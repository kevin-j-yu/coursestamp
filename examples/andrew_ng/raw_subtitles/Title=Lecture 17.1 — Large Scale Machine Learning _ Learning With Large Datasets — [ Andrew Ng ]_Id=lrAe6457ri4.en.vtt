WEBVTT
Kind: captions
Language: en

00:00:00.030 --> 00:00:02.030 align:start position:0%
 
in<00:00:00.359><c> the</c><00:00:00.539><c> next</c><00:00:00.810><c> few</c><00:00:00.989><c> videos</c><00:00:01.140><c> we'll</c><00:00:01.620><c> talk</c><00:00:01.860><c> about</c>

00:00:02.030 --> 00:00:02.040 align:start position:0%
in the next few videos we'll talk about
 

00:00:02.040 --> 00:00:04.880 align:start position:0%
in the next few videos we'll talk about
large-scale<00:00:02.730><c> machine</c><00:00:03.360><c> learning</c><00:00:03.990><c> that</c><00:00:04.170><c> is</c>

00:00:04.880 --> 00:00:04.890 align:start position:0%
large-scale machine learning that is
 

00:00:04.890 --> 00:00:07.369 align:start position:0%
large-scale machine learning that is
algorithms<00:00:05.879><c> for</c><00:00:06.120><c> dealing</c><00:00:06.420><c> with</c><00:00:06.750><c> big</c><00:00:07.140><c> data</c>

00:00:07.369 --> 00:00:07.379 align:start position:0%
algorithms for dealing with big data
 

00:00:07.379 --> 00:00:10.280 align:start position:0%
algorithms for dealing with big data
sets<00:00:07.740><c> if</c><00:00:08.550><c> you</c><00:00:08.700><c> look</c><00:00:08.849><c> back</c><00:00:09.090><c> at</c><00:00:09.360><c> the</c><00:00:09.510><c> recent</c><00:00:09.929><c> five</c>

00:00:10.280 --> 00:00:10.290 align:start position:0%
sets if you look back at the recent five
 

00:00:10.290 --> 00:00:12.259 align:start position:0%
sets if you look back at the recent five
or<00:00:10.559><c> ten-year</c><00:00:10.889><c> history</c><00:00:11.280><c> of</c><00:00:11.790><c> machine</c><00:00:12.150><c> learning</c>

00:00:12.259 --> 00:00:12.269 align:start position:0%
or ten-year history of machine learning
 

00:00:12.269 --> 00:00:14.419 align:start position:0%
or ten-year history of machine learning
one<00:00:13.110><c> of</c><00:00:13.139><c> the</c><00:00:13.320><c> reasons</c><00:00:13.740><c> that</c><00:00:13.889><c> learning</c>

00:00:14.419 --> 00:00:14.429 align:start position:0%
one of the reasons that learning
 

00:00:14.429 --> 00:00:15.980 align:start position:0%
one of the reasons that learning
algorithms<00:00:14.880><c> were</c><00:00:15.059><c> of</c><00:00:15.179><c> so</c><00:00:15.389><c> much</c><00:00:15.540><c> better</c><00:00:15.750><c> now</c>

00:00:15.980 --> 00:00:15.990 align:start position:0%
algorithms were of so much better now
 

00:00:15.990 --> 00:00:18.320 align:start position:0%
algorithms were of so much better now
than<00:00:16.260><c> even</c><00:00:16.650><c> say</c><00:00:16.920><c> five</c><00:00:17.190><c> years</c><00:00:17.220><c> ago</c><00:00:17.699><c> it's</c><00:00:18.210><c> just</c>

00:00:18.320 --> 00:00:18.330 align:start position:0%
than even say five years ago it's just
 

00:00:18.330 --> 00:00:20.450 align:start position:0%
than even say five years ago it's just
the<00:00:18.630><c> sheer</c><00:00:18.900><c> amount</c><00:00:19.170><c> of</c><00:00:19.529><c> data</c><00:00:19.740><c> that</c><00:00:19.980><c> we</c><00:00:20.189><c> have</c>

00:00:20.450 --> 00:00:20.460 align:start position:0%
the sheer amount of data that we have
 

00:00:20.460 --> 00:00:22.250 align:start position:0%
the sheer amount of data that we have
now<00:00:20.670><c> and</c><00:00:21.000><c> that</c><00:00:21.240><c> we</c><00:00:21.330><c> can</c><00:00:21.480><c> train</c><00:00:21.689><c> our</c><00:00:21.869><c> algorithms</c>

00:00:22.250 --> 00:00:22.260 align:start position:0%
now and that we can train our algorithms
 

00:00:22.260 --> 00:00:24.529 align:start position:0%
now and that we can train our algorithms
on<00:00:22.380><c> in</c><00:00:22.650><c> these</c><00:00:23.250><c> next</c><00:00:23.550><c> few</c><00:00:23.670><c> videos</c><00:00:24.090><c> we</c><00:00:24.330><c> talk</c>

00:00:24.529 --> 00:00:24.539 align:start position:0%
on in these next few videos we talk
 

00:00:24.539 --> 00:00:26.689 align:start position:0%
on in these next few videos we talk
about<00:00:24.570><c> algorithms</c><00:00:25.320><c> but</c><00:00:25.590><c> dealing</c><00:00:25.830><c> with</c><00:00:26.010><c> when</c>

00:00:26.689 --> 00:00:26.699 align:start position:0%
about algorithms but dealing with when
 

00:00:26.699 --> 00:00:33.440 align:start position:0%
about algorithms but dealing with when
we<00:00:26.820><c> have</c><00:00:27.029><c> such</c><00:00:27.420><c> massive</c><00:00:27.900><c> data</c><00:00:28.170><c> sets</c><00:00:32.090><c> so</c><00:00:33.090><c> why</c><00:00:33.390><c> do</c>

00:00:33.440 --> 00:00:33.450 align:start position:0%
we have such massive data sets so why do
 

00:00:33.450 --> 00:00:35.479 align:start position:0%
we have such massive data sets so why do
we<00:00:33.630><c> want</c><00:00:33.899><c> to</c><00:00:34.050><c> use</c><00:00:34.079><c> such</c><00:00:34.530><c> large</c><00:00:34.860><c> data</c><00:00:35.130><c> sets</c>

00:00:35.479 --> 00:00:35.489 align:start position:0%
we want to use such large data sets
 

00:00:35.489 --> 00:00:37.790 align:start position:0%
we want to use such large data sets
we've<00:00:36.090><c> already</c><00:00:36.270><c> seen</c><00:00:36.660><c> that</c><00:00:36.989><c> one</c><00:00:37.350><c> of</c><00:00:37.469><c> the</c><00:00:37.559><c> best</c>

00:00:37.790 --> 00:00:37.800 align:start position:0%
we've already seen that one of the best
 

00:00:37.800 --> 00:00:39.860 align:start position:0%
we've already seen that one of the best
ways<00:00:38.010><c> to</c><00:00:38.040><c> get</c><00:00:38.460><c> a</c><00:00:38.489><c> high</c><00:00:38.820><c> performance</c><00:00:39.480><c> machine</c>

00:00:39.860 --> 00:00:39.870 align:start position:0%
ways to get a high performance machine
 

00:00:39.870 --> 00:00:42.200 align:start position:0%
ways to get a high performance machine
learning<00:00:40.170><c> system</c><00:00:40.559><c> is</c><00:00:40.739><c> if</c><00:00:41.340><c> you</c><00:00:41.520><c> take</c><00:00:41.790><c> a</c><00:00:41.820><c> low</c>

00:00:42.200 --> 00:00:42.210 align:start position:0%
learning system is if you take a low
 

00:00:42.210 --> 00:00:44.869 align:start position:0%
learning system is if you take a low
bias<00:00:42.540><c> learning</c><00:00:43.260><c> algorithm</c><00:00:43.739><c> and</c><00:00:43.920><c> train</c><00:00:44.670><c> that</c>

00:00:44.869 --> 00:00:44.879 align:start position:0%
bias learning algorithm and train that
 

00:00:44.879 --> 00:00:48.020 align:start position:0%
bias learning algorithm and train that
on<00:00:45.149><c> a</c><00:00:45.210><c> lot</c><00:00:45.480><c> of</c><00:00:45.629><c> data</c><00:00:45.840><c> and</c><00:00:46.430><c> so</c><00:00:47.430><c> one</c><00:00:47.700><c> early</c>

00:00:48.020 --> 00:00:48.030 align:start position:0%
on a lot of data and so one early
 

00:00:48.030 --> 00:00:49.430 align:start position:0%
on a lot of data and so one early
example<00:00:48.059><c> they</c><00:00:48.539><c> were</c><00:00:48.660><c> already</c><00:00:48.840><c> seeing</c><00:00:49.140><c> was</c>

00:00:49.430 --> 00:00:49.440 align:start position:0%
example they were already seeing was
 

00:00:49.440 --> 00:00:52.310 align:start position:0%
example they were already seeing was
this<00:00:49.680><c> example</c><00:00:50.399><c> of</c><00:00:51.140><c> classifying</c><00:00:52.140><c> between</c>

00:00:52.310 --> 00:00:52.320 align:start position:0%
this example of classifying between
 

00:00:52.320 --> 00:00:54.740 align:start position:0%
this example of classifying between
confusable<00:00:52.530><c> words</c><00:00:53.129><c> so</c><00:00:53.670><c> for</c><00:00:53.879><c> breakfast</c><00:00:54.270><c> I</c><00:00:54.510><c> ate</c>

00:00:54.740 --> 00:00:54.750 align:start position:0%
confusable words so for breakfast I ate
 

00:00:54.750 --> 00:00:58.779 align:start position:0%
confusable words so for breakfast I ate
two<00:00:55.590><c> T</c><00:00:56.219><c> wo</c><00:00:56.640><c> eggs</c><00:00:57.000><c> and</c><00:00:57.270><c> we</c><00:00:57.840><c> saw</c><00:00:58.020><c> in</c><00:00:58.109><c> this</c><00:00:58.199><c> example</c>

00:00:58.779 --> 00:00:58.789 align:start position:0%
two T wo eggs and we saw in this example
 

00:00:58.789 --> 00:01:02.540 align:start position:0%
two T wo eggs and we saw in this example
these<00:00:59.789><c> sorts</c><00:01:00.270><c> of</c><00:01:00.359><c> results</c><00:01:00.539><c> where</c><00:01:01.079><c> you</c><00:01:01.559><c> know</c><00:01:01.680><c> so</c>

00:01:02.540 --> 00:01:02.550 align:start position:0%
these sorts of results where you know so
 

00:01:02.550 --> 00:01:04.399 align:start position:0%
these sorts of results where you know so
long<00:01:02.760><c> as</c><00:01:02.879><c> you</c><00:01:03.030><c> feed</c><00:01:03.239><c> the</c><00:01:03.390><c> algorithm</c><00:01:03.870><c> a</c><00:01:03.989><c> lot</c><00:01:04.260><c> of</c>

00:01:04.399 --> 00:01:04.409 align:start position:0%
long as you feed the algorithm a lot of
 

00:01:04.409 --> 00:01:07.219 align:start position:0%
long as you feed the algorithm a lot of
data<00:01:04.589><c> it</c><00:01:05.070><c> seems</c><00:01:05.339><c> to</c><00:01:05.580><c> do</c><00:01:05.760><c> very</c><00:01:06.000><c> well</c><00:01:06.090><c> and</c><00:01:06.659><c> so</c>

00:01:07.219 --> 00:01:07.229 align:start position:0%
data it seems to do very well and so
 

00:01:07.229 --> 00:01:08.929 align:start position:0%
data it seems to do very well and so
this<00:01:07.320><c> results</c><00:01:07.680><c> ideas</c><00:01:08.010><c> that</c><00:01:08.250><c> has</c><00:01:08.400><c> led</c><00:01:08.610><c> to</c><00:01:08.640><c> the</c>

00:01:08.929 --> 00:01:08.939 align:start position:0%
this results ideas that has led to the
 

00:01:08.939 --> 00:01:11.000 align:start position:0%
this results ideas that has led to the
saying<00:01:09.210><c> in</c><00:01:09.540><c> machine</c><00:01:09.840><c> learning</c><00:01:09.960><c> that</c><00:01:10.409><c> often</c><00:01:10.830><c> is</c>

00:01:11.000 --> 00:01:11.010 align:start position:0%
saying in machine learning that often is
 

00:01:11.010 --> 00:01:12.770 align:start position:0%
saying in machine learning that often is
not<00:01:11.040><c> who</c><00:01:11.430><c> has</c><00:01:11.610><c> the</c><00:01:11.760><c> best</c><00:01:11.909><c> hamburger</c><00:01:12.330><c> wins</c><00:01:12.570><c> is</c>

00:01:12.770 --> 00:01:12.780 align:start position:0%
not who has the best hamburger wins is
 

00:01:12.780 --> 00:01:15.950 align:start position:0%
not who has the best hamburger wins is
who<00:01:13.320><c> has</c><00:01:13.530><c> the</c><00:01:13.710><c> most</c><00:01:13.860><c> data</c><00:01:14.270><c> so</c><00:01:15.270><c> we</c><00:01:15.600><c> want</c><00:01:15.810><c> to</c>

00:01:15.950 --> 00:01:15.960 align:start position:0%
who has the most data so we want to
 

00:01:15.960 --> 00:01:17.960 align:start position:0%
who has the most data so we want to
learn<00:01:16.170><c> from</c><00:01:16.530><c> large</c><00:01:16.799><c> data</c><00:01:17.100><c> sets</c><00:01:17.400><c> at</c><00:01:17.640><c> least</c><00:01:17.790><c> when</c>

00:01:17.960 --> 00:01:17.970 align:start position:0%
learn from large data sets at least when
 

00:01:17.970 --> 00:01:20.870 align:start position:0%
learn from large data sets at least when
we<00:01:18.060><c> can</c><00:01:18.210><c> get</c><00:01:18.420><c> such</c><00:01:18.659><c> large</c><00:01:18.930><c> data</c><00:01:19.229><c> sets</c><00:01:19.880><c> but</c>

00:01:20.870 --> 00:01:20.880 align:start position:0%
we can get such large data sets but
 

00:01:20.880 --> 00:01:22.940 align:start position:0%
we can get such large data sets but
learning<00:01:21.060><c> with</c><00:01:21.450><c> large</c><00:01:21.689><c> data</c><00:01:21.960><c> sets</c><00:01:22.259><c> comes</c><00:01:22.710><c> with</c>

00:01:22.940 --> 00:01:22.950 align:start position:0%
learning with large data sets comes with
 

00:01:22.950 --> 00:01:25.330 align:start position:0%
learning with large data sets comes with
his<00:01:23.100><c> own</c><00:01:23.250><c> unique</c><00:01:23.520><c> problems</c><00:01:24.170><c> specifically</c>

00:01:25.330 --> 00:01:25.340 align:start position:0%
his own unique problems specifically
 

00:01:25.340 --> 00:01:28.190 align:start position:0%
his own unique problems specifically
computational<00:01:26.340><c> problems</c><00:01:26.820><c> let's</c><00:01:27.780><c> say</c><00:01:27.990><c> your</c>

00:01:28.190 --> 00:01:28.200 align:start position:0%
computational problems let's say your
 

00:01:28.200 --> 00:01:32.270 align:start position:0%
computational problems let's say your
training<00:01:28.560><c> set</c><00:01:28.770><c> size</c><00:01:28.979><c> is</c><00:01:29.340><c> M</c><00:01:29.579><c> equals</c><00:01:29.939><c> a</c><00:01:31.280><c> hundred</c>

00:01:32.270 --> 00:01:32.280 align:start position:0%
training set size is M equals a hundred
 

00:01:32.280 --> 00:01:34.819 align:start position:0%
training set size is M equals a hundred
million<00:01:32.490><c> and</c><00:01:33.259><c> this</c><00:01:34.259><c> is</c><00:01:34.380><c> actually</c><00:01:34.680><c> pretty</c>

00:01:34.819 --> 00:01:34.829 align:start position:0%
million and this is actually pretty
 

00:01:34.829 --> 00:01:37.969 align:start position:0%
million and this is actually pretty
realistic<00:01:35.250><c> for</c><00:01:35.790><c> many</c><00:01:36.360><c> modern</c><00:01:36.869><c> data</c><00:01:37.439><c> sets</c><00:01:37.740><c> if</c>

00:01:37.969 --> 00:01:37.979 align:start position:0%
realistic for many modern data sets if
 

00:01:37.979 --> 00:01:39.980 align:start position:0%
realistic for many modern data sets if
you<00:01:38.040><c> look</c><00:01:38.189><c> at</c><00:01:38.310><c> the</c><00:01:38.400><c> US</c><00:01:38.759><c> census</c><00:01:39.299><c> data</c><00:01:39.479><c> set</c><00:01:39.780><c> if</c>

00:01:39.980 --> 00:01:39.990 align:start position:0%
you look at the US census data set if
 

00:01:39.990 --> 00:01:41.780 align:start position:0%
you look at the US census data set if
there<00:01:40.170><c> are</c><00:01:40.259><c> you</c><00:01:40.350><c> know</c><00:01:40.380><c> 300</c><00:01:41.040><c> million</c><00:01:41.369><c> people</c><00:01:41.700><c> in</c>

00:01:41.780 --> 00:01:41.790 align:start position:0%
there are you know 300 million people in
 

00:01:41.790 --> 00:01:43.819 align:start position:0%
there are you know 300 million people in
the<00:01:41.850><c> US</c><00:01:42.150><c> you</c><00:01:42.840><c> can</c><00:01:42.869><c> use</c><00:01:43.110><c> to</c><00:01:43.229><c> get</c><00:01:43.380><c> hundreds</c><00:01:43.770><c> of</c>

00:01:43.819 --> 00:01:43.829 align:start position:0%
the US you can use to get hundreds of
 

00:01:43.829 --> 00:01:45.319 align:start position:0%
the US you can use to get hundreds of
millions<00:01:44.130><c> of</c><00:01:44.189><c> records</c><00:01:44.640><c> if</c><00:01:44.790><c> you</c><00:01:44.909><c> look</c><00:01:45.090><c> at</c><00:01:45.210><c> the</c>

00:01:45.319 --> 00:01:45.329 align:start position:0%
millions of records if you look at the
 

00:01:45.329 --> 00:01:47.179 align:start position:0%
millions of records if you look at the
amount<00:01:45.540><c> of</c><00:01:45.689><c> traffic</c><00:01:45.899><c> that</c><00:01:46.229><c> popular</c><00:01:46.799><c> websites</c>

00:01:47.179 --> 00:01:47.189 align:start position:0%
amount of traffic that popular websites
 

00:01:47.189 --> 00:01:49.550 align:start position:0%
amount of traffic that popular websites
get<00:01:47.490><c> you</c><00:01:47.970><c> easily</c><00:01:48.450><c> get</c><00:01:48.659><c> training</c><00:01:48.930><c> sets</c><00:01:49.259><c> are</c>

00:01:49.550 --> 00:01:49.560 align:start position:0%
get you easily get training sets are
 

00:01:49.560 --> 00:01:51.139 align:start position:0%
get you easily get training sets are
much<00:01:49.740><c> larger</c><00:01:49.950><c> than</c><00:01:50.220><c> hundreds</c><00:01:50.700><c> of</c><00:01:50.790><c> millions</c><00:01:51.090><c> of</c>

00:01:51.139 --> 00:01:51.149 align:start position:0%
much larger than hundreds of millions of
 

00:01:51.149 --> 00:01:54.139 align:start position:0%
much larger than hundreds of millions of
examples<00:01:51.659><c> and</c><00:01:52.399><c> let's</c><00:01:53.399><c> say</c><00:01:53.490><c> you</c><00:01:53.579><c> want</c><00:01:53.759><c> to</c><00:01:53.790><c> train</c>

00:01:54.139 --> 00:01:54.149 align:start position:0%
examples and let's say you want to train
 

00:01:54.149 --> 00:01:56.240 align:start position:0%
examples and let's say you want to train
a<00:01:54.450><c> linear</c><00:01:55.049><c> regression</c><00:01:55.140><c> model</c><00:01:55.500><c> or</c><00:01:55.950><c> maybe</c><00:01:56.130><c> a</c>

00:01:56.240 --> 00:01:56.250 align:start position:0%
a linear regression model or maybe a
 

00:01:56.250 --> 00:01:58.760 align:start position:0%
a linear regression model or maybe a
logistic<00:01:56.700><c> regression</c><00:01:57.119><c> model</c><00:01:57.479><c> in</c><00:01:58.320><c> which</c><00:01:58.469><c> case</c>

00:01:58.760 --> 00:01:58.770 align:start position:0%
logistic regression model in which case
 

00:01:58.770 --> 00:02:02.630 align:start position:0%
logistic regression model in which case
this<00:01:59.189><c> is</c><00:01:59.549><c> the</c><00:01:59.939><c> gradient</c><00:02:00.270><c> descent</c><00:02:00.570><c> rule</c><00:02:00.990><c> and</c><00:02:01.640><c> if</c>

00:02:02.630 --> 00:02:02.640 align:start position:0%
this is the gradient descent rule and if
 

00:02:02.640 --> 00:02:04.219 align:start position:0%
this is the gradient descent rule and if
you<00:02:02.820><c> look</c><00:02:03.030><c> at</c><00:02:03.180><c> what</c><00:02:03.360><c> you</c><00:02:03.479><c> need</c><00:02:03.630><c> to</c><00:02:03.810><c> do</c><00:02:03.990><c> to</c>

00:02:04.219 --> 00:02:04.229 align:start position:0%
you look at what you need to do to
 

00:02:04.229 --> 00:02:06.109 align:start position:0%
you look at what you need to do to
compute<00:02:04.590><c> the</c><00:02:04.770><c> gradient</c><00:02:05.310><c> which</c><00:02:05.640><c> is</c><00:02:05.790><c> this</c><00:02:05.909><c> term</c>

00:02:06.109 --> 00:02:06.119 align:start position:0%
compute the gradient which is this term
 

00:02:06.119 --> 00:02:09.139 align:start position:0%
compute the gradient which is this term
over<00:02:06.390><c> here</c><00:02:06.619><c> then</c><00:02:07.619><c> when</c><00:02:07.979><c> M</c><00:02:08.280><c> is</c><00:02:08.610><c> a</c><00:02:08.640><c> hundred</c>

00:02:09.139 --> 00:02:09.149 align:start position:0%
over here then when M is a hundred
 

00:02:09.149 --> 00:02:11.240 align:start position:0%
over here then when M is a hundred
million<00:02:09.390><c> you</c><00:02:10.140><c> need</c><00:02:10.319><c> to</c><00:02:10.440><c> carry</c><00:02:10.800><c> out</c><00:02:10.920><c> a</c>

00:02:11.240 --> 00:02:11.250 align:start position:0%
million you need to carry out a
 

00:02:11.250 --> 00:02:13.520 align:start position:0%
million you need to carry out a
summation<00:02:11.670><c> over</c><00:02:12.120><c> a</c><00:02:12.360><c> hundred</c><00:02:12.840><c> million</c><00:02:13.050><c> terms</c>

00:02:13.520 --> 00:02:13.530 align:start position:0%
summation over a hundred million terms
 

00:02:13.530 --> 00:02:16.550 align:start position:0%
summation over a hundred million terms
in<00:02:14.370><c> order</c><00:02:14.610><c> to</c><00:02:15.030><c> compute</c><00:02:15.420><c> this</c><00:02:15.660><c> derivative</c><00:02:15.900><c> term</c>

00:02:16.550 --> 00:02:16.560 align:start position:0%
in order to compute this derivative term
 

00:02:16.560 --> 00:02:18.530 align:start position:0%
in order to compute this derivative term
and<00:02:16.739><c> to</c><00:02:17.160><c> perform</c><00:02:17.580><c> a</c><00:02:17.760><c> single</c><00:02:17.970><c> step</c><00:02:18.300><c> of</c><00:02:18.450><c> gradient</c>

00:02:18.530 --> 00:02:18.540 align:start position:0%
and to perform a single step of gradient
 

00:02:18.540 --> 00:02:21.619 align:start position:0%
and to perform a single step of gradient
descent<00:02:19.520><c> because</c><00:02:20.520><c> of</c><00:02:20.700><c> the</c><00:02:20.819><c> computational</c>

00:02:21.619 --> 00:02:21.629 align:start position:0%
descent because of the computational
 

00:02:21.629 --> 00:02:24.140 align:start position:0%
descent because of the computational
expense<00:02:22.050><c> of</c><00:02:22.440><c> summing</c><00:02:23.099><c> over</c><00:02:23.220><c> a</c><00:02:23.640><c> hundred</c>

00:02:24.140 --> 00:02:24.150 align:start position:0%
expense of summing over a hundred
 

00:02:24.150 --> 00:02:26.869 align:start position:0%
expense of summing over a hundred
million<00:02:24.360><c> entries</c><00:02:25.290><c> in</c><00:02:25.739><c> order</c><00:02:26.069><c> to</c><00:02:26.190><c> compute</c><00:02:26.550><c> just</c>

00:02:26.869 --> 00:02:26.879 align:start position:0%
million entries in order to compute just
 

00:02:26.879 --> 00:02:29.449 align:start position:0%
million entries in order to compute just
one<00:02:27.209><c> step</c><00:02:27.450><c> of</c><00:02:27.569><c> gradient</c><00:02:27.599><c> descent</c><00:02:27.900><c> in</c><00:02:28.709><c> the</c><00:02:29.099><c> next</c>

00:02:29.449 --> 00:02:29.459 align:start position:0%
one step of gradient descent in the next
 

00:02:29.459 --> 00:02:31.250 align:start position:0%
one step of gradient descent in the next
few<00:02:29.580><c> videos</c><00:02:29.730><c> we'll</c><00:02:30.090><c> talk</c><00:02:30.269><c> about</c><00:02:30.480><c> techniques</c>

00:02:31.250 --> 00:02:31.260 align:start position:0%
few videos we'll talk about techniques
 

00:02:31.260 --> 00:02:33.320 align:start position:0%
few videos we'll talk about techniques
for<00:02:31.590><c> either</c><00:02:31.890><c> replacing</c><00:02:32.670><c> this</c><00:02:32.790><c> algorithm</c><00:02:33.239><c> or</c>

00:02:33.320 --> 00:02:33.330 align:start position:0%
for either replacing this algorithm or
 

00:02:33.330 --> 00:02:35.660 align:start position:0%
for either replacing this algorithm or
something<00:02:33.629><c> else</c><00:02:33.780><c> or</c><00:02:33.989><c> to</c><00:02:34.650><c> find</c><00:02:34.920><c> more</c><00:02:35.220><c> efficient</c>

00:02:35.660 --> 00:02:35.670 align:start position:0%
something else or to find more efficient
 

00:02:35.670 --> 00:02:38.600 align:start position:0%
something else or to find more efficient
ways<00:02:35.700><c> to</c><00:02:35.849><c> compute</c><00:02:36.510><c> this</c><00:02:36.810><c> derivative</c><00:02:37.550><c> by</c><00:02:38.550><c> the</c>

00:02:38.600 --> 00:02:38.610 align:start position:0%
ways to compute this derivative by the
 

00:02:38.610 --> 00:02:40.490 align:start position:0%
ways to compute this derivative by the
end<00:02:38.879><c> of</c><00:02:39.090><c> the</c><00:02:39.269><c> sequence</c><00:02:39.510><c> of</c><00:02:39.959><c> videos</c><00:02:40.110><c> on</c>

00:02:40.490 --> 00:02:40.500 align:start position:0%
end of the sequence of videos on
 

00:02:40.500 --> 00:02:42.350 align:start position:0%
end of the sequence of videos on
large-scale<00:02:40.800><c> machine</c><00:02:41.160><c> learning</c><00:02:41.670><c> you</c><00:02:42.150><c> know</c>

00:02:42.350 --> 00:02:42.360 align:start position:0%
large-scale machine learning you know
 

00:02:42.360 --> 00:02:45.110 align:start position:0%
large-scale machine learning you know
how<00:02:42.690><c> to</c><00:02:42.750><c> fit</c><00:02:43.250><c> models</c><00:02:44.250><c> linear</c><00:02:44.580><c> regression</c>

00:02:45.110 --> 00:02:45.120 align:start position:0%
how to fit models linear regression
 

00:02:45.120 --> 00:02:46.820 align:start position:0%
how to fit models linear regression
logistic<00:02:45.150><c> Russian</c><00:02:45.870><c> neural</c><00:02:46.170><c> networks</c><00:02:46.560><c> and</c><00:02:46.650><c> so</c>

00:02:46.820 --> 00:02:46.830 align:start position:0%
logistic Russian neural networks and so
 

00:02:46.830 --> 00:02:49.009 align:start position:0%
logistic Russian neural networks and so
on<00:02:46.860><c> even</c><00:02:47.250><c> two</c><00:02:47.610><c> data</c><00:02:47.790><c> sets</c><00:02:48.060><c> with</c><00:02:48.330><c> say</c><00:02:48.750><c> a</c><00:02:48.780><c> hundred</c>

00:02:49.009 --> 00:02:49.019 align:start position:0%
on even two data sets with say a hundred
 

00:02:49.019 --> 00:02:52.070 align:start position:0%
on even two data sets with say a hundred
million<00:02:49.349><c> examples</c><00:02:50.060><c> of</c><00:02:51.060><c> course</c><00:02:51.330><c> before</c><00:02:51.569><c> we</c><00:02:51.780><c> put</c>

00:02:52.070 --> 00:02:52.080 align:start position:0%
million examples of course before we put
 

00:02:52.080 --> 00:02:54.710 align:start position:0%
million examples of course before we put
in<00:02:52.200><c> the</c><00:02:52.560><c> effort</c><00:02:52.769><c> into</c><00:02:53.310><c> training</c><00:02:53.640><c> a</c><00:02:54.060><c> model</c><00:02:54.480><c> with</c>

00:02:54.710 --> 00:02:54.720 align:start position:0%
in the effort into training a model with
 

00:02:54.720 --> 00:02:56.600 align:start position:0%
in the effort into training a model with
100<00:02:55.080><c> million</c><00:02:55.230><c> examples</c><00:02:55.830><c> we</c><00:02:56.099><c> should</c><00:02:56.250><c> also</c><00:02:56.400><c> ask</c>

00:02:56.600 --> 00:02:56.610 align:start position:0%
100 million examples we should also ask
 

00:02:56.610 --> 00:02:59.720 align:start position:0%
100 million examples we should also ask
ourselves<00:02:56.790><c> well</c><00:02:57.629><c> why</c><00:02:57.900><c> not</c><00:02:58.170><c> use</c><00:02:58.769><c> just</c><00:02:59.610><c> a</c>

00:02:59.720 --> 00:02:59.730 align:start position:0%
ourselves well why not use just a
 

00:02:59.730 --> 00:03:02.449 align:start position:0%
ourselves well why not use just a
thousand<00:03:00.209><c> examples</c><00:03:00.840><c> maybe</c><00:03:01.379><c> we</c><00:03:01.470><c> can</c><00:03:01.680><c> randomly</c>

00:03:02.449 --> 00:03:02.459 align:start position:0%
thousand examples maybe we can randomly
 

00:03:02.459 --> 00:03:05.119 align:start position:0%
thousand examples maybe we can randomly
pick<00:03:02.760><c> a</c><00:03:03.120><c> subset</c><00:03:03.569><c> of</c><00:03:03.780><c> a</c><00:03:03.989><c> thousand</c><00:03:04.440><c> examples</c><00:03:04.560><c> out</c>

00:03:05.119 --> 00:03:05.129 align:start position:0%
pick a subset of a thousand examples out
 

00:03:05.129 --> 00:03:07.520 align:start position:0%
pick a subset of a thousand examples out
of<00:03:05.280><c> a</c><00:03:05.370><c> hundred</c><00:03:05.700><c> million</c><00:03:05.879><c> examples</c><00:03:06.209><c> and</c><00:03:06.720><c> train</c>

00:03:07.520 --> 00:03:07.530 align:start position:0%
of a hundred million examples and train
 

00:03:07.530 --> 00:03:09.559 align:start position:0%
of a hundred million examples and train
our<00:03:07.920><c> algorithm</c><00:03:08.459><c> on</c><00:03:08.670><c> just</c><00:03:09.090><c> a</c><00:03:09.180><c> thousand</c>

00:03:09.559 --> 00:03:09.569 align:start position:0%
our algorithm on just a thousand
 

00:03:09.569 --> 00:03:12.199 align:start position:0%
our algorithm on just a thousand
examples<00:03:09.840><c> so</c><00:03:10.739><c> before</c><00:03:11.099><c> investing</c><00:03:11.700><c> the</c><00:03:11.819><c> effort</c>

00:03:12.199 --> 00:03:12.209 align:start position:0%
examples so before investing the effort
 

00:03:12.209 --> 00:03:14.630 align:start position:0%
examples so before investing the effort
into<00:03:12.690><c> actually</c><00:03:13.560><c> developing</c><00:03:14.069><c> the</c><00:03:14.129><c> software</c>

00:03:14.630 --> 00:03:14.640 align:start position:0%
into actually developing the software
 

00:03:14.640 --> 00:03:16.130 align:start position:0%
into actually developing the software
needed<00:03:14.849><c> to</c><00:03:15.060><c> train</c><00:03:15.269><c> these</c><00:03:15.420><c> massive</c><00:03:15.810><c> models</c>

00:03:16.130 --> 00:03:16.140 align:start position:0%
needed to train these massive models
 

00:03:16.140 --> 00:03:18.410 align:start position:0%
needed to train these massive models
there's<00:03:16.379><c> often</c><00:03:16.620><c> good</c><00:03:16.860><c> to</c><00:03:16.920><c> sanity</c><00:03:17.879><c> check</c><00:03:18.120><c> if</c>

00:03:18.410 --> 00:03:18.420 align:start position:0%
there's often good to sanity check if
 

00:03:18.420 --> 00:03:20.780 align:start position:0%
there's often good to sanity check if
training<00:03:19.319><c> on</c><00:03:19.680><c> just</c><00:03:20.010><c> a</c><00:03:20.099><c> thousand</c><00:03:20.519><c> examples</c>

00:03:20.780 --> 00:03:20.790 align:start position:0%
training on just a thousand examples
 

00:03:20.790 --> 00:03:24.199 align:start position:0%
training on just a thousand examples
might<00:03:21.540><c> do</c><00:03:21.720><c> just</c><00:03:21.989><c> as</c><00:03:22.110><c> well</c>

00:03:24.199 --> 00:03:24.209 align:start position:0%
 
 

00:03:24.209 --> 00:03:26.839 align:start position:0%
 
the<00:03:24.930><c> way</c><00:03:25.079><c> it's</c><00:03:25.230><c> a</c><00:03:25.349><c> sanity</c><00:03:25.860><c> check</c><00:03:25.890><c> of</c><00:03:26.280><c> using</c><00:03:26.640><c> a</c>

00:03:26.839 --> 00:03:26.849 align:start position:0%
the way it's a sanity check of using a
 

00:03:26.849 --> 00:03:29.420 align:start position:0%
the way it's a sanity check of using a
much<00:03:27.060><c> smaller</c><00:03:27.090><c> training</c><00:03:27.810><c> set</c><00:03:28.560><c> might</c><00:03:29.010><c> do</c><00:03:29.159><c> just</c>

00:03:29.420 --> 00:03:29.430 align:start position:0%
much smaller training set might do just
 

00:03:29.430 --> 00:03:31.369 align:start position:0%
much smaller training set might do just
as<00:03:29.519><c> well</c><00:03:29.549><c> that</c><00:03:29.970><c> is</c><00:03:30.329><c> if</c><00:03:30.689><c> using</c><00:03:31.110><c> a</c><00:03:31.200><c> much</c><00:03:31.349><c> smaller</c>

00:03:31.369 --> 00:03:31.379 align:start position:0%
as well that is if using a much smaller
 

00:03:31.379 --> 00:03:34.190 align:start position:0%
as well that is if using a much smaller
N<00:03:31.980><c> equals</c><00:03:32.400><c> 1000</c><00:03:33.150><c> size</c><00:03:33.329><c> training</c><00:03:33.720><c> set</c><00:03:33.870><c> might</c><00:03:34.049><c> do</c>

00:03:34.190 --> 00:03:34.200 align:start position:0%
N equals 1000 size training set might do
 

00:03:34.200 --> 00:03:36.500 align:start position:0%
N equals 1000 size training set might do
just<00:03:34.230><c> as</c><00:03:34.590><c> well</c><00:03:34.620><c> it</c><00:03:35.189><c> is</c><00:03:35.370><c> the</c><00:03:35.519><c> usual</c><00:03:35.700><c> method</c><00:03:36.239><c> of</c>

00:03:36.500 --> 00:03:36.510 align:start position:0%
just as well it is the usual method of
 

00:03:36.510 --> 00:03:38.869 align:start position:0%
just as well it is the usual method of
plotting<00:03:36.750><c> the</c><00:03:37.110><c> learning</c><00:03:37.470><c> curves</c><00:03:37.709><c> so</c><00:03:38.159><c> if</c><00:03:38.579><c> you</c>

00:03:38.869 --> 00:03:38.879 align:start position:0%
plotting the learning curves so if you
 

00:03:38.879 --> 00:03:41.179 align:start position:0%
plotting the learning curves so if you
were<00:03:39.120><c> to</c><00:03:39.390><c> plot</c><00:03:39.659><c> the</c><00:03:39.840><c> learning</c><00:03:39.989><c> curves</c><00:03:40.170><c> and</c><00:03:40.769><c> if</c>

00:03:41.179 --> 00:03:41.189 align:start position:0%
were to plot the learning curves and if
 

00:03:41.189 --> 00:03:46.160 align:start position:0%
were to plot the learning curves and if
your<00:03:42.560><c> training</c><00:03:43.560><c> objective</c><00:03:44.870><c> where</c><00:03:45.870><c> to</c><00:03:45.989><c> look</c>

00:03:46.160 --> 00:03:46.170 align:start position:0%
your training objective where to look
 

00:03:46.170 --> 00:03:49.880 align:start position:0%
your training objective where to look
like<00:03:46.319><c> this</c><00:03:46.849><c> that's</c><00:03:47.849><c> J</c><00:03:48.209><c> train</c><00:03:48.510><c> theta</c><00:03:48.870><c> and</c><00:03:49.290><c> it's</c>

00:03:49.880 --> 00:03:49.890 align:start position:0%
like this that's J train theta and it's
 

00:03:49.890 --> 00:03:54.679 align:start position:0%
like this that's J train theta and it's
your<00:03:50.599><c> cross-validation</c><00:03:51.599><c> set</c><00:03:52.439><c> objective</c><00:03:53.689><c> JCV</c>

00:03:54.679 --> 00:03:54.689 align:start position:0%
your cross-validation set objective JCV
 

00:03:54.689 --> 00:03:56.690 align:start position:0%
your cross-validation set objective JCV
of<00:03:54.720><c> theta</c><00:03:55.109><c> would</c><00:03:55.620><c> look</c><00:03:55.890><c> like</c><00:03:56.010><c> this</c><00:03:56.220><c> then</c><00:03:56.489><c> you</c>

00:03:56.690 --> 00:03:56.700 align:start position:0%
of theta would look like this then you
 

00:03:56.700 --> 00:03:58.789 align:start position:0%
of theta would look like this then you
know<00:03:56.819><c> it</c><00:03:57.299><c> looks</c><00:03:57.480><c> like</c><00:03:57.629><c> this</c><00:03:57.810><c> does</c><00:03:57.989><c> like</c><00:03:58.139><c> a</c><00:03:58.260><c> high</c>

00:03:58.789 --> 00:03:58.799 align:start position:0%
know it looks like this does like a high
 

00:03:58.799 --> 00:04:00.860 align:start position:0%
know it looks like this does like a high
variance<00:03:59.069><c> learning</c><00:03:59.700><c> algorithm</c><00:04:00.150><c> and</c><00:04:00.269><c> we</c><00:04:00.659><c> would</c>

00:04:00.860 --> 00:04:00.870 align:start position:0%
variance learning algorithm and we would
 

00:04:00.870 --> 00:04:02.930 align:start position:0%
variance learning algorithm and we would
be<00:04:00.989><c> more</c><00:04:01.200><c> confident</c><00:04:01.799><c> that</c><00:04:02.069><c> adding</c><00:04:02.609><c> extra</c>

00:04:02.930 --> 00:04:02.940 align:start position:0%
be more confident that adding extra
 

00:04:02.940 --> 00:04:04.930 align:start position:0%
be more confident that adding extra
training<00:04:03.269><c> examples</c><00:04:03.750><c> would</c><00:04:04.109><c> improve</c>

00:04:04.930 --> 00:04:04.940 align:start position:0%
training examples would improve
 

00:04:04.940 --> 00:04:07.759 align:start position:0%
training examples would improve
performance<00:04:05.940><c> whereas</c><00:04:06.810><c> in</c><00:04:06.959><c> contrast</c><00:04:07.109><c> if</c><00:04:07.620><c> you</c>

00:04:07.759 --> 00:04:07.769 align:start position:0%
performance whereas in contrast if you
 

00:04:07.769 --> 00:04:11.890 align:start position:0%
performance whereas in contrast if you
were<00:04:07.920><c> to</c><00:04:08.220><c> plot</c><00:04:08.549><c> the</c><00:04:08.939><c> learning</c><00:04:09.209><c> curves</c><00:04:09.420><c> if</c><00:04:10.170><c> your</c>

00:04:11.890 --> 00:04:11.900 align:start position:0%
were to plot the learning curves if your
 

00:04:11.900 --> 00:04:14.179 align:start position:0%
were to plot the learning curves if your
training<00:04:12.900><c> objective</c><00:04:13.470><c> where</c><00:04:13.769><c> to</c><00:04:13.889><c> look</c><00:04:14.040><c> like</c>

00:04:14.179 --> 00:04:14.189 align:start position:0%
training objective where to look like
 

00:04:14.189 --> 00:04:17.650 align:start position:0%
training objective where to look like
this<00:04:14.430><c> and</c><00:04:14.819><c> if</c><00:04:15.269><c> your</c><00:04:15.629><c> cross-validation</c>

00:04:17.650 --> 00:04:17.660 align:start position:0%
this and if your cross-validation
 

00:04:17.660 --> 00:04:20.509 align:start position:0%
this and if your cross-validation
objective<00:04:18.660><c> was</c><00:04:19.530><c> to</c><00:04:19.560><c> look</c><00:04:19.739><c> like</c><00:04:19.889><c> that</c><00:04:20.070><c> then</c>

00:04:20.509 --> 00:04:20.519 align:start position:0%
objective was to look like that then
 

00:04:20.519 --> 00:04:22.629 align:start position:0%
objective was to look like that then
this<00:04:20.729><c> looks</c><00:04:21.030><c> like</c><00:04:21.209><c> the</c><00:04:21.389><c> classical</c><00:04:21.989><c> high</c><00:04:22.289><c> bias</c>

00:04:22.629 --> 00:04:22.639 align:start position:0%
this looks like the classical high bias
 

00:04:22.639 --> 00:04:24.830 align:start position:0%
this looks like the classical high bias
learning<00:04:23.639><c> algorithm</c><00:04:24.060><c> and</c><00:04:24.360><c> in</c><00:04:24.479><c> the</c><00:04:24.630><c> latter</c>

00:04:24.830 --> 00:04:24.840 align:start position:0%
learning algorithm and in the latter
 

00:04:24.840 --> 00:04:27.620 align:start position:0%
learning algorithm and in the latter
case<00:04:25.169><c> you</c><00:04:25.800><c> know</c><00:04:25.919><c> if</c><00:04:26.160><c> you</c><00:04:26.910><c> were</c><00:04:27.060><c> to</c><00:04:27.210><c> plot</c><00:04:27.389><c> this</c>

00:04:27.620 --> 00:04:27.630 align:start position:0%
case you know if you were to plot this
 

00:04:27.630 --> 00:04:30.710 align:start position:0%
case you know if you were to plot this
up<00:04:27.870><c> to</c><00:04:28.139><c> say</c><00:04:28.349><c> M</c><00:04:28.560><c> equals</c><00:04:28.740><c> 1000</c><00:04:29.729><c> and</c><00:04:30.120><c> so</c><00:04:30.330><c> that's</c><00:04:30.539><c> a</c>

00:04:30.710 --> 00:04:30.720 align:start position:0%
up to say M equals 1000 and so that's a
 

00:04:30.720 --> 00:04:33.830 align:start position:0%
up to say M equals 1000 and so that's a
equals<00:04:31.199><c> 500</c><00:04:31.800><c> to</c><00:04:32.130><c> M</c><00:04:32.310><c> equals</c><00:04:32.610><c> 1,000</c><00:04:33.180><c> then</c><00:04:33.690><c> it</c>

00:04:33.830 --> 00:04:33.840 align:start position:0%
equals 500 to M equals 1,000 then it
 

00:04:33.840 --> 00:04:36.800 align:start position:0%
equals 500 to M equals 1,000 then it
seems<00:04:34.080><c> unlikely</c><00:04:34.320><c> that</c><00:04:35.010><c> increasing</c><00:04:35.849><c> m</c><00:04:36.120><c> to</c><00:04:36.780><c> a</c>

00:04:36.800 --> 00:04:36.810 align:start position:0%
seems unlikely that increasing m to a
 

00:04:36.810 --> 00:04:39.770 align:start position:0%
seems unlikely that increasing m to a
hundred<00:04:37.410><c> million</c><00:04:37.639><c> would</c><00:04:38.639><c> do</c><00:04:39.030><c> much</c><00:04:39.210><c> better</c><00:04:39.270><c> and</c>

00:04:39.770 --> 00:04:39.780 align:start position:0%
hundred million would do much better and
 

00:04:39.780 --> 00:04:41.600 align:start position:0%
hundred million would do much better and
then<00:04:39.990><c> you'd</c><00:04:40.349><c> be</c><00:04:40.470><c> just</c><00:04:40.650><c> fine</c><00:04:40.919><c> sticking</c><00:04:41.340><c> to</c><00:04:41.430><c> any</c>

00:04:41.600 --> 00:04:41.610 align:start position:0%
then you'd be just fine sticking to any
 

00:04:41.610 --> 00:04:43.640 align:start position:0%
then you'd be just fine sticking to any
calls<00:04:41.880><c> a</c><00:04:42.000><c> thousand</c><00:04:42.479><c> rather</c><00:04:42.750><c> than</c><00:04:43.020><c> investing</c><00:04:43.560><c> a</c>

00:04:43.640 --> 00:04:43.650 align:start position:0%
calls a thousand rather than investing a
 

00:04:43.650 --> 00:04:45.439 align:start position:0%
calls a thousand rather than investing a
lot<00:04:43.770><c> of</c><00:04:43.919><c> effort</c><00:04:44.250><c> to</c><00:04:44.699><c> figure</c><00:04:44.940><c> out</c><00:04:45.060><c> how</c><00:04:45.180><c> to</c><00:04:45.240><c> scale</c>

00:04:45.439 --> 00:04:45.449 align:start position:0%
lot of effort to figure out how to scale
 

00:04:45.449 --> 00:04:47.870 align:start position:0%
lot of effort to figure out how to scale
at<00:04:45.720><c> the</c><00:04:45.840><c> algorithm</c><00:04:46.080><c> of</c><00:04:46.500><c> course</c><00:04:47.370><c> if</c><00:04:47.610><c> you</c><00:04:47.729><c> were</c>

00:04:47.870 --> 00:04:47.880 align:start position:0%
at the algorithm of course if you were
 

00:04:47.880 --> 00:04:50.390 align:start position:0%
at the algorithm of course if you were
in<00:04:48.240><c> the</c><00:04:48.389><c> situation</c><00:04:48.990><c> shown</c><00:04:49.800><c> by</c><00:04:50.010><c> the</c><00:04:50.070><c> figure</c><00:04:50.310><c> on</c>

00:04:50.390 --> 00:04:50.400 align:start position:0%
in the situation shown by the figure on
 

00:04:50.400 --> 00:04:51.770 align:start position:0%
in the situation shown by the figure on
the<00:04:50.460><c> right</c><00:04:50.490><c> then</c><00:04:50.820><c> one</c><00:04:50.970><c> natural</c><00:04:51.330><c> thing</c><00:04:51.510><c> to</c><00:04:51.630><c> do</c>

00:04:51.770 --> 00:04:51.780 align:start position:0%
the right then one natural thing to do
 

00:04:51.780 --> 00:04:54.740 align:start position:0%
the right then one natural thing to do
would<00:04:51.990><c> be</c><00:04:52.050><c> to</c><00:04:52.199><c> add</c><00:04:52.800><c> extra</c><00:04:53.370><c> features</c><00:04:53.669><c> or</c><00:04:54.210><c> add</c>

00:04:54.740 --> 00:04:54.750 align:start position:0%
would be to add extra features or add
 

00:04:54.750 --> 00:04:56.839 align:start position:0%
would be to add extra features or add
extra<00:04:55.500><c> hidden</c><00:04:55.830><c> units</c><00:04:56.190><c> to</c><00:04:56.370><c> a</c><00:04:56.400><c> neural</c><00:04:56.669><c> network</c>

00:04:56.839 --> 00:04:56.849 align:start position:0%
extra hidden units to a neural network
 

00:04:56.849 --> 00:04:59.719 align:start position:0%
extra hidden units to a neural network
goal<00:04:57.330><c> and</c><00:04:57.810><c> so</c><00:04:58.020><c> on</c><00:04:58.199><c> so</c><00:04:58.800><c> then</c><00:04:58.979><c> you</c><00:04:59.130><c> end</c><00:04:59.310><c> up</c><00:04:59.490><c> with</c><00:04:59.669><c> a</c>

00:04:59.719 --> 00:04:59.729 align:start position:0%
goal and so on so then you end up with a
 

00:04:59.729 --> 00:05:01.850 align:start position:0%
goal and so on so then you end up with a
similar<00:05:00.270><c> situation</c><00:05:00.570><c> and</c><00:05:00.990><c> closer</c><00:05:01.199><c> to</c><00:05:01.530><c> down</c><00:05:01.680><c> on</c>

00:05:01.850 --> 00:05:01.860 align:start position:0%
similar situation and closer to down on
 

00:05:01.860 --> 00:05:03.710 align:start position:0%
similar situation and closer to down on
the<00:05:01.949><c> Left</c><00:05:02.190><c> where</c><00:05:02.789><c> maybe</c><00:05:02.970><c> this</c><00:05:03.210><c> is</c><00:05:03.330><c> up</c><00:05:03.479><c> to</c><00:05:03.510><c> M</c>

00:05:03.710 --> 00:05:03.720 align:start position:0%
the Left where maybe this is up to M
 

00:05:03.720 --> 00:05:05.480 align:start position:0%
the Left where maybe this is up to M
equals<00:05:03.990><c> a</c><00:05:04.050><c> thousand</c><00:05:04.440><c> and</c><00:05:04.620><c> this</c><00:05:05.039><c> then</c><00:05:05.310><c> gives</c>

00:05:05.480 --> 00:05:05.490 align:start position:0%
equals a thousand and this then gives
 

00:05:05.490 --> 00:05:07.689 align:start position:0%
equals a thousand and this then gives
you<00:05:05.610><c> more</c><00:05:05.760><c> confidence</c><00:05:06.330><c> that</c><00:05:06.479><c> trying</c><00:05:06.900><c> to</c>

00:05:07.689 --> 00:05:07.699 align:start position:0%
you more confidence that trying to
 

00:05:07.699 --> 00:05:09.379 align:start position:0%
you more confidence that trying to
infrastructure<00:05:08.699><c> or</c><00:05:08.789><c> change</c><00:05:09.030><c> the</c><00:05:09.150><c> algorithm</c>

00:05:09.379 --> 00:05:09.389 align:start position:0%
infrastructure or change the algorithm
 

00:05:09.389 --> 00:05:11.300 align:start position:0%
infrastructure or change the algorithm
to<00:05:09.780><c> use</c><00:05:09.810><c> much</c><00:05:10.289><c> more</c><00:05:10.530><c> than</c><00:05:10.710><c> a</c><00:05:10.740><c> thousand</c>

00:05:11.300 --> 00:05:11.310 align:start position:0%
to use much more than a thousand
 

00:05:11.310 --> 00:05:13.010 align:start position:0%
to use much more than a thousand
examples<00:05:11.580><c> that</c><00:05:12.060><c> that</c><00:05:12.210><c> might</c><00:05:12.449><c> actually</c><00:05:12.479><c> be</c><00:05:12.900><c> a</c>

00:05:13.010 --> 00:05:13.020 align:start position:0%
examples that that might actually be a
 

00:05:13.020 --> 00:05:15.589 align:start position:0%
examples that that might actually be a
good<00:05:13.229><c> use</c><00:05:13.440><c> of</c><00:05:13.470><c> your</c><00:05:13.650><c> time</c><00:05:13.800><c> so</c><00:05:14.789><c> a</c><00:05:15.030><c> large</c><00:05:15.300><c> scale</c>

00:05:15.589 --> 00:05:15.599 align:start position:0%
good use of your time so a large scale
 

00:05:15.599 --> 00:05:17.750 align:start position:0%
good use of your time so a large scale
machine<00:05:15.780><c> learning</c><00:05:15.990><c> we'd</c><00:05:16.949><c> like</c><00:05:17.159><c> to</c><00:05:17.370><c> come</c><00:05:17.639><c> up</c>

00:05:17.750 --> 00:05:17.760 align:start position:0%
machine learning we'd like to come up
 

00:05:17.760 --> 00:05:19.640 align:start position:0%
machine learning we'd like to come up
with<00:05:17.909><c> computationally</c><00:05:18.900><c> reasonable</c><00:05:19.470><c> weights</c>

00:05:19.640 --> 00:05:19.650 align:start position:0%
with computationally reasonable weights
 

00:05:19.650 --> 00:05:21.860 align:start position:0%
with computationally reasonable weights
or<00:05:19.979><c> computationally</c><00:05:20.940><c> efficient</c><00:05:21.449><c> ways</c><00:05:21.630><c> to</c>

00:05:21.860 --> 00:05:21.870 align:start position:0%
or computationally efficient ways to
 

00:05:21.870 --> 00:05:24.620 align:start position:0%
or computationally efficient ways to
deal<00:05:22.409><c> with</c><00:05:22.590><c> very</c><00:05:22.800><c> big</c><00:05:23.070><c> data</c><00:05:23.310><c> sets</c><00:05:23.669><c> in</c><00:05:23.880><c> the</c><00:05:24.360><c> next</c>

00:05:24.620 --> 00:05:24.630 align:start position:0%
deal with very big data sets in the next
 

00:05:24.630 --> 00:05:26.779 align:start position:0%
deal with very big data sets in the next
few<00:05:24.810><c> videos</c><00:05:24.960><c> we'll</c><00:05:25.409><c> see</c><00:05:25.650><c> two</c><00:05:25.919><c> main</c><00:05:26.130><c> ideas</c><00:05:26.310><c> the</c>

00:05:26.779 --> 00:05:26.789 align:start position:0%
few videos we'll see two main ideas the
 

00:05:26.789 --> 00:05:28.189 align:start position:0%
few videos we'll see two main ideas the
first<00:05:26.970><c> is</c><00:05:27.210><c> called</c><00:05:27.419><c> stochastic</c><00:05:27.840><c> gradient</c>

00:05:28.189 --> 00:05:28.199 align:start position:0%
first is called stochastic gradient
 

00:05:28.199 --> 00:05:28.970 align:start position:0%
first is called stochastic gradient
descent

00:05:28.970 --> 00:05:28.980 align:start position:0%
descent
 

00:05:28.980 --> 00:05:31.610 align:start position:0%
descent
and<00:05:29.070><c> the</c><00:05:29.490><c> second</c><00:05:29.850><c> is</c><00:05:30.060><c> called</c><00:05:30.240><c> MapReduce</c><00:05:30.870><c> but</c>

00:05:31.610 --> 00:05:31.620 align:start position:0%
and the second is called MapReduce but
 

00:05:31.620 --> 00:05:34.370 align:start position:0%
and the second is called MapReduce but
dealing<00:05:31.860><c> with</c><00:05:32.010><c> very</c><00:05:32.460><c> big</c><00:05:32.700><c> datasets</c><00:05:33.060><c> and</c><00:05:33.420><c> after</c>

00:05:34.370 --> 00:05:34.380 align:start position:0%
dealing with very big datasets and after
 

00:05:34.380 --> 00:05:35.930 align:start position:0%
dealing with very big datasets and after
you've<00:05:34.650><c> learned</c><00:05:35.040><c> about</c><00:05:35.280><c> these</c><00:05:35.550><c> methods</c>

00:05:35.930 --> 00:05:35.940 align:start position:0%
you've learned about these methods
 

00:05:35.940 --> 00:05:37.910 align:start position:0%
you've learned about these methods
hopefully<00:05:36.600><c> that</c><00:05:36.870><c> will</c><00:05:36.960><c> allow</c><00:05:37.230><c> you</c><00:05:37.290><c> to</c><00:05:37.470><c> scale</c>

00:05:37.910 --> 00:05:37.920 align:start position:0%
hopefully that will allow you to scale
 

00:05:37.920 --> 00:05:39.860 align:start position:0%
hopefully that will allow you to scale
up<00:05:38.100><c> your</c><00:05:38.130><c> learning</c><00:05:38.610><c> algorithms</c><00:05:39.120><c> to</c><00:05:39.390><c> big</c><00:05:39.600><c> data</c>

00:05:39.860 --> 00:05:39.870 align:start position:0%
up your learning algorithms to big data
 

00:05:39.870 --> 00:05:41.840 align:start position:0%
up your learning algorithms to big data
and<00:05:40.260><c> allow</c><00:05:40.740><c> you</c><00:05:40.800><c> to</c><00:05:40.980><c> get</c><00:05:41.370><c> much</c><00:05:41.640><c> better</c>

00:05:41.840 --> 00:05:41.850 align:start position:0%
and allow you to get much better
 

00:05:41.850 --> 00:05:44.030 align:start position:0%
and allow you to get much better
performance<00:05:42.170><c> on</c><00:05:43.170><c> many</c><00:05:43.680><c> different</c>

00:05:44.030 --> 00:05:44.040 align:start position:0%
performance on many different
 

00:05:44.040 --> 00:05:46.610 align:start position:0%
performance on many different
applications

