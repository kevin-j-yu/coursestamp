WEBVTT
Kind: captions
Language: en

00:00:00.030 --> 00:00:01.910 align:start position:0%
 
in<00:00:00.390><c> the</c><00:00:00.570><c> last</c><00:00:00.750><c> few</c><00:00:00.989><c> videos</c><00:00:01.140><c> we</c><00:00:01.380><c> talked</c><00:00:01.800><c> about</c>

00:00:01.910 --> 00:00:01.920 align:start position:0%
in the last few videos we talked about
 

00:00:01.920 --> 00:00:03.860 align:start position:0%
in the last few videos we talked about
stochastic<00:00:02.429><c> gradient</c><00:00:02.790><c> descent</c><00:00:03.120><c> and</c><00:00:03.629><c> you</c><00:00:03.750><c> know</c>

00:00:03.860 --> 00:00:03.870 align:start position:0%
stochastic gradient descent and you know
 

00:00:03.870 --> 00:00:05.539 align:start position:0%
stochastic gradient descent and you know
other<00:00:04.140><c> variations</c><00:00:04.859><c> of</c><00:00:05.069><c> the</c><00:00:05.190><c> stochastic</c>

00:00:05.539 --> 00:00:05.549 align:start position:0%
other variations of the stochastic
 

00:00:05.549 --> 00:00:07.369 align:start position:0%
other variations of the stochastic
gradient<00:00:05.850><c> descent</c><00:00:06.120><c> algorithm</c><00:00:06.629><c> including</c><00:00:07.230><c> us</c>

00:00:07.369 --> 00:00:07.379 align:start position:0%
gradient descent algorithm including us
 

00:00:07.379 --> 00:00:10.430 align:start position:0%
gradient descent algorithm including us
adaptations<00:00:08.069><c> online</c><00:00:08.519><c> learning</c><00:00:08.870><c> but</c><00:00:09.870><c> all</c><00:00:10.260><c> of</c>

00:00:10.430 --> 00:00:10.440 align:start position:0%
adaptations online learning but all of
 

00:00:10.440 --> 00:00:12.290 align:start position:0%
adaptations online learning but all of
those<00:00:10.559><c> algorithms</c><00:00:11.130><c> could</c><00:00:11.429><c> be</c><00:00:11.580><c> run</c><00:00:11.790><c> on</c><00:00:12.000><c> one</c>

00:00:12.290 --> 00:00:12.300 align:start position:0%
those algorithms could be run on one
 

00:00:12.300 --> 00:00:14.539 align:start position:0%
those algorithms could be run on one
machine<00:00:12.330><c> could</c><00:00:12.929><c> be</c><00:00:12.960><c> run</c><00:00:13.200><c> on</c><00:00:13.440><c> one</c><00:00:13.679><c> computer</c><00:00:14.099><c> and</c>

00:00:14.539 --> 00:00:14.549 align:start position:0%
machine could be run on one computer and
 

00:00:14.549 --> 00:00:16.670 align:start position:0%
machine could be run on one computer and
some<00:00:15.299><c> machine</c><00:00:15.480><c> learning</c><00:00:15.660><c> problems</c><00:00:16.350><c> are</c><00:00:16.500><c> just</c>

00:00:16.670 --> 00:00:16.680 align:start position:0%
some machine learning problems are just
 

00:00:16.680 --> 00:00:18.650 align:start position:0%
some machine learning problems are just
too<00:00:16.859><c> big</c><00:00:17.070><c> to</c><00:00:17.100><c> run</c><00:00:17.490><c> on</c><00:00:17.699><c> one</c><00:00:17.940><c> machine</c><00:00:17.970><c> and</c>

00:00:18.650 --> 00:00:18.660 align:start position:0%
too big to run on one machine and
 

00:00:18.660 --> 00:00:20.210 align:start position:0%
too big to run on one machine and
sometimes<00:00:18.990><c> maybe</c><00:00:19.289><c> you</c><00:00:19.439><c> just</c><00:00:19.680><c> have</c><00:00:19.830><c> so</c><00:00:20.039><c> much</c>

00:00:20.210 --> 00:00:20.220 align:start position:0%
sometimes maybe you just have so much
 

00:00:20.220 --> 00:00:22.790 align:start position:0%
sometimes maybe you just have so much
data<00:00:20.490><c> you</c><00:00:21.180><c> just</c><00:00:21.420><c> don't</c><00:00:21.660><c> ever</c><00:00:21.779><c> want</c><00:00:21.930><c> to</c><00:00:22.230><c> run</c><00:00:22.500><c> all</c>

00:00:22.790 --> 00:00:22.800 align:start position:0%
data you just don't ever want to run all
 

00:00:22.800 --> 00:00:25.400 align:start position:0%
data you just don't ever want to run all
that<00:00:23.010><c> data</c><00:00:23.310><c> through</c><00:00:24.000><c> a</c><00:00:24.090><c> single</c><00:00:24.449><c> computer</c><00:00:24.960><c> no</c>

00:00:25.400 --> 00:00:25.410 align:start position:0%
that data through a single computer no
 

00:00:25.410 --> 00:00:27.109 align:start position:0%
that data through a single computer no
matter<00:00:25.439><c> what</c><00:00:25.740><c> algorithm</c><00:00:26.250><c> you</c><00:00:26.640><c> would</c><00:00:26.789><c> use</c><00:00:26.970><c> on</c>

00:00:27.109 --> 00:00:27.119 align:start position:0%
matter what algorithm you would use on
 

00:00:27.119 --> 00:00:29.870 align:start position:0%
matter what algorithm you would use on
that<00:00:27.300><c> computer</c><00:00:27.650><c> so</c><00:00:28.650><c> in</c><00:00:29.010><c> this</c><00:00:29.160><c> video</c><00:00:29.369><c> I'd</c><00:00:29.760><c> like</c>

00:00:29.870 --> 00:00:29.880 align:start position:0%
that computer so in this video I'd like
 

00:00:29.880 --> 00:00:31.939 align:start position:0%
that computer so in this video I'd like
to<00:00:29.939><c> talk</c><00:00:30.210><c> about</c><00:00:30.269><c> a</c><00:00:30.570><c> different</c><00:00:31.230><c> approach</c><00:00:31.590><c> to</c>

00:00:31.939 --> 00:00:31.949 align:start position:0%
to talk about a different approach to
 

00:00:31.949 --> 00:00:34.130 align:start position:0%
to talk about a different approach to
large<00:00:32.189><c> scale</c><00:00:32.430><c> machine</c><00:00:32.730><c> learning</c><00:00:32.940><c> called</c><00:00:33.899><c> the</c>

00:00:34.130 --> 00:00:34.140 align:start position:0%
large scale machine learning called the
 

00:00:34.140 --> 00:00:38.119 align:start position:0%
large scale machine learning called the
MapReduce<00:00:34.770><c> approach</c><00:00:35.550><c> and</c><00:00:36.800><c> even</c><00:00:37.800><c> though</c><00:00:37.950><c> we</c>

00:00:38.119 --> 00:00:38.129 align:start position:0%
MapReduce approach and even though we
 

00:00:38.129 --> 00:00:39.650 align:start position:0%
MapReduce approach and even though we
had<00:00:38.340><c> quite</c><00:00:38.579><c> a</c><00:00:38.640><c> few</c><00:00:38.910><c> videos</c><00:00:39.030><c> on</c><00:00:39.420><c> stochastic</c>

00:00:39.650 --> 00:00:39.660 align:start position:0%
had quite a few videos on stochastic
 

00:00:39.660 --> 00:00:42.740 align:start position:0%
had quite a few videos on stochastic
gradient<00:00:40.079><c> descent</c><00:00:40.350><c> and</c><00:00:40.940><c> to</c><00:00:41.940><c> spend</c><00:00:42.180><c> relatively</c>

00:00:42.740 --> 00:00:42.750 align:start position:0%
gradient descent and to spend relatively
 

00:00:42.750 --> 00:00:45.410 align:start position:0%
gradient descent and to spend relatively
less<00:00:42.960><c> time</c><00:00:43.260><c> on</c><00:00:43.500><c> MapReduce</c><00:00:44.160><c> don't</c><00:00:44.879><c> judge</c><00:00:45.180><c> the</c>

00:00:45.410 --> 00:00:45.420 align:start position:0%
less time on MapReduce don't judge the
 

00:00:45.420 --> 00:00:47.450 align:start position:0%
less time on MapReduce don't judge the
relative<00:00:45.809><c> importance</c><00:00:46.440><c> of</c><00:00:46.500><c> MapReduce</c><00:00:46.860><c> versus</c>

00:00:47.450 --> 00:00:47.460 align:start position:0%
relative importance of MapReduce versus
 

00:00:47.460 --> 00:00:49.549 align:start position:0%
relative importance of MapReduce versus
the<00:00:47.610><c> costly</c><00:00:47.940><c> penderson</c><00:00:48.450><c> based</c><00:00:48.870><c> on</c><00:00:49.079><c> the</c><00:00:49.260><c> amount</c>

00:00:49.549 --> 00:00:49.559 align:start position:0%
the costly penderson based on the amount
 

00:00:49.559 --> 00:00:51.380 align:start position:0%
the costly penderson based on the amount
of<00:00:49.680><c> time</c><00:00:49.950><c> I</c><00:00:49.980><c> spend</c><00:00:50.399><c> on</c><00:00:50.520><c> these</c><00:00:50.700><c> ideas</c><00:00:50.850><c> in</c>

00:00:51.380 --> 00:00:51.390 align:start position:0%
of time I spend on these ideas in
 

00:00:51.390 --> 00:00:54.049 align:start position:0%
of time I spend on these ideas in
particular<00:00:51.660><c> many</c><00:00:52.440><c> people</c><00:00:52.800><c> will</c><00:00:53.070><c> say</c><00:00:53.309><c> that</c><00:00:53.340><c> Mac</c>

00:00:54.049 --> 00:00:54.059 align:start position:0%
particular many people will say that Mac
 

00:00:54.059 --> 00:00:56.119 align:start position:0%
particular many people will say that Mac
reduces<00:00:54.600><c> and</c><00:00:54.840><c> leaves</c><00:00:55.050><c> an</c><00:00:55.199><c> equally</c><00:00:55.620><c> important</c>

00:00:56.119 --> 00:00:56.129 align:start position:0%
reduces and leaves an equally important
 

00:00:56.129 --> 00:00:57.350 align:start position:0%
reduces and leaves an equally important
and<00:00:56.370><c> some</c><00:00:56.550><c> would</c><00:00:56.670><c> say</c><00:00:56.699><c> an</c><00:00:56.820><c> even</c><00:00:57.090><c> more</c>

00:00:57.350 --> 00:00:57.360 align:start position:0%
and some would say an even more
 

00:00:57.360 --> 00:00:59.510 align:start position:0%
and some would say an even more
important<00:00:57.870><c> idea</c><00:00:58.260><c> compared</c><00:00:58.890><c> to</c><00:00:58.920><c> stochastic</c>

00:00:59.510 --> 00:00:59.520 align:start position:0%
important idea compared to stochastic
 

00:00:59.520 --> 00:01:02.569 align:start position:0%
important idea compared to stochastic
gradient<00:00:59.609><c> descent</c><00:00:59.989><c> only</c><00:01:00.989><c> it</c><00:01:01.350><c> is</c><00:01:01.829><c> relatively</c>

00:01:02.569 --> 00:01:02.579 align:start position:0%
gradient descent only it is relatively
 

00:01:02.579 --> 00:01:04.520 align:start position:0%
gradient descent only it is relatively
simpler<00:01:03.059><c> to</c><00:01:03.210><c> explain</c><00:01:03.690><c> which</c><00:01:03.960><c> is</c><00:01:04.080><c> why</c><00:01:04.229><c> I'm</c>

00:01:04.520 --> 00:01:04.530 align:start position:0%
simpler to explain which is why I'm
 

00:01:04.530 --> 00:01:06.770 align:start position:0%
simpler to explain which is why I'm
going<00:01:04.739><c> to</c><00:01:04.830><c> spend</c><00:01:05.189><c> less</c><00:01:05.400><c> time</c><00:01:05.670><c> on</c><00:01:05.700><c> it</c><00:01:06.000><c> but</c><00:01:06.540><c> using</c>

00:01:06.770 --> 00:01:06.780 align:start position:0%
going to spend less time on it but using
 

00:01:06.780 --> 00:01:08.660 align:start position:0%
going to spend less time on it but using
these<00:01:07.020><c> ideas</c><00:01:07.229><c> you</c><00:01:07.799><c> might</c><00:01:08.010><c> be</c><00:01:08.159><c> with</c><00:01:08.310><c> a</c><00:01:08.369><c> scale</c>

00:01:08.660 --> 00:01:08.670 align:start position:0%
these ideas you might be with a scale
 

00:01:08.670 --> 00:01:11.480 align:start position:0%
these ideas you might be with a scale
learning<00:01:09.390><c> algorithms</c><00:01:10.140><c> to</c><00:01:10.409><c> even</c><00:01:10.890><c> far</c><00:01:11.250><c> larger</c>

00:01:11.480 --> 00:01:11.490 align:start position:0%
learning algorithms to even far larger
 

00:01:11.490 --> 00:01:13.550 align:start position:0%
learning algorithms to even far larger
problems<00:01:12.240><c> than</c><00:01:12.479><c> is</c><00:01:12.630><c> possible</c><00:01:12.810><c> using</c>

00:01:13.550 --> 00:01:13.560 align:start position:0%
problems than is possible using
 

00:01:13.560 --> 00:01:17.899 align:start position:0%
problems than is possible using
stochastic<00:01:13.799><c> gradient</c><00:01:14.310><c> descent</c>

00:01:17.899 --> 00:01:17.909 align:start position:0%
 
 

00:01:17.909 --> 00:01:21.210 align:start position:0%
 
here's<00:01:18.909><c> the</c><00:01:19.060><c> idea</c><00:01:19.329><c> let's</c><00:01:19.930><c> say</c><00:01:20.229><c> we</c><00:01:20.439><c> want</c><00:01:20.680><c> to</c><00:01:20.890><c> fit</c>

00:01:21.210 --> 00:01:21.220 align:start position:0%
here's the idea let's say we want to fit
 

00:01:21.220 --> 00:01:23.580 align:start position:0%
here's the idea let's say we want to fit
a<00:01:21.520><c> linear</c><00:01:22.119><c> regression</c><00:01:22.299><c> model</c><00:01:22.689><c> or</c><00:01:23.110><c> logistic</c>

00:01:23.580 --> 00:01:23.590 align:start position:0%
a linear regression model or logistic
 

00:01:23.590 --> 00:01:25.710 align:start position:0%
a linear regression model or logistic
regression<00:01:23.920><c> model</c><00:01:24.250><c> or</c><00:01:24.399><c> somesuch</c><00:01:24.610><c> and</c><00:01:25.119><c> let's</c>

00:01:25.710 --> 00:01:25.720 align:start position:0%
regression model or somesuch and let's
 

00:01:25.720 --> 00:01:27.420 align:start position:0%
regression model or somesuch and let's
start<00:01:26.020><c> again</c><00:01:26.229><c> with</c><00:01:26.409><c> batch</c><00:01:26.890><c> gradient</c><00:01:27.189><c> descent</c>

00:01:27.420 --> 00:01:27.430 align:start position:0%
start again with batch gradient descent
 

00:01:27.430 --> 00:01:29.460 align:start position:0%
start again with batch gradient descent
so<00:01:27.850><c> that's</c><00:01:28.119><c> our</c><00:01:28.329><c> batch</c><00:01:28.869><c> gradient</c><00:01:29.200><c> descent</c>

00:01:29.460 --> 00:01:29.470 align:start position:0%
so that's our batch gradient descent
 

00:01:29.470 --> 00:01:32.969 align:start position:0%
so that's our batch gradient descent
learning<00:01:30.100><c> rule</c><00:01:30.490><c> and</c><00:01:30.759><c> to</c><00:01:31.720><c> keep</c><00:01:31.960><c> the</c><00:01:32.289><c> writing</c><00:01:32.830><c> on</c>

00:01:32.969 --> 00:01:32.979 align:start position:0%
learning rule and to keep the writing on
 

00:01:32.979 --> 00:01:34.770 align:start position:0%
learning rule and to keep the writing on
this<00:01:33.130><c> slide</c><00:01:33.369><c> tractable</c><00:01:33.939><c> I'm</c><00:01:34.149><c> going</c><00:01:34.390><c> to</c><00:01:34.509><c> assume</c>

00:01:34.770 --> 00:01:34.780 align:start position:0%
this slide tractable I'm going to assume
 

00:01:34.780 --> 00:01:37.679 align:start position:0%
this slide tractable I'm going to assume
throughout<00:01:35.259><c> that</c><00:01:35.890><c> we</c><00:01:36.039><c> have</c><00:01:36.250><c> M</c><00:01:36.490><c> equals</c><00:01:37.030><c> 400</c>

00:01:37.679 --> 00:01:37.689 align:start position:0%
throughout that we have M equals 400
 

00:01:37.689 --> 00:01:40.260 align:start position:0%
throughout that we have M equals 400
examples<00:01:38.200><c> of</c><00:01:38.770><c> course</c><00:01:39.100><c> by</c><00:01:39.340><c> our</c><00:01:39.369><c> standards</c><00:01:39.970><c> in</c>

00:01:40.260 --> 00:01:40.270 align:start position:0%
examples of course by our standards in
 

00:01:40.270 --> 00:01:41.429 align:start position:0%
examples of course by our standards in
terms<00:01:40.299><c> of</c><00:01:40.630><c> large</c><00:01:40.869><c> scale</c><00:01:41.020><c> machine</c><00:01:41.289><c> learning</c>

00:01:41.429 --> 00:01:41.439 align:start position:0%
terms of large scale machine learning
 

00:01:41.439 --> 00:01:43.440 align:start position:0%
terms of large scale machine learning
you<00:01:41.890><c> know</c><00:01:41.950><c> n</c><00:01:42.130><c> might</c><00:01:42.369><c> be</c><00:01:42.490><c> pretty</c><00:01:42.640><c> small</c><00:01:42.970><c> and</c><00:01:43.240><c> so</c>

00:01:43.440 --> 00:01:43.450 align:start position:0%
you know n might be pretty small and so
 

00:01:43.450 --> 00:01:46.200 align:start position:0%
you know n might be pretty small and so
this<00:01:43.929><c> might</c><00:01:44.170><c> be</c><00:01:44.350><c> more</c><00:01:44.920><c> commonly</c><00:01:45.820><c> applied</c><00:01:46.179><c> to</c>

00:01:46.200 --> 00:01:46.210 align:start position:0%
this might be more commonly applied to
 

00:01:46.210 --> 00:01:48.210 align:start position:0%
this might be more commonly applied to
problems<00:01:46.539><c> where</c><00:01:46.990><c> you</c><00:01:47.020><c> have</c><00:01:47.110><c> maybe</c><00:01:47.500><c> closer</c><00:01:47.860><c> to</c>

00:01:48.210 --> 00:01:48.220 align:start position:0%
problems where you have maybe closer to
 

00:01:48.220 --> 00:01:50.940 align:start position:0%
problems where you have maybe closer to
400<00:01:48.909><c> million</c><00:01:49.270><c> examples</c><00:01:49.840><c> or</c><00:01:49.959><c> some</c><00:01:50.140><c> such</c><00:01:50.350><c> but</c>

00:01:50.940 --> 00:01:50.950 align:start position:0%
400 million examples or some such but
 

00:01:50.950 --> 00:01:52.649 align:start position:0%
400 million examples or some such but
just<00:01:51.310><c> to</c><00:01:51.429><c> make</c><00:01:51.580><c> the</c><00:01:51.759><c> writing</c><00:01:51.970><c> on</c><00:01:52.240><c> this</c><00:01:52.390><c> slide</c>

00:01:52.649 --> 00:01:52.659 align:start position:0%
just to make the writing on this slide
 

00:01:52.659 --> 00:01:55.020 align:start position:0%
just to make the writing on this slide
simpler<00:01:53.200><c> and</c><00:01:53.440><c> then</c><00:01:53.560><c> to</c><00:01:53.709><c> pretend</c><00:01:54.190><c> we</c><00:01:54.340><c> have</c><00:01:54.520><c> 100</c>

00:01:55.020 --> 00:01:55.030 align:start position:0%
simpler and then to pretend we have 100
 

00:01:55.030 --> 00:01:57.990 align:start position:0%
simpler and then to pretend we have 100
examples<00:01:55.350><c> so</c><00:01:56.350><c> in</c><00:01:56.409><c> that</c><00:01:56.560><c> case</c><00:01:56.830><c> the</c><00:01:57.610><c> batch</c>

00:01:57.990 --> 00:01:58.000 align:start position:0%
examples so in that case the batch
 

00:01:58.000 --> 00:01:59.880 align:start position:0%
examples so in that case the batch
gradient<00:01:58.270><c> descent</c><00:01:58.509><c> learning</c><00:01:58.899><c> rule</c><00:01:59.289><c> has</c><00:01:59.709><c> this</c>

00:01:59.880 --> 00:01:59.890 align:start position:0%
gradient descent learning rule has this
 

00:01:59.890 --> 00:02:02.100 align:start position:0%
gradient descent learning rule has this
you<00:02:00.100><c> have</c><00:02:00.189><c> 1</c><00:02:00.340><c> over</c><00:02:00.520><c> 400</c><00:02:01.119><c> and</c><00:02:01.330><c> this</c><00:02:01.479><c> sum</c><00:02:01.720><c> from</c><00:02:01.929><c> I</c>

00:02:02.100 --> 00:02:02.110 align:start position:0%
you have 1 over 400 and this sum from I
 

00:02:02.110 --> 00:02:04.919 align:start position:0%
you have 1 over 400 and this sum from I
equals<00:02:02.470><c> 1</c><00:02:02.679><c> through</c><00:02:02.709><c> 400</c><00:02:03.429><c> through</c><00:02:04.179><c> 400</c>

00:02:04.919 --> 00:02:04.929 align:start position:0%
equals 1 through 400 through 400
 

00:02:04.929 --> 00:02:07.679 align:start position:0%
equals 1 through 400 through 400
examples<00:02:05.349><c> here</c><00:02:05.799><c> and</c><00:02:05.979><c> if</c><00:02:06.909><c> M</c><00:02:07.060><c> is</c><00:02:07.179><c> large</c><00:02:07.390><c> then</c>

00:02:07.679 --> 00:02:07.689 align:start position:0%
examples here and if M is large then
 

00:02:07.689 --> 00:02:10.050 align:start position:0%
examples here and if M is large then
this<00:02:07.840><c> is</c><00:02:08.019><c> a</c><00:02:08.310><c> computationally</c><00:02:09.310><c> expensive</c><00:02:09.489><c> step</c>

00:02:10.050 --> 00:02:10.060 align:start position:0%
this is a computationally expensive step
 

00:02:10.060 --> 00:02:13.979 align:start position:0%
this is a computationally expensive step
so<00:02:11.049><c> what</c><00:02:11.440><c> the</c><00:02:11.530><c> MapReduce</c><00:02:12.180><c> idea</c><00:02:13.180><c> does</c><00:02:13.450><c> is</c><00:02:13.720><c> the</c>

00:02:13.979 --> 00:02:13.989 align:start position:0%
so what the MapReduce idea does is the
 

00:02:13.989 --> 00:02:16.080 align:start position:0%
so what the MapReduce idea does is the
following<00:02:14.250><c> and</c><00:02:15.250><c> I</c><00:02:15.340><c> should</c><00:02:15.519><c> say</c><00:02:15.610><c> the</c><00:02:15.790><c> MapReduce</c>

00:02:16.080 --> 00:02:16.090 align:start position:0%
following and I should say the MapReduce
 

00:02:16.090 --> 00:02:20.370 align:start position:0%
following and I should say the MapReduce
idea<00:02:16.599><c> is</c><00:02:16.690><c> due</c><00:02:16.959><c> to</c><00:02:17.140><c> two</c><00:02:18.090><c> researchers</c><00:02:19.090><c> Jeff</c><00:02:20.049><c> Dean</c>

00:02:20.370 --> 00:02:20.380 align:start position:0%
idea is due to two researchers Jeff Dean
 

00:02:20.380 --> 00:02:23.460 align:start position:0%
idea is due to two researchers Jeff Dean
and<00:02:21.000><c> Sanjay</c><00:02:22.000><c> Ghemawat</c><00:02:22.269><c> Jeff</c><00:02:22.870><c> Dean</c><00:02:23.049><c> by</c><00:02:23.260><c> the</c><00:02:23.320><c> way</c>

00:02:23.460 --> 00:02:23.470 align:start position:0%
and Sanjay Ghemawat Jeff Dean by the way
 

00:02:23.470 --> 00:02:25.340 align:start position:0%
and Sanjay Ghemawat Jeff Dean by the way
is<00:02:23.650><c> also</c><00:02:23.890><c> one</c><00:02:24.340><c> of</c><00:02:24.370><c> the</c><00:02:24.549><c> most</c><00:02:24.730><c> legendary</c>

00:02:25.340 --> 00:02:25.350 align:start position:0%
is also one of the most legendary
 

00:02:25.350 --> 00:02:27.750 align:start position:0%
is also one of the most legendary
engineers<00:02:26.350><c> in</c><00:02:26.650><c> all</c><00:02:26.829><c> of</c><00:02:27.010><c> Silicon</c><00:02:27.340><c> Valley</c><00:02:27.370><c> and</c>

00:02:27.750 --> 00:02:27.760 align:start position:0%
engineers in all of Silicon Valley and
 

00:02:27.760 --> 00:02:30.420 align:start position:0%
engineers in all of Silicon Valley and
he<00:02:27.970><c> kind</c><00:02:28.420><c> of</c><00:02:28.480><c> a</c><00:02:28.630><c> belt</c><00:02:29.230><c> a</c><00:02:29.470><c> large</c><00:02:29.769><c> fraction</c><00:02:30.400><c> of</c>

00:02:30.420 --> 00:02:30.430 align:start position:0%
he kind of a belt a large fraction of
 

00:02:30.430 --> 00:02:32.940 align:start position:0%
he kind of a belt a large fraction of
the<00:02:31.109><c> architecture</c><00:02:32.109><c> or</c><00:02:32.140><c> the</c><00:02:32.260><c> infrastructure</c>

00:02:32.940 --> 00:02:32.950 align:start position:0%
the architecture or the infrastructure
 

00:02:32.950 --> 00:02:36.479 align:start position:0%
the architecture or the infrastructure
that<00:02:33.370><c> all</c><00:02:33.579><c> of</c><00:02:33.790><c> Google</c><00:02:34.239><c> runs</c><00:02:34.690><c> on</c><00:02:34.989><c> today</c><00:02:35.319><c> oh</c><00:02:35.590><c> but</c>

00:02:36.479 --> 00:02:36.489 align:start position:0%
that all of Google runs on today oh but
 

00:02:36.489 --> 00:02:38.520 align:start position:0%
that all of Google runs on today oh but
here's<00:02:36.670><c> the</c><00:02:36.790><c> MapReduce</c><00:02:37.120><c> idea</c><00:02:37.690><c> so</c><00:02:38.049><c> let's</c><00:02:38.410><c> say</c><00:02:38.500><c> I</c>

00:02:38.520 --> 00:02:38.530 align:start position:0%
here's the MapReduce idea so let's say I
 

00:02:38.530 --> 00:02:40.410 align:start position:0%
here's the MapReduce idea so let's say I
have<00:02:38.650><c> some</c><00:02:38.680><c> training</c><00:02:39.220><c> set</c><00:02:39.430><c> yeah</c><00:02:39.760><c> denote</c><00:02:40.390><c> by</c>

00:02:40.410 --> 00:02:40.420 align:start position:0%
have some training set yeah denote by
 

00:02:40.420 --> 00:02:43.740 align:start position:0%
have some training set yeah denote by
this<00:02:40.810><c> box</c><00:02:41.109><c> here</c><00:02:41.350><c> of</c><00:02:41.620><c> X</c><00:02:42.519><c> Y</c><00:02:42.730><c> here</c><00:02:43.000><c> it</c><00:02:43.359><c> says</c><00:02:43.510><c> you</c>

00:02:43.740 --> 00:02:43.750 align:start position:0%
this box here of X Y here it says you
 

00:02:43.750 --> 00:02:47.440 align:start position:0%
this box here of X Y here it says you
know<00:02:43.840><c> x1</c><00:02:45.120><c> y1</c>

00:02:47.440 --> 00:02:47.450 align:start position:0%
know x1 y1
 

00:02:47.450 --> 00:02:52.510 align:start position:0%
know x1 y1
down<00:02:47.900><c> to</c><00:02:48.230><c> my</c><00:02:48.590><c> 400</c><00:02:49.580><c> examples</c><00:02:49.820><c> XM</c><00:02:51.010><c> YM</c><00:02:52.040><c> so</c><00:02:52.340><c> that's</c>

00:02:52.510 --> 00:02:52.520 align:start position:0%
down to my 400 examples XM YM so that's
 

00:02:52.520 --> 00:02:55.180 align:start position:0%
down to my 400 examples XM YM so that's
my<00:02:52.670><c> training</c><00:02:53.000><c> sample</c><00:02:53.270><c> 400</c><00:02:53.660><c> ring</c><00:02:53.810><c> examples</c><00:02:54.290><c> in</c>

00:02:55.180 --> 00:02:55.190 align:start position:0%
my training sample 400 ring examples in
 

00:02:55.190 --> 00:02:57.010 align:start position:0%
my training sample 400 ring examples in
the<00:02:55.430><c> MapReduce</c><00:02:55.790><c> idea</c><00:02:56.270><c> what</c><00:02:56.510><c> I'm</c><00:02:56.570><c> going</c><00:02:56.780><c> to</c><00:02:56.810><c> do</c>

00:02:57.010 --> 00:02:57.020 align:start position:0%
the MapReduce idea what I'm going to do
 

00:02:57.020 --> 00:02:59.800 align:start position:0%
the MapReduce idea what I'm going to do
is<00:02:57.200><c> split</c><00:02:57.590><c> this</c><00:02:58.130><c> training</c><00:02:58.490><c> set</c><00:02:58.820><c> into</c>

00:02:59.800 --> 00:02:59.810 align:start position:0%
is split this training set into
 

00:02:59.810 --> 00:03:03.220 align:start position:0%
is split this training set into
different<00:03:00.350><c> subsets</c><00:03:00.980><c> and</c><00:03:01.940><c> I'm</c><00:03:02.690><c> going</c><00:03:02.930><c> to</c><00:03:03.050><c> see</c>

00:03:03.220 --> 00:03:03.230 align:start position:0%
different subsets and I'm going to see
 

00:03:03.230 --> 00:03:05.080 align:start position:0%
different subsets and I'm going to see
you<00:03:03.320><c> for</c><00:03:03.500><c> this</c><00:03:03.620><c> example</c><00:03:03.950><c> that</c><00:03:04.160><c> I</c><00:03:04.340><c> have</c><00:03:04.640><c> for</c>

00:03:05.080 --> 00:03:05.090 align:start position:0%
you for this example that I have for
 

00:03:05.090 --> 00:03:07.300 align:start position:0%
you for this example that I have for
computers<00:03:05.870><c> or</c><00:03:06.020><c> for</c><00:03:06.290><c> machines</c><00:03:06.680><c> to</c><00:03:06.890><c> run</c><00:03:06.920><c> in</c>

00:03:07.300 --> 00:03:07.310 align:start position:0%
computers or for machines to run in
 

00:03:07.310 --> 00:03:09.250 align:start position:0%
computers or for machines to run in
parallel<00:03:07.580><c> on</c><00:03:08.120><c> my</c><00:03:08.300><c> training</c><00:03:08.570><c> set</c><00:03:08.840><c> which</c><00:03:09.020><c> is</c><00:03:09.110><c> not</c>

00:03:09.250 --> 00:03:09.260 align:start position:0%
parallel on my training set which is not
 

00:03:09.260 --> 00:03:11.440 align:start position:0%
parallel on my training set which is not
splitting<00:03:09.620><c> this</c><00:03:09.860><c> into</c><00:03:09.920><c> four</c><00:03:10.370><c> machines</c><00:03:10.700><c> if</c><00:03:11.360><c> you</c>

00:03:11.440 --> 00:03:11.450 align:start position:0%
splitting this into four machines if you
 

00:03:11.450 --> 00:03:13.390 align:start position:0%
splitting this into four machines if you
have<00:03:11.630><c> ten</c><00:03:11.870><c> machines</c><00:03:12.020><c> or</c><00:03:12.410><c> 100</c><00:03:12.770><c> machines</c><00:03:12.890><c> that</c>

00:03:13.390 --> 00:03:13.400 align:start position:0%
have ten machines or 100 machines that
 

00:03:13.400 --> 00:03:14.200 align:start position:0%
have ten machines or 100 machines that
you<00:03:13.490><c> wouldn't</c><00:03:13.730><c> you</c><00:03:13.790><c> know</c><00:03:13.850><c> split</c><00:03:14.060><c> your</c>

00:03:14.200 --> 00:03:14.210 align:start position:0%
you wouldn't you know split your
 

00:03:14.210 --> 00:03:16.090 align:start position:0%
you wouldn't you know split your
training<00:03:14.510><c> set</c><00:03:14.690><c> into</c><00:03:14.990><c> ten</c><00:03:15.230><c> pieces</c><00:03:15.380><c> or</c><00:03:15.770><c> 100</c>

00:03:16.090 --> 00:03:16.100 align:start position:0%
training set into ten pieces or 100
 

00:03:16.100 --> 00:03:18.670 align:start position:0%
training set into ten pieces or 100
pieces<00:03:16.370><c> or</c><00:03:16.550><c> what</c><00:03:16.700><c> have</c><00:03:16.850><c> you</c><00:03:16.880><c> and</c><00:03:17.500><c> what</c><00:03:18.500><c> the</c>

00:03:18.670 --> 00:03:18.680 align:start position:0%
pieces or what have you and what the
 

00:03:18.680 --> 00:03:21.010 align:start position:0%
pieces or what have you and what the
first<00:03:18.950><c> of</c><00:03:19.580><c> my</c><00:03:19.850><c> four</c><00:03:20.120><c> machines</c><00:03:20.450><c> going</c><00:03:20.750><c> to</c><00:03:20.870><c> do</c>

00:03:21.010 --> 00:03:21.020 align:start position:0%
first of my four machines going to do
 

00:03:21.020 --> 00:03:25.300 align:start position:0%
first of my four machines going to do
say<00:03:21.290><c> is</c><00:03:21.590><c> use</c><00:03:21.920><c> just</c><00:03:22.300><c> the</c><00:03:23.300><c> first</c><00:03:23.660><c> 1/4</c><00:03:24.290><c> of</c><00:03:24.860><c> my</c>

00:03:25.300 --> 00:03:25.310 align:start position:0%
say is use just the first 1/4 of my
 

00:03:25.310 --> 00:03:28.570 align:start position:0%
say is use just the first 1/4 of my
training<00:03:25.670><c> sets</c><00:03:25.850><c> reduce</c><00:03:26.360><c> just</c><00:03:26.710><c> the</c><00:03:27.710><c> first</c><00:03:27.980><c> 100</c>

00:03:28.570 --> 00:03:28.580 align:start position:0%
training sets reduce just the first 100
 

00:03:28.580 --> 00:03:31.420 align:start position:0%
training sets reduce just the first 100
training<00:03:28.850><c> examples</c><00:03:29.000><c> and</c><00:03:29.740><c> in</c><00:03:30.740><c> particular</c><00:03:30.950><c> what</c>

00:03:31.420 --> 00:03:31.430 align:start position:0%
training examples and in particular what
 

00:03:31.430 --> 00:03:32.740 align:start position:0%
training examples and in particular what
it's<00:03:31.550><c> going</c><00:03:31.670><c> to</c><00:03:31.730><c> do</c><00:03:31.940><c> is</c><00:03:32.180><c> look</c><00:03:32.510><c> at</c><00:03:32.660><c> this</c>

00:03:32.740 --> 00:03:32.750 align:start position:0%
it's going to do is look at this
 

00:03:32.750 --> 00:03:36.940 align:start position:0%
it's going to do is look at this
summation<00:03:33.110><c> and</c><00:03:34.270><c> compute</c><00:03:35.270><c> that</c><00:03:35.690><c> summation</c><00:03:36.110><c> for</c>

00:03:36.940 --> 00:03:36.950 align:start position:0%
summation and compute that summation for
 

00:03:36.950 --> 00:03:40.180 align:start position:0%
summation and compute that summation for
just<00:03:37.190><c> the</c><00:03:37.310><c> first</c><00:03:37.610><c> 100</c><00:03:38.270><c> training</c><00:03:38.510><c> examples</c><00:03:39.190><c> so</c>

00:03:40.180 --> 00:03:40.190 align:start position:0%
just the first 100 training examples so
 

00:03:40.190 --> 00:03:42.040 align:start position:0%
just the first 100 training examples so
let<00:03:40.430><c> me</c><00:03:40.520><c> write</c><00:03:40.670><c> that</c><00:03:40.700><c> out</c><00:03:40.850><c> and</c><00:03:41.210><c> computer</c><00:03:41.660><c> in</c>

00:03:42.040 --> 00:03:42.050 align:start position:0%
let me write that out and computer in
 

00:03:42.050 --> 00:03:45.100 align:start position:0%
let me write that out and computer in
the<00:03:42.140><c> computer</c><00:03:42.260><c> variable</c><00:03:43.070><c> temp1</c>

00:03:45.100 --> 00:03:45.110 align:start position:0%
the computer variable temp1
 

00:03:45.110 --> 00:03:46.930 align:start position:0%
the computer variable temp1
the<00:03:45.320><c> superscript</c><00:03:45.890><c> 1</c><00:03:46.040><c> denotes</c><00:03:46.370><c> is</c><00:03:46.610><c> the</c><00:03:46.700><c> first</c>

00:03:46.930 --> 00:03:46.940 align:start position:0%
the superscript 1 denotes is the first
 

00:03:46.940 --> 00:03:51.970 align:start position:0%
the superscript 1 denotes is the first
machine<00:03:47.270><c> j</c><00:03:48.790><c> equals</c><00:03:49.790><c> sum</c><00:03:50.750><c> from</c><00:03:51.050><c> I</c><00:03:51.320><c> equals</c><00:03:51.740><c> 1</c>

00:03:51.970 --> 00:03:51.980 align:start position:0%
machine j equals sum from I equals 1
 

00:03:51.980 --> 00:03:54.280 align:start position:0%
machine j equals sum from I equals 1
through<00:03:52.010><c> 100</c><00:03:52.490><c> and</c><00:03:53.390><c> then</c><00:03:53.720><c> I'm</c><00:03:53.870><c> going</c><00:03:54.020><c> to</c><00:03:54.080><c> plug</c>

00:03:54.280 --> 00:03:54.290 align:start position:0%
through 100 and then I'm going to plug
 

00:03:54.290 --> 00:03:56.470 align:start position:0%
through 100 and then I'm going to plug
in<00:03:54.470><c> here</c><00:03:54.650><c> exactly</c><00:03:55.190><c> that</c><00:03:55.370><c> term</c><00:03:55.610><c> and</c><00:03:55.880><c> so</c><00:03:56.180><c> I</c><00:03:56.240><c> have</c>

00:03:56.470 --> 00:03:56.480 align:start position:0%
in here exactly that term and so I have
 

00:03:56.480 --> 00:04:04.360 align:start position:0%
in here exactly that term and so I have
a<00:03:56.770><c> dictator</c><00:03:57.770><c> X</c><00:03:58.580><c> I</c><00:03:59.050><c> -</c><00:04:00.050><c> why</c><00:04:00.290><c> I</c><00:04:01.030><c> times</c><00:04:02.030><c> X</c><00:04:02.420><c> I</c><00:04:02.980><c> J</c><00:04:03.980><c> right</c>

00:04:04.360 --> 00:04:04.370 align:start position:0%
a dictator X I - why I times X I J right
 

00:04:04.370 --> 00:04:06.760 align:start position:0%
a dictator X I - why I times X I J right
so<00:04:04.610><c> that's</c><00:04:04.760><c> just</c><00:04:05.030><c> a</c><00:04:05.240><c> that</c><00:04:05.840><c> green</c><00:04:06.110><c> in</c><00:04:06.260><c> to</c><00:04:06.380><c> center</c>

00:04:06.760 --> 00:04:06.770 align:start position:0%
so that's just a that green in to center
 

00:04:06.770 --> 00:04:10.180 align:start position:0%
so that's just a that green in to center
up<00:04:07.340><c> there</c><00:04:07.400><c> and</c><00:04:07.940><c> then</c><00:04:08.600><c> similarly</c><00:04:09.080><c> I'm</c><00:04:09.620><c> going</c><00:04:10.010><c> to</c>

00:04:10.180 --> 00:04:10.190 align:start position:0%
up there and then similarly I'm going to
 

00:04:10.190 --> 00:04:12.340 align:start position:0%
up there and then similarly I'm going to
take<00:04:10.520><c> the</c><00:04:10.700><c> second</c><00:04:11.180><c> quarter</c><00:04:11.600><c> of</c><00:04:11.660><c> my</c><00:04:11.780><c> data</c><00:04:12.020><c> and</c>

00:04:12.340 --> 00:04:12.350 align:start position:0%
take the second quarter of my data and
 

00:04:12.350 --> 00:04:14.890 align:start position:0%
take the second quarter of my data and
send<00:04:13.100><c> it</c><00:04:13.280><c> to</c><00:04:13.310><c> my</c><00:04:13.760><c> second</c><00:04:14.240><c> machine</c><00:04:14.540><c> and</c><00:04:14.780><c> my</c>

00:04:14.890 --> 00:04:14.900 align:start position:0%
send it to my second machine and my
 

00:04:14.900 --> 00:04:16.240 align:start position:0%
send it to my second machine and my
second<00:04:15.200><c> machine</c><00:04:15.440><c> would</c><00:04:15.680><c> use</c><00:04:15.860><c> training</c>

00:04:16.240 --> 00:04:16.250 align:start position:0%
second machine would use training
 

00:04:16.250 --> 00:04:19.690 align:start position:0%
second machine would use training
examples<00:04:16.670><c> 101</c><00:04:17.480><c> through</c><00:04:18.140><c> 200</c><00:04:18.739><c> and</c><00:04:19.130><c> you</c><00:04:19.640><c> know</c>

00:04:19.690 --> 00:04:19.700 align:start position:0%
examples 101 through 200 and you know
 

00:04:19.700 --> 00:04:22.780 align:start position:0%
examples 101 through 200 and you know
computer<00:04:20.209><c> similar</c><00:04:20.660><c> their</c><00:04:21.350><c> boss</c><00:04:21.620><c> of</c><00:04:21.859><c> at</c><00:04:21.980><c> m2j</c>

00:04:22.780 --> 00:04:22.790 align:start position:0%
computer similar their boss of at m2j
 

00:04:22.790 --> 00:04:25.090 align:start position:0%
computer similar their boss of at m2j
which<00:04:23.090><c> is</c><00:04:23.240><c> the</c><00:04:23.510><c> same</c><00:04:23.720><c> sum</c><00:04:24.020><c> the</c><00:04:24.260><c> index</c><00:04:24.710><c> from</c>

00:04:25.090 --> 00:04:25.100 align:start position:0%
which is the same sum the index from
 

00:04:25.100 --> 00:04:28.540 align:start position:0%
which is the same sum the index from
examples<00:04:25.910><c> 101</c><00:04:26.390><c> through</c><00:04:26.660><c> 200</c><00:04:27.230><c> and</c><00:04:27.550><c> similarly</c>

00:04:28.540 --> 00:04:28.550 align:start position:0%
examples 101 through 200 and similarly
 

00:04:28.550 --> 00:04:30.770 align:start position:0%
examples 101 through 200 and similarly
machines<00:04:29.450><c> to</c><00:04:29.690><c> be</c>

00:04:30.770 --> 00:04:30.780 align:start position:0%
machines to be
 

00:04:30.780 --> 00:04:33.920 align:start position:0%
machines to be
and<00:04:30.870><c> for</c><00:04:31.560><c> we'll</c><00:04:32.430><c> use</c><00:04:32.610><c> the</c><00:04:32.850><c> third</c><00:04:33.120><c> quarter</c><00:04:33.660><c> and</c>

00:04:33.920 --> 00:04:33.930 align:start position:0%
and for we'll use the third quarter and
 

00:04:33.930 --> 00:04:37.640 align:start position:0%
and for we'll use the third quarter and
the<00:04:34.020><c> fourth</c><00:04:34.320><c> quarter</c><00:04:34.770><c> of</c><00:04:35.670><c> my</c><00:04:36.000><c> training</c><00:04:36.540><c> set</c><00:04:36.750><c> so</c>

00:04:37.640 --> 00:04:37.650 align:start position:0%
the fourth quarter of my training set so
 

00:04:37.650 --> 00:04:40.730 align:start position:0%
the fourth quarter of my training set so
now<00:04:37.860><c> each</c><00:04:38.250><c> machine</c><00:04:38.520><c> has</c><00:04:39.300><c> to</c><00:04:39.510><c> sum</c><00:04:39.720><c> over</c><00:04:39.900><c> 100</c>

00:04:40.730 --> 00:04:40.740 align:start position:0%
now each machine has to sum over 100
 

00:04:40.740 --> 00:04:42.920 align:start position:0%
now each machine has to sum over 100
instead<00:04:41.040><c> of</c><00:04:41.190><c> over</c><00:04:41.400><c> 400</c><00:04:41.940><c> examples</c><00:04:42.180><c> and</c><00:04:42.660><c> so</c><00:04:42.780><c> has</c>

00:04:42.920 --> 00:04:42.930 align:start position:0%
instead of over 400 examples and so has
 

00:04:42.930 --> 00:04:44.540 align:start position:0%
instead of over 400 examples and so has
to<00:04:43.050><c> do</c><00:04:43.200><c> only</c><00:04:43.380><c> a</c><00:04:43.530><c> quarter</c><00:04:43.980><c> of</c><00:04:44.010><c> the</c><00:04:44.100><c> work</c><00:04:44.340><c> and</c>

00:04:44.540 --> 00:04:44.550 align:start position:0%
to do only a quarter of the work and
 

00:04:44.550 --> 00:04:46.760 align:start position:0%
to do only a quarter of the work and
thus<00:04:45.210><c> presumably</c><00:04:45.780><c> it</c><00:04:46.080><c> could</c><00:04:46.230><c> do</c><00:04:46.380><c> it</c><00:04:46.500><c> about</c>

00:04:46.760 --> 00:04:46.770 align:start position:0%
thus presumably it could do it about
 

00:04:46.770 --> 00:04:50.390 align:start position:0%
thus presumably it could do it about
four<00:04:46.980><c> times</c><00:04:47.190><c> as</c><00:04:47.460><c> fast</c><00:04:48.860><c> finally</c><00:04:49.860><c> after</c><00:04:50.130><c> all</c>

00:04:50.390 --> 00:04:50.400 align:start position:0%
four times as fast finally after all
 

00:04:50.400 --> 00:04:51.830 align:start position:0%
four times as fast finally after all
these<00:04:50.580><c> machines</c><00:04:50.940><c> have</c><00:04:51.120><c> done</c><00:04:51.240><c> this</c><00:04:51.360><c> work</c><00:04:51.570><c> I'm</c>

00:04:51.830 --> 00:04:51.840 align:start position:0%
these machines have done this work I'm
 

00:04:51.840 --> 00:04:55.090 align:start position:0%
these machines have done this work I'm
going<00:04:52.050><c> to</c><00:04:52.200><c> take</c><00:04:52.500><c> these</c><00:04:52.919><c> temp</c><00:04:53.460><c> variables</c><00:04:54.090><c> and</c>

00:04:55.090 --> 00:04:55.100 align:start position:0%
going to take these temp variables and
 

00:04:55.100 --> 00:04:57.379 align:start position:0%
going to take these temp variables and
put<00:04:56.100><c> them</c><00:04:56.250><c> back</c><00:04:56.430><c> together</c><00:04:56.639><c> so</c><00:04:57.090><c> we'll</c><00:04:57.210><c> take</c>

00:04:57.379 --> 00:04:57.389 align:start position:0%
put them back together so we'll take
 

00:04:57.389 --> 00:04:59.450 align:start position:0%
put them back together so we'll take
these<00:04:57.630><c> variables</c><00:04:58.200><c> and</c><00:04:58.530><c> send</c><00:04:58.830><c> them</c><00:04:58.950><c> all</c><00:04:59.130><c> to</c><00:04:59.430><c> a</c>

00:04:59.450 --> 00:04:59.460 align:start position:0%
these variables and send them all to a
 

00:04:59.460 --> 00:05:02.800 align:start position:0%
these variables and send them all to a
you<00:05:00.060><c> know</c><00:05:00.180><c> centralized</c><00:05:00.780><c> master</c><00:05:01.560><c> server</c><00:05:02.100><c> and</c>

00:05:02.800 --> 00:05:02.810 align:start position:0%
you know centralized master server and
 

00:05:02.810 --> 00:05:05.360 align:start position:0%
you know centralized master server and
what<00:05:03.810><c> the</c><00:05:03.930><c> master</c><00:05:04.320><c> server</c><00:05:04.500><c> will</c><00:05:04.830><c> do</c><00:05:05.010><c> is</c>

00:05:05.360 --> 00:05:05.370 align:start position:0%
what the master server will do is
 

00:05:05.370 --> 00:05:07.640 align:start position:0%
what the master server will do is
combine<00:05:05.850><c> these</c><00:05:06.330><c> results</c><00:05:06.390><c> together</c><00:05:06.900><c> and</c><00:05:07.380><c> in</c>

00:05:07.640 --> 00:05:07.650 align:start position:0%
combine these results together and in
 

00:05:07.650 --> 00:05:10.730 align:start position:0%
combine these results together and in
particular<00:05:07.770><c> it</c><00:05:08.610><c> will</c><00:05:08.730><c> update</c><00:05:09.200><c> my</c><00:05:10.200><c> parameters</c>

00:05:10.730 --> 00:05:10.740 align:start position:0%
particular it will update my parameters
 

00:05:10.740 --> 00:05:13.790 align:start position:0%
particular it will update my parameters
theta<00:05:11.010><c> J</c><00:05:11.120><c> according</c><00:05:12.120><c> to</c><00:05:12.350><c> theta</c><00:05:13.350><c> J</c><00:05:13.560><c> gets</c>

00:05:13.790 --> 00:05:13.800 align:start position:0%
theta J according to theta J gets
 

00:05:13.800 --> 00:05:18.020 align:start position:0%
theta J according to theta J gets
updated<00:05:14.130><c> as</c><00:05:14.400><c> theta</c><00:05:14.430><c> J</c><00:05:14.940><c> minus</c><00:05:16.700><c> the</c><00:05:17.700><c> learning</c>

00:05:18.020 --> 00:05:18.030 align:start position:0%
updated as theta J minus the learning
 

00:05:18.030 --> 00:05:24.230 align:start position:0%
updated as theta J minus the learning
rate<00:05:18.150><c> alpha</c><00:05:18.380><c> times</c><00:05:19.380><c> 1</c><00:05:19.680><c> over</c><00:05:19.890><c> 400</c><00:05:20.730><c> times</c><00:05:22.220><c> temp</c><00:05:23.240><c> 1</c>

00:05:24.230 --> 00:05:24.240 align:start position:0%
rate alpha times 1 over 400 times temp 1
 

00:05:24.240 --> 00:05:33.680 align:start position:0%
rate alpha times 1 over 400 times temp 1
J<00:05:24.840><c> plus</c><00:05:26.060><c> temp</c><00:05:27.060><c> 2</c><00:05:27.660><c> J</c><00:05:28.070><c> plus</c><00:05:29.270><c> temp</c><00:05:30.270><c> 3</c><00:05:31.080><c> J</c><00:05:31.680><c> plus</c><00:05:32.690><c> temp</c>

00:05:33.680 --> 00:05:33.690 align:start position:0%
J plus temp 2 J plus temp 3 J plus temp
 

00:05:33.690 --> 00:05:36.590 align:start position:0%
J plus temp 2 J plus temp 3 J plus temp
4<00:05:34.410><c> J</c><00:05:35.070><c> and</c><00:05:35.340><c> of</c><00:05:35.669><c> course</c><00:05:35.880><c> that</c><00:05:36.090><c> we</c><00:05:36.210><c> have</c><00:05:36.360><c> to</c><00:05:36.479><c> do</c>

00:05:36.590 --> 00:05:36.600 align:start position:0%
4 J and of course that we have to do
 

00:05:36.600 --> 00:05:39.050 align:start position:0%
4 J and of course that we have to do
this<00:05:36.720><c> separately</c><00:05:37.169><c> for</c><00:05:37.229><c> J</c><00:05:37.590><c> equals</c><00:05:37.919><c> 0</c><00:05:38.280><c> you</c><00:05:39.030><c> know</c>

00:05:39.050 --> 00:05:39.060 align:start position:0%
this separately for J equals 0 you know
 

00:05:39.060 --> 00:05:41.710 align:start position:0%
this separately for J equals 0 you know
up<00:05:39.419><c> to</c><00:05:39.780><c> and</c><00:05:40.080><c> within</c><00:05:40.440><c> this</c><00:05:40.680><c> number</c><00:05:41.070><c> of</c><00:05:41.130><c> inches</c>

00:05:41.710 --> 00:05:41.720 align:start position:0%
up to and within this number of inches
 

00:05:41.720 --> 00:05:44.240 align:start position:0%
up to and within this number of inches
so<00:05:42.720><c> sorry</c><00:05:43.200><c> about</c><00:05:43.380><c> breaking</c><00:05:43.560><c> this</c><00:05:43.830><c> equation</c>

00:05:44.240 --> 00:05:44.250 align:start position:0%
so sorry about breaking this equation
 

00:05:44.250 --> 00:05:45.890 align:start position:0%
so sorry about breaking this equation
into<00:05:44.490><c> multiple</c><00:05:44.850><c> lines</c><00:05:45.000><c> but</c><00:05:45.210><c> hope</c><00:05:45.330><c> is</c><00:05:45.450><c> clear</c><00:05:45.660><c> so</c>

00:05:45.890 --> 00:05:45.900 align:start position:0%
into multiple lines but hope is clear so
 

00:05:45.900 --> 00:05:50.480 align:start position:0%
into multiple lines but hope is clear so
what<00:05:46.500><c> this</c><00:05:46.740><c> is</c><00:05:47.039><c> what</c><00:05:47.880><c> this</c><00:05:48.000><c> equation</c><00:05:48.180><c> is</c><00:05:49.490><c> what</c>

00:05:50.480 --> 00:05:50.490 align:start position:0%
what this is what this equation is what
 

00:05:50.490 --> 00:05:54.050 align:start position:0%
what this is what this equation is what
it's<00:05:51.150><c> doing</c><00:05:51.240><c> is</c><00:05:51.690><c> exactly</c><00:05:52.340><c> the</c><00:05:53.340><c> same</c><00:05:53.550><c> as</c><00:05:53.789><c> that</c>

00:05:54.050 --> 00:05:54.060 align:start position:0%
it's doing is exactly the same as that
 

00:05:54.060 --> 00:05:55.969 align:start position:0%
it's doing is exactly the same as that
when<00:05:54.630><c> you</c><00:05:54.720><c> have</c><00:05:54.900><c> a</c><00:05:54.930><c> centralized</c><00:05:55.620><c> master</c>

00:05:55.969 --> 00:05:55.979 align:start position:0%
when you have a centralized master
 

00:05:55.979 --> 00:05:58.310 align:start position:0%
when you have a centralized master
server<00:05:56.190><c> that</c><00:05:56.820><c> takes</c><00:05:56.880><c> the</c><00:05:57.240><c> results</c><00:05:57.660><c> at</c><00:05:57.780><c> 10</c><00:05:57.960><c> 1</c><00:05:58.110><c> J</c>

00:05:58.310 --> 00:05:58.320 align:start position:0%
server that takes the results at 10 1 J
 

00:05:58.320 --> 00:06:01.310 align:start position:0%
server that takes the results at 10 1 J
10<00:05:58.620><c> 2</c><00:05:58.800><c> J</c><00:05:58.979><c> 10</c><00:05:59.220><c> 3</c><00:05:59.430><c> J</c><00:05:59.610><c> 10</c><00:05:59.820><c> 4</c><00:06:00.030><c> J</c><00:06:00.210><c> and</c><00:06:00.390><c> has</c><00:06:00.780><c> them</c><00:06:00.990><c> up</c><00:06:01.110><c> and</c>

00:06:01.310 --> 00:06:01.320 align:start position:0%
10 2 J 10 3 J 10 4 J and has them up and
 

00:06:01.320 --> 00:06:04.909 align:start position:0%
10 2 J 10 3 J 10 4 J and has them up and
so<00:06:02.280><c> of</c><00:06:02.550><c> course</c><00:06:02.760><c> the</c><00:06:03.479><c> sum</c><00:06:03.840><c> of</c><00:06:04.110><c> these</c><00:06:04.410><c> four</c>

00:06:04.909 --> 00:06:04.919 align:start position:0%
so of course the sum of these four
 

00:06:04.919 --> 00:06:08.240 align:start position:0%
so of course the sum of these four
things<00:06:05.510><c> right</c><00:06:06.510><c> that's</c><00:06:06.750><c> just</c><00:06:06.870><c> the</c><00:06:07.260><c> sum</c><00:06:07.590><c> of</c><00:06:07.620><c> this</c>

00:06:08.240 --> 00:06:08.250 align:start position:0%
things right that's just the sum of this
 

00:06:08.250 --> 00:06:11.900 align:start position:0%
things right that's just the sum of this
plus<00:06:08.550><c> the</c><00:06:09.360><c> sum</c><00:06:09.750><c> of</c><00:06:09.780><c> this</c><00:06:10.430><c> versus</c><00:06:11.430><c> some</c><00:06:11.610><c> of</c><00:06:11.640><c> this</c>

00:06:11.900 --> 00:06:11.910 align:start position:0%
plus the sum of this versus some of this
 

00:06:11.910 --> 00:06:13.010 align:start position:0%
plus the sum of this versus some of this
let's

00:06:13.010 --> 00:06:13.020 align:start position:0%
let's
 

00:06:13.020 --> 00:06:14.960 align:start position:0%
let's
some<00:06:13.139><c> of</c><00:06:13.259><c> that</c><00:06:13.470><c> and</c><00:06:13.860><c> those</c><00:06:14.250><c> four</c><00:06:14.460><c> things</c><00:06:14.639><c> just</c>

00:06:14.960 --> 00:06:14.970 align:start position:0%
some of that and those four things just
 

00:06:14.970 --> 00:06:17.900 align:start position:0%
some of that and those four things just
cut<00:06:15.210><c> up</c><00:06:15.389><c> to</c><00:06:15.960><c> be</c><00:06:16.080><c> equal</c><00:06:16.410><c> to</c><00:06:16.590><c> this</c><00:06:16.979><c> some</c><00:06:17.699><c> that</c>

00:06:17.900 --> 00:06:17.910 align:start position:0%
cut up to be equal to this some that
 

00:06:17.910 --> 00:06:19.219 align:start position:0%
cut up to be equal to this some that
were<00:06:18.030><c> originally</c><00:06:18.479><c> computing</c><00:06:18.990><c> a</c><00:06:19.050><c> batch</c>

00:06:19.219 --> 00:06:19.229 align:start position:0%
were originally computing a batch
 

00:06:19.229 --> 00:06:21.230 align:start position:0%
were originally computing a batch
gradient<00:06:19.440><c> descent</c><00:06:19.680><c> and</c><00:06:20.160><c> then</c><00:06:20.789><c> we</c><00:06:20.910><c> have</c><00:06:21.090><c> the</c>

00:06:21.230 --> 00:06:21.240 align:start position:0%
gradient descent and then we have the
 

00:06:21.240 --> 00:06:23.809 align:start position:0%
gradient descent and then we have the
alpha<00:06:21.599><c> times</c><00:06:21.840><c> when</c><00:06:22.020><c> the</c><00:06:22.110><c> 400</c><00:06:22.620><c> alpha</c><00:06:22.979><c> times</c><00:06:23.159><c> 100</c>

00:06:23.809 --> 00:06:23.819 align:start position:0%
alpha times when the 400 alpha times 100
 

00:06:23.819 --> 00:06:26.869 align:start position:0%
alpha times when the 400 alpha times 100
and<00:06:23.940><c> thus</c><00:06:24.569><c> distance</c><00:06:25.050><c> exactly</c><00:06:25.650><c> equivalent</c><00:06:26.550><c> to</c>

00:06:26.869 --> 00:06:26.879 align:start position:0%
and thus distance exactly equivalent to
 

00:06:26.879 --> 00:06:28.580 align:start position:0%
and thus distance exactly equivalent to
the<00:06:27.180><c> batch</c><00:06:27.330><c> gradient</c><00:06:27.690><c> descent</c><00:06:27.960><c> algorithm</c>

00:06:28.580 --> 00:06:28.590 align:start position:0%
the batch gradient descent algorithm
 

00:06:28.590 --> 00:06:31.670 align:start position:0%
the batch gradient descent algorithm
only<00:06:29.490><c> instead</c><00:06:30.270><c> of</c><00:06:30.419><c> needing</c><00:06:30.750><c> to</c><00:06:30.930><c> sum</c><00:06:31.229><c> over</c><00:06:31.470><c> all</c>

00:06:31.670 --> 00:06:31.680 align:start position:0%
only instead of needing to sum over all
 

00:06:31.680 --> 00:06:34.070 align:start position:0%
only instead of needing to sum over all
400<00:06:32.550><c> training</c><00:06:32.879><c> examples</c><00:06:33.389><c> on</c><00:06:33.629><c> just</c><00:06:33.930><c> one</c>

00:06:34.070 --> 00:06:34.080 align:start position:0%
400 training examples on just one
 

00:06:34.080 --> 00:06:36.409 align:start position:0%
400 training examples on just one
machine<00:06:34.259><c> we</c><00:06:35.159><c> can</c><00:06:35.310><c> instead</c><00:06:35.639><c> divide</c><00:06:36.120><c> up</c><00:06:36.300><c> the</c>

00:06:36.409 --> 00:06:36.419 align:start position:0%
machine we can instead divide up the
 

00:06:36.419 --> 00:06:39.980 align:start position:0%
machine we can instead divide up the
workload<00:06:36.659><c> on</c><00:06:37.110><c> four</c><00:06:37.470><c> machines</c><00:06:38.270><c> so</c><00:06:39.270><c> here's</c><00:06:39.539><c> what</c>

00:06:39.980 --> 00:06:39.990 align:start position:0%
workload on four machines so here's what
 

00:06:39.990 --> 00:06:42.200 align:start position:0%
workload on four machines so here's what
the<00:06:40.139><c> general</c><00:06:40.650><c> picture</c><00:06:40.860><c> of</c><00:06:41.039><c> the</c><00:06:41.310><c> MapReduce</c>

00:06:42.200 --> 00:06:42.210 align:start position:0%
the general picture of the MapReduce
 

00:06:42.210 --> 00:06:45.649 align:start position:0%
the general picture of the MapReduce
technique<00:06:43.110><c> feels</c><00:06:43.379><c> like</c><00:06:44.180><c> we</c><00:06:45.180><c> have</c><00:06:45.479><c> some</c>

00:06:45.649 --> 00:06:45.659 align:start position:0%
technique feels like we have some
 

00:06:45.659 --> 00:06:47.659 align:start position:0%
technique feels like we have some
training<00:06:45.900><c> sets</c><00:06:46.289><c> and</c><00:06:46.560><c> if</c><00:06:46.800><c> we</c><00:06:46.919><c> want</c><00:06:47.130><c> to</c><00:06:47.190><c> paralyze</c>

00:06:47.659 --> 00:06:47.669 align:start position:0%
training sets and if we want to paralyze
 

00:06:47.669 --> 00:06:49.370 align:start position:0%
training sets and if we want to paralyze
across<00:06:48.090><c> four</c><00:06:48.419><c> machines</c><00:06:48.840><c> willing</c><00:06:49.080><c> to</c><00:06:49.139><c> take</c><00:06:49.229><c> the</c>

00:06:49.370 --> 00:06:49.380 align:start position:0%
across four machines willing to take the
 

00:06:49.380 --> 00:06:51.320 align:start position:0%
across four machines willing to take the
training<00:06:49.710><c> set</c><00:06:49.919><c> and</c><00:06:50.039><c> split</c><00:06:50.880><c> it</c><00:06:51.060><c> you</c><00:06:51.210><c> know</c>

00:06:51.320 --> 00:06:51.330 align:start position:0%
training set and split it you know
 

00:06:51.330 --> 00:06:53.719 align:start position:0%
training set and split it you know
equally<00:06:52.110><c> or</c><00:06:52.229><c> splitted</c><00:06:52.620><c> as</c><00:06:52.770><c> evenly</c><00:06:53.130><c> as</c><00:06:53.340><c> we</c><00:06:53.520><c> can</c>

00:06:53.719 --> 00:06:53.729 align:start position:0%
equally or splitted as evenly as we can
 

00:06:53.729 --> 00:06:56.270 align:start position:0%
equally or splitted as evenly as we can
into<00:06:54.060><c> four</c><00:06:54.509><c> subsets</c><00:06:55.110><c> then</c><00:06:55.949><c> we're</c><00:06:56.099><c> going</c><00:06:56.250><c> to</c>

00:06:56.270 --> 00:06:56.280 align:start position:0%
into four subsets then we're going to
 

00:06:56.280 --> 00:06:57.890 align:start position:0%
into four subsets then we're going to
take<00:06:56.460><c> the</c><00:06:56.610><c> four</c><00:06:56.819><c> subsets</c><00:06:57.330><c> the</c><00:06:57.449><c> training</c><00:06:57.750><c> data</c>

00:06:57.890 --> 00:06:57.900 align:start position:0%
take the four subsets the training data
 

00:06:57.900 --> 00:06:59.600 align:start position:0%
take the four subsets the training data
and<00:06:58.080><c> send</c><00:06:58.590><c> them</c><00:06:58.770><c> to</c><00:06:58.800><c> four</c><00:06:59.220><c> different</c>

00:06:59.600 --> 00:06:59.610 align:start position:0%
and send them to four different
 

00:06:59.610 --> 00:07:01.879 align:start position:0%
and send them to four different
computers<00:07:00.150><c> and</c><00:07:00.419><c> each</c><00:07:00.870><c> of</c><00:07:01.050><c> the</c><00:07:01.139><c> four</c><00:07:01.319><c> computers</c>

00:07:01.879 --> 00:07:01.889 align:start position:0%
computers and each of the four computers
 

00:07:01.889 --> 00:07:04.399 align:start position:0%
computers and each of the four computers
can<00:07:02.370><c> compute</c><00:07:02.909><c> a</c><00:07:03.000><c> summation</c><00:07:03.330><c> over</c><00:07:03.720><c> just</c><00:07:04.229><c> one</c>

00:07:04.399 --> 00:07:04.409 align:start position:0%
can compute a summation over just one
 

00:07:04.409 --> 00:07:06.379 align:start position:0%
can compute a summation over just one
quarter<00:07:04.860><c> of</c><00:07:04.889><c> the</c><00:07:04.949><c> training</c><00:07:05.310><c> set</c><00:07:05.520><c> and</c><00:07:05.639><c> then</c>

00:07:06.379 --> 00:07:06.389 align:start position:0%
quarter of the training set and then
 

00:07:06.389 --> 00:07:08.330 align:start position:0%
quarter of the training set and then
finally<00:07:06.840><c> it</c><00:07:07.020><c> takes</c><00:07:07.199><c> each</c><00:07:07.650><c> of</c><00:07:07.830><c> the</c><00:07:07.919><c> computers</c>

00:07:08.330 --> 00:07:08.340 align:start position:0%
finally it takes each of the computers
 

00:07:08.340 --> 00:07:10.010 align:start position:0%
finally it takes each of the computers
take<00:07:08.580><c> the</c><00:07:08.699><c> result</c><00:07:09.030><c> sends</c><00:07:09.569><c> them</c><00:07:09.750><c> to</c><00:07:09.990><c> a</c>

00:07:10.010 --> 00:07:10.020 align:start position:0%
take the result sends them to a
 

00:07:10.020 --> 00:07:12.170 align:start position:0%
take the result sends them to a
centralized<00:07:10.620><c> server</c><00:07:10.830><c> which</c><00:07:11.490><c> then</c><00:07:11.520><c> combines</c>

00:07:12.170 --> 00:07:12.180 align:start position:0%
centralized server which then combines
 

00:07:12.180 --> 00:07:14.600 align:start position:0%
centralized server which then combines
the<00:07:12.419><c> results</c><00:07:12.780><c> together</c><00:07:12.900><c> so</c><00:07:13.800><c> on</c><00:07:14.129><c> the</c><00:07:14.370><c> previous</c>

00:07:14.600 --> 00:07:14.610 align:start position:0%
the results together so on the previous
 

00:07:14.610 --> 00:07:16.969 align:start position:0%
the results together so on the previous
slide<00:07:15.060><c> in</c><00:07:15.360><c> that</c><00:07:15.509><c> example</c><00:07:15.930><c> the</c><00:07:16.469><c> bulk</c><00:07:16.770><c> of</c><00:07:16.919><c> the</c>

00:07:16.969 --> 00:07:16.979 align:start position:0%
slide in that example the bulk of the
 

00:07:16.979 --> 00:07:19.070 align:start position:0%
slide in that example the bulk of the
work<00:07:17.190><c> in</c><00:07:17.430><c> gradient</c><00:07:17.789><c> descent</c><00:07:17.940><c> was</c><00:07:18.479><c> computing</c>

00:07:19.070 --> 00:07:19.080 align:start position:0%
work in gradient descent was computing
 

00:07:19.080 --> 00:07:22.219 align:start position:0%
work in gradient descent was computing
the<00:07:19.259><c> sum</c><00:07:19.590><c> from</c><00:07:20.310><c> I</c><00:07:20.580><c> equals</c><00:07:20.940><c> 1</c><00:07:21.030><c> to</c><00:07:21.419><c> 400</c><00:07:22.050><c> or</c>

00:07:22.219 --> 00:07:22.229 align:start position:0%
the sum from I equals 1 to 400 or
 

00:07:22.229 --> 00:07:24.499 align:start position:0%
the sum from I equals 1 to 400 or
something<00:07:22.650><c> so</c><00:07:22.860><c> more</c><00:07:23.039><c> generally</c><00:07:23.069><c> sum</c><00:07:24.060><c> from</c><00:07:24.300><c> I</c>

00:07:24.499 --> 00:07:24.509 align:start position:0%
something so more generally sum from I
 

00:07:24.509 --> 00:07:27.649 align:start position:0%
something so more generally sum from I
equals<00:07:24.900><c> 1</c><00:07:24.990><c> to</c><00:07:25.469><c> M</c><00:07:25.650><c> of</c><00:07:26.039><c> that</c><00:07:26.669><c> you</c><00:07:26.909><c> know</c><00:07:27.030><c> formula</c>

00:07:27.649 --> 00:07:27.659 align:start position:0%
equals 1 to M of that you know formula
 

00:07:27.659 --> 00:07:29.990 align:start position:0%
equals 1 to M of that you know formula
for<00:07:27.900><c> gradient</c><00:07:28.169><c> descent</c><00:07:28.560><c> and</c><00:07:28.830><c> now</c><00:07:29.460><c> because</c>

00:07:29.990 --> 00:07:30.000 align:start position:0%
for gradient descent and now because
 

00:07:30.000 --> 00:07:32.209 align:start position:0%
for gradient descent and now because
each<00:07:30.330><c> of</c><00:07:30.539><c> the</c><00:07:30.690><c> 4</c><00:07:30.930><c> computers</c><00:07:31.259><c> can</c><00:07:31.740><c> do</c><00:07:31.860><c> just</c><00:07:32.130><c> a</c>

00:07:32.209 --> 00:07:32.219 align:start position:0%
each of the 4 computers can do just a
 

00:07:32.219 --> 00:07:34.909 align:start position:0%
each of the 4 computers can do just a
porter<00:07:32.580><c> of</c><00:07:32.610><c> the</c><00:07:32.759><c> work</c><00:07:33.469><c> potentially</c><00:07:34.469><c> you</c><00:07:34.710><c> can</c>

00:07:34.909 --> 00:07:34.919 align:start position:0%
porter of the work potentially you can
 

00:07:34.919 --> 00:07:39.740 align:start position:0%
porter of the work potentially you can
get<00:07:35.130><c> up</c><00:07:35.310><c> to</c><00:07:35.340><c> a</c><00:07:35.550><c> 4x</c><00:07:36.240><c> speed</c><00:07:36.389><c> up</c><00:07:37.880><c> in</c><00:07:38.880><c> particular</c><00:07:39.270><c> if</c>

00:07:39.740 --> 00:07:39.750 align:start position:0%
get up to a 4x speed up in particular if
 

00:07:39.750 --> 00:07:42.230 align:start position:0%
get up to a 4x speed up in particular if
there<00:07:39.990><c> were</c><00:07:40.169><c> no</c><00:07:40.469><c> network</c><00:07:41.250><c> latencies</c><00:07:41.759><c> and</c><00:07:41.819><c> no</c>

00:07:42.230 --> 00:07:42.240 align:start position:0%
there were no network latencies and no
 

00:07:42.240 --> 00:07:44.209 align:start position:0%
there were no network latencies and no
cost<00:07:42.630><c> of</c><00:07:42.900><c> the</c><00:07:43.020><c> network</c><00:07:43.229><c> communication</c><00:07:43.620><c> to</c>

00:07:44.209 --> 00:07:44.219 align:start position:0%
cost of the network communication to
 

00:07:44.219 --> 00:07:45.620 align:start position:0%
cost of the network communication to
send<00:07:44.460><c> the</c><00:07:44.520><c> data</c><00:07:44.639><c> back</c><00:07:44.940><c> and</c><00:07:44.969><c> forth</c><00:07:45.180><c> you</c><00:07:45.479><c> can</c>

00:07:45.620 --> 00:07:45.630 align:start position:0%
send the data back and forth you can
 

00:07:45.630 --> 00:07:48.189 align:start position:0%
send the data back and forth you can
potentially<00:07:46.050><c> get</c><00:07:46.289><c> up</c><00:07:46.529><c> to</c><00:07:46.710><c> a</c><00:07:46.740><c> 4x</c><00:07:47.400><c> speed</c><00:07:47.789><c> up</c><00:07:47.969><c> of</c>

00:07:48.189 --> 00:07:48.199 align:start position:0%
potentially get up to a 4x speed up of
 

00:07:48.199 --> 00:07:51.890 align:start position:0%
potentially get up to a 4x speed up of
course<00:07:49.199><c> in</c><00:07:49.440><c> practice</c><00:07:49.889><c> because</c><00:07:50.279><c> of</c><00:07:50.900><c> network</c>

00:07:51.890 --> 00:07:51.900 align:start position:0%
course in practice because of network
 

00:07:51.900 --> 00:07:54.620 align:start position:0%
course in practice because of network
latency<00:07:52.440><c> is</c><00:07:52.500><c> the</c><00:07:52.860><c> overhead</c><00:07:53.430><c> of</c><00:07:53.819><c> combining</c><00:07:54.419><c> the</c>

00:07:54.620 --> 00:07:54.630 align:start position:0%
latency is the overhead of combining the
 

00:07:54.630 --> 00:07:56.629 align:start position:0%
latency is the overhead of combining the
results<00:07:54.750><c> afterwards</c><00:07:55.500><c> and</c><00:07:55.740><c> other</c><00:07:55.860><c> factors</c><00:07:56.279><c> in</c>

00:07:56.629 --> 00:07:56.639 align:start position:0%
results afterwards and other factors in
 

00:07:56.639 --> 00:07:58.430 align:start position:0%
results afterwards and other factors in
practice<00:07:57.210><c> you</c><00:07:57.419><c> get</c><00:07:57.599><c> a</c><00:07:57.659><c> slightly</c><00:07:58.139><c> less</c><00:07:58.409><c> than</c>

00:07:58.430 --> 00:07:58.440 align:start position:0%
practice you get a slightly less than
 

00:07:58.440 --> 00:08:01.189 align:start position:0%
practice you get a slightly less than
the<00:07:58.710><c> 4x</c><00:07:59.069><c> speed</c><00:07:59.400><c> up</c><00:07:59.580><c> but</c><00:08:00.270><c> nonetheless</c><00:08:00.690><c> this</c>

00:08:01.189 --> 00:08:01.199 align:start position:0%
the 4x speed up but nonetheless this
 

00:08:01.199 --> 00:08:03.080 align:start position:0%
the 4x speed up but nonetheless this
sort<00:08:01.440><c> of</c><00:08:01.529><c> MapReduce</c><00:08:01.860><c> approach</c><00:08:02.370><c> does</c><00:08:02.669><c> offer</c><00:08:02.940><c> us</c>

00:08:03.080 --> 00:08:03.090 align:start position:0%
sort of MapReduce approach does offer us
 

00:08:03.090 --> 00:08:05.689 align:start position:0%
sort of MapReduce approach does offer us
a<00:08:03.479><c> way</c><00:08:03.659><c> to</c><00:08:03.690><c> process</c><00:08:04.409><c> much</c><00:08:04.740><c> larger</c><00:08:05.219><c> data</c><00:08:05.340><c> sets</c>

00:08:05.689 --> 00:08:05.699 align:start position:0%
a way to process much larger data sets
 

00:08:05.699 --> 00:08:08.159 align:start position:0%
a way to process much larger data sets
than<00:08:05.789><c> is</c><00:08:06.060><c> possible</c><00:08:06.240><c> using</c><00:08:07.199><c> a</c><00:08:07.289><c> single</c><00:08:07.620><c> computer</c>

00:08:08.159 --> 00:08:08.169 align:start position:0%
than is possible using a single computer
 

00:08:08.169 --> 00:08:10.369 align:start position:0%
than is possible using a single computer
if<00:08:08.889><c> you</c><00:08:09.069><c> are</c><00:08:09.129><c> thinking</c><00:08:09.430><c> of</c><00:08:09.610><c> applying</c>

00:08:10.369 --> 00:08:10.379 align:start position:0%
if you are thinking of applying
 

00:08:10.379 --> 00:08:13.439 align:start position:0%
if you are thinking of applying
MapReduce<00:08:11.379><c> to</c><00:08:11.740><c> some</c><00:08:12.099><c> learning</c><00:08:12.669><c> algorithm</c><00:08:13.150><c> in</c>

00:08:13.439 --> 00:08:13.449 align:start position:0%
MapReduce to some learning algorithm in
 

00:08:13.449 --> 00:08:15.809 align:start position:0%
MapReduce to some learning algorithm in
order<00:08:13.599><c> to</c><00:08:13.870><c> speed</c><00:08:14.110><c> it</c><00:08:14.259><c> up</c><00:08:14.409><c> by</c><00:08:14.889><c> parallelizing</c>

00:08:15.809 --> 00:08:15.819 align:start position:0%
order to speed it up by parallelizing
 

00:08:15.819 --> 00:08:17.850 align:start position:0%
order to speed it up by parallelizing
the<00:08:15.909><c> computation</c><00:08:16.840><c> over</c><00:08:17.139><c> different</c><00:08:17.289><c> computers</c>

00:08:17.850 --> 00:08:17.860 align:start position:0%
the computation over different computers
 

00:08:17.860 --> 00:08:20.969 align:start position:0%
the computation over different computers
the<00:08:18.490><c> key</c><00:08:18.699><c> question</c><00:08:19.150><c> to</c><00:08:19.180><c> ask</c><00:08:19.419><c> yourself</c><00:08:19.599><c> is</c><00:08:20.050><c> can</c>

00:08:20.969 --> 00:08:20.979 align:start position:0%
the key question to ask yourself is can
 

00:08:20.979 --> 00:08:22.890 align:start position:0%
the key question to ask yourself is can
your<00:08:21.129><c> learning</c><00:08:21.490><c> algorithm</c><00:08:21.969><c> be</c><00:08:22.180><c> expressed</c><00:08:22.689><c> as</c>

00:08:22.890 --> 00:08:22.900 align:start position:0%
your learning algorithm be expressed as
 

00:08:22.900 --> 00:08:25.830 align:start position:0%
your learning algorithm be expressed as
a<00:08:22.960><c> summation</c><00:08:23.590><c> over</c><00:08:24.039><c> the</c><00:08:24.370><c> training</c><00:08:24.819><c> set</c><00:08:25.060><c> and</c><00:08:25.330><c> it</c>

00:08:25.830 --> 00:08:25.840 align:start position:0%
a summation over the training set and it
 

00:08:25.840 --> 00:08:27.360 align:start position:0%
a summation over the training set and it
turns<00:08:26.020><c> out</c><00:08:26.199><c> that</c><00:08:26.409><c> many</c><00:08:26.560><c> learning</c><00:08:26.889><c> algorithms</c>

00:08:27.360 --> 00:08:27.370 align:start position:0%
turns out that many learning algorithms
 

00:08:27.370 --> 00:08:29.369 align:start position:0%
turns out that many learning algorithms
can<00:08:27.610><c> actually</c><00:08:27.999><c> be</c><00:08:28.120><c> expressed</c><00:08:28.449><c> as</c><00:08:28.779><c> computing</c>

00:08:29.369 --> 00:08:29.379 align:start position:0%
can actually be expressed as computing
 

00:08:29.379 --> 00:08:31.290 align:start position:0%
can actually be expressed as computing
sums<00:08:29.710><c> of</c><00:08:29.979><c> functions</c><00:08:30.430><c> over</c><00:08:30.550><c> the</c><00:08:30.759><c> training</c><00:08:31.090><c> set</c>

00:08:31.290 --> 00:08:31.300 align:start position:0%
sums of functions over the training set
 

00:08:31.300 --> 00:08:34.139 align:start position:0%
sums of functions over the training set
and<00:08:31.709><c> the</c><00:08:32.709><c> computational</c><00:08:33.430><c> expense</c><00:08:33.789><c> of</c><00:08:34.029><c> running</c>

00:08:34.139 --> 00:08:34.149 align:start position:0%
and the computational expense of running
 

00:08:34.149 --> 00:08:36.089 align:start position:0%
and the computational expense of running
them<00:08:34.419><c> on</c><00:08:34.599><c> large</c><00:08:34.839><c> data</c><00:08:35.110><c> sets</c><00:08:35.469><c> is</c><00:08:35.620><c> because</c><00:08:35.919><c> they</c>

00:08:36.089 --> 00:08:36.099 align:start position:0%
them on large data sets is because they
 

00:08:36.099 --> 00:08:37.949 align:start position:0%
them on large data sets is because they
need<00:08:36.279><c> to</c><00:08:36.339><c> sum</c><00:08:36.669><c> over</c><00:08:36.880><c> a</c><00:08:37.089><c> very</c><00:08:37.390><c> large</c><00:08:37.690><c> training</c>

00:08:37.949 --> 00:08:37.959 align:start position:0%
need to sum over a very large training
 

00:08:37.959 --> 00:08:40.230 align:start position:0%
need to sum over a very large training
set<00:08:38.289><c> so</c><00:08:38.800><c> whenever</c><00:08:39.370><c> your</c><00:08:39.519><c> learning</c><00:08:39.849><c> algorithm</c>

00:08:40.230 --> 00:08:40.240 align:start position:0%
set so whenever your learning algorithm
 

00:08:40.240 --> 00:08:42.120 align:start position:0%
set so whenever your learning algorithm
can<00:08:40.269><c> be</c><00:08:40.449><c> expressed</c><00:08:40.930><c> as</c><00:08:41.229><c> a</c><00:08:41.469><c> sum</c><00:08:41.500><c> over</c><00:08:41.979><c> the</c>

00:08:42.120 --> 00:08:42.130 align:start position:0%
can be expressed as a sum over the
 

00:08:42.130 --> 00:08:43.889 align:start position:0%
can be expressed as a sum over the
training<00:08:42.190><c> set</c><00:08:42.640><c> whenever</c><00:08:43.060><c> the</c><00:08:43.209><c> bulk</c><00:08:43.570><c> of</c><00:08:43.750><c> the</c>

00:08:43.889 --> 00:08:43.899 align:start position:0%
training set whenever the bulk of the
 

00:08:43.899 --> 00:08:45.449 align:start position:0%
training set whenever the bulk of the
work<00:08:44.110><c> of</c><00:08:44.410><c> a</c><00:08:44.500><c> learning</c><00:08:44.829><c> algorithm</c><00:08:45.070><c> can</c><00:08:45.279><c> be</c>

00:08:45.449 --> 00:08:45.459 align:start position:0%
work of a learning algorithm can be
 

00:08:45.459 --> 00:08:47.040 align:start position:0%
work of a learning algorithm can be
expressed<00:08:45.880><c> as</c><00:08:46.000><c> a</c><00:08:46.029><c> sum</c><00:08:46.149><c> of</c><00:08:46.360><c> the</c><00:08:46.540><c> training</c><00:08:46.870><c> set</c>

00:08:47.040 --> 00:08:47.050 align:start position:0%
expressed as a sum of the training set
 

00:08:47.050 --> 00:08:49.019 align:start position:0%
expressed as a sum of the training set
then<00:08:47.740><c> map</c><00:08:48.010><c> reviews</c><00:08:48.339><c> might</c><00:08:48.610><c> be</c><00:08:48.760><c> a</c><00:08:48.790><c> good</c>

00:08:49.019 --> 00:08:49.029 align:start position:0%
then map reviews might be a good
 

00:08:49.029 --> 00:08:51.389 align:start position:0%
then map reviews might be a good
candidate<00:08:49.630><c> for</c><00:08:49.860><c> scaling</c><00:08:50.860><c> your</c><00:08:50.949><c> learning</c>

00:08:51.389 --> 00:08:51.399 align:start position:0%
candidate for scaling your learning
 

00:08:51.399 --> 00:08:53.280 align:start position:0%
candidate for scaling your learning
algorithms<00:08:51.850><c> to</c><00:08:51.880><c> very</c><00:08:52.269><c> very</c><00:08:52.540><c> big</c><00:08:52.779><c> data</c><00:08:52.959><c> sets</c>

00:08:53.280 --> 00:08:53.290 align:start position:0%
algorithms to very very big data sets
 

00:08:53.290 --> 00:08:56.189 align:start position:0%
algorithms to very very big data sets
let's<00:08:54.040><c> just</c><00:08:54.250><c> look</c><00:08:54.339><c> at</c><00:08:54.490><c> one</c><00:08:54.730><c> more</c><00:08:54.910><c> example</c><00:08:55.199><c> last</c>

00:08:56.189 --> 00:08:56.199 align:start position:0%
let's just look at one more example last
 

00:08:56.199 --> 00:08:57.449 align:start position:0%
let's just look at one more example last
thing<00:08:56.350><c> that</c><00:08:56.380><c> we</c><00:08:56.560><c> want</c><00:08:56.769><c> to</c><00:08:56.800><c> use</c><00:08:57.010><c> one</c><00:08:57.250><c> of</c><00:08:57.279><c> the</c>

00:08:57.449 --> 00:08:57.459 align:start position:0%
thing that we want to use one of the
 

00:08:57.459 --> 00:08:59.100 align:start position:0%
thing that we want to use one of the
advanced<00:08:58.000><c> optimization</c><00:08:58.029><c> algorithms</c><00:08:59.019><c> so</c>

00:08:59.100 --> 00:08:59.110 align:start position:0%
advanced optimization algorithms so
 

00:08:59.110 --> 00:09:00.930 align:start position:0%
advanced optimization algorithms so
things<00:08:59.290><c> on</c><00:08:59.440><c> your</c><00:08:59.620><c> l-bfgs</c><00:08:59.949><c> conjugate</c>

00:09:00.930 --> 00:09:00.940 align:start position:0%
things on your l-bfgs conjugate
 

00:09:00.940 --> 00:09:03.180 align:start position:0%
things on your l-bfgs conjugate
gradients<00:09:01.360><c> and</c><00:09:01.480><c> so</c><00:09:01.630><c> on</c><00:09:01.810><c> and</c><00:09:02.050><c> let's</c><00:09:03.010><c> say</c><00:09:03.070><c> we</c>

00:09:03.180 --> 00:09:03.190 align:start position:0%
gradients and so on and let's say we
 

00:09:03.190 --> 00:09:04.740 align:start position:0%
gradients and so on and let's say we
want<00:09:03.339><c> to</c><00:09:03.370><c> train</c><00:09:03.670><c> in</c><00:09:03.940><c> logistic</c><00:09:04.329><c> regression</c>

00:09:04.740 --> 00:09:04.750 align:start position:0%
want to train in logistic regression
 

00:09:04.750 --> 00:09:07.319 align:start position:0%
want to train in logistic regression
learning<00:09:05.019><c> algorithm</c><00:09:05.440><c> for</c><00:09:06.250><c> that</c><00:09:06.490><c> we</c><00:09:06.970><c> need</c><00:09:07.149><c> to</c>

00:09:07.319 --> 00:09:07.329 align:start position:0%
learning algorithm for that we need to
 

00:09:07.329 --> 00:09:10.139 align:start position:0%
learning algorithm for that we need to
compute<00:09:07.779><c> two</c><00:09:08.290><c> main</c><00:09:08.560><c> quantities</c><00:09:09.130><c> one</c><00:09:09.490><c> is</c><00:09:09.519><c> for</c>

00:09:10.139 --> 00:09:10.149 align:start position:0%
compute two main quantities one is for
 

00:09:10.149 --> 00:09:11.759 align:start position:0%
compute two main quantities one is for
the<00:09:10.269><c> advanced</c><00:09:10.690><c> optimization</c><00:09:10.959><c> algorithms</c>

00:09:11.759 --> 00:09:11.769 align:start position:0%
the advanced optimization algorithms
 

00:09:11.769 --> 00:09:13.530 align:start position:0%
the advanced optimization algorithms
like<00:09:11.949><c> you</c><00:09:12.130><c> know</c><00:09:12.190><c> LPF</c><00:09:12.940><c> GS</c><00:09:13.149><c> and</c><00:09:13.300><c> conjugate</c>

00:09:13.530 --> 00:09:13.540 align:start position:0%
like you know LPF GS and conjugate
 

00:09:13.540 --> 00:09:15.720 align:start position:0%
like you know LPF GS and conjugate
gradient<00:09:13.899><c> we</c><00:09:14.410><c> need</c><00:09:14.560><c> to</c><00:09:14.709><c> provide</c><00:09:15.010><c> it</c><00:09:15.190><c> a</c><00:09:15.279><c> routine</c>

00:09:15.720 --> 00:09:15.730 align:start position:0%
gradient we need to provide it a routine
 

00:09:15.730 --> 00:09:18.420 align:start position:0%
gradient we need to provide it a routine
to<00:09:16.240><c> compute</c><00:09:16.720><c> the</c><00:09:17.050><c> cost</c><00:09:17.649><c> function</c><00:09:17.829><c> of</c><00:09:18.220><c> the</c>

00:09:18.420 --> 00:09:18.430 align:start position:0%
to compute the cost function of the
 

00:09:18.430 --> 00:09:20.819 align:start position:0%
to compute the cost function of the
optimization<00:09:18.640><c> objective</c><00:09:19.630><c> and</c><00:09:19.839><c> so</c><00:09:20.589><c> for</c>

00:09:20.819 --> 00:09:20.829 align:start position:0%
optimization objective and so for
 

00:09:20.829 --> 00:09:21.569 align:start position:0%
optimization objective and so for
logistic<00:09:21.190><c> regression</c>

00:09:21.569 --> 00:09:21.579 align:start position:0%
logistic regression
 

00:09:21.579 --> 00:09:23.730 align:start position:0%
logistic regression
you<00:09:21.820><c> remember</c><00:09:22.300><c> that</c><00:09:22.390><c> the</c><00:09:22.660><c> cost</c><00:09:22.870><c> function</c><00:09:22.959><c> has</c>

00:09:23.730 --> 00:09:23.740 align:start position:0%
you remember that the cost function has
 

00:09:23.740 --> 00:09:25.590 align:start position:0%
you remember that the cost function has
this<00:09:24.010><c> sort</c><00:09:24.310><c> of</c><00:09:24.430><c> sum</c><00:09:24.670><c> over</c><00:09:25.000><c> the</c><00:09:25.089><c> training</c><00:09:25.240><c> set</c>

00:09:25.590 --> 00:09:25.600 align:start position:0%
this sort of sum over the training set
 

00:09:25.600 --> 00:09:28.380 align:start position:0%
this sort of sum over the training set
again<00:09:25.870><c> and</c><00:09:26.199><c> so</c><00:09:26.560><c> if</c><00:09:27.040><c> you're</c><00:09:27.279><c> paralyzing</c><00:09:27.940><c> over</c>

00:09:28.380 --> 00:09:28.390 align:start position:0%
again and so if you're paralyzing over
 

00:09:28.390 --> 00:09:30.600 align:start position:0%
again and so if you're paralyzing over
ten<00:09:29.260><c> machines</c><00:09:29.620><c> you</c><00:09:29.890><c> would</c><00:09:30.040><c> split</c><00:09:30.339><c> up</c><00:09:30.490><c> the</c>

00:09:30.600 --> 00:09:30.610 align:start position:0%
ten machines you would split up the
 

00:09:30.610 --> 00:09:33.030 align:start position:0%
ten machines you would split up the
training<00:09:31.089><c> set</c><00:09:31.269><c> onto</c><00:09:31.570><c> ten</c><00:09:31.930><c> machines</c><00:09:32.050><c> and</c><00:09:32.800><c> have</c>

00:09:33.030 --> 00:09:33.040 align:start position:0%
training set onto ten machines and have
 

00:09:33.040 --> 00:09:35.699 align:start position:0%
training set onto ten machines and have
each<00:09:33.279><c> of</c><00:09:33.459><c> the</c><00:09:33.550><c> ten</c><00:09:33.760><c> machines</c><00:09:33.880><c> compute</c><00:09:34.660><c> the</c><00:09:35.410><c> sum</c>

00:09:35.699 --> 00:09:35.709 align:start position:0%
each of the ten machines compute the sum
 

00:09:35.709 --> 00:09:38.550 align:start position:0%
each of the ten machines compute the sum
of<00:09:35.740><c> this</c><00:09:36.250><c> quantity</c><00:09:36.820><c> over</c><00:09:37.480><c> just</c><00:09:37.720><c> one</c><00:09:37.930><c> tenth</c><00:09:38.290><c> of</c>

00:09:38.550 --> 00:09:38.560 align:start position:0%
of this quantity over just one tenth of
 

00:09:38.560 --> 00:09:41.189 align:start position:0%
of this quantity over just one tenth of
the<00:09:38.620><c> training</c><00:09:39.040><c> data</c><00:09:39.570><c> then</c><00:09:40.570><c> the</c><00:09:40.810><c> other</c><00:09:40.930><c> thing</c>

00:09:41.189 --> 00:09:41.199 align:start position:0%
the training data then the other thing
 

00:09:41.199 --> 00:09:42.960 align:start position:0%
the training data then the other thing
that<00:09:41.230><c> the</c><00:09:41.800><c> advanced</c><00:09:42.339><c> optimization</c>

00:09:42.960 --> 00:09:42.970 align:start position:0%
that the advanced optimization
 

00:09:42.970 --> 00:09:45.210 align:start position:0%
that the advanced optimization
algorithms<00:09:43.329><c> need</c><00:09:43.480><c> is</c><00:09:43.750><c> routine</c><00:09:44.680><c> to</c><00:09:44.709><c> compute</c>

00:09:45.210 --> 00:09:45.220 align:start position:0%
algorithms need is routine to compute
 

00:09:45.220 --> 00:09:47.460 align:start position:0%
algorithms need is routine to compute
these<00:09:45.310><c> partial</c><00:09:45.370><c> derivative</c><00:09:46.329><c> terms</c><00:09:46.630><c> and</c><00:09:46.899><c> once</c>

00:09:47.460 --> 00:09:47.470 align:start position:0%
these partial derivative terms and once
 

00:09:47.470 --> 00:09:49.079 align:start position:0%
these partial derivative terms and once
again<00:09:47.709><c> these</c><00:09:47.949><c> derivative</c><00:09:48.550><c> terms</c><00:09:48.850><c> for</c>

00:09:49.079 --> 00:09:49.089 align:start position:0%
again these derivative terms for
 

00:09:49.089 --> 00:09:51.210 align:start position:0%
again these derivative terms for
logistic<00:09:49.480><c> regression</c><00:09:49.540><c> can</c><00:09:50.529><c> be</c><00:09:50.589><c> expressed</c><00:09:51.070><c> as</c>

00:09:51.210 --> 00:09:51.220 align:start position:0%
logistic regression can be expressed as
 

00:09:51.220 --> 00:09:53.340 align:start position:0%
logistic regression can be expressed as
a<00:09:51.430><c> sum</c><00:09:51.459><c> over</c><00:09:51.940><c> the</c><00:09:52.089><c> training</c><00:09:52.180><c> set</c><00:09:52.660><c> and</c><00:09:52.690><c> so</c><00:09:53.079><c> once</c>

00:09:53.340 --> 00:09:53.350 align:start position:0%
a sum over the training set and so once
 

00:09:53.350 --> 00:09:55.920 align:start position:0%
a sum over the training set and so once
again<00:09:53.470><c> similar</c><00:09:54.160><c> to</c><00:09:54.190><c> our</c><00:09:54.459><c> earlier</c><00:09:54.790><c> example</c><00:09:55.300><c> you</c>

00:09:55.920 --> 00:09:55.930 align:start position:0%
again similar to our earlier example you
 

00:09:55.930 --> 00:09:57.810 align:start position:0%
again similar to our earlier example you
would<00:09:56.110><c> have</c><00:09:56.350><c> each</c><00:09:56.620><c> machine</c><00:09:56.890><c> compute</c><00:09:57.640><c> that</c>

00:09:57.810 --> 00:09:57.820 align:start position:0%
would have each machine compute that
 

00:09:57.820 --> 00:10:00.090 align:start position:0%
would have each machine compute that
summation<00:09:58.240><c> over</c><00:09:58.839><c> just</c><00:09:59.290><c> some</c><00:09:59.470><c> small</c><00:09:59.920><c> fraction</c>

00:10:00.090 --> 00:10:00.100 align:start position:0%
summation over just some small fraction
 

00:10:00.100 --> 00:10:04.530 align:start position:0%
summation over just some small fraction
of<00:10:00.459><c> your</c><00:10:00.880><c> training</c><00:10:01.240><c> data</c><00:10:01.420><c> and</c><00:10:02.459><c> finally</c><00:10:03.540><c> having</c>

00:10:04.530 --> 00:10:04.540 align:start position:0%
of your training data and finally having
 

00:10:04.540 --> 00:10:06.420 align:start position:0%
of your training data and finally having
computed<00:10:05.079><c> all</c><00:10:05.230><c> of</c><00:10:05.380><c> these</c><00:10:05.529><c> things</c><00:10:05.829><c> they</c><00:10:06.250><c> could</c>

00:10:06.420 --> 00:10:06.430 align:start position:0%
computed all of these things they could
 

00:10:06.430 --> 00:10:08.430 align:start position:0%
computed all of these things they could
then<00:10:06.579><c> send</c><00:10:06.850><c> their</c><00:10:07.060><c> results</c><00:10:07.480><c> to</c><00:10:07.690><c> a</c><00:10:07.720><c> centralized</c>

00:10:08.430 --> 00:10:08.440 align:start position:0%
then send their results to a centralized
 

00:10:08.440 --> 00:10:11.400 align:start position:0%
then send their results to a centralized
server<00:10:08.680><c> which</c><00:10:09.370><c> can</c><00:10:09.610><c> then</c><00:10:09.820><c> add</c><00:10:10.180><c> up</c><00:10:10.360><c> the</c><00:10:10.660><c> local</c>

00:10:11.400 --> 00:10:11.410 align:start position:0%
server which can then add up the local
 

00:10:11.410 --> 00:10:11.790 align:start position:0%
server which can then add up the local
very

00:10:11.790 --> 00:10:11.800 align:start position:0%
very
 

00:10:11.800 --> 00:10:13.949 align:start position:0%
very
add<00:10:11.860><c> up</c><00:10:12.160><c> the</c><00:10:12.310><c> partial</c><00:10:12.790><c> sums</c><00:10:13.060><c> this</c><00:10:13.449><c> corresponds</c>

00:10:13.949 --> 00:10:13.959 align:start position:0%
add up the partial sums this corresponds
 

00:10:13.959 --> 00:10:14.740 align:start position:0%
add up the partial sums this corresponds
to<00:10:14.050><c> adding</c><00:10:14.260><c> up</c>

00:10:14.740 --> 00:10:14.750 align:start position:0%
to adding up
 

00:10:14.750 --> 00:10:21.730 align:start position:0%
to adding up
you<00:10:14.930><c> know</c><00:10:15.019><c> temp</c><00:10:15.740><c> I</c><00:10:15.980><c> or</c><00:10:16.850><c> the</c><00:10:17.000><c> temp</c><00:10:18.790><c> IJ</c><00:10:20.740><c> variables</c>

00:10:21.730 --> 00:10:21.740 align:start position:0%
you know temp I or the temp IJ variables
 

00:10:21.740 --> 00:10:23.949 align:start position:0%
you know temp I or the temp IJ variables
which<00:10:22.040><c> were</c><00:10:22.189><c> computer</c><00:10:22.730><c> locally</c><00:10:23.269><c> on</c><00:10:23.509><c> machine</c>

00:10:23.949 --> 00:10:23.959 align:start position:0%
which were computer locally on machine
 

00:10:23.959 --> 00:10:26.230 align:start position:0%
which were computer locally on machine
number<00:10:24.139><c> I</c><00:10:24.439><c> and</c><00:10:24.829><c> so</c><00:10:25.519><c> the</c><00:10:25.639><c> centralized</c><00:10:26.060><c> server</c>

00:10:26.230 --> 00:10:26.240 align:start position:0%
number I and so the centralized server
 

00:10:26.240 --> 00:10:28.420 align:start position:0%
number I and so the centralized server
can<00:10:26.839><c> sum</c><00:10:27.079><c> these</c><00:10:27.199><c> things</c><00:10:27.470><c> up</c><00:10:27.620><c> and</c><00:10:27.800><c> get</c><00:10:28.310><c> the</c>

00:10:28.420 --> 00:10:28.430 align:start position:0%
can sum these things up and get the
 

00:10:28.430 --> 00:10:31.210 align:start position:0%
can sum these things up and get the
overall<00:10:29.230><c> cost</c><00:10:30.230><c> function</c><00:10:30.319><c> and</c><00:10:30.949><c> get</c><00:10:31.160><c> the</c>

00:10:31.210 --> 00:10:31.220 align:start position:0%
overall cost function and get the
 

00:10:31.220 --> 00:10:33.819 align:start position:0%
overall cost function and get the
overall<00:10:31.389><c> partial</c><00:10:32.389><c> diversity</c><00:10:32.930><c> which</c><00:10:33.560><c> you</c><00:10:33.680><c> can</c>

00:10:33.819 --> 00:10:33.829 align:start position:0%
overall partial diversity which you can
 

00:10:33.829 --> 00:10:35.019 align:start position:0%
overall partial diversity which you can
then<00:10:34.040><c> pass</c><00:10:34.310><c> through</c><00:10:34.610><c> the</c><00:10:34.670><c> advanced</c>

00:10:35.019 --> 00:10:35.029 align:start position:0%
then pass through the advanced
 

00:10:35.029 --> 00:10:38.050 align:start position:0%
then pass through the advanced
optimization<00:10:35.300><c> atom</c><00:10:36.069><c> so</c><00:10:37.069><c> more</c><00:10:37.339><c> broadly</c><00:10:37.519><c> by</c>

00:10:38.050 --> 00:10:38.060 align:start position:0%
optimization atom so more broadly by
 

00:10:38.060 --> 00:10:40.960 align:start position:0%
optimization atom so more broadly by
taking<00:10:38.600><c> other</c><00:10:39.110><c> learning</c><00:10:40.040><c> algorithms</c><00:10:40.670><c> and</c>

00:10:40.960 --> 00:10:40.970 align:start position:0%
taking other learning algorithms and
 

00:10:40.970 --> 00:10:42.819 align:start position:0%
taking other learning algorithms and
expressing<00:10:41.569><c> them</c><00:10:41.779><c> in</c><00:10:42.050><c> sort</c><00:10:42.350><c> of</c><00:10:42.439><c> summation</c>

00:10:42.819 --> 00:10:42.829 align:start position:0%
expressing them in sort of summation
 

00:10:42.829 --> 00:10:45.100 align:start position:0%
expressing them in sort of summation
form<00:10:43.339><c> or</c><00:10:43.519><c> by</c><00:10:43.699><c> expressing</c><00:10:44.209><c> them</c><00:10:44.509><c> in</c><00:10:44.540><c> terms</c><00:10:44.930><c> of</c>

00:10:45.100 --> 00:10:45.110 align:start position:0%
form or by expressing them in terms of
 

00:10:45.110 --> 00:10:46.809 align:start position:0%
form or by expressing them in terms of
computing<00:10:45.589><c> sums</c><00:10:45.889><c> of</c><00:10:46.129><c> functions</c><00:10:46.490><c> over</c><00:10:46.730><c> the</c>

00:10:46.809 --> 00:10:46.819 align:start position:0%
computing sums of functions over the
 

00:10:46.819 --> 00:10:48.670 align:start position:0%
computing sums of functions over the
training<00:10:47.089><c> set</c><00:10:47.300><c> you</c><00:10:47.839><c> can</c><00:10:48.019><c> use</c><00:10:48.170><c> the</c><00:10:48.350><c> MapReduce</c>

00:10:48.670 --> 00:10:48.680 align:start position:0%
training set you can use the MapReduce
 

00:10:48.680 --> 00:10:50.710 align:start position:0%
training set you can use the MapReduce
technique<00:10:49.279><c> to</c><00:10:49.490><c> parallelize</c><00:10:50.060><c> other</c><00:10:50.389><c> learning</c>

00:10:50.710 --> 00:10:50.720 align:start position:0%
technique to parallelize other learning
 

00:10:50.720 --> 00:10:53.079 align:start position:0%
technique to parallelize other learning
adverbs<00:10:51.230><c> as</c><00:10:51.410><c> well</c><00:10:51.589><c> and</c><00:10:51.889><c> scale</c><00:10:52.490><c> them</c><00:10:52.699><c> to</c><00:10:52.879><c> very</c>

00:10:53.079 --> 00:10:53.089 align:start position:0%
adverbs as well and scale them to very
 

00:10:53.089 --> 00:10:55.720 align:start position:0%
adverbs as well and scale them to very
large<00:10:53.329><c> training</c><00:10:53.600><c> sets</c><00:10:53.959><c> finally</c><00:10:54.860><c> as</c><00:10:54.949><c> one</c><00:10:55.220><c> last</c>

00:10:55.720 --> 00:10:55.730 align:start position:0%
large training sets finally as one last
 

00:10:55.730 --> 00:10:57.749 align:start position:0%
large training sets finally as one last
comments<00:10:56.449><c> so</c><00:10:56.810><c> far</c><00:10:56.839><c> I've</c><00:10:57.110><c> been</c><00:10:57.139><c> discussing</c>

00:10:57.749 --> 00:10:57.759 align:start position:0%
comments so far I've been discussing
 

00:10:57.759 --> 00:11:00.579 align:start position:0%
comments so far I've been discussing
MapReduce<00:10:58.759><c> algorithms</c><00:10:59.329><c> as</c><00:10:59.629><c> allowing</c><00:11:00.350><c> you</c><00:11:00.500><c> to</c>

00:11:00.579 --> 00:11:00.589 align:start position:0%
MapReduce algorithms as allowing you to
 

00:11:00.589 --> 00:11:03.340 align:start position:0%
MapReduce algorithms as allowing you to
parallelize<00:11:00.819><c> over</c><00:11:01.819><c> multiple</c><00:11:02.509><c> computers</c><00:11:03.050><c> that</c>

00:11:03.340 --> 00:11:03.350 align:start position:0%
parallelize over multiple computers that
 

00:11:03.350 --> 00:11:05.259 align:start position:0%
parallelize over multiple computers that
may<00:11:03.500><c> be</c><00:11:03.560><c> multiple</c><00:11:03.980><c> computers</c><00:11:04.490><c> and</c><00:11:04.759><c> a</c><00:11:04.790><c> computer</c>

00:11:05.259 --> 00:11:05.269 align:start position:0%
may be multiple computers and a computer
 

00:11:05.269 --> 00:11:07.600 align:start position:0%
may be multiple computers and a computer
cluster<00:11:05.449><c> all</c><00:11:05.959><c> the</c><00:11:06.319><c> multiple</c><00:11:06.889><c> computers</c><00:11:07.370><c> in</c>

00:11:07.600 --> 00:11:07.610 align:start position:0%
cluster all the multiple computers in
 

00:11:07.610 --> 00:11:09.879 align:start position:0%
cluster all the multiple computers in
the<00:11:07.699><c> data</c><00:11:07.910><c> center</c><00:11:08.449><c> it</c><00:11:09.230><c> turns</c><00:11:09.500><c> out</c><00:11:09.649><c> that</c>

00:11:09.879 --> 00:11:09.889 align:start position:0%
the data center it turns out that
 

00:11:09.889 --> 00:11:12.009 align:start position:0%
the data center it turns out that
sometimes<00:11:10.310><c> even</c><00:11:10.819><c> if</c><00:11:10.939><c> you</c><00:11:11.000><c> have</c><00:11:11.120><c> just</c><00:11:11.300><c> a</c><00:11:11.509><c> single</c>

00:11:12.009 --> 00:11:12.019 align:start position:0%
sometimes even if you have just a single
 

00:11:12.019 --> 00:11:14.410 align:start position:0%
sometimes even if you have just a single
computer<00:11:12.550><c> MapReduce</c><00:11:13.550><c> can</c><00:11:13.819><c> also</c><00:11:14.000><c> be</c>

00:11:14.410 --> 00:11:14.420 align:start position:0%
computer MapReduce can also be
 

00:11:14.420 --> 00:11:17.319 align:start position:0%
computer MapReduce can also be
applicable<00:11:14.660><c> in</c><00:11:15.589><c> particular</c><00:11:15.860><c> on</c><00:11:16.370><c> many</c><00:11:16.879><c> single</c>

00:11:17.319 --> 00:11:17.329 align:start position:0%
applicable in particular on many single
 

00:11:17.329 --> 00:11:18.939 align:start position:0%
applicable in particular on many single
computers<00:11:17.870><c> now</c><00:11:18.079><c> you</c><00:11:18.139><c> can</c><00:11:18.529><c> have</c><00:11:18.709><c> multiple</c>

00:11:18.939 --> 00:11:18.949 align:start position:0%
computers now you can have multiple
 

00:11:18.949 --> 00:11:21.490 align:start position:0%
computers now you can have multiple
processing<00:11:19.939><c> cores</c><00:11:20.180><c> we</c><00:11:20.420><c> have</c><00:11:20.540><c> multiple</c><00:11:20.870><c> CPUs</c>

00:11:21.490 --> 00:11:21.500 align:start position:0%
processing cores we have multiple CPUs
 

00:11:21.500 --> 00:11:23.860 align:start position:0%
processing cores we have multiple CPUs
and<00:11:22.100><c> within</c><00:11:22.459><c> the</c><00:11:22.670><c> CPU</c><00:11:22.910><c> can</c><00:11:23.269><c> have</c><00:11:23.389><c> multiple</c>

00:11:23.860 --> 00:11:23.870 align:start position:0%
and within the CPU can have multiple
 

00:11:23.870 --> 00:11:27.160 align:start position:0%
and within the CPU can have multiple
protocols<00:11:24.350><c> and</c><00:11:25.000><c> so</c><00:11:26.000><c> if</c><00:11:26.329><c> you</c><00:11:26.600><c> have</c><00:11:26.809><c> a</c><00:11:26.839><c> large</c>

00:11:27.160 --> 00:11:27.170 align:start position:0%
protocols and so if you have a large
 

00:11:27.170 --> 00:11:29.559 align:start position:0%
protocols and so if you have a large
training<00:11:27.350><c> set</c><00:11:27.769><c> what</c><00:11:28.370><c> you</c><00:11:28.430><c> can</c><00:11:28.610><c> do</c><00:11:28.790><c> if</c><00:11:29.089><c> say</c><00:11:29.420><c> you</c>

00:11:29.559 --> 00:11:29.569 align:start position:0%
training set what you can do if say you
 

00:11:29.569 --> 00:11:31.600 align:start position:0%
training set what you can do if say you
have<00:11:29.750><c> a</c><00:11:29.779><c> computer</c><00:11:29.930><c> with</c><00:11:30.319><c> four</c><00:11:30.829><c> cores</c><00:11:31.220><c> with</c>

00:11:31.600 --> 00:11:31.610 align:start position:0%
have a computer with four cores with
 

00:11:31.610 --> 00:11:34.179 align:start position:0%
have a computer with four cores with
four<00:11:31.790><c> computing</c><00:11:32.300><c> cores</c><00:11:32.569><c> we</c><00:11:33.379><c> can</c><00:11:33.589><c> do</c><00:11:33.740><c> is</c><00:11:33.949><c> even</c>

00:11:34.179 --> 00:11:34.189 align:start position:0%
four computing cores we can do is even
 

00:11:34.189 --> 00:11:35.799 align:start position:0%
four computing cores we can do is even
on<00:11:34.370><c> a</c><00:11:34.399><c> single</c><00:11:34.759><c> computer</c><00:11:34.910><c> you</c><00:11:35.300><c> can</c><00:11:35.449><c> split</c><00:11:35.689><c> the</c>

00:11:35.799 --> 00:11:35.809 align:start position:0%
on a single computer you can split the
 

00:11:35.809 --> 00:11:38.049 align:start position:0%
on a single computer you can split the
training<00:11:36.110><c> sense</c><00:11:36.319><c> multiple</c><00:11:36.589><c> pieces</c><00:11:36.949><c> and</c><00:11:37.309><c> send</c>

00:11:38.049 --> 00:11:38.059 align:start position:0%
training sense multiple pieces and send
 

00:11:38.059 --> 00:11:39.850 align:start position:0%
training sense multiple pieces and send
the<00:11:38.149><c> training</c><00:11:38.420><c> set</c><00:11:38.660><c> to</c><00:11:38.839><c> different</c><00:11:39.500><c> cores</c>

00:11:39.850 --> 00:11:39.860 align:start position:0%
the training set to different cores
 

00:11:39.860 --> 00:11:41.980 align:start position:0%
the training set to different cores
within<00:11:40.220><c> a</c><00:11:40.430><c> single</c><00:11:40.850><c> box</c><00:11:41.149><c> like</c><00:11:41.480><c> within</c><00:11:41.660><c> a</c><00:11:41.809><c> single</c>

00:11:41.980 --> 00:11:41.990 align:start position:0%
within a single box like within a single
 

00:11:41.990 --> 00:11:43.990 align:start position:0%
within a single box like within a single
desktop<00:11:42.379><c> computer</c><00:11:43.100><c> or</c><00:11:43.129><c> in</c><00:11:43.370><c> a</c><00:11:43.430><c> single</c><00:11:43.699><c> server</c>

00:11:43.990 --> 00:11:44.000 align:start position:0%
desktop computer or in a single server
 

00:11:44.000 --> 00:11:46.929 align:start position:0%
desktop computer or in a single server
and<00:11:44.389><c> use</c><00:11:45.350><c> MapReduce</c><00:11:45.769><c> this</c><00:11:46.189><c> way</c><00:11:46.370><c> to</c><00:11:46.430><c> divvy</c><00:11:46.910><c> up</c>

00:11:46.929 --> 00:11:46.939 align:start position:0%
and use MapReduce this way to divvy up
 

00:11:46.939 --> 00:11:49.179 align:start position:0%
and use MapReduce this way to divvy up
the<00:11:47.149><c> workload</c><00:11:47.360><c> each</c><00:11:48.110><c> of</c><00:11:48.350><c> the</c><00:11:48.439><c> cores</c><00:11:48.709><c> can</c><00:11:48.829><c> then</c>

00:11:49.179 --> 00:11:49.189 align:start position:0%
the workload each of the cores can then
 

00:11:49.189 --> 00:11:51.879 align:start position:0%
the workload each of the cores can then
carry<00:11:49.550><c> out</c><00:11:49.670><c> the</c><00:11:49.939><c> sum</c><00:11:50.180><c> over</c><00:11:50.420><c> say</c><00:11:51.110><c> one</c><00:11:51.410><c> quarter</c>

00:11:51.879 --> 00:11:51.889 align:start position:0%
carry out the sum over say one quarter
 

00:11:51.889 --> 00:11:53.710 align:start position:0%
carry out the sum over say one quarter
of<00:11:51.920><c> your</c><00:11:52.100><c> training</c><00:11:52.339><c> set</c><00:11:52.670><c> and</c><00:11:52.910><c> then</c><00:11:53.420><c> they</c><00:11:53.569><c> can</c>

00:11:53.710 --> 00:11:53.720 align:start position:0%
of your training set and then they can
 

00:11:53.720 --> 00:11:55.480 align:start position:0%
of your training set and then they can
take<00:11:53.870><c> the</c><00:11:53.990><c> partial</c><00:11:54.259><c> sums</c><00:11:54.680><c> and</c><00:11:55.009><c> you</c><00:11:55.370><c> know</c>

00:11:55.480 --> 00:11:55.490 align:start position:0%
take the partial sums and you know
 

00:11:55.490 --> 00:11:57.970 align:start position:0%
take the partial sums and you know
combine<00:11:55.939><c> them</c><00:11:56.209><c> in</c><00:11:56.870><c> order</c><00:11:57.110><c> to</c><00:11:57.290><c> get</c><00:11:57.709><c> the</c>

00:11:57.970 --> 00:11:57.980 align:start position:0%
combine them in order to get the
 

00:11:57.980 --> 00:11:59.889 align:start position:0%
combine them in order to get the
summation<00:11:58.670><c> over</c><00:11:58.879><c> the</c><00:11:59.029><c> entire</c><00:11:59.389><c> training</c><00:11:59.660><c> set</c>

00:11:59.889 --> 00:11:59.899 align:start position:0%
summation over the entire training set
 

00:11:59.899 --> 00:12:01.720 align:start position:0%
summation over the entire training set
the<00:12:00.649><c> advantage</c><00:12:01.100><c> of</c><00:12:01.279><c> thinking</c><00:12:01.610><c> about</c>

00:12:01.720 --> 00:12:01.730 align:start position:0%
the advantage of thinking about
 

00:12:01.730 --> 00:12:04.119 align:start position:0%
the advantage of thinking about
MapReduce<00:12:02.389><c> this</c><00:12:02.600><c> way</c><00:12:02.809><c> as</c><00:12:03.050><c> paralyzing</c><00:12:03.829><c> over</c>

00:12:04.119 --> 00:12:04.129 align:start position:0%
MapReduce this way as paralyzing over
 

00:12:04.129 --> 00:12:05.710 align:start position:0%
MapReduce this way as paralyzing over
cause<00:12:04.399><c> within</c><00:12:04.759><c> a</c><00:12:04.819><c> single</c><00:12:05.029><c> machine</c><00:12:05.269><c> rather</c>

00:12:05.710 --> 00:12:05.720 align:start position:0%
cause within a single machine rather
 

00:12:05.720 --> 00:12:07.600 align:start position:0%
cause within a single machine rather
than<00:12:05.990><c> paralyzing</c><00:12:06.559><c> across</c><00:12:06.860><c> multiple</c><00:12:07.189><c> machines</c>

00:12:07.600 --> 00:12:07.610 align:start position:0%
than paralyzing across multiple machines
 

00:12:07.610 --> 00:12:10.329 align:start position:0%
than paralyzing across multiple machines
is<00:12:08.209><c> that</c><00:12:08.269><c> this</c><00:12:09.199><c> way</c><00:12:09.439><c> you</c><00:12:09.500><c> don't</c><00:12:09.800><c> have</c><00:12:09.889><c> to</c><00:12:10.100><c> worry</c>

00:12:10.329 --> 00:12:10.339 align:start position:0%
is that this way you don't have to worry
 

00:12:10.339 --> 00:12:12.400 align:start position:0%
is that this way you don't have to worry
about<00:12:10.610><c> network</c><00:12:11.180><c> latency</c><00:12:11.720><c> because</c><00:12:11.870><c> all</c><00:12:12.230><c> the</c>

00:12:12.400 --> 00:12:12.410 align:start position:0%
about network latency because all the
 

00:12:12.410 --> 00:12:14.290 align:start position:0%
about network latency because all the
communication<00:12:13.100><c> all</c><00:12:13.399><c> the</c><00:12:13.430><c> sending</c>

00:12:14.290 --> 00:12:14.300 align:start position:0%
communication all the sending
 

00:12:14.300 --> 00:12:16.420 align:start position:0%
communication all the sending
of<00:12:14.420><c> the</c><00:12:14.690><c> temp</c><00:12:15.050><c> jayveer</c><00:12:15.560><c> Vols</c><00:12:15.830><c> back</c><00:12:16.070><c> and</c><00:12:16.220><c> forth</c>

00:12:16.420 --> 00:12:16.430 align:start position:0%
of the temp jayveer Vols back and forth
 

00:12:16.430 --> 00:12:18.190 align:start position:0%
of the temp jayveer Vols back and forth
all<00:12:16.580><c> that</c><00:12:16.820><c> happens</c><00:12:17.270><c> within</c><00:12:17.510><c> a</c><00:12:17.690><c> single</c><00:12:17.900><c> machine</c>

00:12:18.190 --> 00:12:18.200 align:start position:0%
all that happens within a single machine
 

00:12:18.200 --> 00:12:20.950 align:start position:0%
all that happens within a single machine
and<00:12:18.860><c> so</c><00:12:19.130><c> network</c><00:12:19.580><c> latency</c><00:12:20.150><c> becomes</c><00:12:20.510><c> much</c><00:12:20.780><c> less</c>

00:12:20.950 --> 00:12:20.960 align:start position:0%
and so network latency becomes much less
 

00:12:20.960 --> 00:12:22.720 align:start position:0%
and so network latency becomes much less
of<00:12:21.140><c> an</c><00:12:21.230><c> issue</c><00:12:21.350><c> compared</c><00:12:22.010><c> to</c><00:12:22.160><c> if</c><00:12:22.370><c> you</c><00:12:22.580><c> were</c>

00:12:22.720 --> 00:12:22.730 align:start position:0%
of an issue compared to if you were
 

00:12:22.730 --> 00:12:24.790 align:start position:0%
of an issue compared to if you were
using<00:12:22.940><c> this</c><00:12:23.240><c> to</c><00:12:23.450><c> paralyze</c><00:12:23.900><c> over</c><00:12:24.350><c> different</c>

00:12:24.790 --> 00:12:24.800 align:start position:0%
using this to paralyze over different
 

00:12:24.800 --> 00:12:27.520 align:start position:0%
using this to paralyze over different
computers<00:12:25.490><c> within</c><00:12:25.760><c> the</c><00:12:25.940><c> data</c><00:12:26.150><c> center</c><00:12:26.530><c> finally</c>

00:12:27.520 --> 00:12:27.530 align:start position:0%
computers within the data center finally
 

00:12:27.530 --> 00:12:29.980 align:start position:0%
computers within the data center finally
one<00:12:27.800><c> last</c><00:12:27.830><c> caveat</c><00:12:28.310><c> on</c><00:12:28.820><c> parallelizing</c><00:12:29.660><c> within</c>

00:12:29.980 --> 00:12:29.990 align:start position:0%
one last caveat on parallelizing within
 

00:12:29.990 --> 00:12:32.080 align:start position:0%
one last caveat on parallelizing within
a<00:12:30.140><c> multi-core</c><00:12:30.770><c> machine</c><00:12:31.070><c> depending</c><00:12:31.940><c> on</c><00:12:32.000><c> the</c>

00:12:32.080 --> 00:12:32.090 align:start position:0%
a multi-core machine depending on the
 

00:12:32.090 --> 00:12:34.330 align:start position:0%
a multi-core machine depending on the
details<00:12:32.510><c> of</c><00:12:32.690><c> implementation</c><00:12:33.050><c> if</c><00:12:33.950><c> you</c><00:12:34.130><c> have</c><00:12:34.310><c> a</c>

00:12:34.330 --> 00:12:34.340 align:start position:0%
details of implementation if you have a
 

00:12:34.340 --> 00:12:36.460 align:start position:0%
details of implementation if you have a
multi-core<00:12:34.940><c> machine</c><00:12:35.270><c> and</c><00:12:35.570><c> if</c><00:12:36.050><c> you</c><00:12:36.260><c> have</c>

00:12:36.460 --> 00:12:36.470 align:start position:0%
multi-core machine and if you have
 

00:12:36.470 --> 00:12:37.810 align:start position:0%
multi-core machine and if you have
certain<00:12:36.950><c> numerical</c><00:12:37.130><c> linear</c><00:12:37.640><c> algebra</c>

00:12:37.810 --> 00:12:37.820 align:start position:0%
certain numerical linear algebra
 

00:12:37.820 --> 00:12:40.000 align:start position:0%
certain numerical linear algebra
libraries<00:12:38.570><c> it</c><00:12:39.080><c> turns</c><00:12:39.290><c> out</c><00:12:39.470><c> that</c><00:12:39.620><c> the</c><00:12:39.740><c> some</c>

00:12:40.000 --> 00:12:40.010 align:start position:0%
libraries it turns out that the some
 

00:12:40.010 --> 00:12:41.770 align:start position:0%
libraries it turns out that the some
numerical<00:12:40.490><c> in</c><00:12:40.640><c> the</c><00:12:40.730><c> area</c><00:12:41.030><c> libraries</c><00:12:41.390><c> that</c><00:12:41.630><c> can</c>

00:12:41.770 --> 00:12:41.780 align:start position:0%
numerical in the area libraries that can
 

00:12:41.780 --> 00:12:44.950 align:start position:0%
numerical in the area libraries that can
automatically<00:12:42.590><c> paralyze</c><00:12:43.400><c> their</c><00:12:44.150><c> linear</c>

00:12:44.950 --> 00:12:44.960 align:start position:0%
automatically paralyze their linear
 

00:12:44.960 --> 00:12:47.530 align:start position:0%
automatically paralyze their linear
algebra<00:12:45.440><c> operations</c><00:12:46.340><c> across</c><00:12:46.730><c> multiple</c><00:12:47.210><c> holes</c>

00:12:47.530 --> 00:12:47.540 align:start position:0%
algebra operations across multiple holes
 

00:12:47.540 --> 00:12:49.330 align:start position:0%
algebra operations across multiple holes
within<00:12:47.900><c> the</c><00:12:47.960><c> machine</c><00:12:48.290><c> so</c><00:12:48.890><c> if</c><00:12:49.160><c> you're</c>

00:12:49.330 --> 00:12:49.340 align:start position:0%
within the machine so if you're
 

00:12:49.340 --> 00:12:50.980 align:start position:0%
within the machine so if you're
unfortunate<00:12:49.640><c> enough</c><00:12:50.120><c> to</c><00:12:50.180><c> be</c><00:12:50.420><c> using</c><00:12:50.810><c> one</c><00:12:50.960><c> of</c>

00:12:50.980 --> 00:12:50.990 align:start position:0%
unfortunate enough to be using one of
 

00:12:50.990 --> 00:12:52.180 align:start position:0%
unfortunate enough to be using one of
those<00:12:51.200><c> numerical</c><00:12:51.560><c> than</c><00:12:51.890><c> the</c><00:12:51.980><c> arcuate</c>

00:12:52.180 --> 00:12:52.190 align:start position:0%
those numerical than the arcuate
 

00:12:52.190 --> 00:12:53.980 align:start position:0%
those numerical than the arcuate
libraries<00:12:52.700><c> and</c><00:12:53.000><c> certainly</c><00:12:53.390><c> this</c><00:12:53.630><c> does</c><00:12:53.840><c> not</c>

00:12:53.980 --> 00:12:53.990 align:start position:0%
libraries and certainly this does not
 

00:12:53.990 --> 00:12:56.170 align:start position:0%
libraries and certainly this does not
apply<00:12:54.230><c> to</c><00:12:54.350><c> every</c><00:12:54.800><c> single</c><00:12:54.980><c> library</c><00:12:55.340><c> but</c><00:12:56.090><c> I'll</c>

00:12:56.170 --> 00:12:56.180 align:start position:0%
apply to every single library but I'll
 

00:12:56.180 --> 00:12:58.270 align:start position:0%
apply to every single library but I'll
be<00:12:56.240><c> using</c><00:12:56.540><c> one</c><00:12:56.630><c> of</c><00:12:56.660><c> those</c><00:12:56.840><c> libraries</c><00:12:57.350><c> and</c><00:12:57.860><c> if</c>

00:12:58.270 --> 00:12:58.280 align:start position:0%
be using one of those libraries and if
 

00:12:58.280 --> 00:12:59.650 align:start position:0%
be using one of those libraries and if
you<00:12:58.400><c> have</c><00:12:58.580><c> a</c><00:12:58.610><c> very</c><00:12:58.880><c> good</c><00:12:59.120><c> vectorized</c>

00:12:59.650 --> 00:12:59.660 align:start position:0%
you have a very good vectorized
 

00:12:59.660 --> 00:13:01.150 align:start position:0%
you have a very good vectorized
implementation<00:13:00.470><c> of</c><00:13:00.560><c> a</c><00:13:00.620><c> learning</c><00:13:00.800><c> algorithm</c>

00:13:01.150 --> 00:13:01.160 align:start position:0%
implementation of a learning algorithm
 

00:13:01.160 --> 00:13:03.310 align:start position:0%
implementation of a learning algorithm
sometimes<00:13:02.150><c> you</c><00:13:02.330><c> can</c><00:13:02.480><c> just</c><00:13:02.690><c> implement</c><00:13:02.930><c> your</c>

00:13:03.310 --> 00:13:03.320 align:start position:0%
sometimes you can just implement your
 

00:13:03.320 --> 00:13:05.230 align:start position:0%
sometimes you can just implement your
standard<00:13:04.070><c> learning</c><00:13:04.370><c> algorithm</c><00:13:04.790><c> in</c><00:13:05.120><c> a</c>

00:13:05.230 --> 00:13:05.240 align:start position:0%
standard learning algorithm in a
 

00:13:05.240 --> 00:13:07.060 align:start position:0%
standard learning algorithm in a
vectorized<00:13:05.600><c> fashion</c><00:13:06.170><c> and</c><00:13:06.380><c> not</c><00:13:06.650><c> worry</c><00:13:06.890><c> about</c>

00:13:07.060 --> 00:13:07.070 align:start position:0%
vectorized fashion and not worry about
 

00:13:07.070 --> 00:13:09.190 align:start position:0%
vectorized fashion and not worry about
parallelization<00:13:07.850><c> and</c><00:13:08.210><c> your</c><00:13:08.780><c> numerical</c>

00:13:09.190 --> 00:13:09.200 align:start position:0%
parallelization and your numerical
 

00:13:09.200 --> 00:13:10.930 align:start position:0%
parallelization and your numerical
linearity<00:13:09.680><c> library</c><00:13:10.040><c> could</c><00:13:10.310><c> take</c><00:13:10.490><c> care</c><00:13:10.730><c> of</c>

00:13:10.930 --> 00:13:10.940 align:start position:0%
linearity library could take care of
 

00:13:10.940 --> 00:13:12.700 align:start position:0%
linearity library could take care of
some<00:13:11.570><c> of</c><00:13:11.690><c> it</c><00:13:11.810><c> for</c><00:13:12.050><c> you</c><00:13:12.140><c> so</c><00:13:12.470><c> that</c><00:13:12.620><c> you</c><00:13:12.680><c> don't</c>

00:13:12.700 --> 00:13:12.710 align:start position:0%
some of it for you so that you don't
 

00:13:12.710 --> 00:13:15.010 align:start position:0%
some of it for you so that you don't
need<00:13:12.950><c> to</c><00:13:13.010><c> implement</c><00:13:13.340><c> MapReduce</c><00:13:13.910><c> but</c><00:13:14.660><c> for</c>

00:13:15.010 --> 00:13:15.020 align:start position:0%
need to implement MapReduce but for
 

00:13:15.020 --> 00:13:17.080 align:start position:0%
need to implement MapReduce but for
other<00:13:15.170><c> learning</c><00:13:15.530><c> problems</c><00:13:15.950><c> taking</c><00:13:16.580><c> advantage</c>

00:13:17.080 --> 00:13:17.090 align:start position:0%
other learning problems taking advantage
 

00:13:17.090 --> 00:13:18.460 align:start position:0%
other learning problems taking advantage
of<00:13:17.300><c> this</c><00:13:17.420><c> sort</c><00:13:17.660><c> of</c><00:13:17.750><c> MapReduce</c><00:13:18.290><c> implementation</c>

00:13:18.460 --> 00:13:18.470 align:start position:0%
of this sort of MapReduce implementation
 

00:13:18.470 --> 00:13:21.370 align:start position:0%
of this sort of MapReduce implementation
while<00:13:19.250><c> finding</c><00:13:19.610><c> a</c><00:13:19.720><c> using</c><00:13:20.720><c> this</c><00:13:20.840><c> MapReduce</c>

00:13:21.370 --> 00:13:21.380 align:start position:0%
while finding a using this MapReduce
 

00:13:21.380 --> 00:13:23.350 align:start position:0%
while finding a using this MapReduce
formalism<00:13:22.040><c> to</c><00:13:22.070><c> paralyze</c><00:13:22.610><c> across</c><00:13:23.000><c> course</c>

00:13:23.350 --> 00:13:23.360 align:start position:0%
formalism to paralyze across course
 

00:13:23.360 --> 00:13:25.240 align:start position:0%
formalism to paralyze across course
explicitly<00:13:23.990><c> yourself</c><00:13:24.380><c> might</c><00:13:24.830><c> be</c><00:13:24.860><c> a</c><00:13:24.980><c> good</c><00:13:25.160><c> idea</c>

00:13:25.240 --> 00:13:25.250 align:start position:0%
explicitly yourself might be a good idea
 

00:13:25.250 --> 00:13:27.220 align:start position:0%
explicitly yourself might be a good idea
as<00:13:25.550><c> well</c><00:13:25.760><c> and</c><00:13:25.790><c> could</c><00:13:26.270><c> that</c><00:13:26.420><c> you</c><00:13:26.480><c> speed</c><00:13:26.900><c> up</c><00:13:27.080><c> your</c>

00:13:27.220 --> 00:13:27.230 align:start position:0%
as well and could that you speed up your
 

00:13:27.230 --> 00:13:29.180 align:start position:0%
as well and could that you speed up your
learning<00:13:27.290><c> algorithm</c>

00:13:29.180 --> 00:13:29.190 align:start position:0%
learning algorithm
 

00:13:29.190 --> 00:13:32.180 align:start position:0%
learning algorithm
in<00:13:30.000><c> this</c><00:13:30.149><c> video</c><00:13:30.420><c> we</c><00:13:30.990><c> talked</c><00:13:31.350><c> about</c><00:13:31.500><c> the</c><00:13:31.680><c> Map</c>

00:13:32.180 --> 00:13:32.190 align:start position:0%
in this video we talked about the Map
 

00:13:32.190 --> 00:13:34.790 align:start position:0%
in this video we talked about the Map
Reduce<00:13:32.490><c> approach</c><00:13:33.209><c> to</c><00:13:33.569><c> parallelizing</c><00:13:34.470><c> machine</c>

00:13:34.790 --> 00:13:34.800 align:start position:0%
Reduce approach to parallelizing machine
 

00:13:34.800 --> 00:13:36.410 align:start position:0%
Reduce approach to parallelizing machine
learning<00:13:35.220><c> by</c><00:13:35.430><c> taking</c><00:13:35.790><c> your</c><00:13:35.910><c> data</c><00:13:36.060><c> and</c>

00:13:36.410 --> 00:13:36.420 align:start position:0%
learning by taking your data and
 

00:13:36.420 --> 00:13:38.329 align:start position:0%
learning by taking your data and
spreading<00:13:36.839><c> them</c><00:13:37.050><c> across</c><00:13:37.769><c> maybe</c><00:13:38.009><c> many</c>

00:13:38.329 --> 00:13:38.339 align:start position:0%
spreading them across maybe many
 

00:13:38.339 --> 00:13:39.980 align:start position:0%
spreading them across maybe many
computers<00:13:38.910><c> in</c><00:13:39.060><c> the</c><00:13:39.149><c> data</c><00:13:39.300><c> center</c>

00:13:39.980 --> 00:13:39.990 align:start position:0%
computers in the data center
 

00:13:39.990 --> 00:13:42.590 align:start position:0%
computers in the data center
although<00:13:40.410><c> these</c><00:13:40.829><c> years</c><00:13:41.790><c> are</c><00:13:42.000><c> applicable</c><00:13:42.449><c> to</c>

00:13:42.590 --> 00:13:42.600 align:start position:0%
although these years are applicable to
 

00:13:42.600 --> 00:13:45.050 align:start position:0%
although these years are applicable to
paralyzing<00:13:43.470><c> across</c><00:13:43.889><c> multiple</c><00:13:44.250><c> cores</c><00:13:44.670><c> within</c>

00:13:45.050 --> 00:13:45.060 align:start position:0%
paralyzing across multiple cores within
 

00:13:45.060 --> 00:13:47.900 align:start position:0%
paralyzing across multiple cores within
a<00:13:45.120><c> single</c><00:13:45.300><c> computer</c><00:13:45.899><c> as</c><00:13:46.019><c> well</c><00:13:46.170><c> and</c><00:13:46.910><c> today</c>

00:13:47.900 --> 00:13:47.910 align:start position:0%
a single computer as well and today
 

00:13:47.910 --> 00:13:49.790 align:start position:0%
a single computer as well and today
there<00:13:48.149><c> are</c><00:13:48.180><c> some</c><00:13:48.540><c> good</c><00:13:48.810><c> open-source</c>

00:13:49.790 --> 00:13:49.800 align:start position:0%
there are some good open-source
 

00:13:49.800 --> 00:13:52.040 align:start position:0%
there are some good open-source
implementations<00:13:50.670><c> of</c><00:13:50.790><c> MapReduce</c><00:13:51.120><c> so</c><00:13:51.600><c> there</c>

00:13:52.040 --> 00:13:52.050 align:start position:0%
implementations of MapReduce so there
 

00:13:52.050 --> 00:13:53.720 align:start position:0%
implementations of MapReduce so there
are<00:13:52.110><c> many</c><00:13:52.199><c> users</c><00:13:52.620><c> of</c><00:13:52.769><c> an</c><00:13:52.860><c> open-source</c><00:13:53.160><c> system</c>

00:13:53.720 --> 00:13:53.730 align:start position:0%
are many users of an open-source system
 

00:13:53.730 --> 00:13:56.119 align:start position:0%
are many users of an open-source system
called<00:13:54.029><c> Hadoop</c><00:13:54.360><c> and</c><00:13:54.620><c> using</c><00:13:55.620><c> either</c><00:13:55.769><c> your</c><00:13:56.009><c> own</c>

00:13:56.119 --> 00:13:56.129 align:start position:0%
called Hadoop and using either your own
 

00:13:56.129 --> 00:13:57.949 align:start position:0%
called Hadoop and using either your own
implementation<00:13:56.399><c> or</c><00:13:57.089><c> using</c><00:13:57.420><c> someone</c><00:13:57.899><c> else's</c>

00:13:57.949 --> 00:13:57.959 align:start position:0%
implementation or using someone else's
 

00:13:57.959 --> 00:14:00.410 align:start position:0%
implementation or using someone else's
open-source<00:13:58.680><c> implementation</c><00:13:59.370><c> you</c><00:13:59.910><c> can</c><00:13:59.939><c> use</c>

00:14:00.410 --> 00:14:00.420 align:start position:0%
open-source implementation you can use
 

00:14:00.420 --> 00:14:02.480 align:start position:0%
open-source implementation you can use
these<00:14:00.600><c> ideas</c><00:14:00.750><c> to</c><00:14:01.290><c> paralyze</c><00:14:01.800><c> learning</c>

00:14:02.480 --> 00:14:02.490 align:start position:0%
these ideas to paralyze learning
 

00:14:02.490 --> 00:14:04.850 align:start position:0%
these ideas to paralyze learning
algorithms<00:14:02.970><c> and</c><00:14:03.180><c> get</c><00:14:03.689><c> them</c><00:14:03.930><c> to</c><00:14:04.110><c> run</c><00:14:04.319><c> on</c><00:14:04.620><c> much</c>

00:14:04.850 --> 00:14:04.860 align:start position:0%
algorithms and get them to run on much
 

00:14:04.860 --> 00:14:06.679 align:start position:0%
algorithms and get them to run on much
larger<00:14:05.100><c> data</c><00:14:05.459><c> sets</c><00:14:05.790><c> than</c><00:14:05.910><c> is</c><00:14:06.149><c> possible</c>

00:14:06.679 --> 00:14:06.689 align:start position:0%
larger data sets than is possible
 

00:14:06.689 --> 00:14:09.889 align:start position:0%
larger data sets than is possible
using<00:14:07.230><c> just</c><00:14:07.410><c> a</c><00:14:07.500><c> single</c><00:14:07.709><c> machine</c>

