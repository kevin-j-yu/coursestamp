WEBVTT
Kind: captions
Language: en

00:00:00.060 --> 00:00:02.540 align:start position:0%
 
in<00:00:00.359><c> previous</c><00:00:01.050><c> videos</c><00:00:01.380><c> we</c><00:00:01.410><c> talked</c><00:00:01.949><c> about</c><00:00:02.159><c> the</c>

00:00:02.540 --> 00:00:02.550 align:start position:0%
in previous videos we talked about the
 

00:00:02.550 --> 00:00:04.849 align:start position:0%
in previous videos we talked about the
gradient<00:00:03.179><c> descent</c><00:00:03.510><c> algorithm</c><00:00:03.870><c> and</c><00:00:04.350><c> we</c><00:00:04.560><c> talked</c>

00:00:04.849 --> 00:00:04.859 align:start position:0%
gradient descent algorithm and we talked
 

00:00:04.859 --> 00:00:07.220 align:start position:0%
gradient descent algorithm and we talked
about<00:00:05.040><c> the</c><00:00:05.370><c> linear</c><00:00:05.670><c> regression</c><00:00:06.270><c> model</c><00:00:07.080><c> and</c>

00:00:07.220 --> 00:00:07.230 align:start position:0%
about the linear regression model and
 

00:00:07.230 --> 00:00:09.799 align:start position:0%
about the linear regression model and
the<00:00:07.470><c> squared</c><00:00:07.770><c> error</c><00:00:07.980><c> cost</c><00:00:08.309><c> function</c><00:00:08.460><c> in</c><00:00:09.090><c> this</c>

00:00:09.799 --> 00:00:09.809 align:start position:0%
the squared error cost function in this
 

00:00:09.809 --> 00:00:11.299 align:start position:0%
the squared error cost function in this
video<00:00:10.139><c> we're</c><00:00:10.440><c> going</c><00:00:10.559><c> to</c><00:00:10.650><c> put</c><00:00:10.860><c> together</c><00:00:10.980><c> a</c>

00:00:11.299 --> 00:00:11.309 align:start position:0%
video we're going to put together a
 

00:00:11.309 --> 00:00:13.700 align:start position:0%
video we're going to put together a
gradient<00:00:11.639><c> descent</c><00:00:12.120><c> with</c><00:00:12.450><c> our</c><00:00:12.719><c> cost</c><00:00:13.259><c> function</c>

00:00:13.700 --> 00:00:13.710 align:start position:0%
gradient descent with our cost function
 

00:00:13.710 --> 00:00:16.279 align:start position:0%
gradient descent with our cost function
and<00:00:13.860><c> that</c><00:00:14.610><c> will</c><00:00:14.670><c> give</c><00:00:15.030><c> us</c><00:00:15.059><c> an</c><00:00:15.420><c> algorithm</c><00:00:15.960><c> for</c>

00:00:16.279 --> 00:00:16.289 align:start position:0%
and that will give us an algorithm for
 

00:00:16.289 --> 00:00:17.840 align:start position:0%
and that will give us an algorithm for
linear<00:00:16.619><c> regression</c><00:00:17.220><c> for</c><00:00:17.430><c> fitting</c><00:00:17.730><c> a</c><00:00:17.820><c> straight</c>

00:00:17.840 --> 00:00:17.850 align:start position:0%
linear regression for fitting a straight
 

00:00:17.850 --> 00:00:22.939 align:start position:0%
linear regression for fitting a straight
line<00:00:18.090><c> to</c><00:00:18.420><c> our</c><00:00:18.449><c> data</c><00:00:19.880><c> so</c><00:00:21.439><c> this</c><00:00:22.439><c> is</c><00:00:22.619><c> what</c><00:00:22.830><c> we</c>

00:00:22.939 --> 00:00:22.949 align:start position:0%
line to our data so this is what we
 

00:00:22.949 --> 00:00:24.740 align:start position:0%
line to our data so this is what we
worked<00:00:23.160><c> out</c><00:00:23.400><c> in</c><00:00:23.730><c> their</c><00:00:23.970><c> previous</c><00:00:24.359><c> videos</c>

00:00:24.740 --> 00:00:24.750 align:start position:0%
worked out in their previous videos
 

00:00:24.750 --> 00:00:26.240 align:start position:0%
worked out in their previous videos
there's<00:00:25.109><c> our</c><00:00:25.260><c> gradient</c><00:00:25.560><c> descent</c><00:00:25.980><c> algorithm</c>

00:00:26.240 --> 00:00:26.250 align:start position:0%
there's our gradient descent algorithm
 

00:00:26.250 --> 00:00:29.210 align:start position:0%
there's our gradient descent algorithm
which<00:00:27.029><c> should</c><00:00:27.420><c> be</c><00:00:27.539><c> familiar</c><00:00:27.779><c> and</c><00:00:28.320><c> here's</c><00:00:29.070><c> the</c>

00:00:29.210 --> 00:00:29.220 align:start position:0%
which should be familiar and here's the
 

00:00:29.220 --> 00:00:31.970 align:start position:0%
which should be familiar and here's the
linear<00:00:29.550><c> regression</c><00:00:29.670><c> model</c><00:00:30.359><c> with</c><00:00:30.869><c> our</c><00:00:31.140><c> linear</c>

00:00:31.970 --> 00:00:31.980 align:start position:0%
linear regression model with our linear
 

00:00:31.980 --> 00:00:34.970 align:start position:0%
linear regression model with our linear
linear<00:00:32.369><c> hypothesis</c><00:00:33.270><c> and</c><00:00:33.510><c> our</c><00:00:34.230><c> squared</c><00:00:34.770><c> error</c>

00:00:34.970 --> 00:00:34.980 align:start position:0%
linear hypothesis and our squared error
 

00:00:34.980 --> 00:00:37.549 align:start position:0%
linear hypothesis and our squared error
cost<00:00:35.219><c> function</c><00:00:35.660><c> what</c><00:00:36.660><c> we're</c><00:00:36.780><c> going</c><00:00:36.960><c> to</c><00:00:37.020><c> do</c><00:00:37.230><c> is</c>

00:00:37.549 --> 00:00:37.559 align:start position:0%
cost function what we're going to do is
 

00:00:37.559 --> 00:00:44.020 align:start position:0%
cost function what we're going to do is
apply<00:00:38.190><c> gradient</c><00:00:38.730><c> descent</c><00:00:39.980><c> to</c><00:00:40.980><c> minimize</c><00:00:41.010><c> our</c>

00:00:44.020 --> 00:00:44.030 align:start position:0%
 
 

00:00:44.030 --> 00:00:48.740 align:start position:0%
 
squared<00:00:45.030><c> error</c><00:00:45.180><c> cost</c><00:00:45.420><c> function</c><00:00:46.879><c> now</c><00:00:47.879><c> in</c><00:00:48.270><c> order</c>

00:00:48.740 --> 00:00:48.750 align:start position:0%
squared error cost function now in order
 

00:00:48.750 --> 00:00:50.660 align:start position:0%
squared error cost function now in order
to<00:00:48.870><c> apply</c><00:00:49.079><c> gradient</c><00:00:49.410><c> descent</c><00:00:49.890><c> in</c><00:00:50.070><c> order</c><00:00:50.370><c> to</c>

00:00:50.660 --> 00:00:50.670 align:start position:0%
to apply gradient descent in order to
 

00:00:50.670 --> 00:00:53.240 align:start position:0%
to apply gradient descent in order to
you<00:00:51.000><c> know</c><00:00:51.090><c> write</c><00:00:51.360><c> this</c><00:00:51.600><c> piece</c><00:00:51.809><c> of</c><00:00:51.989><c> code</c><00:00:52.250><c> the</c>

00:00:53.240 --> 00:00:53.250 align:start position:0%
you know write this piece of code the
 

00:00:53.250 --> 00:00:56.360 align:start position:0%
you know write this piece of code the
key<00:00:53.610><c> term</c><00:00:53.940><c> we</c><00:00:54.149><c> need</c><00:00:54.360><c> is</c><00:00:54.629><c> this</c><00:00:55.230><c> derivative</c><00:00:56.010><c> term</c>

00:00:56.360 --> 00:00:56.370 align:start position:0%
key term we need is this derivative term
 

00:00:56.370 --> 00:01:00.770 align:start position:0%
key term we need is this derivative term
over<00:00:56.850><c> here</c><00:00:58.640><c> so</c><00:00:59.640><c> you</c><00:00:59.940><c> need</c><00:01:00.120><c> to</c><00:01:00.210><c> figure</c><00:01:00.359><c> out</c><00:01:00.480><c> what</c>

00:01:00.770 --> 00:01:00.780 align:start position:0%
over here so you need to figure out what
 

00:01:00.780 --> 00:01:03.369 align:start position:0%
over here so you need to figure out what
is<00:01:00.899><c> this</c><00:01:01.079><c> partial</c><00:01:01.410><c> derivative</c><00:01:01.739><c> term</c><00:01:02.430><c> and</c>

00:01:03.369 --> 00:01:03.379 align:start position:0%
is this partial derivative term and
 

00:01:03.379 --> 00:01:06.260 align:start position:0%
is this partial derivative term and
plugging<00:01:04.379><c> in</c><00:01:04.439><c> the</c><00:01:04.530><c> definition</c><00:01:04.559><c> of</c><00:01:05.369><c> the</c><00:01:06.090><c> cost</c>

00:01:06.260 --> 00:01:06.270 align:start position:0%
plugging in the definition of the cost
 

00:01:06.270 --> 00:01:13.190 align:start position:0%
plugging in the definition of the cost
function<00:01:06.450><c> J</c><00:01:06.860><c> this</c><00:01:07.860><c> turns</c><00:01:08.189><c> out</c><00:01:08.490><c> to</c><00:01:08.760><c> be</c><00:01:10.250><c> this</c><00:01:12.200><c> sum</c>

00:01:13.190 --> 00:01:13.200 align:start position:0%
function J this turns out to be this sum
 

00:01:13.200 --> 00:01:16.780 align:start position:0%
function J this turns out to be this sum
from<00:01:13.320><c> I</c><00:01:13.470><c> equals</c><00:01:13.799><c> 1</c><00:01:14.010><c> through</c><00:01:14.040><c> n</c><00:01:14.490><c> of</c><00:01:14.869><c> this</c>

00:01:16.780 --> 00:01:16.790 align:start position:0%
from I equals 1 through n of this
 

00:01:16.790 --> 00:01:21.770 align:start position:0%
from I equals 1 through n of this
squared<00:01:17.790><c> error</c><00:01:18.030><c> cost</c><00:01:19.460><c> function</c><00:01:20.460><c> term</c><00:01:20.970><c> and</c><00:01:21.210><c> all</c>

00:01:21.770 --> 00:01:21.780 align:start position:0%
squared error cost function term and all
 

00:01:21.780 --> 00:01:23.899 align:start position:0%
squared error cost function term and all
I<00:01:21.810><c> did</c><00:01:22.080><c> here</c><00:01:22.110><c> was</c><00:01:22.350><c> I</c><00:01:22.710><c> just</c><00:01:22.979><c> you</c><00:01:23.580><c> know</c><00:01:23.610><c> plugged</c>

00:01:23.899 --> 00:01:23.909 align:start position:0%
I did here was I just you know plugged
 

00:01:23.909 --> 00:01:25.550 align:start position:0%
I did here was I just you know plugged
in<00:01:24.060><c> the</c><00:01:24.150><c> definition</c><00:01:24.720><c> of</c><00:01:24.960><c> the</c><00:01:25.259><c> cost</c><00:01:25.409><c> function</c>

00:01:25.550 --> 00:01:25.560 align:start position:0%
in the definition of the cost function
 

00:01:25.560 --> 00:01:28.749 align:start position:0%
in the definition of the cost function
there<00:01:26.100><c> and</c><00:01:26.900><c> simplifying</c><00:01:27.900><c> a</c><00:01:28.020><c> little</c><00:01:28.229><c> bit</c><00:01:28.320><c> more</c>

00:01:28.749 --> 00:01:28.759 align:start position:0%
there and simplifying a little bit more
 

00:01:28.759 --> 00:01:34.910 align:start position:0%
there and simplifying a little bit more
this<00:01:29.759><c> turns</c><00:01:30.030><c> out</c><00:01:30.270><c> to</c><00:01:30.329><c> be</c><00:01:30.570><c> equal</c><00:01:31.079><c> to</c><00:01:32.479><c> this</c><00:01:33.920><c> sum</c>

00:01:34.910 --> 00:01:34.920 align:start position:0%
this turns out to be equal to this sum
 

00:01:34.920 --> 00:01:37.190 align:start position:0%
this turns out to be equal to this sum
from<00:01:35.100><c> I</c><00:01:35.250><c> equals</c><00:01:35.520><c> 1</c><00:01:35.549><c> through</c><00:01:35.700><c> m</c><00:01:36.030><c> of</c><00:01:36.299><c> theta</c><00:01:36.750><c> 0</c>

00:01:37.190 --> 00:01:37.200 align:start position:0%
from I equals 1 through m of theta 0
 

00:01:37.200 --> 00:01:43.280 align:start position:0%
from I equals 1 through m of theta 0
plus<00:01:37.380><c> theta</c><00:01:37.590><c> 1</c><00:01:38.100><c> X</c><00:01:39.060><c> I</c><00:01:40.280><c> minus</c><00:01:41.280><c> y</c><00:01:41.490><c> I</c><00:01:42.049><c> squared</c><00:01:43.049><c> and</c>

00:01:43.280 --> 00:01:43.290 align:start position:0%
plus theta 1 X I minus y I squared and
 

00:01:43.290 --> 00:01:44.719 align:start position:0%
plus theta 1 X I minus y I squared and
all<00:01:43.560><c> I</c><00:01:43.619><c> did</c><00:01:43.829><c> there</c><00:01:44.040><c> was</c><00:01:44.310><c> I</c><00:01:44.369><c> took</c><00:01:44.490><c> the</c>

00:01:44.719 --> 00:01:44.729 align:start position:0%
all I did there was I took the
 

00:01:44.729 --> 00:01:47.780 align:start position:0%
all I did there was I took the
definition<00:01:44.759><c> for</c><00:01:45.570><c> my</c><00:01:45.799><c> hypothesis</c><00:01:46.799><c> and</c><00:01:47.220><c> plugged</c>

00:01:47.780 --> 00:01:47.790 align:start position:0%
definition for my hypothesis and plugged
 

00:01:47.790 --> 00:01:50.780 align:start position:0%
definition for my hypothesis and plugged
it<00:01:47.939><c> in</c><00:01:48.060><c> there</c><00:01:48.299><c> and</c><00:01:49.399><c> turns</c><00:01:50.399><c> out</c><00:01:50.520><c> we</c><00:01:50.640><c> need</c><00:01:50.759><c> to</c>

00:01:50.780 --> 00:01:50.790 align:start position:0%
it in there and turns out we need to
 

00:01:50.790 --> 00:01:52.069 align:start position:0%
it in there and turns out we need to
figure<00:01:51.299><c> out</c><00:01:51.450><c> what</c><00:01:51.659><c> is</c><00:01:51.780><c> this</c><00:01:51.930><c> partial</c>

00:01:52.069 --> 00:01:52.079 align:start position:0%
figure out what is this partial
 

00:01:52.079 --> 00:01:55.490 align:start position:0%
figure out what is this partial
derivative<00:01:52.530><c> for</c><00:01:53.189><c> two</c><00:01:53.340><c> cases</c><00:01:53.549><c> for</c><00:01:54.210><c> J</c><00:01:54.869><c> equals</c><00:01:55.140><c> 0</c>

00:01:55.490 --> 00:01:55.500 align:start position:0%
derivative for two cases for J equals 0
 

00:01:55.500 --> 00:01:57.440 align:start position:0%
derivative for two cases for J equals 0
and<00:01:55.770><c> for</c><00:01:56.070><c> J</c><00:01:56.219><c> equals</c><00:01:56.520><c> 1</c><00:01:56.579><c> so</c><00:01:56.939><c> we</c><00:01:57.060><c> want</c><00:01:57.240><c> to</c><00:01:57.329><c> figure</c>

00:01:57.440 --> 00:01:57.450 align:start position:0%
and for J equals 1 so we want to figure
 

00:01:57.450 --> 00:01:59.539 align:start position:0%
and for J equals 1 so we want to figure
out<00:01:57.570><c> what</c><00:01:58.110><c> is</c><00:01:58.200><c> this</c><00:01:58.320><c> partial</c><00:01:58.439><c> derivative</c><00:01:59.250><c> for</c>

00:01:59.539 --> 00:01:59.549 align:start position:0%
out what is this partial derivative for
 

00:01:59.549 --> 00:02:02.179 align:start position:0%
out what is this partial derivative for
you<00:02:00.090><c> know</c><00:02:00.210><c> both</c><00:02:00.450><c> the</c><00:02:00.630><c> theta</c><00:02:00.899><c> 0</c><00:02:01.320><c> case</c><00:02:01.590><c> and</c><00:02:01.890><c> the</c>

00:02:02.179 --> 00:02:02.189 align:start position:0%
you know both the theta 0 case and the
 

00:02:02.189 --> 00:02:05.450 align:start position:0%
you know both the theta 0 case and the
theta<00:02:02.790><c> 1</c><00:02:03.119><c> case</c><00:02:03.390><c> and</c><00:02:03.950><c> I'm</c><00:02:04.950><c> just</c><00:02:05.159><c> going</c><00:02:05.280><c> to</c><00:02:05.310><c> write</c>

00:02:05.450 --> 00:02:05.460 align:start position:0%
theta 1 case and I'm just going to write
 

00:02:05.460 --> 00:02:07.880 align:start position:0%
theta 1 case and I'm just going to write
out<00:02:05.579><c> the</c><00:02:05.700><c> answers</c><00:02:06.090><c> it</c><00:02:06.990><c> turns</c><00:02:07.200><c> out</c><00:02:07.409><c> this</c><00:02:07.590><c> first</c>

00:02:07.880 --> 00:02:07.890 align:start position:0%
out the answers it turns out this first
 

00:02:07.890 --> 00:02:09.500 align:start position:0%
out the answers it turns out this first
term<00:02:08.129><c> is</c>

00:02:09.500 --> 00:02:09.510 align:start position:0%
term is
 

00:02:09.510 --> 00:02:14.400 align:start position:0%
term is
simplifies<00:02:10.510><c> to</c><00:02:10.540><c> 1</c><00:02:11.230><c> over</c><00:02:11.440><c> m</c><00:02:12.360><c> sum</c><00:02:13.360><c> from</c><00:02:13.870><c> open</c><00:02:14.320><c> my</c>

00:02:14.400 --> 00:02:14.410 align:start position:0%
simplifies to 1 over m sum from open my
 

00:02:14.410 --> 00:02:19.550 align:start position:0%
simplifies to 1 over m sum from open my
training<00:02:14.770><c> set</c><00:02:15.010><c> of</c><00:02:15.360><c> just</c><00:02:16.360><c> that</c><00:02:16.990><c> X</c><00:02:17.590><c> I</c><00:02:18.070><c> minus</c><00:02:18.790><c> y</c><00:02:19.090><c> i</c>

00:02:19.550 --> 00:02:19.560 align:start position:0%
training set of just that X I minus y i
 

00:02:19.560 --> 00:02:23.070 align:start position:0%
training set of just that X I minus y i
and<00:02:20.610><c> for</c><00:02:21.610><c> this</c><00:02:21.760><c> term</c><00:02:22.000><c> partial</c><00:02:22.420><c> derivative</c>

00:02:23.070 --> 00:02:23.080 align:start position:0%
and for this term partial derivative
 

00:02:23.080 --> 00:02:26.610 align:start position:0%
and for this term partial derivative
respect<00:02:23.320><c> to</c><00:02:24.160><c> theta</c><00:02:24.310><c> 1</c><00:02:24.840><c> it</c><00:02:25.840><c> turns</c><00:02:26.110><c> out</c><00:02:26.260><c> I</c><00:02:26.440><c> get</c>

00:02:26.610 --> 00:02:26.620 align:start position:0%
respect to theta 1 it turns out I get
 

00:02:26.620 --> 00:02:35.370 align:start position:0%
respect to theta 1 it turns out I get
this<00:02:26.830><c> term</c><00:02:29.700><c> minus</c><00:02:30.700><c> y</c><00:02:30.940><c> I</c><00:02:31.530><c> times</c><00:02:32.530><c> X</c><00:02:33.400><c> I</c><00:02:34.200><c> okay</c>

00:02:35.370 --> 00:02:35.380 align:start position:0%
this term minus y I times X I okay
 

00:02:35.380 --> 00:02:39.360 align:start position:0%
this term minus y I times X I okay
and<00:02:36.960><c> computing</c><00:02:37.960><c> these</c><00:02:38.230><c> partial</c><00:02:38.770><c> derivatives</c>

00:02:39.360 --> 00:02:39.370 align:start position:0%
and computing these partial derivatives
 

00:02:39.370 --> 00:02:41.670 align:start position:0%
and computing these partial derivatives
to<00:02:39.580><c> let</c><00:02:39.730><c> you</c><00:02:39.880><c> know</c><00:02:39.940><c> going</c><00:02:40.270><c> from</c><00:02:40.480><c> this</c><00:02:41.170><c> equation</c>

00:02:41.670 --> 00:02:41.680 align:start position:0%
to let you know going from this equation
 

00:02:41.680 --> 00:02:44.280 align:start position:0%
to let you know going from this equation
right<00:02:42.580><c> going</c><00:02:42.850><c> from</c><00:02:43.000><c> this</c><00:02:43.210><c> equation</c><00:02:43.720><c> to</c><00:02:44.050><c> either</c>

00:02:44.280 --> 00:02:44.290 align:start position:0%
right going from this equation to either
 

00:02:44.290 --> 00:02:46.470 align:start position:0%
right going from this equation to either
of<00:02:44.530><c> the</c><00:02:44.620><c> equations</c><00:02:44.740><c> down</c><00:02:45.280><c> there</c><00:02:45.550><c> computing</c>

00:02:46.470 --> 00:02:46.480 align:start position:0%
of the equations down there computing
 

00:02:46.480 --> 00:02:48.720 align:start position:0%
of the equations down there computing
those<00:02:46.660><c> partial</c><00:02:47.140><c> derivative</c><00:02:47.560><c> terms</c><00:02:47.860><c> requires</c>

00:02:48.720 --> 00:02:48.730 align:start position:0%
those partial derivative terms requires
 

00:02:48.730 --> 00:02:51.570 align:start position:0%
those partial derivative terms requires
some<00:02:49.120><c> multivariate</c><00:02:49.810><c> calculus</c><00:02:50.220><c> if</c><00:02:51.220><c> you</c><00:02:51.400><c> know</c>

00:02:51.570 --> 00:02:51.580 align:start position:0%
some multivariate calculus if you know
 

00:02:51.580 --> 00:02:53.640 align:start position:0%
some multivariate calculus if you know
calculus<00:02:51.820><c> feel</c><00:02:52.540><c> free</c><00:02:52.840><c> to</c><00:02:52.870><c> work</c><00:02:53.260><c> through</c><00:02:53.530><c> the</c>

00:02:53.640 --> 00:02:53.650 align:start position:0%
calculus feel free to work through the
 

00:02:53.650 --> 00:02:55.440 align:start position:0%
calculus feel free to work through the
derivations<00:02:53.920><c> yourself</c><00:02:54.670><c> and</c><00:02:54.880><c> check</c><00:02:55.060><c> that</c><00:02:55.330><c> if</c>

00:02:55.440 --> 00:02:55.450 align:start position:0%
derivations yourself and check that if
 

00:02:55.450 --> 00:02:57.270 align:start position:0%
derivations yourself and check that if
you<00:02:55.540><c> take</c><00:02:55.690><c> the</c><00:02:55.810><c> derivatives</c><00:02:56.350><c> you</c><00:02:56.860><c> actually</c>

00:02:57.270 --> 00:02:57.280 align:start position:0%
you take the derivatives you actually
 

00:02:57.280 --> 00:03:00.270 align:start position:0%
you take the derivatives you actually
get<00:02:57.550><c> the</c><00:02:57.700><c> answers</c><00:02:58.090><c> that</c><00:02:58.120><c> I</c><00:02:58.720><c> got</c><00:02:58.930><c> but</c><00:02:59.590><c> if</c><00:03:00.040><c> you</c>

00:03:00.270 --> 00:03:00.280 align:start position:0%
get the answers that I got but if you
 

00:03:00.280 --> 00:03:02.280 align:start position:0%
get the answers that I got but if you
are<00:03:00.400><c> less</c><00:03:00.610><c> familiar</c><00:03:00.790><c> with</c><00:03:01.150><c> calculus</c><00:03:01.420><c> you</c><00:03:02.200><c> know</c>

00:03:02.280 --> 00:03:02.290 align:start position:0%
are less familiar with calculus you know
 

00:03:02.290 --> 00:03:04.590 align:start position:0%
are less familiar with calculus you know
don't<00:03:02.530><c> worry</c><00:03:02.710><c> about</c><00:03:02.800><c> it</c><00:03:03.160><c> and</c><00:03:03.970><c> it's</c><00:03:04.180><c> fine</c><00:03:04.390><c> to</c>

00:03:04.590 --> 00:03:04.600 align:start position:0%
don't worry about it and it's fine to
 

00:03:04.600 --> 00:03:06.480 align:start position:0%
don't worry about it and it's fine to
just<00:03:04.630><c> take</c><00:03:05.380><c> these</c><00:03:05.530><c> equations</c><00:03:05.770><c> that</c><00:03:06.130><c> worked</c>

00:03:06.480 --> 00:03:06.490 align:start position:0%
just take these equations that worked
 

00:03:06.490 --> 00:03:08.490 align:start position:0%
just take these equations that worked
out<00:03:06.700><c> and</c><00:03:07.090><c> you</c><00:03:07.840><c> won't</c><00:03:08.050><c> need</c><00:03:08.200><c> to</c><00:03:08.260><c> know</c><00:03:08.380><c> calculus</c>

00:03:08.490 --> 00:03:08.500 align:start position:0%
out and you won't need to know calculus
 

00:03:08.500 --> 00:03:10.830 align:start position:0%
out and you won't need to know calculus
or<00:03:09.220><c> anything</c><00:03:09.370><c> like</c><00:03:09.550><c> that</c><00:03:09.610><c> in</c><00:03:10.060><c> order</c><00:03:10.510><c> to</c><00:03:10.630><c> do</c><00:03:10.750><c> the</c>

00:03:10.830 --> 00:03:10.840 align:start position:0%
or anything like that in order to do the
 

00:03:10.840 --> 00:03:12.090 align:start position:0%
or anything like that in order to do the
homeworks<00:03:11.200><c> or</c><00:03:11.350><c> to</c><00:03:11.530><c> implement</c><00:03:11.800><c> gradient</c>

00:03:12.090 --> 00:03:12.100 align:start position:0%
homeworks or to implement gradient
 

00:03:12.100 --> 00:03:14.880 align:start position:0%
homeworks or to implement gradient
descent<00:03:12.580><c> and</c><00:03:12.730><c> get</c><00:03:12.850><c> that</c><00:03:12.940><c> to</c><00:03:13.120><c> work</c><00:03:13.770><c> but</c><00:03:14.770><c> so</c>

00:03:14.880 --> 00:03:14.890 align:start position:0%
descent and get that to work but so
 

00:03:14.890 --> 00:03:16.830 align:start position:0%
descent and get that to work but so
armed<00:03:15.310><c> with</c><00:03:15.370><c> these</c><00:03:15.580><c> definitions</c><00:03:15.790><c> or</c><00:03:16.570><c> armed</c>

00:03:16.830 --> 00:03:16.840 align:start position:0%
armed with these definitions or armed
 

00:03:16.840 --> 00:03:18.600 align:start position:0%
armed with these definitions or armed
with<00:03:16.959><c> what</c><00:03:17.290><c> we</c><00:03:17.680><c> worked</c><00:03:17.920><c> out</c><00:03:18.130><c> to</c><00:03:18.310><c> be</c><00:03:18.340><c> the</c>

00:03:18.600 --> 00:03:18.610 align:start position:0%
with what we worked out to be the
 

00:03:18.610 --> 00:03:20.490 align:start position:0%
with what we worked out to be the
derivatives<00:03:19.540><c> which</c><00:03:19.780><c> is</c><00:03:19.870><c> really</c><00:03:20.110><c> just</c><00:03:20.170><c> the</c>

00:03:20.490 --> 00:03:20.500 align:start position:0%
derivatives which is really just the
 

00:03:20.500 --> 00:03:23.760 align:start position:0%
derivatives which is really just the
slope<00:03:20.650><c> of</c><00:03:21.040><c> the</c><00:03:21.280><c> cost</c><00:03:21.670><c> function</c><00:03:21.880><c> J</c><00:03:22.500><c> we</c><00:03:23.500><c> can</c><00:03:23.650><c> now</c>

00:03:23.760 --> 00:03:23.770 align:start position:0%
slope of the cost function J we can now
 

00:03:23.770 --> 00:03:25.860 align:start position:0%
slope of the cost function J we can now
plug<00:03:24.040><c> them</c><00:03:24.250><c> back</c><00:03:24.430><c> into</c><00:03:24.700><c> our</c><00:03:25.180><c> gradient</c><00:03:25.600><c> descent</c>

00:03:25.860 --> 00:03:25.870 align:start position:0%
plug them back into our gradient descent
 

00:03:25.870 --> 00:03:26.340 align:start position:0%
plug them back into our gradient descent
algorithm

00:03:26.340 --> 00:03:26.350 align:start position:0%
algorithm
 

00:03:26.350 --> 00:03:28.770 align:start position:0%
algorithm
so<00:03:27.280><c> here's</c><00:03:27.550><c> gradient</c><00:03:27.970><c> descent</c><00:03:28.300><c> for</c><00:03:28.570><c> linear</c>

00:03:28.770 --> 00:03:28.780 align:start position:0%
so here's gradient descent for linear
 

00:03:28.780 --> 00:03:29.400 align:start position:0%
so here's gradient descent for linear
regression

00:03:29.400 --> 00:03:29.410 align:start position:0%
regression
 

00:03:29.410 --> 00:03:31.490 align:start position:0%
regression
we're<00:03:29.590><c> going</c><00:03:29.800><c> to</c><00:03:29.860><c> repeat</c><00:03:30.190><c> until</c><00:03:30.220><c> convergence</c>

00:03:31.490 --> 00:03:31.500 align:start position:0%
we're going to repeat until convergence
 

00:03:31.500 --> 00:03:35.160 align:start position:0%
we're going to repeat until convergence
theta<00:03:32.500><c> 0</c><00:03:32.860><c> and</c><00:03:32.950><c> theta</c><00:03:33.220><c> 1</c><00:03:33.520><c> get</c><00:03:34.030><c> updated</c><00:03:34.450><c> as</c><00:03:35.110><c> you</c>

00:03:35.160 --> 00:03:35.170 align:start position:0%
theta 0 and theta 1 get updated as you
 

00:03:35.170 --> 00:03:37.620 align:start position:0%
theta 0 and theta 1 get updated as you
know<00:03:35.620><c> just</c><00:03:35.950><c> a</c><00:03:36.070><c> minus</c><00:03:36.490><c> alpha</c><00:03:36.910><c> times</c><00:03:37.300><c> the</c>

00:03:37.620 --> 00:03:37.630 align:start position:0%
know just a minus alpha times the
 

00:03:37.630 --> 00:03:42.560 align:start position:0%
know just a minus alpha times the
derivative<00:03:37.810><c> term</c><00:03:38.520><c> so</c><00:03:39.520><c> this</c><00:03:40.330><c> term</c><00:03:40.600><c> here</c><00:03:40.930><c> so</c>

00:03:42.560 --> 00:03:42.570 align:start position:0%
derivative term so this term here so
 

00:03:42.570 --> 00:03:46.310 align:start position:0%
derivative term so this term here so
here's<00:03:43.570><c> our</c><00:03:43.780><c> linear</c><00:03:44.260><c> regression</c><00:03:44.890><c> algorithm</c>

00:03:46.310 --> 00:03:46.320 align:start position:0%
here's our linear regression algorithm
 

00:03:46.320 --> 00:03:51.990 align:start position:0%
here's our linear regression algorithm
this<00:03:47.320><c> first</c><00:03:47.740><c> term</c><00:03:48.040><c> here</c>

00:03:51.990 --> 00:03:52.000 align:start position:0%
 
 

00:03:52.000 --> 00:03:54.750 align:start position:0%
 
that<00:03:52.720><c> term</c><00:03:53.080><c> is</c><00:03:53.110><c> of</c><00:03:53.650><c> course</c><00:03:53.920><c> just</c><00:03:54.010><c> the</c><00:03:54.520><c> partial</c>

00:03:54.750 --> 00:03:54.760 align:start position:0%
that term is of course just the partial
 

00:03:54.760 --> 00:03:57.450 align:start position:0%
that term is of course just the partial
derivative<00:03:55.390><c> respect</c><00:03:55.420><c> to</c><00:03:55.930><c> theta</c><00:03:56.110><c> 0</c><00:03:56.340><c> it</c><00:03:57.340><c> will</c>

00:03:57.450 --> 00:03:57.460 align:start position:0%
derivative respect to theta 0 it will
 

00:03:57.460 --> 00:04:00.360 align:start position:0%
derivative respect to theta 0 it will
worked<00:03:57.700><c> out</c><00:03:57.880><c> on</c><00:03:58.030><c> a</c><00:03:58.060><c> previous</c><00:03:58.510><c> slide</c><00:03:58.540><c> and</c><00:03:59.370><c> this</c>

00:04:00.360 --> 00:04:00.370 align:start position:0%
worked out on a previous slide and this
 

00:04:00.370 --> 00:04:04.470 align:start position:0%
worked out on a previous slide and this
second<00:04:01.030><c> term</c><00:04:01.180><c> here</c><00:04:02.460><c> that</c><00:04:03.460><c> term</c><00:04:03.760><c> is</c><00:04:04.060><c> just</c><00:04:04.300><c> the</c>

00:04:04.470 --> 00:04:04.480 align:start position:0%
second term here that term is just the
 

00:04:04.480 --> 00:04:08.090 align:start position:0%
second term here that term is just the
partial<00:04:04.690><c> derivative</c><00:04:05.290><c> respect</c><00:04:05.530><c> to</c><00:04:06.160><c> theta</c><00:04:06.370><c> 1</c>

00:04:08.090 --> 00:04:08.100 align:start position:0%
partial derivative respect to theta 1
 

00:04:08.100 --> 00:04:10.470 align:start position:0%
partial derivative respect to theta 1
then<00:04:09.100><c> we</c><00:04:09.190><c> worked</c><00:04:09.400><c> out</c><00:04:09.580><c> on</c><00:04:09.820><c> the</c><00:04:10.030><c> previous</c><00:04:10.390><c> line</c>

00:04:10.470 --> 00:04:10.480 align:start position:0%
then we worked out on the previous line
 

00:04:10.480 --> 00:04:13.620 align:start position:0%
then we worked out on the previous line
and<00:04:10.930><c> just</c><00:04:11.740><c> as</c><00:04:11.920><c> a</c><00:04:11.950><c> quick</c><00:04:12.040><c> reminder</c><00:04:12.459><c> you</c><00:04:12.940><c> must</c>

00:04:13.620 --> 00:04:13.630 align:start position:0%
and just as a quick reminder you must
 

00:04:13.630 --> 00:04:15.390 align:start position:0%
and just as a quick reminder you must
when<00:04:14.020><c> implementing</c><00:04:14.530><c> great</c><00:04:14.740><c> incentives</c>

00:04:15.390 --> 00:04:15.400 align:start position:0%
when implementing great incentives
 

00:04:15.400 --> 00:04:16.949 align:start position:0%
when implementing great incentives
actually<00:04:15.640><c> this</c><00:04:15.760><c> detail</c><00:04:16.180><c> that</c><00:04:16.209><c> you</c><00:04:16.780><c> know</c><00:04:16.900><c> you</c>

00:04:16.949 --> 00:04:16.959 align:start position:0%
actually this detail that you know you
 

00:04:16.959 --> 00:04:19.680 align:start position:0%
actually this detail that you know you
should<00:04:17.230><c> be</c><00:04:17.890><c> implementing</c><00:04:18.700><c> it</c><00:04:18.850><c> so</c><00:04:18.880><c> the</c><00:04:19.209><c> update</c>

00:04:19.680 --> 00:04:19.690 align:start position:0%
should be implementing it so the update
 

00:04:19.690 --> 00:04:24.710 align:start position:0%
should be implementing it so the update
theta<00:04:19.930><c> 0</c><00:04:20.109><c> and</c><00:04:20.470><c> theta</c><00:04:20.709><c> 1</c><00:04:21.270><c> simultaneously</c><00:04:23.520><c> so</c>

00:04:24.710 --> 00:04:24.720 align:start position:0%
theta 0 and theta 1 simultaneously so
 

00:04:24.720 --> 00:04:28.170 align:start position:0%
theta 0 and theta 1 simultaneously so
let's<00:04:25.720><c> see</c><00:04:25.870><c> how</c><00:04:26.050><c> gradient</c><00:04:26.470><c> descent</c><00:04:26.770><c> works</c><00:04:27.180><c> one</c>

00:04:28.170 --> 00:04:28.180 align:start position:0%
let's see how gradient descent works one
 

00:04:28.180 --> 00:04:30.030 align:start position:0%
let's see how gradient descent works one
of<00:04:28.210><c> the</c><00:04:28.419><c> issues</c><00:04:28.810><c> we</c><00:04:28.990><c> solve</c><00:04:29.260><c> gradient</c><00:04:29.740><c> descent</c>

00:04:30.030 --> 00:04:30.040 align:start position:0%
of the issues we solve gradient descent
 

00:04:30.040 --> 00:04:31.950 align:start position:0%
of the issues we solve gradient descent
is<00:04:30.220><c> that</c><00:04:30.490><c> it</c><00:04:30.669><c> can</c><00:04:30.850><c> be</c><00:04:31.000><c> susceptible</c><00:04:31.360><c> to</c><00:04:31.660><c> local</c>

00:04:31.950 --> 00:04:31.960 align:start position:0%
is that it can be susceptible to local
 

00:04:31.960 --> 00:04:34.200 align:start position:0%
is that it can be susceptible to local
optima<00:04:32.680><c> so</c><00:04:32.919><c> when</c><00:04:33.190><c> I</c><00:04:33.220><c> first</c><00:04:33.430><c> explained</c><00:04:33.820><c> radians</c>

00:04:34.200 --> 00:04:34.210 align:start position:0%
optima so when I first explained radians
 

00:04:34.210 --> 00:04:36.420 align:start position:0%
optima so when I first explained radians
then<00:04:34.300><c> I</c><00:04:34.480><c> showed</c><00:04:34.720><c> you</c><00:04:34.870><c> this</c><00:04:34.990><c> picture</c><00:04:35.080><c> of</c><00:04:35.560><c> it</c><00:04:36.220><c> you</c>

00:04:36.420 --> 00:04:36.430 align:start position:0%
then I showed you this picture of it you
 

00:04:36.430 --> 00:04:38.730 align:start position:0%
then I showed you this picture of it you
know<00:04:36.550><c> going</c><00:04:36.910><c> down</c><00:04:37.120><c> along</c><00:04:37.419><c> the</c><00:04:37.600><c> surface</c><00:04:37.810><c> and</c><00:04:38.260><c> we</c>

00:04:38.730 --> 00:04:38.740 align:start position:0%
know going down along the surface and we
 

00:04:38.740 --> 00:04:40.200 align:start position:0%
know going down along the surface and we
saw<00:04:38.950><c> how</c><00:04:39.220><c> depending</c><00:04:39.790><c> on</c><00:04:39.880><c> where</c><00:04:40.060><c> you</c>

00:04:40.200 --> 00:04:40.210 align:start position:0%
saw how depending on where you
 

00:04:40.210 --> 00:04:41.700 align:start position:0%
saw how depending on where you
initialize<00:04:40.600><c> it</c><00:04:40.900><c> you</c><00:04:41.020><c> can</c><00:04:41.169><c> end</c><00:04:41.350><c> up</c><00:04:41.500><c> at</c>

00:04:41.700 --> 00:04:41.710 align:start position:0%
initialize it you can end up at
 

00:04:41.710 --> 00:04:44.040 align:start position:0%
initialize it you can end up at
different<00:04:41.860><c> local</c><00:04:42.190><c> optima</c><00:04:42.700><c> and</c><00:04:42.880><c> 1wo</c><00:04:43.450><c> here</c><00:04:43.870><c> or</c>

00:04:44.040 --> 00:04:44.050 align:start position:0%
different local optima and 1wo here or
 

00:04:44.050 --> 00:04:47.550 align:start position:0%
different local optima and 1wo here or
here<00:04:44.380><c> but</c><00:04:45.060><c> it</c><00:04:46.060><c> turns</c><00:04:46.360><c> out</c><00:04:46.600><c> that</c><00:04:46.960><c> the</c><00:04:47.320><c> cost</c>

00:04:47.550 --> 00:04:47.560 align:start position:0%
here but it turns out that the cost
 

00:04:47.560 --> 00:04:50.220 align:start position:0%
here but it turns out that the cost
function<00:04:47.710><c> for</c><00:04:48.520><c> gradient</c><00:04:49.270><c> of</c><00:04:49.510><c> cost</c><00:04:49.900><c> function</c>

00:04:50.220 --> 00:04:50.230 align:start position:0%
function for gradient of cost function
 

00:04:50.230 --> 00:04:52.740 align:start position:0%
function for gradient of cost function
for<00:04:50.260><c> linear</c><00:04:50.500><c> regression</c><00:04:51.160><c> is</c><00:04:51.370><c> always</c><00:04:52.330><c> going</c><00:04:52.690><c> to</c>

00:04:52.740 --> 00:04:52.750 align:start position:0%
for linear regression is always going to
 

00:04:52.750 --> 00:04:55.110 align:start position:0%
for linear regression is always going to
be<00:04:52.840><c> a</c><00:04:52.960><c> bow</c><00:04:53.470><c> shaped</c><00:04:53.830><c> function</c><00:04:53.950><c> like</c><00:04:54.550><c> this</c><00:04:54.820><c> um</c>

00:04:55.110 --> 00:04:55.120 align:start position:0%
be a bow shaped function like this um
 

00:04:55.120 --> 00:04:58.020 align:start position:0%
be a bow shaped function like this um
the<00:04:56.110><c> technical</c><00:04:56.650><c> term</c><00:04:56.890><c> for</c><00:04:56.919><c> this</c><00:04:57.250><c> is</c><00:04:57.310><c> that</c><00:04:57.640><c> this</c>

00:04:58.020 --> 00:04:58.030 align:start position:0%
the technical term for this is that this
 

00:04:58.030 --> 00:05:04.890 align:start position:0%
the technical term for this is that this
is<00:04:58.210><c> called</c><00:04:58.479><c> a</c><00:04:58.720><c> convex</c><00:04:59.440><c> function</c><00:04:59.860><c> and</c><00:05:03.900><c> not</c>

00:05:04.890 --> 00:05:04.900 align:start position:0%
is called a convex function and not
 

00:05:04.900 --> 00:05:06.330 align:start position:0%
is called a convex function and not
going<00:05:05.020><c> to</c><00:05:05.080><c> give</c><00:05:05.229><c> the</c><00:05:05.350><c> formal</c><00:05:05.650><c> definition</c><00:05:06.190><c> so</c>

00:05:06.330 --> 00:05:06.340 align:start position:0%
going to give the formal definition so
 

00:05:06.340 --> 00:05:09.570 align:start position:0%
going to give the formal definition so
what<00:05:06.520><c> is</c><00:05:06.640><c> a</c><00:05:06.760><c> convex</c><00:05:07.270><c> function</c><00:05:07.450><c> Co</c><00:05:08.110><c> and</c><00:05:08.440><c> V</c><00:05:08.650><c> X</c><00:05:08.950><c> but</c>

00:05:09.570 --> 00:05:09.580 align:start position:0%
what is a convex function Co and V X but
 

00:05:09.580 --> 00:05:11.820 align:start position:0%
what is a convex function Co and V X but
then<00:05:09.729><c> formally</c><00:05:10.240><c> a</c><00:05:10.270><c> convex</c><00:05:10.840><c> function</c><00:05:10.960><c> means</c><00:05:11.590><c> a</c>

00:05:11.820 --> 00:05:11.830 align:start position:0%
then formally a convex function means a
 

00:05:11.830 --> 00:05:14.130 align:start position:0%
then formally a convex function means a
bowl-shaped<00:05:12.160><c> function</c><00:05:12.940><c> you</c><00:05:13.450><c> know</c><00:05:13.570><c> kinda</c><00:05:14.050><c> like</c>

00:05:14.130 --> 00:05:14.140 align:start position:0%
bowl-shaped function you know kinda like
 

00:05:14.140 --> 00:05:17.159 align:start position:0%
bowl-shaped function you know kinda like
a<00:05:14.260><c> bow</c><00:05:14.440><c> shaped</c><00:05:14.770><c> and</c><00:05:15.180><c> so</c><00:05:16.180><c> this</c><00:05:16.750><c> function</c>

00:05:17.159 --> 00:05:17.169 align:start position:0%
a bow shaped and so this function
 

00:05:17.169 --> 00:05:20.159 align:start position:0%
a bow shaped and so this function
doesn't<00:05:17.800><c> have</c><00:05:18.010><c> any</c><00:05:18.160><c> local</c><00:05:18.490><c> optima</c><00:05:19.330><c> except</c><00:05:19.900><c> for</c>

00:05:20.159 --> 00:05:20.169 align:start position:0%
doesn't have any local optima except for
 

00:05:20.169 --> 00:05:23.159 align:start position:0%
doesn't have any local optima except for
the<00:05:20.560><c> one</c><00:05:20.740><c> global</c><00:05:21.250><c> optimum</c><00:05:21.669><c> and</c><00:05:21.850><c> does</c><00:05:22.690><c> gradient</c>

00:05:23.159 --> 00:05:23.169 align:start position:0%
the one global optimum and does gradient
 

00:05:23.169 --> 00:05:25.380 align:start position:0%
the one global optimum and does gradient
descent<00:05:23.470><c> on</c><00:05:23.650><c> this</c><00:05:24.400><c> type</c><00:05:24.640><c> of</c><00:05:24.790><c> cost</c><00:05:24.970><c> function</c>

00:05:25.380 --> 00:05:25.390 align:start position:0%
descent on this type of cost function
 

00:05:25.390 --> 00:05:26.909 align:start position:0%
descent on this type of cost function
which<00:05:25.660><c> you</c><00:05:25.810><c> get</c><00:05:25.990><c> whenever</c><00:05:26.410><c> you're</c><00:05:26.680><c> using</c>

00:05:26.909 --> 00:05:26.919 align:start position:0%
which you get whenever you're using
 

00:05:26.919 --> 00:05:28.650 align:start position:0%
which you get whenever you're using
linear<00:05:27.520><c> regression</c><00:05:27.550><c> it</c><00:05:28.180><c> will</c><00:05:28.300><c> always</c>

00:05:28.650 --> 00:05:28.660 align:start position:0%
linear regression it will always
 

00:05:28.660 --> 00:05:30.630 align:start position:0%
linear regression it will always
converge<00:05:28.990><c> to</c><00:05:29.410><c> the</c><00:05:29.530><c> global</c><00:05:29.560><c> optimum</c><00:05:30.340><c> because</c>

00:05:30.630 --> 00:05:30.640 align:start position:0%
converge to the global optimum because
 

00:05:30.640 --> 00:05:32.580 align:start position:0%
converge to the global optimum because
there<00:05:30.790><c> are</c><00:05:30.940><c> no</c><00:05:31.120><c> other</c><00:05:31.390><c> local</c><00:05:31.840><c> optima</c><00:05:32.169><c> auditing</c>

00:05:32.580 --> 00:05:32.590 align:start position:0%
there are no other local optima auditing
 

00:05:32.590 --> 00:05:35.430 align:start position:0%
there are no other local optima auditing
or<00:05:32.710><c> global</c><00:05:33.040><c> optimum</c><00:05:33.460><c> so</c><00:05:34.419><c> now</c><00:05:34.600><c> let's</c><00:05:35.080><c> see</c><00:05:35.290><c> this</c>

00:05:35.430 --> 00:05:35.440 align:start position:0%
or global optimum so now let's see this
 

00:05:35.440 --> 00:05:39.150 align:start position:0%
or global optimum so now let's see this
algorithm<00:05:35.979><c> in</c><00:05:36.160><c> action</c><00:05:36.370><c> as</c><00:05:37.680><c> usual</c><00:05:38.680><c> here</c><00:05:38.979><c> are</c>

00:05:39.150 --> 00:05:39.160 align:start position:0%
algorithm in action as usual here are
 

00:05:39.160 --> 00:05:42.330 align:start position:0%
algorithm in action as usual here are
plots<00:05:39.460><c> of</c><00:05:39.790><c> the</c><00:05:40.180><c> hypothesis</c><00:05:40.990><c> function</c><00:05:41.140><c> and</c><00:05:41.740><c> of</c>

00:05:42.330 --> 00:05:42.340 align:start position:0%
plots of the hypothesis function and of
 

00:05:42.340 --> 00:05:46.950 align:start position:0%
plots of the hypothesis function and of
my<00:05:42.900><c> cost</c><00:05:43.900><c> function</c><00:05:44.290><c> J</c><00:05:44.650><c> and</c><00:05:45.300><c> so</c><00:05:46.300><c> let's</c><00:05:46.690><c> say</c><00:05:46.840><c> I've</c>

00:05:46.950 --> 00:05:46.960 align:start position:0%
my cost function J and so let's say I've
 

00:05:46.960 --> 00:05:49.140 align:start position:0%
my cost function J and so let's say I've
initialized<00:05:47.260><c> my</c><00:05:47.710><c> parameters</c><00:05:48.370><c> at</c><00:05:48.700><c> this</c><00:05:48.910><c> value</c>

00:05:49.140 --> 00:05:49.150 align:start position:0%
initialized my parameters at this value
 

00:05:49.150 --> 00:05:51.570 align:start position:0%
initialized my parameters at this value
you<00:05:49.600><c> know</c><00:05:50.050><c> let's</c><00:05:50.260><c> say</c><00:05:50.380><c> instead</c><00:05:50.680><c> usually</c><00:05:51.430><c> you</c>

00:05:51.570 --> 00:05:51.580 align:start position:0%
you know let's say instead usually you
 

00:05:51.580 --> 00:05:54.450 align:start position:0%
you know let's say instead usually you
initialize<00:05:52.450><c> your</c><00:05:52.630><c> parameters</c><00:05:53.080><c> at</c><00:05:53.290><c> 0</c><00:05:53.710><c> 0</c><00:05:53.740><c> theta</c>

00:05:54.450 --> 00:05:54.460 align:start position:0%
initialize your parameters at 0 0 theta
 

00:05:54.460 --> 00:05:54.890 align:start position:0%
initialize your parameters at 0 0 theta
0

00:05:54.890 --> 00:05:54.900 align:start position:0%
0
 

00:05:54.900 --> 00:05:58.219 align:start position:0%
0
there<00:05:55.080><c> we</c><00:05:55.259><c> go</c><00:05:55.380><c> zero</c><00:05:55.740><c> but</c><00:05:56.460><c> for</c><00:05:56.490><c> illustration</c><00:05:57.270><c> in</c>

00:05:58.219 --> 00:05:58.229 align:start position:0%
there we go zero but for illustration in
 

00:05:58.229 --> 00:05:59.540 align:start position:0%
there we go zero but for illustration in
this<00:05:58.530><c> in</c><00:05:58.710><c> this</c><00:05:58.979><c> particular</c><00:05:59.250><c> implementation</c>

00:05:59.540 --> 00:05:59.550 align:start position:0%
this in this particular implementation
 

00:05:59.550 --> 00:06:01.879 align:start position:0%
this in this particular implementation
of<00:06:00.030><c> unison</c><00:06:00.479><c> I've</c><00:06:00.660><c> initialized</c><00:06:01.350><c> you</c><00:06:01.830><c> know</c>

00:06:01.879 --> 00:06:01.889 align:start position:0%
of unison I've initialized you know
 

00:06:01.889 --> 00:06:03.770 align:start position:0%
of unison I've initialized you know
theta<00:06:02.160><c> zero</c><00:06:02.340><c> at</c><00:06:02.850><c> about</c><00:06:03.150><c> nine</c><00:06:03.360><c> hundred</c><00:06:03.389><c> and</c>

00:06:03.770 --> 00:06:03.780 align:start position:0%
theta zero at about nine hundred and
 

00:06:03.780 --> 00:06:06.110 align:start position:0%
theta zero at about nine hundred and
theta<00:06:04.139><c> one</c><00:06:04.380><c> at</c><00:06:04.530><c> about</c><00:06:04.620><c> minus</c><00:06:04.860><c> 0.1</c><00:06:05.520><c> okay</c>

00:06:06.110 --> 00:06:06.120 align:start position:0%
theta one at about minus 0.1 okay
 

00:06:06.120 --> 00:06:10.850 align:start position:0%
theta one at about minus 0.1 okay
and<00:06:06.240><c> so</c><00:06:06.990><c> this</c><00:06:07.199><c> corresponds</c><00:06:07.919><c> to</c><00:06:08.060><c> H</c><00:06:09.060><c> of</c><00:06:09.690><c> x</c><00:06:09.870><c> equals</c>

00:06:10.850 --> 00:06:10.860 align:start position:0%
and so this corresponds to H of x equals
 

00:06:10.860 --> 00:06:15.710 align:start position:0%
and so this corresponds to H of x equals
you<00:06:11.580><c> know</c><00:06:11.610><c> minus</c><00:06:12.620><c> 900</c><00:06:13.620><c> minus</c><00:06:14.490><c> 0.1</c><00:06:15.120><c> X</c><00:06:15.150><c> so</c><00:06:15.509><c> this</c>

00:06:15.710 --> 00:06:15.720 align:start position:0%
you know minus 900 minus 0.1 X so this
 

00:06:15.720 --> 00:06:17.960 align:start position:0%
you know minus 900 minus 0.1 X so this
is<00:06:15.780><c> this</c><00:06:16.110><c> line</c><00:06:16.289><c> so</c><00:06:16.680><c> out</c><00:06:16.979><c> here</c><00:06:17.250><c> on</c><00:06:17.370><c> on</c><00:06:17.639><c> the</c><00:06:17.759><c> cost</c>

00:06:17.960 --> 00:06:17.970 align:start position:0%
is this line so out here on on the cost
 

00:06:17.970 --> 00:06:20.779 align:start position:0%
is this line so out here on on the cost
function<00:06:18.410><c> now</c><00:06:19.410><c> if</c><00:06:19.680><c> we</c><00:06:19.979><c> take</c><00:06:20.130><c> one</c><00:06:20.400><c> step</c><00:06:20.639><c> with</c>

00:06:20.779 --> 00:06:20.789 align:start position:0%
function now if we take one step with
 

00:06:20.789 --> 00:06:23.779 align:start position:0%
function now if we take one step with
great<00:06:20.940><c> descent</c><00:06:21.330><c> we</c><00:06:22.080><c> end</c><00:06:22.199><c> up</c><00:06:22.350><c> going</c><00:06:22.620><c> from</c><00:06:22.860><c> this</c>

00:06:23.779 --> 00:06:23.789 align:start position:0%
great descent we end up going from this
 

00:06:23.789 --> 00:06:28.340 align:start position:0%
great descent we end up going from this
point<00:06:24.060><c> out</c><00:06:24.300><c> here</c><00:06:24.720><c> a</c><00:06:25.910><c> little</c><00:06:26.910><c> bit</c><00:06:27.150><c> to</c><00:06:27.449><c> the</c><00:06:28.020><c> down</c>

00:06:28.340 --> 00:06:28.350 align:start position:0%
point out here a little bit to the down
 

00:06:28.350 --> 00:06:30.680 align:start position:0%
point out here a little bit to the down
and<00:06:28.560><c> left</c><00:06:28.740><c> to</c><00:06:29.490><c> that</c><00:06:29.669><c> second</c><00:06:30.090><c> point</c><00:06:30.210><c> over</c><00:06:30.539><c> there</c>

00:06:30.680 --> 00:06:30.690 align:start position:0%
and left to that second point over there
 

00:06:30.690 --> 00:06:34.550 align:start position:0%
and left to that second point over there
and<00:06:30.979><c> you</c><00:06:31.979><c> notice</c><00:06:32.340><c> that</c><00:06:33.180><c> my</c><00:06:33.840><c> line</c><00:06:34.080><c> changed</c><00:06:34.530><c> a</c>

00:06:34.550 --> 00:06:34.560 align:start position:0%
and you notice that my line changed a
 

00:06:34.560 --> 00:06:36.469 align:start position:0%
and you notice that my line changed a
little<00:06:34.650><c> bit</c><00:06:34.860><c> and</c><00:06:35.070><c> as</c><00:06:35.460><c> I</c><00:06:35.580><c> take</c><00:06:35.820><c> another</c><00:06:36.090><c> step</c><00:06:36.330><c> of</c>

00:06:36.469 --> 00:06:36.479 align:start position:0%
little bit and as I take another step of
 

00:06:36.479 --> 00:06:39.230 align:start position:0%
little bit and as I take another step of
grading<00:06:36.690><c> the</c><00:06:36.720><c> Zen</c><00:06:36.960><c> my</c><00:06:37.949><c> line</c><00:06:38.250><c> on</c><00:06:38.610><c> the</c><00:06:38.669><c> Left</c><00:06:38.820><c> will</c>

00:06:39.230 --> 00:06:39.240 align:start position:0%
grading the Zen my line on the Left will
 

00:06:39.240 --> 00:06:44.480 align:start position:0%
grading the Zen my line on the Left will
change<00:06:40.340><c> right</c><00:06:41.370><c> and</c><00:06:42.080><c> I've</c><00:06:43.080><c> also</c><00:06:43.320><c> moved</c><00:06:43.830><c> to</c><00:06:44.190><c> a</c>

00:06:44.480 --> 00:06:44.490 align:start position:0%
change right and I've also moved to a
 

00:06:44.490 --> 00:06:48.170 align:start position:0%
change right and I've also moved to a
new<00:06:44.699><c> point</c><00:06:44.940><c> on</c><00:06:45.419><c> my</c><00:06:45.690><c> cost</c><00:06:45.930><c> function</c><00:06:46.380><c> and</c><00:06:47.030><c> as</c><00:06:48.030><c> I</c>

00:06:48.170 --> 00:06:48.180 align:start position:0%
new point on my cost function and as I
 

00:06:48.180 --> 00:06:49.520 align:start position:0%
new point on my cost function and as I
take<00:06:48.360><c> further</c><00:06:48.570><c> steps</c><00:06:48.780><c> of</c><00:06:49.080><c> grading</c><00:06:49.289><c> descent</c>

00:06:49.520 --> 00:06:49.530 align:start position:0%
take further steps of grading descent
 

00:06:49.530 --> 00:06:53.180 align:start position:0%
take further steps of grading descent
I'm<00:06:50.370><c> going</c><00:06:50.910><c> down</c><00:06:51.210><c> in</c><00:06:51.419><c> cost</c><00:06:51.710><c> right</c><00:06:52.710><c> so</c><00:06:52.979><c> my</c>

00:06:53.180 --> 00:06:53.190 align:start position:0%
I'm going down in cost right so my
 

00:06:53.190 --> 00:06:55.249 align:start position:0%
I'm going down in cost right so my
parameter<00:06:54.150><c> is</c><00:06:54.210><c> instead</c><00:06:54.570><c> of</c><00:06:54.660><c> following</c><00:06:55.110><c> this</c>

00:06:55.249 --> 00:06:55.259 align:start position:0%
parameter is instead of following this
 

00:06:55.259 --> 00:06:58.750 align:start position:0%
parameter is instead of following this
trajectory<00:06:55.530><c> and</c><00:06:56.750><c> if</c><00:06:57.750><c> you</c><00:06:58.050><c> look</c><00:06:58.169><c> on</c><00:06:58.320><c> the</c><00:06:58.380><c> left</c>

00:06:58.750 --> 00:06:58.760 align:start position:0%
trajectory and if you look on the left
 

00:06:58.760 --> 00:07:03.050 align:start position:0%
trajectory and if you look on the left
this<00:06:59.760><c> corresponds</c><00:07:00.330><c> to</c><00:07:00.539><c> hypotheses</c><00:07:01.500><c> that</c><00:07:02.060><c> you</c>

00:07:03.050 --> 00:07:03.060 align:start position:0%
this corresponds to hypotheses that you
 

00:07:03.060 --> 00:07:05.270 align:start position:0%
this corresponds to hypotheses that you
know<00:07:03.210><c> seem</c><00:07:03.960><c> to</c><00:07:03.990><c> be</c><00:07:04.139><c> getting</c><00:07:04.410><c> to</c><00:07:04.740><c> be</c><00:07:04.860><c> better</c><00:07:05.099><c> and</c>

00:07:05.270 --> 00:07:05.280 align:start position:0%
know seem to be getting to be better and
 

00:07:05.280 --> 00:07:08.689 align:start position:0%
know seem to be getting to be better and
better<00:07:05.340><c> fits</c><00:07:05.849><c> in</c><00:07:06.030><c> the</c><00:07:06.120><c> data</c><00:07:07.430><c> until</c><00:07:08.430><c> eventually</c>

00:07:08.689 --> 00:07:08.699 align:start position:0%
better fits in the data until eventually
 

00:07:08.699 --> 00:07:11.899 align:start position:0%
better fits in the data until eventually
I<00:07:09.240><c> have</c><00:07:10.139><c> now</c><00:07:10.380><c> wound</c><00:07:10.740><c> up</c><00:07:10.979><c> at</c><00:07:11.190><c> the</c><00:07:11.490><c> global</c>

00:07:11.899 --> 00:07:11.909 align:start position:0%
I have now wound up at the global
 

00:07:11.909 --> 00:07:14.180 align:start position:0%
I have now wound up at the global
minimum<00:07:12.300><c> and</c><00:07:12.510><c> this</c><00:07:13.409><c> global</c><00:07:13.620><c> minimum</c>

00:07:14.180 --> 00:07:14.190 align:start position:0%
minimum and this global minimum
 

00:07:14.190 --> 00:07:17.870 align:start position:0%
minimum and this global minimum
corresponds<00:07:15.180><c> to</c><00:07:15.539><c> this</c><00:07:16.320><c> hypothesis</c><00:07:17.130><c> which</c>

00:07:17.870 --> 00:07:17.880 align:start position:0%
corresponds to this hypothesis which
 

00:07:17.880 --> 00:07:20.120 align:start position:0%
corresponds to this hypothesis which
gives<00:07:18.479><c> me</c><00:07:18.690><c> a</c><00:07:18.720><c> good</c><00:07:19.229><c> fit</c><00:07:19.470><c> to</c><00:07:19.680><c> the</c><00:07:19.770><c> data</c>

00:07:20.120 --> 00:07:20.130 align:start position:0%
gives me a good fit to the data
 

00:07:20.130 --> 00:07:24.409 align:start position:0%
gives me a good fit to the data
and<00:07:20.960><c> so</c><00:07:21.960><c> that's</c><00:07:22.800><c> gradient</c><00:07:23.490><c> descent</c><00:07:23.820><c> and</c><00:07:24.090><c> we've</c>

00:07:24.409 --> 00:07:24.419 align:start position:0%
and so that's gradient descent and we've
 

00:07:24.419 --> 00:07:28.129 align:start position:0%
and so that's gradient descent and we've
just<00:07:24.449><c> run</c><00:07:24.960><c> it</c><00:07:25.110><c> and</c><00:07:25.349><c> gotten</c><00:07:25.979><c> a</c><00:07:26.370><c> good</c><00:07:26.849><c> fit</c><00:07:27.150><c> to</c><00:07:27.659><c> my</c>

00:07:28.129 --> 00:07:28.139 align:start position:0%
just run it and gotten a good fit to my
 

00:07:28.139 --> 00:07:31.550 align:start position:0%
just run it and gotten a good fit to my
data<00:07:28.250><c> set</c><00:07:29.250><c> of</c><00:07:29.430><c> housing</c><00:07:29.760><c> prices</c><00:07:30.060><c> and</c><00:07:30.659><c> you</c><00:07:31.380><c> can</c>

00:07:31.550 --> 00:07:31.560 align:start position:0%
data set of housing prices and you can
 

00:07:31.560 --> 00:07:34.640 align:start position:0%
data set of housing prices and you can
now<00:07:31.680><c> use</c><00:07:31.979><c> it</c><00:07:32.010><c> to</c><00:07:32.520><c> predict</c><00:07:33.449><c> you</c><00:07:34.169><c> know</c><00:07:34.199><c> if</c><00:07:34.500><c> your</c>

00:07:34.640 --> 00:07:34.650 align:start position:0%
now use it to predict you know if your
 

00:07:34.650 --> 00:07:37.640 align:start position:0%
now use it to predict you know if your
friend<00:07:34.919><c> has</c><00:07:35.130><c> a</c><00:07:35.159><c> house</c><00:07:35.460><c> that</c><00:07:36.180><c> costs</c><00:07:36.659><c> website</c>

00:07:37.640 --> 00:07:37.650 align:start position:0%
friend has a house that costs website
 

00:07:37.650 --> 00:07:39.649 align:start position:0%
friend has a house that costs website
1250<00:07:38.280><c> square</c><00:07:38.400><c> feet</c><00:07:38.729><c> you</c><00:07:39.000><c> can</c><00:07:39.210><c> now</c><00:07:39.360><c> read</c><00:07:39.630><c> off</c>

00:07:39.649 --> 00:07:39.659 align:start position:0%
1250 square feet you can now read off
 

00:07:39.659 --> 00:07:42.170 align:start position:0%
1250 square feet you can now read off
the<00:07:39.990><c> value</c><00:07:40.349><c> and</c><00:07:40.560><c> tell</c><00:07:40.710><c> them</c><00:07:40.919><c> that</c><00:07:41.220><c> I</c><00:07:41.430><c> know</c>

00:07:42.170 --> 00:07:42.180 align:start position:0%
the value and tell them that I know
 

00:07:42.180 --> 00:07:43.760 align:start position:0%
the value and tell them that I know
maybe<00:07:42.510><c> they</c><00:07:42.720><c> can</c><00:07:42.870><c> get</c><00:07:43.050><c> two</c><00:07:43.229><c> hundred</c><00:07:43.470><c> and</c><00:07:43.530><c> fifty</c>

00:07:43.760 --> 00:07:43.770 align:start position:0%
maybe they can get two hundred and fifty
 

00:07:43.770 --> 00:07:48.890 align:start position:0%
maybe they can get two hundred and fifty
thousand<00:07:44.130><c> dollars</c><00:07:44.430><c> for</c><00:07:45.389><c> their</c><00:07:45.539><c> house</c><00:07:47.900><c> finally</c>

00:07:48.890 --> 00:07:48.900 align:start position:0%
thousand dollars for their house finally
 

00:07:48.900 --> 00:07:51.080 align:start position:0%
thousand dollars for their house finally
just<00:07:49.560><c> to</c><00:07:49.650><c> give</c><00:07:49.800><c> this</c><00:07:49.919><c> another</c><00:07:50.250><c> name</c><00:07:50.490><c> it</c><00:07:50.789><c> turns</c>

00:07:51.080 --> 00:07:51.090 align:start position:0%
just to give this another name it turns
 

00:07:51.090 --> 00:07:53.240 align:start position:0%
just to give this another name it turns
out<00:07:51.330><c> that</c><00:07:51.630><c> the</c><00:07:51.900><c> algorithm</c><00:07:52.289><c> that</c><00:07:52.770><c> we</c><00:07:52.919><c> just</c><00:07:53.099><c> went</c>

00:07:53.240 --> 00:07:53.250 align:start position:0%
out that the algorithm that we just went
 

00:07:53.250 --> 00:07:56.240 align:start position:0%
out that the algorithm that we just went
over<00:07:53.490><c> is</c><00:07:54.050><c> sometimes</c><00:07:55.050><c> called</c><00:07:55.320><c> batch</c><00:07:55.800><c> gradient</c>

00:07:56.240 --> 00:07:56.250 align:start position:0%
over is sometimes called batch gradient
 

00:07:56.250 --> 00:07:58.760 align:start position:0%
over is sometimes called batch gradient
descent<00:07:56.550><c> and</c><00:07:57.090><c> it</c><00:07:57.960><c> turns</c><00:07:58.139><c> out</c><00:07:58.320><c> in</c><00:07:58.530><c> machine</c>

00:07:58.760 --> 00:07:58.770 align:start position:0%
descent and it turns out in machine
 

00:07:58.770 --> 00:08:00.379 align:start position:0%
descent and it turns out in machine
learning<00:07:58.889><c> on</c><00:07:59.280><c> there</c><00:07:59.430><c> I</c><00:07:59.490><c> feel</c><00:07:59.669><c> like</c><00:07:59.789><c> us</c><00:07:59.970><c> machine</c>

00:08:00.379 --> 00:08:00.389 align:start position:0%
learning on there I feel like us machine
 

00:08:00.389 --> 00:08:02.360 align:start position:0%
learning on there I feel like us machine
learning<00:08:00.690><c> people</c><00:08:00.780><c> were</c><00:08:01.169><c> not</c><00:08:01.320><c> always</c><00:08:01.770><c> great</c><00:08:02.130><c> at</c>

00:08:02.360 --> 00:08:02.370 align:start position:0%
learning people were not always great at
 

00:08:02.370 --> 00:08:04.790 align:start position:0%
learning people were not always great at
giving<00:08:02.490><c> me</c><00:08:02.820><c> into</c><00:08:03.060><c> algorithms</c><00:08:03.450><c> but</c><00:08:04.380><c> the</c><00:08:04.529><c> term</c>

00:08:04.790 --> 00:08:04.800 align:start position:0%
giving me into algorithms but the term
 

00:08:04.800 --> 00:08:06.490 align:start position:0%
giving me into algorithms but the term
batch<00:08:05.219><c> gradient</c><00:08:05.700><c> descent</c>

00:08:06.490 --> 00:08:06.500 align:start position:0%
batch gradient descent
 

00:08:06.500 --> 00:08:08.650 align:start position:0%
batch gradient descent
means<00:08:06.800><c> that</c><00:08:07.130><c> refers</c><00:08:07.760><c> to</c><00:08:07.790><c> the</c><00:08:07.970><c> fact</c><00:08:08.150><c> that</c><00:08:08.330><c> in</c>

00:08:08.650 --> 00:08:08.660 align:start position:0%
means that refers to the fact that in
 

00:08:08.660 --> 00:08:10.780 align:start position:0%
means that refers to the fact that in
every<00:08:09.200><c> step</c><00:08:09.410><c> of</c><00:08:09.590><c> gradient</c><00:08:09.710><c> descent</c><00:08:09.980><c> were</c>

00:08:10.780 --> 00:08:10.790 align:start position:0%
every step of gradient descent were
 

00:08:10.790 --> 00:08:13.540 align:start position:0%
every step of gradient descent were
looking<00:08:11.420><c> at</c><00:08:11.570><c> all</c><00:08:11.780><c> of</c><00:08:12.080><c> the</c><00:08:12.230><c> training</c><00:08:12.620><c> examples</c>

00:08:13.540 --> 00:08:13.550 align:start position:0%
looking at all of the training examples
 

00:08:13.550 --> 00:08:15.970 align:start position:0%
looking at all of the training examples
so<00:08:14.000><c> in</c><00:08:14.300><c> gradient</c><00:08:15.050><c> descent</c><00:08:15.140><c> you</c><00:08:15.620><c> know</c><00:08:15.800><c> when</c>

00:08:15.970 --> 00:08:15.980 align:start position:0%
so in gradient descent you know when
 

00:08:15.980 --> 00:08:17.860 align:start position:0%
so in gradient descent you know when
computing<00:08:16.430><c> different</c><00:08:16.730><c> derivatives</c><00:08:16.970><c> we're</c>

00:08:17.860 --> 00:08:17.870 align:start position:0%
computing different derivatives we're
 

00:08:17.870 --> 00:08:20.430 align:start position:0%
computing different derivatives we're
computing<00:08:18.200><c> these</c><00:08:18.590><c> sums</c><00:08:18.980><c> right</c><00:08:19.250><c> this</c><00:08:19.550><c> summer</c>

00:08:20.430 --> 00:08:20.440 align:start position:0%
computing these sums right this summer
 

00:08:20.440 --> 00:08:22.960 align:start position:0%
computing these sums right this summer
so<00:08:21.440><c> in</c><00:08:21.560><c> every</c><00:08:21.800><c> step</c><00:08:22.010><c> of</c><00:08:22.040><c> gradient</c><00:08:22.250><c> descent</c><00:08:22.790><c> we</c>

00:08:22.960 --> 00:08:22.970 align:start position:0%
so in every step of gradient descent we
 

00:08:22.970 --> 00:08:24.430 align:start position:0%
so in every step of gradient descent we
end<00:08:23.120><c> up</c><00:08:23.270><c> computing</c><00:08:23.600><c> something</c><00:08:24.050><c> like</c><00:08:24.260><c> this</c>

00:08:24.430 --> 00:08:24.440 align:start position:0%
end up computing something like this
 

00:08:24.440 --> 00:08:27.370 align:start position:0%
end up computing something like this
that<00:08:24.980><c> sums</c><00:08:25.400><c> over</c><00:08:25.760><c> our</c><00:08:26.180><c> M</c><00:08:26.720><c> training</c><00:08:27.230><c> examples</c>

00:08:27.370 --> 00:08:27.380 align:start position:0%
that sums over our M training examples
 

00:08:27.380 --> 00:08:29.890 align:start position:0%
that sums over our M training examples
and<00:08:28.100><c> so</c><00:08:28.730><c> the</c><00:08:28.850><c> term</c><00:08:29.060><c> batch</c><00:08:29.360><c> gradient</c><00:08:29.660><c> descent</c>

00:08:29.890 --> 00:08:29.900 align:start position:0%
and so the term batch gradient descent
 

00:08:29.900 --> 00:08:31.660 align:start position:0%
and so the term batch gradient descent
refers<00:08:30.530><c> the</c><00:08:30.710><c> fact</c><00:08:30.920><c> that</c><00:08:31.010><c> we're</c><00:08:31.250><c> looking</c><00:08:31.370><c> at</c>

00:08:31.660 --> 00:08:31.670 align:start position:0%
refers the fact that we're looking at
 

00:08:31.670 --> 00:08:34.029 align:start position:0%
refers the fact that we're looking at
the<00:08:32.060><c> entire</c><00:08:32.390><c> batch</c><00:08:32.810><c> of</c><00:08:33.110><c> training</c><00:08:33.530><c> examples</c>

00:08:34.029 --> 00:08:34.039 align:start position:0%
the entire batch of training examples
 

00:08:34.039 --> 00:08:35.950 align:start position:0%
the entire batch of training examples
again<00:08:34.430><c> this</c><00:08:34.640><c> is</c><00:08:34.729><c> really</c><00:08:34.909><c> maybe</c><00:08:35.419><c> not</c><00:08:35.659><c> a</c><00:08:35.690><c> great</c>

00:08:35.950 --> 00:08:35.960 align:start position:0%
again this is really maybe not a great
 

00:08:35.960 --> 00:08:37.990 align:start position:0%
again this is really maybe not a great
name<00:08:36.169><c> but</c><00:08:36.380><c> this</c><00:08:36.500><c> is</c><00:08:36.620><c> what</c><00:08:36.830><c> machine</c><00:08:37.669><c> learning</c>

00:08:37.990 --> 00:08:38.000 align:start position:0%
name but this is what machine learning
 

00:08:38.000 --> 00:08:40.390 align:start position:0%
name but this is what machine learning
people<00:08:38.120><c> call</c><00:08:38.510><c> it</c><00:08:38.659><c> and</c><00:08:39.010><c> it</c><00:08:40.010><c> turns</c><00:08:40.219><c> out</c><00:08:40.340><c> that</c>

00:08:40.390 --> 00:08:40.400 align:start position:0%
people call it and it turns out that
 

00:08:40.400 --> 00:08:42.850 align:start position:0%
people call it and it turns out that
there<00:08:41.120><c> are</c><00:08:41.240><c> sometimes</c><00:08:41.570><c> other</c><00:08:41.930><c> versions</c><00:08:42.680><c> of</c>

00:08:42.850 --> 00:08:42.860 align:start position:0%
there are sometimes other versions of
 

00:08:42.860 --> 00:08:44.260 align:start position:0%
there are sometimes other versions of
gradient<00:08:43.190><c> descent</c><00:08:43.460><c> that</c><00:08:43.550><c> are</c><00:08:43.760><c> not</c><00:08:43.969><c> batch</c>

00:08:44.260 --> 00:08:44.270 align:start position:0%
gradient descent that are not batch
 

00:08:44.270 --> 00:08:46.900 align:start position:0%
gradient descent that are not batch
versions<00:08:44.960><c> but</c><00:08:45.200><c> that</c><00:08:45.350><c> instead</c><00:08:45.530><c> do</c><00:08:46.340><c> not</c><00:08:46.550><c> look</c><00:08:46.790><c> at</c>

00:08:46.900 --> 00:08:46.910 align:start position:0%
versions but that instead do not look at
 

00:08:46.910 --> 00:08:49.210 align:start position:0%
versions but that instead do not look at
the<00:08:47.000><c> entire</c><00:08:47.270><c> training</c><00:08:47.510><c> set</c><00:08:47.840><c> but</c><00:08:48.020><c> look</c><00:08:49.010><c> at</c>

00:08:49.210 --> 00:08:49.220 align:start position:0%
the entire training set but look at
 

00:08:49.220 --> 00:08:51.010 align:start position:0%
the entire training set but look at
small<00:08:49.550><c> subsets</c><00:08:50.030><c> on</c><00:08:50.180><c> the</c><00:08:50.240><c> training</c><00:08:50.480><c> set</c><00:08:50.720><c> at</c><00:08:50.930><c> a</c>

00:08:51.010 --> 00:08:51.020 align:start position:0%
small subsets on the training set at a
 

00:08:51.020 --> 00:08:53.110 align:start position:0%
small subsets on the training set at a
time<00:08:51.230><c> and</c><00:08:51.440><c> we'll</c><00:08:52.070><c> talk</c><00:08:52.130><c> about</c><00:08:52.550><c> those</c><00:08:52.910><c> versions</c>

00:08:53.110 --> 00:08:53.120 align:start position:0%
time and we'll talk about those versions
 

00:08:53.120 --> 00:08:55.540 align:start position:0%
time and we'll talk about those versions
later<00:08:53.570><c> in</c><00:08:53.780><c> the</c><00:08:53.840><c> schools</c><00:08:54.140><c> as</c><00:08:54.260><c> well</c><00:08:54.410><c> but</c><00:08:55.280><c> for</c><00:08:55.430><c> now</c>

00:08:55.540 --> 00:08:55.550 align:start position:0%
later in the schools as well but for now
 

00:08:55.550 --> 00:08:56.920 align:start position:0%
later in the schools as well but for now
using<00:08:56.000><c> the</c><00:08:56.150><c> album</c><00:08:56.450><c> you</c><00:08:56.510><c> just</c><00:08:56.690><c> learned</c><00:08:56.840><c> about</c>

00:08:56.920 --> 00:08:56.930 align:start position:0%
using the album you just learned about
 

00:08:56.930 --> 00:08:59.410 align:start position:0%
using the album you just learned about
or<00:08:57.260><c> using</c><00:08:57.590><c> batch</c><00:08:57.860><c> gradient</c><00:08:58.220><c> descent</c><00:08:58.430><c> you</c><00:08:58.910><c> now</c>

00:08:59.410 --> 00:08:59.420 align:start position:0%
or using batch gradient descent you now
 

00:08:59.420 --> 00:09:02.050 align:start position:0%
or using batch gradient descent you now
know<00:08:59.690><c> how</c><00:09:00.050><c> to</c><00:09:00.080><c> implement</c><00:09:00.710><c> gradient</c><00:09:01.490><c> descent</c>

00:09:02.050 --> 00:09:02.060 align:start position:0%
know how to implement gradient descent
 

00:09:02.060 --> 00:09:06.940 align:start position:0%
know how to implement gradient descent
for<00:09:02.690><c> linear</c><00:09:02.870><c> regression</c><00:09:05.110><c> so</c><00:09:06.110><c> that's</c><00:09:06.620><c> linear</c>

00:09:06.940 --> 00:09:06.950 align:start position:0%
for linear regression so that's linear
 

00:09:06.950 --> 00:09:08.880 align:start position:0%
for linear regression so that's linear
regression<00:09:07.550><c> with</c><00:09:07.850><c> gradient</c><00:09:07.880><c> descent</c><00:09:08.300><c> if</c>

00:09:08.880 --> 00:09:08.890 align:start position:0%
regression with gradient descent if
 

00:09:08.890 --> 00:09:11.290 align:start position:0%
regression with gradient descent if
you've<00:09:09.890><c> seen</c><00:09:10.190><c> advanced</c><00:09:10.790><c> linear</c><00:09:11.120><c> algebra</c>

00:09:11.290 --> 00:09:11.300 align:start position:0%
you've seen advanced linear algebra
 

00:09:11.300 --> 00:09:13.180 align:start position:0%
you've seen advanced linear algebra
before<00:09:11.900><c> so</c><00:09:12.260><c> some</c><00:09:12.470><c> of</c><00:09:12.530><c> you</c><00:09:12.620><c> may</c><00:09:12.740><c> have</c><00:09:12.860><c> taken</c><00:09:12.980><c> a</c>

00:09:13.180 --> 00:09:13.190 align:start position:0%
before so some of you may have taken a
 

00:09:13.190 --> 00:09:14.290 align:start position:0%
before so some of you may have taken a
calls<00:09:13.400><c> in</c><00:09:13.580><c> you</c><00:09:13.730><c> know</c><00:09:13.790><c> advanced</c><00:09:14.060><c> linear</c>

00:09:14.290 --> 00:09:14.300 align:start position:0%
calls in you know advanced linear
 

00:09:14.300 --> 00:09:17.110 align:start position:0%
calls in you know advanced linear
algebra<00:09:14.480><c> you</c><00:09:15.050><c> might</c><00:09:16.040><c> know</c><00:09:16.250><c> that</c><00:09:16.280><c> there</c><00:09:16.700><c> exists</c>

00:09:17.110 --> 00:09:17.120 align:start position:0%
algebra you might know that there exists
 

00:09:17.120 --> 00:09:19.840 align:start position:0%
algebra you might know that there exists
a<00:09:17.150><c> solution</c><00:09:17.330><c> for</c><00:09:18.290><c> numerically</c><00:09:19.010><c> solving</c><00:09:19.550><c> for</c>

00:09:19.840 --> 00:09:19.850 align:start position:0%
a solution for numerically solving for
 

00:09:19.850 --> 00:09:22.060 align:start position:0%
a solution for numerically solving for
the<00:09:19.910><c> minimum</c><00:09:20.300><c> of</c><00:09:20.420><c> the</c><00:09:20.540><c> cost</c><00:09:20.750><c> function</c><00:09:20.960><c> J</c><00:09:21.230><c> we're</c>

00:09:22.060 --> 00:09:22.070 align:start position:0%
the minimum of the cost function J we're
 

00:09:22.070 --> 00:09:23.590 align:start position:0%
the minimum of the cost function J we're
about<00:09:22.220><c> needing</c><00:09:22.580><c> to</c><00:09:22.610><c> use</c><00:09:22.880><c> an</c><00:09:23.120><c> iterative</c>

00:09:23.590 --> 00:09:23.600 align:start position:0%
about needing to use an iterative
 

00:09:23.600 --> 00:09:26.650 align:start position:0%
about needing to use an iterative
algorithm<00:09:24.170><c> like</c><00:09:24.530><c> gradient</c><00:09:24.589><c> descent</c><00:09:25.510><c> later</c><00:09:26.510><c> in</c>

00:09:26.650 --> 00:09:26.660 align:start position:0%
algorithm like gradient descent later in
 

00:09:26.660 --> 00:09:27.940 align:start position:0%
algorithm like gradient descent later in
this<00:09:26.780><c> course</c><00:09:26.990><c> we'll</c><00:09:27.170><c> talk</c><00:09:27.320><c> about</c><00:09:27.470><c> that</c><00:09:27.740><c> method</c>

00:09:27.940 --> 00:09:27.950 align:start position:0%
this course we'll talk about that method
 

00:09:27.950 --> 00:09:30.160 align:start position:0%
this course we'll talk about that method
as<00:09:28.190><c> well</c><00:09:28.370><c> that</c><00:09:28.640><c> just</c><00:09:28.790><c> solves</c><00:09:29.270><c> for</c><00:09:29.570><c> the</c><00:09:29.810><c> minimum</c>

00:09:30.160 --> 00:09:30.170 align:start position:0%
as well that just solves for the minimum
 

00:09:30.170 --> 00:09:31.930 align:start position:0%
as well that just solves for the minimum
the<00:09:30.350><c> cost</c><00:09:30.560><c> function</c><00:09:30.680><c> J</c><00:09:30.950><c> without</c><00:09:31.430><c> needing</c><00:09:31.760><c> you</c>

00:09:31.930 --> 00:09:31.940 align:start position:0%
the cost function J without needing you
 

00:09:31.940 --> 00:09:33.280 align:start position:0%
the cost function J without needing you
know<00:09:32.000><c> this</c><00:09:32.180><c> multiple</c><00:09:32.660><c> steps</c><00:09:33.020><c> of</c><00:09:33.170><c> gradient</c>

00:09:33.280 --> 00:09:33.290 align:start position:0%
know this multiple steps of gradient
 

00:09:33.290 --> 00:09:35.680 align:start position:0%
know this multiple steps of gradient
descent<00:09:33.610><c> that</c><00:09:34.610><c> other</c><00:09:34.820><c> method</c><00:09:35.180><c> is</c><00:09:35.300><c> called</c><00:09:35.540><c> the</c>

00:09:35.680 --> 00:09:35.690 align:start position:0%
descent that other method is called the
 

00:09:35.690 --> 00:09:38.860 align:start position:0%
descent that other method is called the
normal<00:09:36.200><c> equations</c><00:09:36.800><c> method</c><00:09:37.220><c> and</c><00:09:37.460><c> but</c><00:09:38.150><c> in</c><00:09:38.690><c> case</c>

00:09:38.860 --> 00:09:38.870 align:start position:0%
normal equations method and but in case
 

00:09:38.870 --> 00:09:40.360 align:start position:0%
normal equations method and but in case
you've<00:09:39.020><c> heard</c><00:09:39.230><c> of</c><00:09:39.320><c> that</c><00:09:39.410><c> method</c><00:09:39.620><c> it</c><00:09:39.980><c> turns</c><00:09:40.160><c> out</c>

00:09:40.360 --> 00:09:40.370 align:start position:0%
you've heard of that method it turns out
 

00:09:40.370 --> 00:09:42.310 align:start position:0%
you've heard of that method it turns out
that<00:09:40.580><c> gradient</c><00:09:41.330><c> descent</c><00:09:41.600><c> will</c><00:09:41.810><c> scale</c><00:09:42.050><c> better</c>

00:09:42.310 --> 00:09:42.320 align:start position:0%
that gradient descent will scale better
 

00:09:42.320 --> 00:09:44.920 align:start position:0%
that gradient descent will scale better
to<00:09:43.010><c> larger</c><00:09:43.490><c> data</c><00:09:43.670><c> sets</c><00:09:44.000><c> than</c><00:09:44.240><c> that</c><00:09:44.450><c> normal</c>

00:09:44.920 --> 00:09:44.930 align:start position:0%
to larger data sets than that normal
 

00:09:44.930 --> 00:09:47.620 align:start position:0%
to larger data sets than that normal
equation<00:09:45.410><c> method</c><00:09:45.440><c> and</c><00:09:46.180><c> now</c><00:09:47.180><c> that</c><00:09:47.360><c> we</c><00:09:47.450><c> know</c>

00:09:47.620 --> 00:09:47.630 align:start position:0%
equation method and now that we know
 

00:09:47.630 --> 00:09:49.270 align:start position:0%
equation method and now that we know
about<00:09:47.660><c> grading</c><00:09:48.110><c> descent</c><00:09:48.380><c> we'll</c><00:09:48.920><c> be</c><00:09:49.040><c> able</c><00:09:49.130><c> to</c>

00:09:49.270 --> 00:09:49.280 align:start position:0%
about grading descent we'll be able to
 

00:09:49.280 --> 00:09:51.340 align:start position:0%
about grading descent we'll be able to
use<00:09:49.490><c> it</c><00:09:49.850><c> in</c><00:09:49.970><c> lots</c><00:09:50.240><c> of</c><00:09:50.390><c> different</c><00:09:50.450><c> contexts</c><00:09:51.140><c> and</c>

00:09:51.340 --> 00:09:51.350 align:start position:0%
use it in lots of different contexts and
 

00:09:51.350 --> 00:09:52.300 align:start position:0%
use it in lots of different contexts and
we'll<00:09:51.410><c> use</c><00:09:51.500><c> it</c><00:09:51.680><c> in</c><00:09:51.890><c> lots</c><00:09:52.130><c> of</c><00:09:52.250><c> different</c>

00:09:52.300 --> 00:09:52.310 align:start position:0%
we'll use it in lots of different
 

00:09:52.310 --> 00:09:55.620 align:start position:0%
we'll use it in lots of different
machine<00:09:52.850><c> learning</c><00:09:52.880><c> problems</c><00:09:53.480><c> as</c><00:09:53.630><c> well</c><00:09:54.400><c> so</c>

00:09:55.620 --> 00:09:55.630 align:start position:0%
machine learning problems as well so
 

00:09:55.630 --> 00:09:58.240 align:start position:0%
machine learning problems as well so
congrats<00:09:56.630><c> on</c><00:09:56.930><c> learning</c><00:09:57.290><c> about</c><00:09:57.620><c> your</c><00:09:57.920><c> first</c>

00:09:58.240 --> 00:09:58.250 align:start position:0%
congrats on learning about your first
 

00:09:58.250 --> 00:10:00.850 align:start position:0%
congrats on learning about your first
machine<00:09:58.730><c> learning</c><00:09:59.089><c> algorithm</c><00:09:59.620><c> well</c><00:10:00.620><c> later</c>

00:10:00.850 --> 00:10:00.860 align:start position:0%
machine learning algorithm well later
 

00:10:00.860 --> 00:10:03.310 align:start position:0%
machine learning algorithm well later
have<00:10:01.190><c> exercises</c><00:10:01.670><c> in</c><00:10:02.240><c> which</c><00:10:02.540><c> we'll</c><00:10:02.780><c> ask</c><00:10:02.990><c> you</c><00:10:03.230><c> to</c>

00:10:03.310 --> 00:10:03.320 align:start position:0%
have exercises in which we'll ask you to
 

00:10:03.320 --> 00:10:05.380 align:start position:0%
have exercises in which we'll ask you to
implement<00:10:03.680><c> gradient</c><00:10:04.190><c> descent</c><00:10:04.730><c> and</c><00:10:04.910><c> hopefully</c>

00:10:05.380 --> 00:10:05.390 align:start position:0%
implement gradient descent and hopefully
 

00:10:05.390 --> 00:10:06.820 align:start position:0%
implement gradient descent and hopefully
see<00:10:05.600><c> these</c><00:10:05.780><c> albums</c><00:10:06.140><c> were</c><00:10:06.260><c> open</c><00:10:06.500><c> yourselves</c>

00:10:06.820 --> 00:10:06.830 align:start position:0%
see these albums were open yourselves
 

00:10:06.830 --> 00:10:10.210 align:start position:0%
see these albums were open yourselves
but<00:10:07.550><c> before</c><00:10:07.910><c> that</c><00:10:08.060><c> I</c><00:10:08.360><c> first</c><00:10:08.780><c> want</c><00:10:09.140><c> to</c><00:10:09.380><c> tell</c><00:10:10.010><c> you</c>

00:10:10.210 --> 00:10:10.220 align:start position:0%
but before that I first want to tell you
 

00:10:10.220 --> 00:10:11.740 align:start position:0%
but before that I first want to tell you
in<00:10:10.460><c> the</c><00:10:10.550><c> NICS</c><00:10:10.760><c> of</c><00:10:10.940><c> the</c><00:10:11.030><c> video</c><00:10:11.270><c> so</c><00:10:11.480><c> the</c><00:10:11.540><c> first</c>

00:10:11.740 --> 00:10:11.750 align:start position:0%
in the NICS of the video so the first
 

00:10:11.750 --> 00:10:14.050 align:start position:0%
in the NICS of the video so the first
one<00:10:11.960><c> to</c><00:10:12.080><c> tell</c><00:10:12.320><c> you</c><00:10:12.350><c> about</c><00:10:12.920><c> a</c><00:10:13.339><c> generalization</c>

00:10:14.050 --> 00:10:14.060 align:start position:0%
one to tell you about a generalization
 

00:10:14.060 --> 00:10:15.610 align:start position:0%
one to tell you about a generalization
of<00:10:14.480><c> the</c><00:10:14.600><c> gradient</c><00:10:14.959><c> descent</c><00:10:15.200><c> algorithm</c>

00:10:15.610 --> 00:10:15.620 align:start position:0%
of the gradient descent algorithm
 

00:10:15.620 --> 00:10:18.220 align:start position:0%
of the gradient descent algorithm
they'll<00:10:16.220><c> make</c><00:10:16.400><c> it</c><00:10:16.580><c> much</c><00:10:16.790><c> more</c><00:10:17.000><c> powerful</c><00:10:17.450><c> and</c><00:10:17.600><c> I</c>

00:10:18.220 --> 00:10:18.230 align:start position:0%
they'll make it much more powerful and I
 

00:10:18.230 --> 00:10:19.870 align:start position:0%
they'll make it much more powerful and I
guess<00:10:18.470><c> I'll</c><00:10:18.620><c> tell</c><00:10:18.920><c> you</c><00:10:19.040><c> about</c><00:10:19.160><c> that</c><00:10:19.279><c> in</c><00:10:19.760><c> the</c>

00:10:19.870 --> 00:10:19.880 align:start position:0%
guess I'll tell you about that in the
 

00:10:19.880 --> 00:10:22.420 align:start position:0%
guess I'll tell you about that in the
next<00:10:20.150><c> video</c>

