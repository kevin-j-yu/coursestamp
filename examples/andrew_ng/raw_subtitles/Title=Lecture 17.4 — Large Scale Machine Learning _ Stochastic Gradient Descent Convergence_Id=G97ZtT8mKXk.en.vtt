WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:01.880 align:start position:0%
 
you<00:00:00.539><c> now</c><00:00:00.690><c> know</c><00:00:00.960><c> about</c><00:00:01.260><c> these</c><00:00:01.650><c> stochastic</c>

00:00:01.880 --> 00:00:01.890 align:start position:0%
you now know about these stochastic
 

00:00:01.890 --> 00:00:04.400 align:start position:0%
you now know about these stochastic
gradient<00:00:02.429><c> descent</c><00:00:02.760><c> algorithm</c><00:00:03.570><c> but</c><00:00:04.110><c> when</c>

00:00:04.400 --> 00:00:04.410 align:start position:0%
gradient descent algorithm but when
 

00:00:04.410 --> 00:00:05.900 align:start position:0%
gradient descent algorithm but when
you're<00:00:04.589><c> running</c><00:00:04.770><c> the</c><00:00:05.009><c> algorithm</c><00:00:05.220><c> how</c><00:00:05.700><c> do</c><00:00:05.759><c> you</c>

00:00:05.900 --> 00:00:05.910 align:start position:0%
you're running the algorithm how do you
 

00:00:05.910 --> 00:00:07.670 align:start position:0%
you're running the algorithm how do you
make<00:00:06.060><c> sure</c><00:00:06.089><c> that</c><00:00:06.600><c> is</c><00:00:06.839><c> sort</c><00:00:07.170><c> of</c><00:00:07.259><c> completely</c>

00:00:07.670 --> 00:00:07.680 align:start position:0%
make sure that is sort of completely
 

00:00:07.680 --> 00:00:10.030 align:start position:0%
make sure that is sort of completely
debug<00:00:08.130><c> than</c><00:00:08.490><c> this</c><00:00:08.639><c> conversion</c><00:00:09.179><c> okay</c><00:00:09.719><c> and</c>

00:00:10.030 --> 00:00:10.040 align:start position:0%
debug than this conversion okay and
 

00:00:10.040 --> 00:00:13.009 align:start position:0%
debug than this conversion okay and
equally<00:00:11.040><c> important</c><00:00:11.519><c> how</c><00:00:11.700><c> do</c><00:00:12.090><c> you</c><00:00:12.330><c> tune</c><00:00:12.690><c> the</c>

00:00:13.009 --> 00:00:13.019 align:start position:0%
equally important how do you tune the
 

00:00:13.019 --> 00:00:15.079 align:start position:0%
equally important how do you tune the
learning<00:00:13.380><c> rate</c><00:00:13.559><c> alpha</c><00:00:13.860><c> was</c><00:00:14.549><c> the</c><00:00:14.580><c> costly</c>

00:00:15.079 --> 00:00:15.089 align:start position:0%
learning rate alpha was the costly
 

00:00:15.089 --> 00:00:17.210 align:start position:0%
learning rate alpha was the costly
gradient<00:00:15.450><c> descent</c><00:00:15.750><c> in</c><00:00:16.049><c> this</c><00:00:16.590><c> video</c><00:00:16.949><c> we'll</c>

00:00:17.210 --> 00:00:17.220 align:start position:0%
gradient descent in this video we'll
 

00:00:17.220 --> 00:00:19.099 align:start position:0%
gradient descent in this video we'll
talk<00:00:17.369><c> about</c><00:00:17.430><c> some</c><00:00:17.850><c> techniques</c><00:00:18.000><c> for</c><00:00:18.510><c> doing</c>

00:00:19.099 --> 00:00:19.109 align:start position:0%
talk about some techniques for doing
 

00:00:19.109 --> 00:00:20.359 align:start position:0%
talk about some techniques for doing
these<00:00:19.289><c> things</c><00:00:19.529><c> for</c><00:00:19.710><c> making</c><00:00:20.100><c> sure</c><00:00:20.250><c> it's</c>

00:00:20.359 --> 00:00:20.369 align:start position:0%
these things for making sure it's
 

00:00:20.369 --> 00:00:23.240 align:start position:0%
these things for making sure it's
converging<00:00:20.970><c> and</c><00:00:21.090><c> for</c><00:00:21.810><c> picking</c><00:00:22.680><c> the</c><00:00:22.920><c> learning</c>

00:00:23.240 --> 00:00:23.250 align:start position:0%
converging and for picking the learning
 

00:00:23.250 --> 00:00:27.769 align:start position:0%
converging and for picking the learning
rate<00:00:23.400><c> alpha</c><00:00:25.580><c> back</c><00:00:26.580><c> when</c><00:00:26.849><c> we</c><00:00:27.029><c> were</c><00:00:27.150><c> using</c><00:00:27.330><c> batch</c>

00:00:27.769 --> 00:00:27.779 align:start position:0%
rate alpha back when we were using batch
 

00:00:27.779 --> 00:00:28.849 align:start position:0%
rate alpha back when we were using batch
gradient<00:00:28.109><c> descent</c>

00:00:28.849 --> 00:00:28.859 align:start position:0%
gradient descent
 

00:00:28.859 --> 00:00:30.529 align:start position:0%
gradient descent
I'll<00:00:28.920><c> stand</c><00:00:29.340><c> away</c><00:00:29.490><c> from</c><00:00:29.640><c> making</c><00:00:30.119><c> sure</c><00:00:30.330><c> that</c>

00:00:30.529 --> 00:00:30.539 align:start position:0%
I'll stand away from making sure that
 

00:00:30.539 --> 00:00:32.540 align:start position:0%
I'll stand away from making sure that
gradient<00:00:31.080><c> descent</c><00:00:31.349><c> was</c><00:00:31.439><c> converging</c><00:00:32.070><c> was</c><00:00:32.309><c> we</c>

00:00:32.540 --> 00:00:32.550 align:start position:0%
gradient descent was converging was we
 

00:00:32.550 --> 00:00:34.400 align:start position:0%
gradient descent was converging was we
would<00:00:32.700><c> plot</c><00:00:33.000><c> the</c><00:00:33.420><c> optimization</c><00:00:34.140><c> cost</c>

00:00:34.400 --> 00:00:34.410 align:start position:0%
would plot the optimization cost
 

00:00:34.410 --> 00:00:35.870 align:start position:0%
would plot the optimization cost
function<00:00:34.829><c> as</c><00:00:35.010><c> a</c><00:00:35.040><c> function</c><00:00:35.399><c> of</c><00:00:35.489><c> the</c><00:00:35.579><c> number</c><00:00:35.610><c> of</c>

00:00:35.870 --> 00:00:35.880 align:start position:0%
function as a function of the number of
 

00:00:35.880 --> 00:00:39.229 align:start position:0%
function as a function of the number of
iterations<00:00:36.450><c> so</c><00:00:36.660><c> that</c><00:00:37.559><c> was</c><00:00:37.770><c> the</c><00:00:37.950><c> cost</c><00:00:38.730><c> function</c>

00:00:39.229 --> 00:00:39.239 align:start position:0%
iterations so that was the cost function
 

00:00:39.239 --> 00:00:41.720 align:start position:0%
iterations so that was the cost function
and<00:00:39.420><c> we</c><00:00:39.870><c> would</c><00:00:40.020><c> make</c><00:00:40.200><c> sure</c><00:00:40.230><c> that</c><00:00:40.440><c> this</c><00:00:41.430><c> cost</c>

00:00:41.720 --> 00:00:41.730 align:start position:0%
and we would make sure that this cost
 

00:00:41.730 --> 00:00:43.760 align:start position:0%
and we would make sure that this cost
function<00:00:41.910><c> is</c><00:00:42.420><c> decreasing</c><00:00:42.690><c> on</c><00:00:43.350><c> every</c>

00:00:43.760 --> 00:00:43.770 align:start position:0%
function is decreasing on every
 

00:00:43.770 --> 00:00:46.040 align:start position:0%
function is decreasing on every
iteration<00:00:43.940><c> when</c><00:00:44.940><c> the</c><00:00:45.059><c> training</c><00:00:45.360><c> set</c><00:00:45.570><c> size</c><00:00:45.780><c> was</c>

00:00:46.040 --> 00:00:46.050 align:start position:0%
iteration when the training set size was
 

00:00:46.050 --> 00:00:47.959 align:start position:0%
iteration when the training set size was
small<00:00:46.079><c> we</c><00:00:46.649><c> could</c><00:00:46.829><c> do</c><00:00:46.980><c> that</c><00:00:47.160><c> because</c><00:00:47.489><c> we</c><00:00:47.850><c> could</c>

00:00:47.959 --> 00:00:47.969 align:start position:0%
small we could do that because we could
 

00:00:47.969 --> 00:00:50.959 align:start position:0%
small we could do that because we could
compute<00:00:48.360><c> this</c><00:00:48.629><c> sum</c><00:00:48.989><c> pretty</c><00:00:49.739><c> efficiently</c><00:00:50.309><c> that</c>

00:00:50.959 --> 00:00:50.969 align:start position:0%
compute this sum pretty efficiently that
 

00:00:50.969 --> 00:00:52.910 align:start position:0%
compute this sum pretty efficiently that
when<00:00:51.180><c> you</c><00:00:51.300><c> have</c><00:00:51.510><c> a</c><00:00:51.750><c> massive</c><00:00:52.230><c> training</c><00:00:52.800><c> set</c>

00:00:52.910 --> 00:00:52.920 align:start position:0%
when you have a massive training set
 

00:00:52.920 --> 00:00:55.610 align:start position:0%
when you have a massive training set
size<00:00:53.300><c> then</c><00:00:54.300><c> you</c><00:00:54.570><c> don't</c><00:00:54.809><c> want</c><00:00:55.050><c> to</c><00:00:55.170><c> have</c><00:00:55.440><c> to</c>

00:00:55.610 --> 00:00:55.620 align:start position:0%
size then you don't want to have to
 

00:00:55.620 --> 00:00:58.189 align:start position:0%
size then you don't want to have to
pause<00:00:55.920><c> your</c><00:00:56.250><c> algorithm</c><00:00:56.850><c> periodically</c><00:00:57.629><c> you</c>

00:00:58.189 --> 00:00:58.199 align:start position:0%
pause your algorithm periodically you
 

00:00:58.199 --> 00:00:59.990 align:start position:0%
pause your algorithm periodically you
don't<00:00:58.379><c> want</c><00:00:58.530><c> to</c><00:00:58.649><c> have</c><00:00:59.129><c> the</c><00:00:59.250><c> paused</c><00:00:59.550><c> stochastic</c>

00:00:59.990 --> 00:01:00.000 align:start position:0%
don't want to have the paused stochastic
 

00:01:00.000 --> 00:01:02.209 align:start position:0%
don't want to have the paused stochastic
gradient<00:01:00.329><c> descent</c><00:01:00.629><c> periodically</c><00:01:01.440><c> in</c><00:01:01.739><c> order</c>

00:01:02.209 --> 00:01:02.219 align:start position:0%
gradient descent periodically in order
 

00:01:02.219 --> 00:01:04.609 align:start position:0%
gradient descent periodically in order
to<00:01:02.460><c> compute</c><00:01:03.059><c> this</c><00:01:03.420><c> cost</c><00:01:03.719><c> function</c><00:01:03.899><c> since</c><00:01:04.500><c> it</c>

00:01:04.609 --> 00:01:04.619 align:start position:0%
to compute this cost function since it
 

00:01:04.619 --> 00:01:06.679 align:start position:0%
to compute this cost function since it
requires<00:01:04.979><c> a</c><00:01:05.250><c> sum</c><00:01:05.580><c> over</c><00:01:05.790><c> your</c><00:01:06.119><c> entire</c><00:01:06.450><c> training</c>

00:01:06.679 --> 00:01:06.689 align:start position:0%
requires a sum over your entire training
 

00:01:06.689 --> 00:01:08.929 align:start position:0%
requires a sum over your entire training
set<00:01:07.020><c> size</c><00:01:07.260><c> and</c><00:01:08.189><c> the</c><00:01:08.340><c> whole</c><00:01:08.520><c> point</c><00:01:08.790><c> of</c>

00:01:08.929 --> 00:01:08.939 align:start position:0%
set size and the whole point of
 

00:01:08.939 --> 00:01:10.940 align:start position:0%
set size and the whole point of
stochastic<00:01:09.450><c> gradient</c><00:01:09.750><c> descent</c><00:01:10.020><c> was</c><00:01:10.530><c> that</c><00:01:10.740><c> you</c>

00:01:10.940 --> 00:01:10.950 align:start position:0%
stochastic gradient descent was that you
 

00:01:10.950 --> 00:01:12.710 align:start position:0%
stochastic gradient descent was that you
wanted<00:01:11.250><c> to</c><00:01:11.400><c> start</c><00:01:11.610><c> to</c><00:01:11.790><c> make</c><00:01:11.970><c> progress</c><00:01:12.210><c> after</c>

00:01:12.710 --> 00:01:12.720 align:start position:0%
wanted to start to make progress after
 

00:01:12.720 --> 00:01:15.080 align:start position:0%
wanted to start to make progress after
looking<00:01:13.170><c> at</c><00:01:13.290><c> just</c><00:01:13.530><c> a</c><00:01:13.619><c> single</c><00:01:13.979><c> example</c><00:01:14.130><c> what</c>

00:01:15.080 --> 00:01:15.090 align:start position:0%
looking at just a single example what
 

00:01:15.090 --> 00:01:17.270 align:start position:0%
looking at just a single example what
about<00:01:15.210><c> needing</c><00:01:15.570><c> to</c><00:01:15.750><c> occasionally</c><00:01:16.710><c> scan</c>

00:01:17.270 --> 00:01:17.280 align:start position:0%
about needing to occasionally scan
 

00:01:17.280 --> 00:01:19.640 align:start position:0%
about needing to occasionally scan
through<00:01:17.640><c> your</c><00:01:17.670><c> entire</c><00:01:18.360><c> training</c><00:01:18.810><c> set</c><00:01:19.110><c> right</c>

00:01:19.640 --> 00:01:19.650 align:start position:0%
through your entire training set right
 

00:01:19.650 --> 00:01:21.830 align:start position:0%
through your entire training set right
in<00:01:19.680><c> the</c><00:01:19.979><c> middle</c><00:01:20.280><c> of</c><00:01:20.460><c> the</c><00:01:20.610><c> algorithm</c><00:01:21.150><c> just</c><00:01:21.330><c> to</c>

00:01:21.830 --> 00:01:21.840 align:start position:0%
in the middle of the algorithm just to
 

00:01:21.840 --> 00:01:23.810 align:start position:0%
in the middle of the algorithm just to
compute<00:01:22.290><c> things</c><00:01:22.500><c> like</c><00:01:22.799><c> the</c><00:01:23.250><c> cost</c><00:01:23.610><c> function</c>

00:01:23.810 --> 00:01:23.820 align:start position:0%
compute things like the cost function
 

00:01:23.820 --> 00:01:26.899 align:start position:0%
compute things like the cost function
over<00:01:24.330><c> your</c><00:01:24.450><c> entire</c><00:01:24.540><c> training</c><00:01:24.960><c> set</c><00:01:25.520><c> so</c><00:01:26.520><c> first</c>

00:01:26.899 --> 00:01:26.909 align:start position:0%
over your entire training set so first
 

00:01:26.909 --> 00:01:29.359 align:start position:0%
over your entire training set so first
oh<00:01:27.090><c> constant</c><00:01:27.540><c> gradient</c><00:01:27.630><c> descent</c><00:01:28.170><c> in</c><00:01:28.470><c> order</c><00:01:29.009><c> to</c>

00:01:29.359 --> 00:01:29.369 align:start position:0%
oh constant gradient descent in order to
 

00:01:29.369 --> 00:01:31.039 align:start position:0%
oh constant gradient descent in order to
check<00:01:29.670><c> that</c><00:01:29.850><c> the</c><00:01:29.970><c> Abril</c><00:01:30.299><c> is</c><00:01:30.390><c> converging</c>

00:01:31.039 --> 00:01:31.049 align:start position:0%
check that the Abril is converging
 

00:01:31.049 --> 00:01:33.499 align:start position:0%
check that the Abril is converging
here's<00:01:31.530><c> what</c><00:01:31.740><c> we</c><00:01:31.799><c> can</c><00:01:31.950><c> do</c><00:01:32.070><c> instead</c><00:01:32.270><c> let's</c><00:01:33.270><c> take</c>

00:01:33.499 --> 00:01:33.509 align:start position:0%
here's what we can do instead let's take
 

00:01:33.509 --> 00:01:35.480 align:start position:0%
here's what we can do instead let's take
the<00:01:33.720><c> definition</c><00:01:34.200><c> of</c><00:01:34.560><c> the</c><00:01:34.680><c> cost</c><00:01:34.920><c> that</c><00:01:35.130><c> we</c><00:01:35.310><c> have</c>

00:01:35.480 --> 00:01:35.490 align:start position:0%
the definition of the cost that we have
 

00:01:35.490 --> 00:01:37.249 align:start position:0%
the definition of the cost that we have
previously<00:01:35.850><c> so</c><00:01:36.270><c> the</c><00:01:36.390><c> cost</c><00:01:36.630><c> of</c><00:01:36.750><c> the</c><00:01:36.869><c> parameters</c>

00:01:37.249 --> 00:01:37.259 align:start position:0%
previously so the cost of the parameters
 

00:01:37.259 --> 00:01:38.810 align:start position:0%
previously so the cost of the parameters
theta<00:01:37.409><c> with</c><00:01:37.860><c> respect</c><00:01:37.890><c> to</c><00:01:38.310><c> a</c><00:01:38.460><c> single</c><00:01:38.640><c> training</c>

00:01:38.810 --> 00:01:38.820 align:start position:0%
theta with respect to a single training
 

00:01:38.820 --> 00:01:41.480 align:start position:0%
theta with respect to a single training
example<00:01:39.150><c> is</c><00:01:39.900><c> just</c><00:01:40.290><c> 1/2</c><00:01:40.680><c> of</c><00:01:40.890><c> the</c><00:01:40.979><c> squared</c><00:01:41.280><c> error</c>

00:01:41.480 --> 00:01:41.490 align:start position:0%
example is just 1/2 of the squared error
 

00:01:41.490 --> 00:01:43.000 align:start position:0%
example is just 1/2 of the squared error
on<00:01:41.729><c> that</c><00:01:41.939><c> tree</c>

00:01:43.000 --> 00:01:43.010 align:start position:0%
on that tree
 

00:01:43.010 --> 00:01:45.530 align:start position:0%
on that tree
then<00:01:44.010><c> while</c><00:01:44.400><c> stochastic</c><00:01:44.880><c> gradient</c><00:01:45.240><c> descent</c>

00:01:45.530 --> 00:01:45.540 align:start position:0%
then while stochastic gradient descent
 

00:01:45.540 --> 00:01:48.860 align:start position:0%
then while stochastic gradient descent
is<00:01:45.930><c> learning</c><00:01:46.229><c> right</c><00:01:47.130><c> before</c><00:01:47.640><c> we</c><00:01:48.150><c> train</c><00:01:48.659><c> on</c><00:01:48.840><c> a</c>

00:01:48.860 --> 00:01:48.870 align:start position:0%
is learning right before we train on a
 

00:01:48.870 --> 00:01:50.930 align:start position:0%
is learning right before we train on a
specific<00:01:49.290><c> example</c><00:01:49.590><c> so</c><00:01:50.070><c> in</c><00:01:50.280><c> Stoke</c><00:01:50.490><c> African</c>

00:01:50.930 --> 00:01:50.940 align:start position:0%
specific example so in Stoke African
 

00:01:50.940 --> 00:01:52.160 align:start position:0%
specific example so in Stoke African
descent<00:01:51.300><c> we're</c><00:01:51.479><c> going</c><00:01:51.630><c> to</c><00:01:51.750><c> look</c><00:01:51.930><c> at</c><00:01:52.050><c> the</c>

00:01:52.160 --> 00:01:52.170 align:start position:0%
descent we're going to look at the
 

00:01:52.170 --> 00:01:54.770 align:start position:0%
descent we're going to look at the
examples<00:01:52.650><c> X</c><00:01:52.890><c> I</c><00:01:53.070><c> Y</c><00:01:53.369><c> I</c><00:01:53.400><c> in</c><00:01:53.909><c> order</c><00:01:54.300><c> and</c><00:01:54.600><c> then</c><00:01:54.750><c> sort</c>

00:01:54.770 --> 00:01:54.780 align:start position:0%
examples X I Y I in order and then sort
 

00:01:54.780 --> 00:01:56.690 align:start position:0%
examples X I Y I in order and then sort
of<00:01:55.080><c> take</c><00:01:55.140><c> a</c><00:01:55.320><c> little</c><00:01:55.590><c> update</c><00:01:55.950><c> with</c><00:01:56.190><c> respect</c><00:01:56.580><c> to</c>

00:01:56.690 --> 00:01:56.700 align:start position:0%
of take a little update with respect to
 

00:01:56.700 --> 00:01:58.820 align:start position:0%
of take a little update with respect to
this<00:01:56.850><c> example</c><00:01:57.210><c> and</c><00:01:57.720><c> then</c><00:01:58.020><c> go</c><00:01:58.200><c> on</c><00:01:58.350><c> to</c><00:01:58.440><c> the</c><00:01:58.530><c> next</c>

00:01:58.820 --> 00:01:58.830 align:start position:0%
this example and then go on to the next
 

00:01:58.830 --> 00:01:59.390 align:start position:0%
this example and then go on to the next
example

00:01:59.390 --> 00:01:59.400 align:start position:0%
example
 

00:01:59.400 --> 00:02:04.040 align:start position:0%
example
X<00:01:59.759><c> I</c><00:02:00.060><c> plus</c><00:02:00.300><c> 1</c><00:02:00.540><c> y</c><00:02:01.500><c> I</c><00:02:01.530><c> plus</c><00:02:02.040><c> 1</c><00:02:02.310><c> and</c><00:02:03.210><c> so</c><00:02:03.360><c> on</c><00:02:03.450><c> right</c>

00:02:04.040 --> 00:02:04.050 align:start position:0%
X I plus 1 y I plus 1 and so on right
 

00:02:04.050 --> 00:02:05.600 align:start position:0%
X I plus 1 y I plus 1 and so on right
that's<00:02:04.410><c> what's</c><00:02:04.619><c> the</c><00:02:04.710><c> cost</c><00:02:04.920><c> appear</c><00:02:05.190><c> descent</c>

00:02:05.600 --> 00:02:05.610 align:start position:0%
that's what's the cost appear descent
 

00:02:05.610 --> 00:02:09.499 align:start position:0%
that's what's the cost appear descent
does<00:02:05.930><c> so</c><00:02:06.930><c> while</c><00:02:07.590><c> the</c><00:02:07.800><c> algorithm</c><00:02:08.310><c> is</c><00:02:08.550><c> looking</c>

00:02:09.499 --> 00:02:09.509 align:start position:0%
does so while the algorithm is looking
 

00:02:09.509 --> 00:02:13.490 align:start position:0%
does so while the algorithm is looking
at<00:02:09.630><c> the</c><00:02:09.750><c> example</c><00:02:10.200><c> X</c><00:02:10.350><c> I</c><00:02:10.470><c> Y</c><00:02:10.679><c> I</c><00:02:10.790><c> but</c><00:02:11.790><c> before</c><00:02:12.360><c> it</c><00:02:13.290><c> has</c>

00:02:13.490 --> 00:02:13.500 align:start position:0%
at the example X I Y I but before it has
 

00:02:13.500 --> 00:02:16.420 align:start position:0%
at the example X I Y I but before it has
updated<00:02:14.040><c> the</c><00:02:14.400><c> parameters</c><00:02:14.910><c> data</c><00:02:15.120><c> using</c><00:02:15.690><c> that</c>

00:02:16.420 --> 00:02:16.430 align:start position:0%
updated the parameters data using that
 

00:02:16.430 --> 00:02:19.760 align:start position:0%
updated the parameters data using that
example<00:02:17.430><c> let's</c><00:02:18.239><c> compute</c><00:02:18.600><c> the</c><00:02:18.690><c> cost</c><00:02:19.110><c> of</c><00:02:19.380><c> that</c>

00:02:19.760 --> 00:02:19.770 align:start position:0%
example let's compute the cost of that
 

00:02:19.770 --> 00:02:22.130 align:start position:0%
example let's compute the cost of that
example<00:02:20.370><c> just</c><00:02:21.150><c> to</c><00:02:21.239><c> say</c><00:02:21.390><c> the</c><00:02:21.570><c> same</c><00:02:21.720><c> thing</c><00:02:21.930><c> again</c>

00:02:22.130 --> 00:02:22.140 align:start position:0%
example just to say the same thing again
 

00:02:22.140 --> 00:02:24.140 align:start position:0%
example just to say the same thing again
by<00:02:22.500><c> using</c><00:02:22.650><c> slightly</c><00:02:23.010><c> different</c><00:02:23.100><c> words</c><00:02:23.489><c> as</c>

00:02:24.140 --> 00:02:24.150 align:start position:0%
by using slightly different words as
 

00:02:24.150 --> 00:02:26.420 align:start position:0%
by using slightly different words as
stochastic<00:02:24.780><c> gradient</c><00:02:25.140><c> descent</c><00:02:25.440><c> is</c><00:02:25.950><c> scanning</c>

00:02:26.420 --> 00:02:26.430 align:start position:0%
stochastic gradient descent is scanning
 

00:02:26.430 --> 00:02:29.540 align:start position:0%
stochastic gradient descent is scanning
through<00:02:26.850><c> our</c><00:02:27.060><c> training</c><00:02:27.269><c> set</c><00:02:27.739><c> right</c><00:02:28.739><c> before</c><00:02:28.920><c> we</c>

00:02:29.540 --> 00:02:29.550 align:start position:0%
through our training set right before we
 

00:02:29.550 --> 00:02:31.400 align:start position:0%
through our training set right before we
have<00:02:29.730><c> updated</c><00:02:30.150><c> theta</c><00:02:30.510><c> using</c><00:02:30.930><c> a</c><00:02:31.140><c> particular</c>

00:02:31.400 --> 00:02:31.410 align:start position:0%
have updated theta using a particular
 

00:02:31.410 --> 00:02:33.830 align:start position:0%
have updated theta using a particular
training<00:02:31.800><c> example</c><00:02:32.220><c> X</c><00:02:32.370><c> I</c><00:02:32.489><c> comma</c><00:02:32.760><c> Y</c><00:02:32.940><c> I</c><00:02:32.970><c> let's</c>

00:02:33.830 --> 00:02:33.840 align:start position:0%
training example X I comma Y I let's
 

00:02:33.840 --> 00:02:36.830 align:start position:0%
training example X I comma Y I let's
compute<00:02:34.350><c> how</c><00:02:34.800><c> well</c><00:02:35.070><c> our</c><00:02:35.459><c> hypothesis</c><00:02:36.360><c> is</c><00:02:36.510><c> doing</c>

00:02:36.830 --> 00:02:36.840 align:start position:0%
compute how well our hypothesis is doing
 

00:02:36.840 --> 00:02:38.930 align:start position:0%
compute how well our hypothesis is doing
on<00:02:37.080><c> that</c><00:02:37.290><c> training</c><00:02:37.709><c> example</c><00:02:38.220><c> yeah</c><00:02:38.700><c> and</c><00:02:38.880><c> we</c>

00:02:38.930 --> 00:02:38.940 align:start position:0%
on that training example yeah and we
 

00:02:38.940 --> 00:02:40.610 align:start position:0%
on that training example yeah and we
want<00:02:39.060><c> to</c><00:02:39.209><c> do</c><00:02:39.330><c> this</c><00:02:39.480><c> before</c><00:02:39.660><c> updating</c><00:02:40.200><c> theta</c>

00:02:40.610 --> 00:02:40.620 align:start position:0%
want to do this before updating theta
 

00:02:40.620 --> 00:02:42.860 align:start position:0%
want to do this before updating theta
because<00:02:41.160><c> if</c><00:02:41.640><c> we've</c><00:02:41.850><c> just</c><00:02:41.880><c> updated</c><00:02:42.300><c> theta</c>

00:02:42.860 --> 00:02:42.870 align:start position:0%
because if we've just updated theta
 

00:02:42.870 --> 00:02:44.690 align:start position:0%
because if we've just updated theta
using<00:02:43.200><c> that</c><00:02:43.470><c> training</c><00:02:43.769><c> example</c><00:02:44.190><c> you</c><00:02:44.580><c> know</c>

00:02:44.690 --> 00:02:44.700 align:start position:0%
using that training example you know
 

00:02:44.700 --> 00:02:46.370 align:start position:0%
using that training example you know
then<00:02:44.880><c> it</c><00:02:45.030><c> might</c><00:02:45.180><c> be</c><00:02:45.209><c> doing</c><00:02:45.660><c> better</c><00:02:45.870><c> on</c><00:02:46.080><c> that</c>

00:02:46.370 --> 00:02:46.380 align:start position:0%
then it might be doing better on that
 

00:02:46.380 --> 00:02:49.030 align:start position:0%
then it might be doing better on that
example<00:02:46.739><c> then</c><00:02:47.220><c> would</c><00:02:47.700><c> be</c><00:02:47.850><c> representative</c>

00:02:49.030 --> 00:02:49.040 align:start position:0%
example then would be representative
 

00:02:49.040 --> 00:02:51.470 align:start position:0%
example then would be representative
finally<00:02:50.040><c> in</c><00:02:50.280><c> order</c><00:02:50.640><c> to</c><00:02:51.030><c> check</c><00:02:51.209><c> for</c><00:02:51.420><c> the</c>

00:02:51.470 --> 00:02:51.480 align:start position:0%
finally in order to check for the
 

00:02:51.480 --> 00:02:52.819 align:start position:0%
finally in order to check for the
convergence<00:02:52.049><c> of</c><00:02:52.200><c> stochastic</c><00:02:52.530><c> gradient</c>

00:02:52.819 --> 00:02:52.829 align:start position:0%
convergence of stochastic gradient
 

00:02:52.829 --> 00:02:56.030 align:start position:0%
convergence of stochastic gradient
descent<00:02:53.040><c> what</c><00:02:53.790><c> we</c><00:02:53.910><c> can</c><00:02:54.030><c> do</c><00:02:54.209><c> is</c><00:02:54.600><c> every</c><00:02:55.260><c> say</c>

00:02:56.030 --> 00:02:56.040 align:start position:0%
descent what we can do is every say
 

00:02:56.040 --> 00:02:58.430 align:start position:0%
descent what we can do is every say
every<00:02:56.459><c> thousand</c><00:02:56.880><c> innovations</c><00:02:57.420><c> we</c><00:02:57.900><c> can</c><00:02:58.079><c> plot</c>

00:02:58.430 --> 00:02:58.440 align:start position:0%
every thousand innovations we can plot
 

00:02:58.440 --> 00:03:00.710 align:start position:0%
every thousand innovations we can plot
these<00:02:59.070><c> costs</c><00:02:59.700><c> that</c><00:02:59.820><c> we've</c><00:03:00.060><c> been</c><00:03:00.209><c> computing</c>

00:03:00.710 --> 00:03:00.720 align:start position:0%
these costs that we've been computing
 

00:03:00.720 --> 00:03:02.420 align:start position:0%
these costs that we've been computing
and<00:03:00.900><c> previous</c><00:03:01.260><c> step</c><00:03:01.470><c> we</c><00:03:01.709><c> can</c><00:03:01.860><c> plot</c><00:03:02.100><c> those</c>

00:03:02.420 --> 00:03:02.430 align:start position:0%
and previous step we can plot those
 

00:03:02.430 --> 00:03:05.420 align:start position:0%
and previous step we can plot those
costs<00:03:02.850><c> average</c><00:03:03.360><c> over</c><00:03:04.170><c> say</c><00:03:04.560><c> the</c><00:03:04.590><c> last</c><00:03:04.920><c> thousand</c>

00:03:05.420 --> 00:03:05.430 align:start position:0%
costs average over say the last thousand
 

00:03:05.430 --> 00:03:07.580 align:start position:0%
costs average over say the last thousand
examples<00:03:05.730><c> processed</c><00:03:06.600><c> by</c><00:03:06.780><c> the</c><00:03:06.840><c> algorithm</c><00:03:07.230><c> and</c>

00:03:07.580 --> 00:03:07.590 align:start position:0%
examples processed by the algorithm and
 

00:03:07.590 --> 00:03:10.100 align:start position:0%
examples processed by the algorithm and
if<00:03:08.579><c> you</c><00:03:08.670><c> do</c><00:03:08.820><c> this</c><00:03:09.000><c> it</c><00:03:09.299><c> kind</c><00:03:09.570><c> of</c><00:03:09.690><c> gives</c><00:03:09.870><c> you</c><00:03:09.989><c> a</c>

00:03:10.100 --> 00:03:10.110 align:start position:0%
if you do this it kind of gives you a
 

00:03:10.110 --> 00:03:11.840 align:start position:0%
if you do this it kind of gives you a
running<00:03:10.500><c> estimate</c><00:03:10.950><c> of</c><00:03:11.190><c> how</c><00:03:11.489><c> well</c><00:03:11.670><c> the</c>

00:03:11.840 --> 00:03:11.850 align:start position:0%
running estimate of how well the
 

00:03:11.850 --> 00:03:13.850 align:start position:0%
running estimate of how well the
algorithm<00:03:12.120><c> is</c><00:03:12.360><c> doing</c><00:03:12.390><c> on</c><00:03:12.930><c> you</c><00:03:13.350><c> know</c><00:03:13.470><c> the</c><00:03:13.680><c> last</c>

00:03:13.850 --> 00:03:13.860 align:start position:0%
algorithm is doing on you know the last
 

00:03:13.860 --> 00:03:16.220 align:start position:0%
algorithm is doing on you know the last
1,000<00:03:14.610><c> training</c><00:03:14.940><c> examples</c><00:03:15.420><c> that</c><00:03:15.690><c> your</c>

00:03:16.220 --> 00:03:16.230 align:start position:0%
1,000 training examples that your
 

00:03:16.230 --> 00:03:19.420 align:start position:0%
1,000 training examples that your
algorithm<00:03:16.470><c> has</c><00:03:16.739><c> seen</c><00:03:17.000><c> so</c><00:03:18.000><c> in</c><00:03:18.329><c> contrast</c><00:03:18.900><c> to</c>

00:03:19.420 --> 00:03:19.430 align:start position:0%
algorithm has seen so in contrast to
 

00:03:19.430 --> 00:03:22.100 align:start position:0%
algorithm has seen so in contrast to
computing<00:03:20.430><c> g-train</c><00:03:21.090><c> periodically</c><00:03:21.810><c> which</c>

00:03:22.100 --> 00:03:22.110 align:start position:0%
computing g-train periodically which
 

00:03:22.110 --> 00:03:23.210 align:start position:0%
computing g-train periodically which
needed<00:03:22.470><c> to</c><00:03:22.560><c> scan</c><00:03:22.829><c> through</c><00:03:23.070><c> the</c><00:03:23.190><c> entire</c>

00:03:23.210 --> 00:03:23.220 align:start position:0%
needed to scan through the entire
 

00:03:23.220 --> 00:03:25.280 align:start position:0%
needed to scan through the entire
training<00:03:23.790><c> set</c><00:03:24.060><c> with</c><00:03:24.690><c> this</c><00:03:24.870><c> order</c><00:03:25.079><c> procedure</c>

00:03:25.280 --> 00:03:25.290 align:start position:0%
training set with this order procedure
 

00:03:25.290 --> 00:03:27.530 align:start position:0%
training set with this order procedure
well<00:03:26.250><c> as</c><00:03:26.489><c> part</c><00:03:26.820><c> of</c><00:03:26.910><c> stochastic</c><00:03:27.239><c> gradient</c>

00:03:27.530 --> 00:03:27.540 align:start position:0%
well as part of stochastic gradient
 

00:03:27.540 --> 00:03:29.960 align:start position:0%
well as part of stochastic gradient
descent<00:03:27.810><c> it</c><00:03:28.320><c> doesn't</c><00:03:28.890><c> cost</c><00:03:29.070><c> much</c><00:03:29.340><c> to</c><00:03:29.370><c> compute</c>

00:03:29.960 --> 00:03:29.970 align:start position:0%
descent it doesn't cost much to compute
 

00:03:29.970 --> 00:03:31.789 align:start position:0%
descent it doesn't cost much to compute
these<00:03:30.090><c> costs</c><00:03:30.510><c> as</c><00:03:30.600><c> well</c><00:03:30.780><c> right</c><00:03:31.650><c> before</c>

00:03:31.789 --> 00:03:31.799 align:start position:0%
these costs as well right before
 

00:03:31.799 --> 00:03:33.740 align:start position:0%
these costs as well right before
updating<00:03:32.190><c> to</c><00:03:32.400><c> parameter</c><00:03:32.730><c> theta</c><00:03:32.880><c> and</c><00:03:33.360><c> all</c>

00:03:33.740 --> 00:03:33.750 align:start position:0%
updating to parameter theta and all
 

00:03:33.750 --> 00:03:35.270 align:start position:0%
updating to parameter theta and all
we're<00:03:33.870><c> doing</c><00:03:34.140><c> is</c><00:03:34.260><c> every</c><00:03:34.680><c> thousand</c><00:03:35.100><c> iterations</c>

00:03:35.270 --> 00:03:35.280 align:start position:0%
we're doing is every thousand iterations
 

00:03:35.280 --> 00:03:38.150 align:start position:0%
we're doing is every thousand iterations
or<00:03:35.670><c> so</c><00:03:35.910><c> we</c><00:03:36.329><c> just</c><00:03:36.540><c> average</c><00:03:36.989><c> the</c><00:03:37.260><c> last</c><00:03:37.410><c> 1,000</c>

00:03:38.150 --> 00:03:38.160 align:start position:0%
or so we just average the last 1,000
 

00:03:38.160 --> 00:03:40.220 align:start position:0%
or so we just average the last 1,000
cost<00:03:38.400><c> that</c><00:03:38.609><c> we</c><00:03:38.760><c> computed</c><00:03:39.209><c> and</c><00:03:39.480><c> we</c><00:03:39.540><c> plot</c><00:03:39.810><c> that</c>

00:03:40.220 --> 00:03:40.230 align:start position:0%
cost that we computed and we plot that
 

00:03:40.230 --> 00:03:43.610 align:start position:0%
cost that we computed and we plot that
and<00:03:40.970><c> by</c><00:03:41.970><c> looking</c><00:03:42.299><c> at</c><00:03:42.390><c> those</c><00:03:42.510><c> plots</c><00:03:42.870><c> this</c><00:03:43.410><c> will</c>

00:03:43.610 --> 00:03:43.620 align:start position:0%
and by looking at those plots this will
 

00:03:43.620 --> 00:03:45.380 align:start position:0%
and by looking at those plots this will
allow<00:03:43.859><c> us</c><00:03:43.890><c> to</c>

00:03:45.380 --> 00:03:45.390 align:start position:0%
allow us to
 

00:03:45.390 --> 00:03:46.940 align:start position:0%
allow us to
if<00:03:45.660><c> stochastic</c><00:03:46.290><c> great</c><00:03:46.440><c> into</c><00:03:46.590><c> census</c>

00:03:46.940 --> 00:03:46.950 align:start position:0%
if stochastic great into census
 

00:03:46.950 --> 00:03:49.640 align:start position:0%
if stochastic great into census
conversion<00:03:47.490><c> so</c><00:03:48.240><c> here</c><00:03:48.630><c> are</c><00:03:48.750><c> a</c><00:03:48.780><c> few</c><00:03:48.810><c> examples</c><00:03:49.050><c> of</c>

00:03:49.640 --> 00:03:49.650 align:start position:0%
conversion so here are a few examples of
 

00:03:49.650 --> 00:03:52.880 align:start position:0%
conversion so here are a few examples of
what<00:03:50.160><c> these</c><00:03:50.370><c> plots</c><00:03:50.880><c> might</c><00:03:51.210><c> look</c><00:03:51.420><c> like</c><00:03:51.890><c> suppose</c>

00:03:52.880 --> 00:03:52.890 align:start position:0%
what these plots might look like suppose
 

00:03:52.890 --> 00:03:54.500 align:start position:0%
what these plots might look like suppose
we<00:03:53.010><c> were</c><00:03:53.100><c> to</c><00:03:53.130><c> plot</c><00:03:53.370><c> the</c><00:03:53.550><c> cost</c><00:03:53.820><c> average</c><00:03:54.270><c> over</c>

00:03:54.500 --> 00:03:54.510 align:start position:0%
we were to plot the cost average over
 

00:03:54.510 --> 00:03:56.690 align:start position:0%
we were to plot the cost average over
the<00:03:54.600><c> last</c><00:03:54.750><c> thousand</c><00:03:55.110><c> examples</c><00:03:55.430><c> because</c><00:03:56.430><c> these</c>

00:03:56.690 --> 00:03:56.700 align:start position:0%
the last thousand examples because these
 

00:03:56.700 --> 00:03:58.520 align:start position:0%
the last thousand examples because these
are<00:03:57.030><c> average</c><00:03:57.570><c> over</c><00:03:57.900><c> just</c><00:03:58.110><c> a</c><00:03:58.200><c> thousand</c>

00:03:58.520 --> 00:03:58.530 align:start position:0%
are average over just a thousand
 

00:03:58.530 --> 00:04:00.050 align:start position:0%
are average over just a thousand
examples<00:03:58.770><c> they</c><00:03:59.190><c> are</c><00:03:59.310><c> going</c><00:03:59.520><c> to</c><00:03:59.640><c> be</c><00:03:59.760><c> a</c><00:03:59.790><c> little</c>

00:04:00.050 --> 00:04:00.060 align:start position:0%
examples they are going to be a little
 

00:04:00.060 --> 00:04:02.720 align:start position:0%
examples they are going to be a little
bit<00:04:00.180><c> noisy</c><00:04:00.390><c> and</c><00:04:00.870><c> so</c><00:04:01.230><c> it</c><00:04:01.620><c> may</c><00:04:01.830><c> not</c><00:04:01.860><c> decrease</c><00:04:02.310><c> on</c>

00:04:02.720 --> 00:04:02.730 align:start position:0%
bit noisy and so it may not decrease on
 

00:04:02.730 --> 00:04:04.910 align:start position:0%
bit noisy and so it may not decrease on
every<00:04:03.000><c> single</c><00:04:03.180><c> iteration</c><00:04:03.510><c> but</c><00:04:04.470><c> if</c><00:04:04.650><c> you</c><00:04:04.740><c> get</c><00:04:04.890><c> a</c>

00:04:04.910 --> 00:04:04.920 align:start position:0%
every single iteration but if you get a
 

00:04:04.920 --> 00:04:07.220 align:start position:0%
every single iteration but if you get a
figure<00:04:05.280><c> that</c><00:04:05.310><c> looks</c><00:04:05.760><c> like</c><00:04:05.880><c> this</c><00:04:06.150><c> so</c><00:04:06.930><c> the</c><00:04:07.020><c> plot</c>

00:04:07.220 --> 00:04:07.230 align:start position:0%
figure that looks like this so the plot
 

00:04:07.230 --> 00:04:09.140 align:start position:0%
figure that looks like this so the plot
is<00:04:07.470><c> noisy</c><00:04:07.980><c> because</c><00:04:08.280><c> this</c><00:04:08.400><c> average</c><00:04:08.790><c> over</c>

00:04:09.140 --> 00:04:09.150 align:start position:0%
is noisy because this average over
 

00:04:09.150 --> 00:04:10.730 align:start position:0%
is noisy because this average over
you're<00:04:09.390><c> just</c><00:04:09.630><c> a</c><00:04:09.720><c> small</c><00:04:09.990><c> subset</c><00:04:10.260><c> say</c><00:04:10.710><c> a</c>

00:04:10.730 --> 00:04:10.740 align:start position:0%
you're just a small subset say a
 

00:04:10.740 --> 00:04:13.100 align:start position:0%
you're just a small subset say a
thousand<00:04:11.220><c> training</c><00:04:11.430><c> examples</c><00:04:11.850><c> you</c><00:04:12.630><c> can</c><00:04:12.810><c> think</c>

00:04:13.100 --> 00:04:13.110 align:start position:0%
thousand training examples you can think
 

00:04:13.110 --> 00:04:14.930 align:start position:0%
thousand training examples you can think
of<00:04:13.410><c> that</c><00:04:13.530><c> looks</c><00:04:13.710><c> like</c><00:04:13.830><c> this</c><00:04:14.010><c> you</c><00:04:14.550><c> know</c><00:04:14.670><c> that</c>

00:04:14.930 --> 00:04:14.940 align:start position:0%
of that looks like this you know that
 

00:04:14.940 --> 00:04:16.970 align:start position:0%
of that looks like this you know that
would<00:04:15.060><c> be</c><00:04:15.210><c> a</c><00:04:15.240><c> pretty</c><00:04:15.840><c> decent</c><00:04:16.109><c> run</c><00:04:16.650><c> with</c><00:04:16.890><c> the</c>

00:04:16.970 --> 00:04:16.980 align:start position:0%
would be a pretty decent run with the
 

00:04:16.980 --> 00:04:18.979 align:start position:0%
would be a pretty decent run with the
algorithm<00:04:17.250><c> maybe</c><00:04:17.640><c> where</c><00:04:17.970><c> it</c><00:04:18.060><c> looks</c><00:04:18.269><c> like</c><00:04:18.359><c> the</c>

00:04:18.979 --> 00:04:18.989 align:start position:0%
algorithm maybe where it looks like the
 

00:04:18.989 --> 00:04:21.199 align:start position:0%
algorithm maybe where it looks like the
cost<00:04:19.260><c> has</c><00:04:19.500><c> gone</c><00:04:19.709><c> down</c><00:04:19.980><c> and</c><00:04:20.280><c> then</c><00:04:20.730><c> this</c><00:04:20.850><c> plateau</c>

00:04:21.199 --> 00:04:21.209 align:start position:0%
cost has gone down and then this plateau
 

00:04:21.209 --> 00:04:22.910 align:start position:0%
cost has gone down and then this plateau
that<00:04:21.570><c> was</c><00:04:21.780><c> kind</c><00:04:21.989><c> of</c><00:04:22.049><c> flattened</c><00:04:22.500><c> now</c><00:04:22.650><c> you're</c>

00:04:22.910 --> 00:04:22.920 align:start position:0%
that was kind of flattened now you're
 

00:04:22.920 --> 00:04:24.590 align:start position:0%
that was kind of flattened now you're
starting<00:04:23.400><c> from</c><00:04:23.550><c> around</c><00:04:23.790><c> that</c><00:04:24.060><c> point</c><00:04:24.330><c> so</c><00:04:24.570><c> it</c>

00:04:24.590 --> 00:04:24.600 align:start position:0%
starting from around that point so it
 

00:04:24.600 --> 00:04:26.390 align:start position:0%
starting from around that point so it
looks<00:04:24.780><c> like</c><00:04:24.960><c> that</c><00:04:25.680><c> there's</c><00:04:25.890><c> no</c><00:04:25.980><c> way</c><00:04:26.070><c> your</c><00:04:26.190><c> cost</c>

00:04:26.390 --> 00:04:26.400 align:start position:0%
looks like that there's no way your cost
 

00:04:26.400 --> 00:04:28.490 align:start position:0%
looks like that there's no way your cost
looks<00:04:26.610><c> like</c><00:04:26.820><c> then</c><00:04:27.450><c> maybe</c><00:04:27.780><c> your</c><00:04:28.170><c> learning</c>

00:04:28.490 --> 00:04:28.500 align:start position:0%
looks like then maybe your learning
 

00:04:28.500 --> 00:04:31.040 align:start position:0%
looks like then maybe your learning
algorithm<00:04:28.860><c> has</c><00:04:28.890><c> converged</c><00:04:29.630><c> if</c><00:04:30.630><c> you</c><00:04:30.780><c> were</c><00:04:30.900><c> to</c>

00:04:31.040 --> 00:04:31.050 align:start position:0%
algorithm has converged if you were to
 

00:04:31.050 --> 00:04:32.780 align:start position:0%
algorithm has converged if you were to
try<00:04:31.260><c> using</c><00:04:31.500><c> a</c><00:04:31.710><c> smaller</c><00:04:32.100><c> learning</c><00:04:32.610><c> rate</c>

00:04:32.780 --> 00:04:32.790 align:start position:0%
try using a smaller learning rate
 

00:04:32.790 --> 00:04:34.670 align:start position:0%
try using a smaller learning rate
something<00:04:33.210><c> that</c><00:04:33.510><c> you</c><00:04:33.630><c> might</c><00:04:33.750><c> see</c><00:04:33.810><c> it</c><00:04:34.200><c> is</c><00:04:34.320><c> that</c>

00:04:34.670 --> 00:04:34.680 align:start position:0%
something that you might see it is that
 

00:04:34.680 --> 00:04:36.830 align:start position:0%
something that you might see it is that
the<00:04:35.460><c> opera</c><00:04:35.760><c> may</c><00:04:35.940><c> initially</c><00:04:36.240><c> learn</c><00:04:36.570><c> more</c>

00:04:36.830 --> 00:04:36.840 align:start position:0%
the opera may initially learn more
 

00:04:36.840 --> 00:04:39.220 align:start position:0%
the opera may initially learn more
slowly<00:04:37.110><c> so</c><00:04:37.710><c> the</c><00:04:37.860><c> cost</c><00:04:38.100><c> goes</c><00:04:38.340><c> down</c><00:04:38.370><c> more</c><00:04:38.850><c> slowly</c>

00:04:39.220 --> 00:04:39.230 align:start position:0%
slowly so the cost goes down more slowly
 

00:04:39.230 --> 00:04:41.420 align:start position:0%
slowly so the cost goes down more slowly
but<00:04:40.230><c> then</c><00:04:40.350><c> eventually</c><00:04:40.650><c> you</c><00:04:40.920><c> have</c><00:04:41.070><c> a</c><00:04:41.160><c> smaller</c>

00:04:41.420 --> 00:04:41.430 align:start position:0%
but then eventually you have a smaller
 

00:04:41.430 --> 00:04:43.160 align:start position:0%
but then eventually you have a smaller
learning<00:04:41.700><c> rate</c><00:04:42.030><c> there's</c><00:04:42.300><c> actually</c><00:04:42.630><c> possible</c>

00:04:43.160 --> 00:04:43.170 align:start position:0%
learning rate there's actually possible
 

00:04:43.170 --> 00:04:46.010 align:start position:0%
learning rate there's actually possible
for<00:04:43.410><c> the</c><00:04:43.830><c> algorithm</c><00:04:44.100><c> to</c><00:04:44.340><c> end</c><00:04:44.820><c> up</c><00:04:45.060><c> at</c><00:04:45.240><c> a</c><00:04:45.270><c> maybe</c><00:04:45.900><c> a</c>

00:04:46.010 --> 00:04:46.020 align:start position:0%
for the algorithm to end up at a maybe a
 

00:04:46.020 --> 00:04:48.200 align:start position:0%
for the algorithm to end up at a maybe a
very<00:04:46.350><c> slightly</c><00:04:46.770><c> better</c><00:04:47.040><c> solution</c><00:04:47.640><c> so</c><00:04:47.910><c> the</c><00:04:48.030><c> red</c>

00:04:48.200 --> 00:04:48.210 align:start position:0%
very slightly better solution so the red
 

00:04:48.210 --> 00:04:50.060 align:start position:0%
very slightly better solution so the red
line<00:04:48.240><c> may</c><00:04:48.690><c> represent</c><00:04:49.200><c> the</c><00:04:49.380><c> behavior</c><00:04:49.770><c> of</c>

00:04:50.060 --> 00:04:50.070 align:start position:0%
line may represent the behavior of
 

00:04:50.070 --> 00:04:51.830 align:start position:0%
line may represent the behavior of
stochastic<00:04:50.400><c> we</c><00:04:50.700><c> understand</c><00:04:51.060><c> using</c><00:04:51.300><c> a</c><00:04:51.510><c> slower</c>

00:04:51.830 --> 00:04:51.840 align:start position:0%
stochastic we understand using a slower
 

00:04:51.840 --> 00:04:54.530 align:start position:0%
stochastic we understand using a slower
using<00:04:52.410><c> a</c><00:04:52.500><c> smaller</c><00:04:52.800><c> learning</c><00:04:53.040><c> rate</c><00:04:53.430><c> and</c><00:04:53.610><c> the</c>

00:04:54.530 --> 00:04:54.540 align:start position:0%
using a smaller learning rate and the
 

00:04:54.540 --> 00:04:56.600 align:start position:0%
using a smaller learning rate and the
reason<00:04:54.900><c> this</c><00:04:55.080><c> is</c><00:04:55.230><c> the</c><00:04:55.260><c> case</c><00:04:55.380><c> is</c><00:04:55.890><c> because</c><00:04:56.220><c> you</c>

00:04:56.600 --> 00:04:56.610 align:start position:0%
reason this is the case is because you
 

00:04:56.610 --> 00:04:58.250 align:start position:0%
reason this is the case is because you
remember<00:04:57.060><c> stochastic</c><00:04:57.690><c> gradient</c><00:04:57.990><c> descent</c>

00:04:58.250 --> 00:04:58.260 align:start position:0%
remember stochastic gradient descent
 

00:04:58.260 --> 00:05:00.470 align:start position:0%
remember stochastic gradient descent
doesn't<00:04:58.919><c> just</c><00:04:59.100><c> converge</c><00:04:59.520><c> the</c><00:04:59.760><c> global</c><00:05:00.150><c> minimum</c>

00:05:00.470 --> 00:05:00.480 align:start position:0%
doesn't just converge the global minimum
 

00:05:00.480 --> 00:05:02.870 align:start position:0%
doesn't just converge the global minimum
is<00:05:00.720><c> that</c><00:05:01.320><c> what</c><00:05:01.560><c> it</c><00:05:01.680><c> does</c><00:05:01.830><c> is</c><00:05:02.130><c> the</c><00:05:02.280><c> parameters</c>

00:05:02.870 --> 00:05:02.880 align:start position:0%
is that what it does is the parameters
 

00:05:02.880 --> 00:05:04.640 align:start position:0%
is that what it does is the parameters
will<00:05:03.060><c> oscillate</c><00:05:03.270><c> a</c><00:05:03.660><c> bit</c><00:05:03.900><c> around</c><00:05:04.169><c> the</c><00:05:04.410><c> global</c>

00:05:04.640 --> 00:05:04.650 align:start position:0%
will oscillate a bit around the global
 

00:05:04.650 --> 00:05:06.680 align:start position:0%
will oscillate a bit around the global
minimum<00:05:05.100><c> and</c><00:05:05.280><c> so</c><00:05:05.370><c> by</c><00:05:05.760><c> using</c><00:05:06.000><c> a</c><00:05:06.330><c> smaller</c>

00:05:06.680 --> 00:05:06.690 align:start position:0%
minimum and so by using a smaller
 

00:05:06.690 --> 00:05:08.540 align:start position:0%
minimum and so by using a smaller
learning<00:05:06.900><c> rate</c><00:05:07.260><c> you</c><00:05:07.770><c> end</c><00:05:07.919><c> up</c><00:05:08.070><c> with</c><00:05:08.220><c> smaller</c>

00:05:08.540 --> 00:05:08.550 align:start position:0%
learning rate you end up with smaller
 

00:05:08.550 --> 00:05:11.360 align:start position:0%
learning rate you end up with smaller
oscillations<00:05:09.330><c> and</c><00:05:09.890><c> sometimes</c><00:05:10.890><c> this</c><00:05:11.130><c> little</c>

00:05:11.360 --> 00:05:11.370 align:start position:0%
oscillations and sometimes this little
 

00:05:11.370 --> 00:05:13.510 align:start position:0%
oscillations and sometimes this little
difference<00:05:11.790><c> will</c><00:05:12.300><c> be</c><00:05:12.419><c> negligible</c><00:05:12.750><c> and</c>

00:05:13.510 --> 00:05:13.520 align:start position:0%
difference will be negligible and
 

00:05:13.520 --> 00:05:15.560 align:start position:0%
difference will be negligible and
sometimes<00:05:14.520><c> with</c><00:05:14.700><c> a</c><00:05:14.760><c> smaller</c><00:05:15.030><c> learning</c><00:05:15.240><c> ready</c>

00:05:15.560 --> 00:05:15.570 align:start position:0%
sometimes with a smaller learning ready
 

00:05:15.570 --> 00:05:19.070 align:start position:0%
sometimes with a smaller learning ready
can<00:05:15.750><c> get</c><00:05:15.900><c> a</c><00:05:15.930><c> slightly</c><00:05:16.380><c> better</c><00:05:17.810><c> value</c><00:05:18.810><c> for</c><00:05:18.870><c> the</c>

00:05:19.070 --> 00:05:19.080 align:start position:0%
can get a slightly better value for the
 

00:05:19.080 --> 00:05:20.810 align:start position:0%
can get a slightly better value for the
parameters<00:05:19.680><c> here</c><00:05:20.280><c> are</c><00:05:20.340><c> some</c><00:05:20.550><c> other</c><00:05:20.730><c> things</c>

00:05:20.810 --> 00:05:20.820 align:start position:0%
parameters here are some other things
 

00:05:20.820 --> 00:05:22.760 align:start position:0%
parameters here are some other things
that<00:05:21.240><c> might</c><00:05:21.360><c> happen</c><00:05:21.540><c> let's</c><00:05:22.050><c> say</c><00:05:22.140><c> he</c><00:05:22.350><c> runs</c><00:05:22.620><c> the</c>

00:05:22.760 --> 00:05:22.770 align:start position:0%
that might happen let's say he runs the
 

00:05:22.770 --> 00:05:24.590 align:start position:0%
that might happen let's say he runs the
Kospi<00:05:23.190><c> in</c><00:05:23.280><c> descent</c><00:05:23.580><c> and</c><00:05:23.760><c> you</c><00:05:23.850><c> average</c><00:05:24.030><c> over</c><00:05:24.419><c> a</c>

00:05:24.590 --> 00:05:24.600 align:start position:0%
Kospi in descent and you average over a
 

00:05:24.600 --> 00:05:27.409 align:start position:0%
Kospi in descent and you average over a
thousand<00:05:25.050><c> examples</c><00:05:25.730><c> when</c><00:05:26.730><c> when</c><00:05:26.970><c> plotting</c>

00:05:27.409 --> 00:05:27.419 align:start position:0%
thousand examples when when plotting
 

00:05:27.419 --> 00:05:29.480 align:start position:0%
thousand examples when when plotting
these<00:05:27.570><c> costs</c><00:05:28.080><c> so</c><00:05:28.590><c> you</c><00:05:28.650><c> know</c><00:05:28.860><c> here</c><00:05:29.250><c> it</c><00:05:29.310><c> might</c><00:05:29.430><c> be</c>

00:05:29.480 --> 00:05:29.490 align:start position:0%
these costs so you know here it might be
 

00:05:29.490 --> 00:05:32.360 align:start position:0%
these costs so you know here it might be
a<00:05:29.940><c> result</c><00:05:30.930><c> of</c><00:05:31.140><c> another</c><00:05:31.440><c> one</c><00:05:31.800><c> of</c><00:05:31.830><c> these</c><00:05:32.040><c> plots</c>

00:05:32.360 --> 00:05:32.370 align:start position:0%
a result of another one of these plots
 

00:05:32.370 --> 00:05:33.830 align:start position:0%
a result of another one of these plots
and<00:05:32.760><c> again</c><00:05:33.000><c> the</c><00:05:33.150><c> trial</c><00:05:33.390><c> looks</c><00:05:33.570><c> like</c><00:05:33.720><c> this</c>

00:05:33.830 --> 00:05:33.840 align:start position:0%
and again the trial looks like this
 

00:05:33.840 --> 00:05:34.540 align:start position:0%
and again the trial looks like this
converge

00:05:34.540 --> 00:05:34.550 align:start position:0%
converge
 

00:05:34.550 --> 00:05:38.050 align:start position:0%
converge
if<00:05:35.240><c> you</c><00:05:35.960><c> were</c><00:05:36.139><c> to</c><00:05:36.319><c> take</c><00:05:36.530><c> this</c><00:05:36.680><c> number</c><00:05:37.060><c> 8,000</c>

00:05:38.050 --> 00:05:38.060 align:start position:0%
if you were to take this number 8,000
 

00:05:38.060 --> 00:05:41.710 align:start position:0%
if you were to take this number 8,000
and<00:05:38.210><c> increase</c><00:05:39.110><c> it</c><00:05:39.319><c> to</c><00:05:39.530><c> averaging</c><00:05:40.099><c> over</c><00:05:40.720><c> 5,000</c>

00:05:41.710 --> 00:05:41.720 align:start position:0%
and increase it to averaging over 5,000
 

00:05:41.720 --> 00:05:43.930 align:start position:0%
and increase it to averaging over 5,000
examples<00:05:42.169><c> then</c><00:05:42.800><c> it's</c><00:05:42.979><c> possible</c><00:05:43.460><c> that</c><00:05:43.699><c> you</c>

00:05:43.930 --> 00:05:43.940 align:start position:0%
examples then it's possible that you
 

00:05:43.940 --> 00:05:47.170 align:start position:0%
examples then it's possible that you
might<00:05:44.120><c> get</c><00:05:44.599><c> a</c><00:05:44.840><c> smoother</c><00:05:45.440><c> curve</c><00:05:46.190><c> that</c><00:05:46.940><c> looks</c>

00:05:47.170 --> 00:05:47.180 align:start position:0%
might get a smoother curve that looks
 

00:05:47.180 --> 00:05:50.920 align:start position:0%
might get a smoother curve that looks
more<00:05:47.389><c> like</c><00:05:47.449><c> this</c><00:05:47.599><c> and</c><00:05:48.280><c> by</c><00:05:49.280><c> averaging</c><00:05:49.909><c> over</c><00:05:50.210><c> say</c>

00:05:50.920 --> 00:05:50.930 align:start position:0%
more like this and by averaging over say
 

00:05:50.930 --> 00:05:54.100 align:start position:0%
more like this and by averaging over say
5,000<00:05:51.740><c> examples</c><00:05:52.340><c> instead</c><00:05:52.610><c> of</c><00:05:52.819><c> 1,000</c><00:05:53.479><c> you</c>

00:05:54.100 --> 00:05:54.110 align:start position:0%
5,000 examples instead of 1,000 you
 

00:05:54.110 --> 00:05:55.990 align:start position:0%
5,000 examples instead of 1,000 you
might<00:05:54.349><c> be</c><00:05:54.530><c> really</c><00:05:54.740><c> get</c><00:05:54.949><c> a</c><00:05:55.159><c> smoother</c><00:05:55.669><c> curve</c>

00:05:55.990 --> 00:05:56.000 align:start position:0%
might be really get a smoother curve
 

00:05:56.000 --> 00:05:57.999 align:start position:0%
might be really get a smoother curve
like<00:05:56.300><c> this</c><00:05:56.539><c> and</c><00:05:56.870><c> so</c><00:05:57.259><c> that's</c><00:05:57.440><c> the</c><00:05:57.620><c> effect</c><00:05:57.710><c> of</c>

00:05:57.999 --> 00:05:58.009 align:start position:0%
like this and so that's the effect of
 

00:05:58.009 --> 00:05:59.680 align:start position:0%
like this and so that's the effect of
increasing<00:05:58.159><c> the</c><00:05:58.789><c> number</c><00:05:59.030><c> of</c><00:05:59.060><c> examples</c><00:05:59.569><c> your</c>

00:05:59.680 --> 00:05:59.690 align:start position:0%
increasing the number of examples your
 

00:05:59.690 --> 00:06:01.689 align:start position:0%
increasing the number of examples your
average<00:05:59.960><c> over</c><00:06:00.259><c> the</c><00:06:00.860><c> disadvantage</c><00:06:01.550><c> of</c><00:06:01.669><c> making</c>

00:06:01.689 --> 00:06:01.699 align:start position:0%
average over the disadvantage of making
 

00:06:01.699 --> 00:06:03.460 align:start position:0%
average over the disadvantage of making
this<00:06:02.030><c> too</c><00:06:02.210><c> big</c><00:06:02.360><c> of</c><00:06:02.599><c> course</c><00:06:02.629><c> is</c><00:06:03.050><c> that</c><00:06:03.199><c> now</c><00:06:03.409><c> you</c>

00:06:03.460 --> 00:06:03.470 align:start position:0%
this too big of course is that now you
 

00:06:03.470 --> 00:06:05.860 align:start position:0%
this too big of course is that now you
get<00:06:03.889><c> one</c><00:06:04.159><c> day</c><00:06:04.310><c> to</c><00:06:04.370><c> point</c><00:06:04.699><c> only</c><00:06:04.879><c> every</c><00:06:05.210><c> 5,000</c>

00:06:05.860 --> 00:06:05.870 align:start position:0%
get one day to point only every 5,000
 

00:06:05.870 --> 00:06:08.469 align:start position:0%
get one day to point only every 5,000
examples<00:06:06.349><c> and</c><00:06:06.680><c> so</c><00:06:07.190><c> the</c><00:06:07.370><c> feedback</c><00:06:07.759><c> you</c><00:06:08.000><c> get</c><00:06:08.210><c> on</c>

00:06:08.469 --> 00:06:08.479 align:start position:0%
examples and so the feedback you get on
 

00:06:08.479 --> 00:06:10.029 align:start position:0%
examples and so the feedback you get on
how<00:06:08.690><c> well</c><00:06:08.900><c> your</c><00:06:09.050><c> learning</c><00:06:09.080><c> outcome</c><00:06:09.620><c> is</c><00:06:09.740><c> doing</c>

00:06:10.029 --> 00:06:10.039 align:start position:0%
how well your learning outcome is doing
 

00:06:10.039 --> 00:06:11.950 align:start position:0%
how well your learning outcome is doing
is<00:06:10.250><c> sort</c><00:06:10.460><c> of</c><00:06:10.550><c> maybe</c><00:06:10.819><c> it's</c><00:06:10.970><c> more</c><00:06:11.180><c> delay</c><00:06:11.509><c> because</c>

00:06:11.950 --> 00:06:11.960 align:start position:0%
is sort of maybe it's more delay because
 

00:06:11.960 --> 00:06:14.110 align:start position:0%
is sort of maybe it's more delay because
you<00:06:12.380><c> get</c><00:06:12.590><c> one</c><00:06:12.860><c> data</c><00:06:13.069><c> point</c><00:06:13.190><c> on</c><00:06:13.580><c> your</c><00:06:13.699><c> plot</c><00:06:13.909><c> or</c>

00:06:14.110 --> 00:06:14.120 align:start position:0%
you get one data point on your plot or
 

00:06:14.120 --> 00:06:16.270 align:start position:0%
you get one data point on your plot or
the<00:06:14.150><c> every</c><00:06:14.449><c> 5,000</c><00:06:15.289><c> examples</c><00:06:15.740><c> rather</c><00:06:15.979><c> than</c>

00:06:16.270 --> 00:06:16.280 align:start position:0%
the every 5,000 examples rather than
 

00:06:16.280 --> 00:06:20.499 align:start position:0%
the every 5,000 examples rather than
early<00:06:17.680><c> 1001</c><00:06:18.680><c> was</c><00:06:18.830><c> similar</c><00:06:19.130><c> vein</c><00:06:19.509><c> sometimes</c>

00:06:20.499 --> 00:06:20.509 align:start position:0%
early 1001 was similar vein sometimes
 

00:06:20.509 --> 00:06:22.360 align:start position:0%
early 1001 was similar vein sometimes
you<00:06:20.750><c> may</c><00:06:20.870><c> run</c><00:06:21.050><c> stochastic</c><00:06:21.620><c> great</c><00:06:21.770><c> descent</c><00:06:22.159><c> and</c>

00:06:22.360 --> 00:06:22.370 align:start position:0%
you may run stochastic great descent and
 

00:06:22.370 --> 00:06:24.010 align:start position:0%
you may run stochastic great descent and
end<00:06:22.550><c> up</c><00:06:22.699><c> with</c><00:06:22.880><c> a</c><00:06:22.909><c> plot</c><00:06:23.210><c> that</c><00:06:23.419><c> looks</c><00:06:23.479><c> like</c><00:06:23.810><c> this</c>

00:06:24.010 --> 00:06:24.020 align:start position:0%
end up with a plot that looks like this
 

00:06:24.020 --> 00:06:26.950 align:start position:0%
end up with a plot that looks like this
and<00:06:24.409><c> with</c><00:06:25.280><c> a</c><00:06:25.310><c> plot</c><00:06:25.639><c> that</c><00:06:25.819><c> looks</c><00:06:25.880><c> like</c><00:06:26.240><c> this</c><00:06:26.479><c> you</c>

00:06:26.950 --> 00:06:26.960 align:start position:0%
and with a plot that looks like this you
 

00:06:26.960 --> 00:06:31.089 align:start position:0%
and with a plot that looks like this you
know<00:06:27.370><c> it</c><00:06:28.370><c> looks</c><00:06:28.580><c> like</c><00:06:28.750><c> the</c><00:06:29.750><c> cost</c><00:06:30.050><c> just</c><00:06:30.530><c> is</c><00:06:30.770><c> not</c>

00:06:31.089 --> 00:06:31.099 align:start position:0%
know it looks like the cost just is not
 

00:06:31.099 --> 00:06:32.770 align:start position:0%
know it looks like the cost just is not
the<00:06:31.370><c> appeasing</c><00:06:31.819><c> at</c><00:06:31.909><c> all</c><00:06:32.060><c> it</c><00:06:32.300><c> looks</c><00:06:32.479><c> like</c><00:06:32.659><c> the</c>

00:06:32.770 --> 00:06:32.780 align:start position:0%
the appeasing at all it looks like the
 

00:06:32.780 --> 00:06:34.450 align:start position:0%
the appeasing at all it looks like the
algorithm<00:06:33.139><c> is</c><00:06:33.199><c> just</c><00:06:33.289><c> not</c><00:06:33.650><c> learning</c><00:06:33.919><c> well</c><00:06:34.370><c> it</c>

00:06:34.450 --> 00:06:34.460 align:start position:0%
algorithm is just not learning well it
 

00:06:34.460 --> 00:06:36.790 align:start position:0%
algorithm is just not learning well it
just<00:06:34.490><c> looks</c><00:06:34.849><c> like</c><00:06:35.449><c> bithia</c><00:06:35.719><c> flat</c><00:06:36.199><c> curve</c><00:06:36.590><c> and</c>

00:06:36.790 --> 00:06:36.800 align:start position:0%
just looks like bithia flat curve and
 

00:06:36.800 --> 00:06:38.649 align:start position:0%
just looks like bithia flat curve and
it's<00:06:36.919><c> just</c><00:06:37.159><c> the</c><00:06:37.819><c> cost</c><00:06:38.090><c> is</c><00:06:38.210><c> just</c><00:06:38.419><c> not</c>

00:06:38.649 --> 00:06:38.659 align:start position:0%
it's just the cost is just not
 

00:06:38.659 --> 00:06:41.890 align:start position:0%
it's just the cost is just not
decreasing<00:06:39.319><c> but</c><00:06:40.280><c> again</c><00:06:40.940><c> if</c><00:06:41.270><c> you</c><00:06:41.449><c> were</c><00:06:41.630><c> to</c>

00:06:41.890 --> 00:06:41.900 align:start position:0%
decreasing but again if you were to
 

00:06:41.900 --> 00:06:45.279 align:start position:0%
decreasing but again if you were to
increase<00:06:42.349><c> this</c><00:06:42.909><c> to</c><00:06:43.909><c> averaging</c><00:06:44.539><c> over</c><00:06:44.750><c> a</c><00:06:44.900><c> larger</c>

00:06:45.279 --> 00:06:45.289 align:start position:0%
increase this to averaging over a larger
 

00:06:45.289 --> 00:06:47.649 align:start position:0%
increase this to averaging over a larger
number<00:06:45.440><c> of</c><00:06:45.620><c> examples</c><00:06:46.099><c> it's</c><00:06:46.669><c> possible</c><00:06:47.360><c> that</c>

00:06:47.649 --> 00:06:47.659 align:start position:0%
number of examples it's possible that
 

00:06:47.659 --> 00:06:49.689 align:start position:0%
number of examples it's possible that
you<00:06:47.870><c> see</c><00:06:48.110><c> something</c><00:06:48.530><c> like</c><00:06:48.680><c> this</c><00:06:48.919><c> red</c><00:06:49.190><c> line</c><00:06:49.430><c> it</c>

00:06:49.689 --> 00:06:49.699 align:start position:0%
you see something like this red line it
 

00:06:49.699 --> 00:06:51.070 align:start position:0%
you see something like this red line it
looks<00:06:49.880><c> like</c><00:06:50.029><c> the</c><00:06:50.240><c> cost</c><00:06:50.479><c> actually</c><00:06:50.900><c> is</c>

00:06:51.070 --> 00:06:51.080 align:start position:0%
looks like the cost actually is
 

00:06:51.080 --> 00:06:53.230 align:start position:0%
looks like the cost actually is
decreasing<00:06:51.650><c> it's</c><00:06:52.069><c> just</c><00:06:52.310><c> that</c><00:06:52.490><c> the</c><00:06:53.029><c> blue</c><00:06:53.210><c> line</c>

00:06:53.230 --> 00:06:53.240 align:start position:0%
decreasing it's just that the blue line
 

00:06:53.240 --> 00:06:55.899 align:start position:0%
decreasing it's just that the blue line
averaging<00:06:54.110><c> over</c><00:06:54.229><c> two</c><00:06:54.500><c> three</c><00:06:54.680><c> examples</c><00:06:55.190><c> the</c>

00:06:55.899 --> 00:06:55.909 align:start position:0%
averaging over two three examples the
 

00:06:55.909 --> 00:06:57.790 align:start position:0%
averaging over two three examples the
blue<00:06:56.120><c> line</c><00:06:56.330><c> was</c><00:06:56.509><c> too</c><00:06:56.719><c> noisy</c><00:06:56.900><c> so</c><00:06:57.349><c> you</c><00:06:57.440><c> couldn't</c>

00:06:57.790 --> 00:06:57.800 align:start position:0%
blue line was too noisy so you couldn't
 

00:06:57.800 --> 00:07:00.070 align:start position:0%
blue line was too noisy so you couldn't
see<00:06:58.009><c> the</c><00:06:58.130><c> actual</c><00:06:58.520><c> trend</c><00:06:58.940><c> in</c><00:06:59.539><c> the</c><00:06:59.779><c> cost</c>

00:07:00.070 --> 00:07:00.080 align:start position:0%
see the actual trend in the cost
 

00:07:00.080 --> 00:07:03.279 align:start position:0%
see the actual trend in the cost
actually<00:07:00.949><c> decreasing</c><00:07:01.580><c> and</c><00:07:02.289><c> possibly</c>

00:07:03.279 --> 00:07:03.289 align:start position:0%
actually decreasing and possibly
 

00:07:03.289 --> 00:07:05.709 align:start position:0%
actually decreasing and possibly
averaging<00:07:03.830><c> over</c><00:07:03.949><c> 5,000</c><00:07:04.759><c> examples</c><00:07:05.419><c> and</c><00:07:05.569><c> said</c>

00:07:05.709 --> 00:07:05.719 align:start position:0%
averaging over 5,000 examples and said
 

00:07:05.719 --> 00:07:07.870 align:start position:0%
averaging over 5,000 examples and said
1,000<00:07:06.319><c> may</c><00:07:06.409><c> help</c><00:07:06.620><c> of</c><00:07:07.250><c> course</c><00:07:07.490><c> when</c><00:07:07.789><c> you</c>

00:07:07.870 --> 00:07:07.880 align:start position:0%
1,000 may help of course when you
 

00:07:07.880 --> 00:07:09.580 align:start position:0%
1,000 may help of course when you
average<00:07:08.150><c> over</c><00:07:08.240><c> a</c><00:07:08.479><c> larger</c><00:07:08.810><c> number</c><00:07:08.930><c> of</c><00:07:09.050><c> examples</c>

00:07:09.580 --> 00:07:09.590 align:start position:0%
average over a larger number of examples
 

00:07:09.590 --> 00:07:11.950 align:start position:0%
average over a larger number of examples
which<00:07:10.219><c> was</c><00:07:10.430><c> average</c><00:07:10.819><c> here</c><00:07:11.060><c> over</c><00:07:11.360><c> 5,000</c>

00:07:11.950 --> 00:07:11.960 align:start position:0%
which was average here over 5,000
 

00:07:11.960 --> 00:07:13.450 align:start position:0%
which was average here over 5,000
examples<00:07:12.409><c> to</c><00:07:12.560><c> just</c><00:07:12.770><c> use</c><00:07:12.919><c> a</c><00:07:13.009><c> different</c><00:07:13.250><c> color</c>

00:07:13.450 --> 00:07:13.460 align:start position:0%
examples to just use a different color
 

00:07:13.460 --> 00:07:15.430 align:start position:0%
examples to just use a different color
it's<00:07:14.000><c> also</c><00:07:14.120><c> possible</c><00:07:14.449><c> that</c><00:07:14.690><c> you</c><00:07:14.870><c> see</c><00:07:15.080><c> that</c><00:07:15.319><c> the</c>

00:07:15.430 --> 00:07:15.440 align:start position:0%
it's also possible that you see that the
 

00:07:15.440 --> 00:07:17.050 align:start position:0%
it's also possible that you see that the
learning<00:07:15.590><c> curve</c><00:07:15.740><c> ends</c><00:07:16.250><c> up</c><00:07:16.430><c> looking</c><00:07:16.550><c> like</c><00:07:16.729><c> this</c>

00:07:17.050 --> 00:07:17.060 align:start position:0%
learning curve ends up looking like this
 

00:07:17.060 --> 00:07:19.029 align:start position:0%
learning curve ends up looking like this
there's<00:07:17.990><c> still</c><00:07:18.169><c> fact</c><00:07:18.469><c> even</c><00:07:18.710><c> when</c><00:07:18.919><c> you're</c>

00:07:19.029 --> 00:07:19.039 align:start position:0%
there's still fact even when you're
 

00:07:19.039 --> 00:07:20.379 align:start position:0%
there's still fact even when you're
averaging<00:07:19.370><c> over</c><00:07:19.520><c> a</c><00:07:19.639><c> larger</c><00:07:20.029><c> number</c><00:07:20.270><c> of</c>

00:07:20.379 --> 00:07:20.389 align:start position:0%
averaging over a larger number of
 

00:07:20.389 --> 00:07:23.409 align:start position:0%
averaging over a larger number of
examples<00:07:20.870><c> and</c><00:07:21.080><c> if</c><00:07:21.620><c> you</c><00:07:21.800><c> get</c><00:07:22.009><c> that</c><00:07:22.310><c> then</c><00:07:22.669><c> that's</c>

00:07:23.409 --> 00:07:23.419 align:start position:0%
examples and if you get that then that's
 

00:07:23.419 --> 00:07:26.080 align:start position:0%
examples and if you get that then that's
maybe<00:07:23.690><c> just</c><00:07:23.840><c> a</c><00:07:24.110><c> more</c><00:07:24.800><c> firm</c><00:07:25.099><c> verification</c><00:07:25.880><c> that</c>

00:07:26.080 --> 00:07:26.090 align:start position:0%
maybe just a more firm verification that
 

00:07:26.090 --> 00:07:28.059 align:start position:0%
maybe just a more firm verification that
unfortunately<00:07:26.810><c> the</c><00:07:26.990><c> outer</c><00:07:27.139><c> just</c><00:07:27.560><c> isn't</c><00:07:27.919><c> very</c>

00:07:28.059 --> 00:07:28.069 align:start position:0%
unfortunately the outer just isn't very
 

00:07:28.069 --> 00:07:29.440 align:start position:0%
unfortunately the outer just isn't very
much<00:07:28.370><c> for</c><00:07:28.610><c> whatever</c><00:07:28.789><c> reason</c>

00:07:29.440 --> 00:07:29.450 align:start position:0%
much for whatever reason
 

00:07:29.450 --> 00:07:31.930 align:start position:0%
much for whatever reason
and<00:07:29.680><c> you</c><00:07:30.680><c> need</c><00:07:30.920><c> to</c><00:07:31.130><c> either</c><00:07:31.370><c> change</c><00:07:31.730><c> the</c>

00:07:31.930 --> 00:07:31.940 align:start position:0%
and you need to either change the
 

00:07:31.940 --> 00:07:33.550 align:start position:0%
and you need to either change the
learning<00:07:32.030><c> rate</c><00:07:32.330><c> or</c><00:07:32.600><c> change</c><00:07:32.840><c> the</c><00:07:32.960><c> features</c><00:07:33.140><c> or</c>

00:07:33.550 --> 00:07:33.560 align:start position:0%
learning rate or change the features or
 

00:07:33.560 --> 00:07:34.570 align:start position:0%
learning rate or change the features or
change<00:07:33.770><c> something</c><00:07:33.980><c> else</c><00:07:34.220><c> about</c><00:07:34.370><c> the</c>

00:07:34.570 --> 00:07:34.580 align:start position:0%
change something else about the
 

00:07:34.580 --> 00:07:36.520 align:start position:0%
change something else about the
algorithm<00:07:35.000><c> finally</c><00:07:35.750><c> one</c><00:07:36.020><c> last</c><00:07:36.050><c> thing</c><00:07:36.470><c> that</c>

00:07:36.520 --> 00:07:36.530 align:start position:0%
algorithm finally one last thing that
 

00:07:36.530 --> 00:07:38.320 align:start position:0%
algorithm finally one last thing that
you<00:07:36.710><c> might</c><00:07:36.860><c> see</c><00:07:37.100><c> would</c><00:07:37.310><c> be</c><00:07:37.370><c> if</c><00:07:37.730><c> you</c><00:07:37.880><c> were</c><00:07:38.060><c> to</c>

00:07:38.320 --> 00:07:38.330 align:start position:0%
you might see would be if you were to
 

00:07:38.330 --> 00:07:40.510 align:start position:0%
you might see would be if you were to
plot<00:07:38.630><c> these</c><00:07:38.870><c> curves</c><00:07:38.930><c> and</c><00:07:39.560><c> you</c><00:07:39.770><c> see</c><00:07:40.040><c> a</c><00:07:40.070><c> curve</c>

00:07:40.510 --> 00:07:40.520 align:start position:0%
plot these curves and you see a curve
 

00:07:40.520 --> 00:07:41.950 align:start position:0%
plot these curves and you see a curve
that<00:07:40.730><c> looks</c><00:07:40.790><c> like</c><00:07:41.030><c> this</c><00:07:41.150><c> where</c><00:07:41.690><c> it</c><00:07:41.780><c> actually</c>

00:07:41.950 --> 00:07:41.960 align:start position:0%
that looks like this where it actually
 

00:07:41.960 --> 00:07:44.350 align:start position:0%
that looks like this where it actually
looks<00:07:42.290><c> like</c><00:07:42.410><c> this</c><00:07:42.590><c> increasing</c><00:07:43.280><c> and</c><00:07:43.460><c> if</c><00:07:44.180><c> that's</c>

00:07:44.350 --> 00:07:44.360 align:start position:0%
looks like this increasing and if that's
 

00:07:44.360 --> 00:07:46.810 align:start position:0%
looks like this increasing and if that's
the<00:07:44.540><c> case</c><00:07:44.600><c> then</c><00:07:45.260><c> this</c><00:07:45.560><c> is</c><00:07:45.770><c> a</c><00:07:45.800><c> sign</c><00:07:46.190><c> that</c><00:07:46.220><c> the</c>

00:07:46.810 --> 00:07:46.820 align:start position:0%
the case then this is a sign that the
 

00:07:46.820 --> 00:07:49.420 align:start position:0%
the case then this is a sign that the
algorithm<00:07:47.330><c> is</c><00:07:47.420><c> diverging</c><00:07:48.260><c> and</c><00:07:48.440><c> what</c><00:07:49.310><c> you</c>

00:07:49.420 --> 00:07:49.430 align:start position:0%
algorithm is diverging and what you
 

00:07:49.430 --> 00:07:51.580 align:start position:0%
algorithm is diverging and what you
really<00:07:49.640><c> should</c><00:07:49.730><c> do</c><00:07:50.060><c> is</c><00:07:50.390><c> use</c><00:07:50.990><c> a</c><00:07:51.020><c> smaller</c><00:07:51.470><c> value</c>

00:07:51.580 --> 00:07:51.590 align:start position:0%
really should do is use a smaller value
 

00:07:51.590 --> 00:07:55.660 align:start position:0%
really should do is use a smaller value
of<00:07:52.190><c> the</c><00:07:52.760><c> learning</c><00:07:53.090><c> rate</c><00:07:53.300><c> alpha</c><00:07:54.250><c> so</c><00:07:55.250><c> hopefully</c>

00:07:55.660 --> 00:07:55.670 align:start position:0%
of the learning rate alpha so hopefully
 

00:07:55.670 --> 00:07:57.220 align:start position:0%
of the learning rate alpha so hopefully
this<00:07:55.880><c> gives</c><00:07:55.910><c> you</c><00:07:56.240><c> a</c><00:07:56.300><c> sense</c><00:07:56.330><c> of</c><00:07:56.780><c> the</c><00:07:56.870><c> range</c><00:07:57.020><c> of</c>

00:07:57.220 --> 00:07:57.230 align:start position:0%
this gives you a sense of the range of
 

00:07:57.230 --> 00:07:58.840 align:start position:0%
this gives you a sense of the range of
phenomena<00:07:57.770><c> you</c><00:07:57.890><c> might</c><00:07:58.070><c> see</c><00:07:58.250><c> when</c><00:07:58.520><c> you</c><00:07:58.640><c> plot</c>

00:07:58.840 --> 00:07:58.850 align:start position:0%
phenomena you might see when you plot
 

00:07:58.850 --> 00:08:01.480 align:start position:0%
phenomena you might see when you plot
these<00:07:59.090><c> cost</c><00:07:59.480><c> average</c><00:08:00.050><c> over</c><00:08:00.470><c> some</c><00:08:01.070><c> range</c><00:08:01.310><c> of</c>

00:08:01.480 --> 00:08:01.490 align:start position:0%
these cost average over some range of
 

00:08:01.490 --> 00:08:04.570 align:start position:0%
these cost average over some range of
examples<00:08:02.030><c> as</c><00:08:02.600><c> well</c><00:08:02.810><c> as</c><00:08:03.050><c> suggest</c><00:08:04.040><c> the</c><00:08:04.220><c> sorts</c><00:08:04.520><c> of</c>

00:08:04.570 --> 00:08:04.580 align:start position:0%
examples as well as suggest the sorts of
 

00:08:04.580 --> 00:08:06.430 align:start position:0%
examples as well as suggest the sorts of
things<00:08:04.640><c> you</c><00:08:04.970><c> might</c><00:08:05.150><c> try</c><00:08:05.360><c> to</c><00:08:05.390><c> do</c><00:08:05.690><c> in</c><00:08:05.990><c> response</c>

00:08:06.430 --> 00:08:06.440 align:start position:0%
things you might try to do in response
 

00:08:06.440 --> 00:08:08.710 align:start position:0%
things you might try to do in response
to<00:08:06.680><c> seeing</c><00:08:06.980><c> different</c><00:08:07.490><c> plots</c><00:08:07.760><c> I</c><00:08:08.090><c> serve</c><00:08:08.540><c> the</c>

00:08:08.710 --> 00:08:08.720 align:start position:0%
to seeing different plots I serve the
 

00:08:08.720 --> 00:08:11.020 align:start position:0%
to seeing different plots I serve the
plot<00:08:08.990><c> looks</c><00:08:09.350><c> too</c><00:08:09.530><c> noisy</c><00:08:09.830><c> of</c><00:08:10.160><c> it</c><00:08:10.280><c> so</c><00:08:10.460><c> it</c><00:08:10.700><c> goes</c><00:08:10.850><c> up</c>

00:08:11.020 --> 00:08:11.030 align:start position:0%
plot looks too noisy of it so it goes up
 

00:08:11.030 --> 00:08:13.210 align:start position:0%
plot looks too noisy of it so it goes up
and<00:08:11.120><c> down</c><00:08:11.180><c> too</c><00:08:11.450><c> much</c><00:08:11.630><c> then</c><00:08:12.350><c> try</c><00:08:12.860><c> increasing</c>

00:08:13.210 --> 00:08:13.220 align:start position:0%
and down too much then try increasing
 

00:08:13.220 --> 00:08:15.670 align:start position:0%
and down too much then try increasing
the<00:08:14.150><c> number</c><00:08:14.600><c> of</c><00:08:14.690><c> examples</c><00:08:14.960><c> you're</c><00:08:15.290><c> averaging</c>

00:08:15.670 --> 00:08:15.680 align:start position:0%
the number of examples you're averaging
 

00:08:15.680 --> 00:08:17.950 align:start position:0%
the number of examples you're averaging
over<00:08:15.830><c> so</c><00:08:16.220><c> you</c><00:08:16.310><c> can</c><00:08:16.460><c> see</c><00:08:16.700><c> the</c><00:08:16.850><c> overall</c><00:08:17.180><c> trend</c><00:08:17.690><c> in</c>

00:08:17.950 --> 00:08:17.960 align:start position:0%
over so you can see the overall trend in
 

00:08:17.960 --> 00:08:21.070 align:start position:0%
over so you can see the overall trend in
the<00:08:18.080><c> plot</c><00:08:18.320><c> better</c><00:08:18.560><c> and</c><00:08:18.860><c> if</c><00:08:19.850><c> you</c><00:08:19.970><c> see</c><00:08:20.270><c> that</c><00:08:20.600><c> the</c>

00:08:21.070 --> 00:08:21.080 align:start position:0%
the plot better and if you see that the
 

00:08:21.080 --> 00:08:22.840 align:start position:0%
the plot better and if you see that the
errors<00:08:21.440><c> are</c><00:08:21.620><c> actually</c><00:08:21.740><c> increasing</c><00:08:22.190><c> the</c><00:08:22.520><c> costs</c>

00:08:22.840 --> 00:08:22.850 align:start position:0%
errors are actually increasing the costs
 

00:08:22.850 --> 00:08:24.730 align:start position:0%
errors are actually increasing the costs
are<00:08:22.940><c> actually</c><00:08:23.000><c> increasing</c><00:08:23.420><c> try</c><00:08:24.260><c> using</c><00:08:24.530><c> a</c>

00:08:24.730 --> 00:08:24.740 align:start position:0%
are actually increasing try using a
 

00:08:24.740 --> 00:08:27.460 align:start position:0%
are actually increasing try using a
smaller<00:08:25.070><c> value</c><00:08:25.400><c> of</c><00:08:25.640><c> alpha</c><00:08:26.290><c> finally</c><00:08:27.290><c> it's</c>

00:08:27.460 --> 00:08:27.470 align:start position:0%
smaller value of alpha finally it's
 

00:08:27.470 --> 00:08:29.800 align:start position:0%
smaller value of alpha finally it's
worth<00:08:27.530><c> examining</c><00:08:28.100><c> the</c><00:08:28.730><c> issue</c><00:08:29.060><c> of</c><00:08:29.600><c> the</c>

00:08:29.800 --> 00:08:29.810 align:start position:0%
worth examining the issue of the
 

00:08:29.810 --> 00:08:32.380 align:start position:0%
worth examining the issue of the
learning<00:08:30.230><c> rate</c><00:08:30.410><c> just</c><00:08:30.800><c> a</c><00:08:30.920><c> little</c><00:08:31.190><c> bit</c><00:08:31.340><c> more</c><00:08:31.520><c> we</c>

00:08:32.380 --> 00:08:32.390 align:start position:0%
learning rate just a little bit more we
 

00:08:32.390 --> 00:08:34.060 align:start position:0%
learning rate just a little bit more we
saw<00:08:32.570><c> that</c><00:08:32.840><c> when</c><00:08:32.960><c> you</c><00:08:33.020><c> run</c><00:08:33.260><c> stochastic</c><00:08:33.680><c> green</c>

00:08:34.060 --> 00:08:34.070 align:start position:0%
saw that when you run stochastic green
 

00:08:34.070 --> 00:08:36.520 align:start position:0%
saw that when you run stochastic green
descent<00:08:34.460><c> the</c><00:08:34.970><c> algorithm</c><00:08:35.570><c> will</c><00:08:35.840><c> start</c><00:08:36.200><c> here</c>

00:08:36.520 --> 00:08:36.530 align:start position:0%
descent the algorithm will start here
 

00:08:36.530 --> 00:08:39.100 align:start position:0%
descent the algorithm will start here
and<00:08:36.740><c> sort</c><00:08:37.070><c> of</c><00:08:37.210><c> meander</c><00:08:38.210><c> towards</c><00:08:38.450><c> the</c><00:08:38.720><c> minimum</c>

00:08:39.100 --> 00:08:39.110 align:start position:0%
and sort of meander towards the minimum
 

00:08:39.110 --> 00:08:41.140 align:start position:0%
and sort of meander towards the minimum
and<00:08:39.860><c> then</c><00:08:40.100><c> it</c><00:08:40.220><c> won't</c><00:08:40.370><c> really</c><00:08:40.520><c> converge</c><00:08:40.970><c> but</c>

00:08:41.140 --> 00:08:41.150 align:start position:0%
and then it won't really converge but
 

00:08:41.150 --> 00:08:42.730 align:start position:0%
and then it won't really converge but
instead<00:08:41.450><c> a</c><00:08:41.510><c> wonder</c><00:08:41.990><c> around</c><00:08:42.290><c> the</c><00:08:42.470><c> minimum</c>

00:08:42.730 --> 00:08:42.740 align:start position:0%
instead a wonder around the minimum
 

00:08:42.740 --> 00:08:45.040 align:start position:0%
instead a wonder around the minimum
forever<00:08:43.280><c> and</c><00:08:44.180><c> so</c><00:08:44.330><c> you</c><00:08:44.390><c> end</c><00:08:44.690><c> up</c><00:08:44.810><c> with</c><00:08:44.870><c> a</c>

00:08:45.040 --> 00:08:45.050 align:start position:0%
forever and so you end up with a
 

00:08:45.050 --> 00:08:46.930 align:start position:0%
forever and so you end up with a
parameter<00:08:45.590><c> value</c><00:08:45.740><c> that</c><00:08:46.160><c> is</c><00:08:46.310><c> hopefully</c><00:08:46.670><c> close</c>

00:08:46.930 --> 00:08:46.940 align:start position:0%
parameter value that is hopefully close
 

00:08:46.940 --> 00:08:48.670 align:start position:0%
parameter value that is hopefully close
to<00:08:47.090><c> the</c><00:08:47.150><c> global</c><00:08:47.330><c> minimum</c><00:08:47.570><c> that</c><00:08:48.350><c> won't</c><00:08:48.560><c> be</c>

00:08:48.670 --> 00:08:48.680 align:start position:0%
to the global minimum that won't be
 

00:08:48.680 --> 00:08:51.340 align:start position:0%
to the global minimum that won't be
exactly<00:08:49.160><c> at</c><00:08:49.430><c> the</c><00:08:49.580><c> global</c><00:08:49.940><c> minimum</c><00:08:50.240><c> in</c><00:08:50.990><c> most</c>

00:08:51.340 --> 00:08:51.350 align:start position:0%
exactly at the global minimum in most
 

00:08:51.350 --> 00:08:53.560 align:start position:0%
exactly at the global minimum in most
typical<00:08:51.800><c> implementations</c><00:08:52.580><c> of</c><00:08:52.820><c> stochastic</c>

00:08:53.560 --> 00:08:53.570 align:start position:0%
typical implementations of stochastic
 

00:08:53.570 --> 00:08:56.050 align:start position:0%
typical implementations of stochastic
gradient<00:08:53.930><c> descent</c><00:08:54.230><c> the</c><00:08:55.070><c> learning</c><00:08:55.460><c> rate</c><00:08:55.670><c> alpha</c>

00:08:56.050 --> 00:08:56.060 align:start position:0%
gradient descent the learning rate alpha
 

00:08:56.060 --> 00:08:58.900 align:start position:0%
gradient descent the learning rate alpha
is<00:08:56.540><c> typically</c><00:08:56.960><c> held</c><00:08:57.290><c> constant</c><00:08:58.040><c> and</c><00:08:58.280><c> so</c><00:08:58.640><c> what</c>

00:08:58.900 --> 00:08:58.910 align:start position:0%
is typically held constant and so what
 

00:08:58.910 --> 00:09:00.970 align:start position:0%
is typically held constant and so what
you<00:08:59.030><c> typically</c><00:08:59.420><c> ends</c><00:08:59.660><c> up</c><00:08:59.720><c> with</c><00:08:59.930><c> is</c><00:09:00.230><c> exactly</c>

00:09:00.970 --> 00:09:00.980 align:start position:0%
you typically ends up with is exactly
 

00:09:00.980 --> 00:09:03.580 align:start position:0%
you typically ends up with is exactly
the<00:09:01.250><c> picture</c><00:09:01.550><c> like</c><00:09:01.700><c> this</c><00:09:01.990><c> if</c><00:09:02.990><c> you</c><00:09:03.320><c> want</c>

00:09:03.580 --> 00:09:03.590 align:start position:0%
the picture like this if you want
 

00:09:03.590 --> 00:09:05.320 align:start position:0%
the picture like this if you want
stochastic<00:09:04.100><c> Granderson</c><00:09:04.820><c> to</c><00:09:04.970><c> actually</c>

00:09:05.320 --> 00:09:05.330 align:start position:0%
stochastic Granderson to actually
 

00:09:05.330 --> 00:09:07.240 align:start position:0%
stochastic Granderson to actually
converge<00:09:05.720><c> to</c><00:09:05.930><c> the</c><00:09:06.020><c> global</c><00:09:06.050><c> minimum</c><00:09:06.650><c> there's</c>

00:09:07.240 --> 00:09:07.250 align:start position:0%
converge to the global minimum there's
 

00:09:07.250 --> 00:09:09.220 align:start position:0%
converge to the global minimum there's
one<00:09:07.430><c> thing</c><00:09:07.670><c> which</c><00:09:08.270><c> you</c><00:09:08.300><c> can</c><00:09:08.450><c> do</c><00:09:08.750><c> which</c><00:09:08.990><c> is</c><00:09:09.140><c> you</c>

00:09:09.220 --> 00:09:09.230 align:start position:0%
one thing which you can do which is you
 

00:09:09.230 --> 00:09:10.930 align:start position:0%
one thing which you can do which is you
can<00:09:09.380><c> slowly</c><00:09:09.680><c> decrease</c><00:09:10.250><c> the</c><00:09:10.520><c> learning</c><00:09:10.850><c> rate</c>

00:09:10.930 --> 00:09:10.940 align:start position:0%
can slowly decrease the learning rate
 

00:09:10.940 --> 00:09:14.710 align:start position:0%
can slowly decrease the learning rate
alpha<00:09:11.150><c> over</c><00:09:11.720><c> time</c><00:09:12.220><c> so</c><00:09:13.220><c> a</c><00:09:13.520><c> pretty</c><00:09:13.970><c> typical</c><00:09:14.210><c> way</c>

00:09:14.710 --> 00:09:14.720 align:start position:0%
alpha over time so a pretty typical way
 

00:09:14.720 --> 00:09:17.350 align:start position:0%
alpha over time so a pretty typical way
of<00:09:14.990><c> doing</c><00:09:15.290><c> that</c><00:09:15.440><c> would</c><00:09:16.070><c> be</c><00:09:16.280><c> to</c><00:09:16.580><c> set</c><00:09:16.880><c> alpha</c>

00:09:17.350 --> 00:09:17.360 align:start position:0%
of doing that would be to set alpha
 

00:09:17.360 --> 00:09:19.990 align:start position:0%
of doing that would be to set alpha
equals<00:09:17.930><c> some</c><00:09:18.740><c> constant</c><00:09:18.770><c> 1</c><00:09:19.460><c> divided</c><00:09:19.970><c> by</c>

00:09:19.990 --> 00:09:20.000 align:start position:0%
equals some constant 1 divided by
 

00:09:20.000 --> 00:09:22.900 align:start position:0%
equals some constant 1 divided by
iteration<00:09:20.600><c> number</c><00:09:20.960><c> plus</c><00:09:21.500><c> constant</c><00:09:21.890><c> 2</c><00:09:22.220><c> so</c>

00:09:22.900 --> 00:09:22.910 align:start position:0%
iteration number plus constant 2 so
 

00:09:22.910 --> 00:09:24.580 align:start position:0%
iteration number plus constant 2 so
aeration<00:09:23.450><c> number</c><00:09:23.780><c> is</c><00:09:24.020><c> the</c><00:09:24.170><c> number</c><00:09:24.200><c> of</c>

00:09:24.580 --> 00:09:24.590 align:start position:0%
aeration number is the number of
 

00:09:24.590 --> 00:09:26.800 align:start position:0%
aeration number is the number of
iterations<00:09:25.130><c> you've</c><00:09:25.340><c> run</c><00:09:25.520><c> of</c><00:09:25.850><c> Circosta</c>

00:09:26.800 --> 00:09:26.810 align:start position:0%
iterations you've run of Circosta
 

00:09:26.810 --> 00:09:28.210 align:start position:0%
iterations you've run of Circosta
quintessence<00:09:27.320><c> is</c><00:09:27.440><c> really</c><00:09:27.740><c> the</c><00:09:27.860><c> number</c><00:09:28.010><c> of</c>

00:09:28.210 --> 00:09:28.220 align:start position:0%
quintessence is really the number of
 

00:09:28.220 --> 00:09:30.580 align:start position:0%
quintessence is really the number of
training<00:09:28.430><c> examples</c><00:09:28.580><c> you've</c><00:09:29.000><c> seen</c><00:09:29.240><c> and</c><00:09:29.900><c> cons</c><00:09:30.290><c> 1</c>

00:09:30.580 --> 00:09:30.590 align:start position:0%
training examples you've seen and cons 1
 

00:09:30.590 --> 00:09:33.520 align:start position:0%
training examples you've seen and cons 1
and<00:09:30.890><c> cos</c><00:09:31.070><c> 2</c><00:09:31.460><c> are</c><00:09:31.820><c> additional</c><00:09:32.630><c> parameters</c><00:09:33.290><c> of</c>

00:09:33.520 --> 00:09:33.530 align:start position:0%
and cos 2 are additional parameters of
 

00:09:33.530 --> 00:09:33.820 align:start position:0%
and cos 2 are additional parameters of
the

00:09:33.820 --> 00:09:33.830 align:start position:0%
the
 

00:09:33.830 --> 00:09:35.500 align:start position:0%
the
then<00:09:34.250><c> you</c><00:09:34.400><c> might</c><00:09:34.550><c> have</c><00:09:34.610><c> to</c><00:09:34.850><c> play</c><00:09:34.970><c> with</c><00:09:35.060><c> a</c><00:09:35.330><c> bit</c>

00:09:35.500 --> 00:09:35.510 align:start position:0%
then you might have to play with a bit
 

00:09:35.510 --> 00:09:39.070 align:start position:0%
then you might have to play with a bit
in<00:09:35.870><c> order</c><00:09:36.290><c> to</c><00:09:36.620><c> get</c><00:09:37.430><c> good</c><00:09:37.460><c> performance</c><00:09:37.930><c> one</c><00:09:38.930><c> of</c>

00:09:39.070 --> 00:09:39.080 align:start position:0%
in order to get good performance one of
 

00:09:39.080 --> 00:09:40.780 align:start position:0%
in order to get good performance one of
the<00:09:39.140><c> reason</c><00:09:39.500><c> is</c><00:09:39.560><c> people</c><00:09:39.890><c> tend</c><00:09:40.070><c> not</c><00:09:40.160><c> to</c><00:09:40.340><c> do</c><00:09:40.640><c> this</c>

00:09:40.780 --> 00:09:40.790 align:start position:0%
the reason is people tend not to do this
 

00:09:40.790 --> 00:09:42.730 align:start position:0%
the reason is people tend not to do this
is<00:09:41.030><c> because</c><00:09:41.510><c> you</c><00:09:41.780><c> end</c><00:09:41.930><c> up</c><00:09:42.080><c> needing</c><00:09:42.260><c> to</c><00:09:42.560><c> spend</c>

00:09:42.730 --> 00:09:42.740 align:start position:0%
is because you end up needing to spend
 

00:09:42.740 --> 00:09:44.170 align:start position:0%
is because you end up needing to spend
time<00:09:42.980><c> playing</c><00:09:43.310><c> with</c><00:09:43.490><c> these</c><00:09:43.640><c> two</c><00:09:43.850><c> extra</c>

00:09:44.170 --> 00:09:44.180 align:start position:0%
time playing with these two extra
 

00:09:44.180 --> 00:09:46.720 align:start position:0%
time playing with these two extra
parameters<00:09:44.690><c> cause</c><00:09:44.990><c> when</c><00:09:45.290><c> it</c><00:09:45.410><c> comes</c><00:09:45.440><c> to</c><00:09:45.950><c> and</c><00:09:46.280><c> so</c>

00:09:46.720 --> 00:09:46.730 align:start position:0%
parameters cause when it comes to and so
 

00:09:46.730 --> 00:09:48.250 align:start position:0%
parameters cause when it comes to and so
this<00:09:46.880><c> makes</c><00:09:47.060><c> the</c><00:09:47.180><c> average</c><00:09:47.480><c> more</c><00:09:47.630><c> finicky</c><00:09:47.960><c> it</c>

00:09:48.250 --> 00:09:48.260 align:start position:0%
this makes the average more finicky it
 

00:09:48.260 --> 00:09:50.260 align:start position:0%
this makes the average more finicky it
was<00:09:48.380><c> just</c><00:09:48.560><c> more</c><00:09:48.680><c> parameters</c><00:09:49.250><c> easy</c><00:09:49.430><c> to</c><00:09:49.760><c> fiddle</c>

00:09:50.260 --> 00:09:50.270 align:start position:0%
was just more parameters easy to fiddle
 

00:09:50.270 --> 00:09:51.790 align:start position:0%
was just more parameters easy to fiddle
with<00:09:50.420><c> in</c><00:09:50.810><c> order</c><00:09:50.930><c> to</c><00:09:51.170><c> meet</c><00:09:51.320><c> the</c><00:09:51.380><c> other</c><00:09:51.560><c> work</c>

00:09:51.790 --> 00:09:51.800 align:start position:0%
with in order to meet the other work
 

00:09:51.800 --> 00:09:54.190 align:start position:0%
with in order to meet the other work
well<00:09:52.070><c> but</c><00:09:52.940><c> if</c><00:09:53.180><c> you</c><00:09:53.210><c> manage</c><00:09:53.660><c> to</c><00:09:53.690><c> tune</c><00:09:54.080><c> the</c>

00:09:54.190 --> 00:09:54.200 align:start position:0%
well but if you manage to tune the
 

00:09:54.200 --> 00:09:56.740 align:start position:0%
well but if you manage to tune the
parameters<00:09:54.650><c> well</c><00:09:54.890><c> then</c><00:09:55.670><c> the</c><00:09:56.180><c> picture</c><00:09:56.510><c> you</c><00:09:56.600><c> can</c>

00:09:56.740 --> 00:09:56.750 align:start position:0%
parameters well then the picture you can
 

00:09:56.750 --> 00:09:58.350 align:start position:0%
parameters well then the picture you can
get<00:09:56.960><c> is</c><00:09:57.170><c> that</c><00:09:57.500><c> the</c><00:09:57.650><c> algorithm</c><00:09:58.040><c> will</c><00:09:58.160><c> actually</c>

00:09:58.350 --> 00:09:58.360 align:start position:0%
get is that the algorithm will actually
 

00:09:58.360 --> 00:10:01.900 align:start position:0%
get is that the algorithm will actually
meander<00:09:59.360><c> around</c><00:10:00.280><c> towards</c><00:10:01.280><c> the</c><00:10:01.340><c> minimum</c><00:10:01.700><c> but</c>

00:10:01.900 --> 00:10:01.910 align:start position:0%
meander around towards the minimum but
 

00:10:01.910 --> 00:10:03.490 align:start position:0%
meander around towards the minimum but
as<00:10:02.030><c> it</c><00:10:02.210><c> gets</c><00:10:02.420><c> closer</c><00:10:02.690><c> because</c><00:10:03.350><c> you're</c>

00:10:03.490 --> 00:10:03.500 align:start position:0%
as it gets closer because you're
 

00:10:03.500 --> 00:10:05.260 align:start position:0%
as it gets closer because you're
decreasing<00:10:04.100><c> the</c><00:10:04.220><c> learning</c><00:10:04.550><c> rate</c><00:10:04.730><c> the</c>

00:10:05.260 --> 00:10:05.270 align:start position:0%
decreasing the learning rate the
 

00:10:05.270 --> 00:10:06.760 align:start position:0%
decreasing the learning rate the
meanderings<00:10:05.900><c> would</c><00:10:06.110><c> get</c><00:10:06.260><c> smaller</c><00:10:06.410><c> and</c>

00:10:06.760 --> 00:10:06.770 align:start position:0%
meanderings would get smaller and
 

00:10:06.770 --> 00:10:08.620 align:start position:0%
meanderings would get smaller and
smaller<00:10:06.860><c> until</c><00:10:07.550><c> it</c><00:10:08.060><c> pretty</c><00:10:08.360><c> much</c><00:10:08.570><c> just</c>

00:10:08.620 --> 00:10:08.630 align:start position:0%
smaller until it pretty much just
 

00:10:08.630 --> 00:10:11.950 align:start position:0%
smaller until it pretty much just
converges<00:10:09.590><c> to</c><00:10:10.160><c> the</c><00:10:10.340><c> global</c><00:10:10.730><c> minimum</c><00:10:11.120><c> I</c><00:10:11.420><c> hope</c>

00:10:11.950 --> 00:10:11.960 align:start position:0%
converges to the global minimum I hope
 

00:10:11.960 --> 00:10:13.900 align:start position:0%
converges to the global minimum I hope
this<00:10:12.110><c> makes</c><00:10:12.290><c> sense</c><00:10:12.470><c> right</c><00:10:12.770><c> and</c><00:10:12.830><c> the</c><00:10:13.520><c> reason</c>

00:10:13.900 --> 00:10:13.910 align:start position:0%
this makes sense right and the reason
 

00:10:13.910 --> 00:10:15.640 align:start position:0%
this makes sense right and the reason
this<00:10:14.030><c> formula</c><00:10:14.450><c> makes</c><00:10:14.630><c> sense</c><00:10:14.840><c> is</c><00:10:15.080><c> because</c><00:10:15.230><c> as</c>

00:10:15.640 --> 00:10:15.650 align:start position:0%
this formula makes sense is because as
 

00:10:15.650 --> 00:10:17.740 align:start position:0%
this formula makes sense is because as
the<00:10:15.800><c> album</c><00:10:16.100><c> runs</c><00:10:16.340><c> the</c><00:10:16.910><c> iteration</c><00:10:17.570><c> number</c>

00:10:17.740 --> 00:10:17.750 align:start position:0%
the album runs the iteration number
 

00:10:17.750 --> 00:10:20.710 align:start position:0%
the album runs the iteration number
becomes<00:10:18.470><c> large</c><00:10:18.770><c> and</c><00:10:19.070><c> so</c><00:10:19.340><c> alpha</c><00:10:19.850><c> will</c><00:10:20.390><c> slowly</c>

00:10:20.710 --> 00:10:20.720 align:start position:0%
becomes large and so alpha will slowly
 

00:10:20.720 --> 00:10:23.350 align:start position:0%
becomes large and so alpha will slowly
become<00:10:21.080><c> small</c><00:10:21.680><c> and</c><00:10:22.010><c> so</c><00:10:22.460><c> you</c><00:10:22.520><c> take</c><00:10:22.790><c> smaller</c><00:10:23.120><c> and</c>

00:10:23.350 --> 00:10:23.360 align:start position:0%
become small and so you take smaller and
 

00:10:23.360 --> 00:10:25.570 align:start position:0%
become small and so you take smaller and
smaller<00:10:23.630><c> steps</c><00:10:23.990><c> until</c><00:10:24.650><c> it</c><00:10:24.800><c> some</c><00:10:25.250><c> of</c><00:10:25.340><c> you</c><00:10:25.520><c> know</c>

00:10:25.570 --> 00:10:25.580 align:start position:0%
smaller steps until it some of you know
 

00:10:25.580 --> 00:10:27.280 align:start position:0%
smaller steps until it some of you know
hopefully<00:10:25.850><c> converges</c><00:10:26.630><c> to</c><00:10:26.840><c> the</c><00:10:26.930><c> global</c>

00:10:27.280 --> 00:10:27.290 align:start position:0%
hopefully converges to the global
 

00:10:27.290 --> 00:10:30.550 align:start position:0%
hopefully converges to the global
minimum<00:10:28.060><c> so</c><00:10:29.060><c> if</c><00:10:29.390><c> you</c><00:10:29.510><c> do</c><00:10:29.690><c> slowly</c><00:10:30.050><c> decrease</c>

00:10:30.550 --> 00:10:30.560 align:start position:0%
minimum so if you do slowly decrease
 

00:10:30.560 --> 00:10:32.200 align:start position:0%
minimum so if you do slowly decrease
alpha<00:10:30.890><c> to</c><00:10:31.010><c> zero</c><00:10:31.340><c> you</c><00:10:31.580><c> can</c><00:10:31.700><c> end</c><00:10:31.910><c> up</c><00:10:32.000><c> with</c><00:10:32.060><c> a</c>

00:10:32.200 --> 00:10:32.210 align:start position:0%
alpha to zero you can end up with a
 

00:10:32.210 --> 00:10:34.660 align:start position:0%
alpha to zero you can end up with a
slightly<00:10:32.660><c> better</c><00:10:32.780><c> hypothesis</c><00:10:33.650><c> but</c><00:10:34.310><c> because</c>

00:10:34.660 --> 00:10:34.670 align:start position:0%
slightly better hypothesis but because
 

00:10:34.670 --> 00:10:36.790 align:start position:0%
slightly better hypothesis but because
of<00:10:34.700><c> the</c><00:10:35.000><c> extra</c><00:10:35.030><c> work</c><00:10:35.570><c> needed</c><00:10:36.110><c> to</c><00:10:36.290><c> fit</c><00:10:36.470><c> of</c><00:10:36.620><c> the</c>

00:10:36.790 --> 00:10:36.800 align:start position:0%
of the extra work needed to fit of the
 

00:10:36.800 --> 00:10:39.340 align:start position:0%
of the extra work needed to fit of the
constants<00:10:37.310><c> and</c><00:10:37.580><c> because</c><00:10:38.540><c> frankly</c><00:10:38.930><c> usually</c>

00:10:39.340 --> 00:10:39.350 align:start position:0%
constants and because frankly usually
 

00:10:39.350 --> 00:10:41.560 align:start position:0%
constants and because frankly usually
were<00:10:39.500><c> pretty</c><00:10:39.530><c> happy</c><00:10:40.040><c> with</c><00:10:40.670><c> any</c><00:10:40.970><c> parameter</c>

00:10:41.560 --> 00:10:41.570 align:start position:0%
were pretty happy with any parameter
 

00:10:41.570 --> 00:10:43.210 align:start position:0%
were pretty happy with any parameter
value<00:10:41.960><c> that's</c><00:10:42.230><c> you</c><00:10:42.350><c> know</c><00:10:42.470><c> pretty</c><00:10:42.710><c> close</c><00:10:42.980><c> to</c>

00:10:43.210 --> 00:10:43.220 align:start position:0%
value that's you know pretty close to
 

00:10:43.220 --> 00:10:45.640 align:start position:0%
value that's you know pretty close to
global<00:10:43.550><c> minimum</c><00:10:43.940><c> typically</c><00:10:44.660><c> this</c><00:10:44.930><c> process</c><00:10:45.470><c> of</c>

00:10:45.640 --> 00:10:45.650 align:start position:0%
global minimum typically this process of
 

00:10:45.650 --> 00:10:48.250 align:start position:0%
global minimum typically this process of
decreasing<00:10:46.190><c> alpha</c><00:10:46.550><c> slowly</c><00:10:47.030><c> is</c><00:10:47.450><c> usually</c><00:10:48.080><c> not</c>

00:10:48.250 --> 00:10:48.260 align:start position:0%
decreasing alpha slowly is usually not
 

00:10:48.260 --> 00:10:50.290 align:start position:0%
decreasing alpha slowly is usually not
done<00:10:48.560><c> and</c><00:10:48.830><c> keeping</c><00:10:49.550><c> the</c><00:10:49.640><c> learning</c><00:10:49.970><c> rate</c><00:10:50.120><c> alpha</c>

00:10:50.290 --> 00:10:50.300 align:start position:0%
done and keeping the learning rate alpha
 

00:10:50.300 --> 00:10:53.230 align:start position:0%
done and keeping the learning rate alpha
constant<00:10:50.780><c> is</c><00:10:51.320><c> the</c><00:10:51.620><c> more</c><00:10:51.830><c> common</c><00:10:52.240><c> application</c>

00:10:53.230 --> 00:10:53.240 align:start position:0%
constant is the more common application
 

00:10:53.240 --> 00:10:55.390 align:start position:0%
constant is the more common application
mr.<00:10:53.510><c> Casas</c><00:10:53.750><c> Grandes</c><00:10:54.140><c> and</c><00:10:54.350><c> although</c><00:10:54.980><c> you</c><00:10:55.250><c> will</c>

00:10:55.390 --> 00:10:55.400 align:start position:0%
mr. Casas Grandes and although you will
 

00:10:55.400 --> 00:10:58.420 align:start position:0%
mr. Casas Grandes and although you will
see<00:10:55.640><c> people</c><00:10:55.820><c> using</c><00:10:56.270><c> either</c><00:10:56.540><c> version</c>

00:10:58.420 --> 00:10:58.430 align:start position:0%
see people using either version
 

00:10:58.430 --> 00:11:01.400 align:start position:0%
see people using either version
to<00:10:59.430><c> summarize</c><00:10:59.910><c> in</c><00:11:00.300><c> this</c><00:11:00.570><c> video</c><00:11:00.870><c> we</c><00:11:01.140><c> talked</c>

00:11:01.400 --> 00:11:01.410 align:start position:0%
to summarize in this video we talked
 

00:11:01.410 --> 00:11:03.920 align:start position:0%
to summarize in this video we talked
about<00:11:01.530><c> a</c><00:11:01.680><c> way</c><00:11:01.920><c> for</c><00:11:02.370><c> approximately</c><00:11:03.240><c> monitoring</c>

00:11:03.920 --> 00:11:03.930 align:start position:0%
about a way for approximately monitoring
 

00:11:03.930 --> 00:11:06.140 align:start position:0%
about a way for approximately monitoring
how<00:11:04.380><c> stochastic</c><00:11:05.010><c> gradient</c><00:11:05.100><c> descent</c><00:11:05.400><c> is</c><00:11:05.790><c> doing</c>

00:11:06.140 --> 00:11:06.150 align:start position:0%
how stochastic gradient descent is doing
 

00:11:06.150 --> 00:11:08.450 align:start position:0%
how stochastic gradient descent is doing
in<00:11:06.390><c> terms</c><00:11:06.750><c> of</c><00:11:06.930><c> optimizing</c><00:11:07.410><c> the</c><00:11:07.770><c> cost</c><00:11:07.980><c> function</c>

00:11:08.450 --> 00:11:08.460 align:start position:0%
in terms of optimizing the cost function
 

00:11:08.460 --> 00:11:10.430 align:start position:0%
in terms of optimizing the cost function
and<00:11:08.610><c> this</c><00:11:09.240><c> is</c><00:11:09.390><c> a</c><00:11:09.420><c> method</c><00:11:09.840><c> that</c><00:11:10.020><c> does</c><00:11:10.080><c> not</c>

00:11:10.430 --> 00:11:10.440 align:start position:0%
and this is a method that does not
 

00:11:10.440 --> 00:11:12.860 align:start position:0%
and this is a method that does not
require<00:11:11.030><c> scanning</c><00:11:12.030><c> over</c><00:11:12.390><c> the</c><00:11:12.510><c> entire</c>

00:11:12.860 --> 00:11:12.870 align:start position:0%
require scanning over the entire
 

00:11:12.870 --> 00:11:15.230 align:start position:0%
require scanning over the entire
training<00:11:13.230><c> set</c><00:11:13.530><c> periodically</c><00:11:14.220><c> to</c><00:11:14.490><c> compute</c><00:11:14.910><c> the</c>

00:11:15.230 --> 00:11:15.240 align:start position:0%
training set periodically to compute the
 

00:11:15.240 --> 00:11:17.150 align:start position:0%
training set periodically to compute the
cost<00:11:15.420><c> function</c><00:11:15.570><c> on</c><00:11:16.110><c> the</c><00:11:16.260><c> entire</c><00:11:16.530><c> training</c><00:11:16.830><c> set</c>

00:11:17.150 --> 00:11:17.160 align:start position:0%
cost function on the entire training set
 

00:11:17.160 --> 00:11:19.160 align:start position:0%
cost function on the entire training set
but<00:11:17.340><c> instead</c><00:11:17.970><c> it</c><00:11:18.150><c> looks</c><00:11:18.300><c> at</c><00:11:18.480><c> I'd</c><00:11:18.540><c> say</c><00:11:18.750><c> only</c><00:11:19.050><c> the</c>

00:11:19.160 --> 00:11:19.170 align:start position:0%
but instead it looks at I'd say only the
 

00:11:19.170 --> 00:11:22.520 align:start position:0%
but instead it looks at I'd say only the
last<00:11:19.350><c> thousand</c><00:11:19.830><c> examples</c><00:11:20.070><c> or</c><00:11:20.490><c> so</c><00:11:20.610><c> and</c><00:11:21.480><c> you</c><00:11:22.350><c> can</c>

00:11:22.520 --> 00:11:22.530 align:start position:0%
last thousand examples or so and you can
 

00:11:22.530 --> 00:11:24.380 align:start position:0%
last thousand examples or so and you can
use<00:11:22.680><c> this</c><00:11:22.860><c> method</c><00:11:23.070><c> both</c><00:11:23.580><c> to</c><00:11:23.880><c> make</c><00:11:24.030><c> sure</c><00:11:24.210><c> that</c>

00:11:24.380 --> 00:11:24.390 align:start position:0%
use this method both to make sure that
 

00:11:24.390 --> 00:11:26.060 align:start position:0%
use this method both to make sure that
stochastic<00:11:24.960><c> gradient</c><00:11:25.020><c> descent</c><00:11:25.320><c> is</c><00:11:25.740><c> really</c>

00:11:26.060 --> 00:11:26.070 align:start position:0%
stochastic gradient descent is really
 

00:11:26.070 --> 00:11:29.330 align:start position:0%
stochastic gradient descent is really
okay<00:11:26.250><c> and</c><00:11:26.790><c> is</c><00:11:27.000><c> converging</c><00:11:27.630><c> or</c><00:11:27.960><c> to</c><00:11:28.650><c> use</c><00:11:28.830><c> it</c><00:11:29.070><c> to</c>

00:11:29.330 --> 00:11:29.340 align:start position:0%
okay and is converging or to use it to
 

00:11:29.340 --> 00:11:32.750 align:start position:0%
okay and is converging or to use it to
tune<00:11:29.640><c> the</c><00:11:29.970><c> learning</c><00:11:30.300><c> rate</c><00:11:30.480><c> alpha</c>

