{
    "-EIfb6vFJzc": {
        "0": " \n",
        "1": "in this video we'll talk about how to\n",
        "4": "get logistic regression to work for\n",
        "6": "multi-class classification problems and\n",
        "8": "in particular want to tell you about an\n",
        "10": "algorithm called one versus all\n",
        "12": "classification what is a multi-class\n",
        "14": "classification problem here are some\n",
        "17": "examples let's say you want a learning\n",
        "19": "algorithm to automatically put your\n",
        "21": "email into different folders or to\n",
        "23": "automatically to hack your emails so you\n",
        "24": "might have different folders or\n",
        "27": "different tags for work email email from\n",
        "29": "your friends email from your family and\n",
        "32": "emails about your hobby and so here we\n",
        "34": "have a classification problem with four\n",
        "36": "classes which we might assign the\n",
        "38": "numbers the classes y equals one y\n",
        "42": "equals two y equals 3 and y equals 4 2\n",
        "46": "and another example for medical\n",
        "48": "diagnosis if a patient comes into your\n",
        "50": "office with you know maybe a stuffy nose\n",
        "52": "the possible diagnosis could be that\n",
        "54": "they're not ill maybe that's y equals 1\n",
        "57": "or they have a code 2 or they have a flu\n",
        "60": "and the third and final example if you\n",
        "62": "are using machine learning to classify\n",
        "64": "the weather you know maybe you want to\n",
        "66": "decide it the weather is sunny cloudy\n",
        "69": "rainy or snow or is going to be snow and\n",
        "72": "so in all of these examples Y can take\n",
        "75": "on a small number of discrete values\n",
        "77": "maybe one two three one two four and so\n",
        "78": "on and these are multi-class\n",
        "81": "classification problems and by the way\n",
        "82": "it doesn't really matter whether we\n",
        "87": "indexes at 0 1 2 3 or s 1 2 3 4 I tend\n",
        "90": "to index the my classes starting from 1\n",
        "91": "rather than starting from 0 but either\n",
        "92": "rather than starting from 0 but either\n",
        "93": "way where often it really doesn't matter\n",
        "95": "where as previously for a binary\n",
        "97": "classification problem our data sets\n",
        "100": "look like this for a multi-class\n",
        "102": "classification problem our data sets may\n",
        "104": "look like this where here I'm using\n",
        "106": "three different symbols to represent all\n",
        "109": "three classes so the question is given\n",
        "111": "the data set with three classes where\n",
        "114": "you know this is a example of one class\n",
        "115": "that's an example of a different class\n",
        "117": "and that's an example of yet a third\n",
        "119": "class how do we get a learning algorithm\n",
        "122": "to work for the setting we already know\n",
        "124": "how to do binary classification using\n",
        "126": "logistic regression we know how the you\n",
        "127": "know maybe for the straight line to\n",
        "129": "separate the positive and negative\n",
        "132": "classes using an idea called one versus\n",
        "135": "classification we can then take this and\n",
        "136": "make it work for multi-class\n",
        "139": "classification as well here's how one\n",
        "142": "versus all classification works and this\n",
        "144": "is also sometimes called one versus rest\n",
        "147": "let's say we have a training set like\n",
        "149": "that shown on the Left where we have 3\n",
        "152": "classes so if y equals 1 we do know that\n",
        "154": "when we try and go if y equals to the\n",
        "157": "square and y equals 3 then the cross\n",
        "159": "what we're going to do is take our\n",
        "162": "training set and turn this into three\n",
        "164": "separate binary classification problems\n",
        "166": "or turn this into three separate two\n",
        "169": "class classification problems so let's\n",
        "171": "start with class one which is a triangle\n",
        "173": "we're going to essentially create a new\n",
        "175": "sort of fake training set where it\n",
        "177": "crosses two and three get assigned to\n",
        "179": "the negative class and class one gets\n",
        "180": "the negative class and class one gets\n",
        "181": "assigned to the positive class so when\n",
        "183": "we create a new training set like that\n",
        "185": "shown on the right and we're going to\n",
        "188": "fit a classifier which I'm going to call\n",
        "191": "H subscript theta superscript 1 of X\n",
        "195": "we're here the triangles are the\n",
        "197": "positive examples and the circles are\n",
        "199": "the negative examples so think of the\n",
        "201": "triangles being assigned the value of 1\n",
        "204": "and the circles the sum of the value of\n",
        "206": "0 and we're just going to train a\n",
        "208": "standard logistic regression classifier\n",
        "211": "and maybe that will give us a decision\n",
        "215": "boundary that looks like that ok this\n",
        "217": "superscript 1 here says the class ones\n",
        "219": "that we're doing this for the triangles\n",
        "221": "of first clause next we do the same\n",
        "223": "favorite class to going to take the\n",
        "225": "squares and assign the squares as the\n",
        "227": "positive class and assign everything\n",
        "229": "else the triangles and crosses as a\n",
        "231": "negative class and then we fit a second\n",
        "234": "logistic regression classifier we call\n",
        "238": "this H of X superscript 2 where the\n",
        "240": "superscript 2 denotes that we're now\n",
        "242": "doing this treating the square class as\n",
        "245": "via positive class and maybe we get a\n",
        "248": "classifier like that and finally we do\n",
        "250": "the same thing for the third Clause and\n",
        "254": "fit a third classifier H superscript 3\n",
        "257": "of X and maybe this will give us a\n",
        "258": "decision boundary give us a classifier\n",
        "260": "that separates the positive and negative\n",
        "262": "examples like that\n",
        "265": "so to summarize what we've done is we'll\n",
        "268": "fit three classifiers so for I equals\n",
        "271": "one two three will fit a classifier H\n",
        "274": "superscript I subscript theta of X\n",
        "276": "there's trying to estimate what is the\n",
        "279": "probability that Y is equal to class I\n",
        "281": "given X and parametrized by theta right\n",
        "284": "so in the first instance for this first\n",
        "287": "one up here this classifier with\n",
        "289": "learning to recognize the triangles so\n",
        "290": "it's thinking of the triangles as a\n",
        "293": "positive class so X superscript one is\n",
        "295": "essentially trying to estimate what is\n",
        "298": "the probability that the Y is equal to 1\n",
        "301": "given X and parametrized by theta and\n",
        "305": "similarly this is treating you know the\n",
        "307": "square class is a positive class and so\n",
        "308": "it's trying to estimate the probability\n",
        "311": "that Y is equal to 2 and so on so we now\n",
        "313": "have three classifiers each of which was\n",
        "315": "trained to recognize one of the three\n",
        "317": "classes just to summarize what we're\n",
        "320": "done is with we want to train a logistic\n",
        "323": "regression classifier H superscript I of\n",
        "325": "X for each class I to predict the\n",
        "328": "probably y is equal to I finally to make\n",
        "330": "a prediction when we given a new input X\n",
        "333": "and we want to make a prediction what we\n",
        "336": "do is we just run all let's say three\n",
        "339": "won't run all three of our classifiers\n",
        "341": "on the input X and we then pick the\n",
        "344": "class I that maximizes a three so we\n",
        "345": "just you know basically pick the\n",
        "347": "classifier pick whichever one of the\n",
        "350": "three classifiers is most confident or\n",
        "352": "most enthusiastically says that in\n",
        "354": "things that has a red class so whichever\n",
        "357": "value of I gives us the highest\n",
        "359": "probability we then predict Y to be that\n",
        "361": " \n",
        "364": "so that's it for multi-class\n",
        "366": "classification and one versus all method\n",
        "367": "classification and one versus all method\n",
        "369": "and with this little method you can now\n",
        "371": "take the logistic regression classifier\n",
        "373": "and make it work on multi-class\n"
    },
    "-la3q9d7AKQ": {
        "0": " \n",
        "2": "in this and the next few videos I want\n",
        "4": "to start to talk about classification\n",
        "7": "problems where the variable Y that you\n",
        "9": "want to predict is discrete values we'll\n",
        "11": "develop an algorithm called logistic\n",
        "13": "regression which is a one of the most\n",
        "15": "popular and most widely used learning\n",
        "20": "algorithms today here are some examples\n",
        "23": "of classification problems earlier we\n",
        "25": "talked about email spam classification\n",
        "28": "as an example of a classification\n",
        "30": "problem another example would be\n",
        "33": "classifying online transactions so if\n",
        "35": "you have a website that sell stuff and\n",
        "37": "if you want to know if a particular\n",
        "39": "transaction is fraudulent or not whether\n",
        "41": "someone has you know you is using a\n",
        "43": "stolen credit card or has stolen the\n",
        "45": "user's password that's another\n",
        "47": "classification problem and earlier we\n",
        "49": "also talked about the example of\n",
        "52": "classifying tumors as cancerous\n",
        "55": "malignant or as benign tumors in all of\n",
        "57": "these problems the variable that we're\n",
        "59": "trying to predict is a variable Y that\n",
        "61": "we can think of as taking on two values\n",
        "65": "either 0 or 1 either spam or non-spam\n",
        "68": "fraudulent or not fraudulent malignant\n",
        "71": "or benign another name for the class\n",
        "74": "that we denote with 0 is the negative\n",
        "76": "class and another name for the class\n",
        "78": "that we denote with 1 is the positive\n",
        "82": "class so 0 may denote the benign tumor\n",
        "85": "and 1 positive class may denote a\n",
        "88": "malignant tumor the assignment of the\n",
        "90": "two classes you know spam not spam and\n",
        "92": "so on the assignment of the two classes\n",
        "93": "so on the assignment of the two classes\n",
        "95": "to positive and negative to 0 and 1 is\n",
        "97": "somewhat arbitrary and it doesn't really\n",
        "100": "matter but often there is this intuition\n",
        "102": "that the negative Clause is conveying\n",
        "104": "the absence of something like the\n",
        "107": "absence of a malignant tumor whereas a 1\n",
        "110": "the positive class is conveying the\n",
        "111": "presence of something that we may be\n",
        "112": "looking for\n",
        "115": "but the declination of which is negative\n",
        "116": "and which is positive is somewhat\n",
        "118": "arbitrary and it doesn't matter that\n",
        "121": "much for now we're going to start with\n",
        "123": "classification problems with just two\n",
        "124": "classes 0 &amp; 1\n",
        "127": "later on we'll talk about multi class\n",
        "129": "problems as well where the variable Y\n",
        "133": "may take on say 4 values 0 1\n",
        "135": "two and three this is called a\n",
        "137": "multi-class classification problem but\n",
        "139": "for the next view videos let's start\n",
        "142": "with the two class or the binary\n",
        "144": "classification problem and we'll worry\n",
        "147": "about the multi class setting later so\n",
        "148": "how do we develop a classification\n",
        "151": "algorithm here's an example of a\n",
        "153": "training set for a classification task\n",
        "157": "for classifying a tumor as malignant or\n",
        "159": "benign and notice that malignancy takes\n",
        "163": "on only two values 0 or no 1 or 1 or yes\n",
        "166": "so one thing we could do given this\n",
        "169": "training set is to apply the algorithm\n",
        "172": "that we already know linear regression\n",
        "174": "to this data set and just try to fit a\n",
        "177": "straight line to the data so if you take\n",
        "178": "this training set and for the straight\n",
        "182": "line to it maybe you get a hypothesis\n",
        "184": "that looks like that right so that's my\n",
        "187": "hypothesis H of x equals theta transpose\n",
        "191": "X if you want to make predictions one\n",
        "193": "thing you could try doing is then\n",
        "196": "threshold the classifier output at 0.5\n",
        "197": "threshold the classifier output at 0.5\n",
        "200": "that is at the vertical axis value 0.5\n",
        "204": "and if the hypothesis outputs a value\n",
        "206": "that is greater than equal to 0.5 you\n",
        "208": "predict y equals 1 this is less than 0.5\n",
        "211": "you predict y equals 0 let's see what\n",
        "213": "happens we do that so let's take 0.5 and\n",
        "216": "so you know that's where the threshold\n",
        "219": "is and thus using linear regression this\n",
        "221": "way everything to the right of this\n",
        "223": "point we would end up predicting as the\n",
        "226": "positive class because the output values\n",
        "228": "is greater than 0.5 on the vertical axis\n",
        "231": "and everything to the left of that point\n",
        "234": "we will end up predicting as a negative\n",
        "237": "value in this particular example it\n",
        "239": "looks like linear regression is actually\n",
        "242": "doing something reasonable even though\n",
        "244": "this is a classification task we're\n",
        "246": "interested in but now let's try changing\n",
        "249": "the problem a bit let me extend out the\n",
        "251": "horizontal axis a little bit and let's\n",
        "253": "think we got one more training example\n",
        "257": "way out there on the right notice that\n",
        "259": "that additional training example this\n",
        "260": "one out here it doesn't actually change\n",
        "262": "anything right this is you know looking\n",
        "264": "at the training set is pretty clear what\n",
        "265": "a good hypothesis\n",
        "268": "instead well everything to the right of\n",
        "269": "somewhere around here to the right of\n",
        "270": "this we should predict this positive and\n",
        "271": "everything to the left\n",
        "273": "we should probably predict this negative\n",
        "275": "because from this training set it looks\n",
        "278": "like all the tumors larger than you know\n",
        "280": "a certain value around here on malignant\n",
        "282": "and all the tumors smaller than that are\n",
        "284": "not malignant at least for this training\n",
        "287": "set but once we've added that extra\n",
        "290": "example out here if you now run linear\n",
        "292": "regression you instead get a straight\n",
        "295": "line fit to the data that might maybe\n",
        "299": "look like this and if you now threshold\n",
        "303": "this hypothesis at 0.5 you end up with a\n",
        "306": "threshold that's around here so that\n",
        "308": "everything to the right of this point\n",
        "309": "you predict as positive and everything\n",
        "311": "to the left at that point you predict as\n",
        "316": "negative and this seems a pretty bad\n",
        "318": "thing for linear regression to have done\n",
        "319": "right because you know these are\n",
        "321": "positive examples these are negative\n",
        "324": "examples it's pretty clear which really\n",
        "326": "should be separating the two classes\n",
        "328": "somewhere around there but somehow by\n",
        "330": "adding one example way out here to the\n",
        "333": "right this example really isn't giving\n",
        "334": "us any new information I mean it should\n",
        "336": "be no surprise to the learning algorithm\n",
        "338": "that the example way out here turns out\n",
        "339": "to be malignant\n",
        "341": "but somehow add in that example out\n",
        "344": "there cause linear regression to change\n",
        "345": "there cause linear regression to change\n",
        "347": "is straight line fit to the data from\n",
        "349": "this magenta from this magenta line out\n",
        "352": "here to this blue line over here and\n",
        "356": "caused it to give us a worse hypothesis\n",
        "359": "so applying linear regression to a\n",
        "361": "classification problem usually isn't\n",
        "365": "often isn't a great idea in the first\n",
        "366": "instance in the first example before I\n",
        "369": "added this extra extra training example\n",
        "371": "previously linear regression was just\n",
        "374": "getting lucky and it got us a hypothesis\n",
        "376": "that you know worked well for that\n",
        "379": "particular example but usually applying\n",
        "382": "linear regression to a data set you know\n",
        "384": "you might get lucky but often it isn't a\n",
        "386": "good idea so I wouldn't use linear\n",
        "388": "regression for classification problems\n",
        "392": "here's one other funny thing about what\n",
        "393": "would happen if we were to use linear\n",
        "395": "regression for\n",
        "398": "problem for classification we know that\n",
        "401": "Y is either 0 or 1 but if you are using\n",
        "402": "linear regression\n",
        "405": "while the hypothesis can output values\n",
        "407": "that are much larger than 1 or less than\n",
        "410": "0 even if all of your training examples\n",
        "414": "have labels y equals 0 1 and it seems\n",
        "416": "kind of strange that even though we know\n",
        "419": "that the labels should be 0 1 it seems\n",
        "421": "kind of strange if the algorithm can\n",
        "424": "output values much larger than 1 or much\n",
        "428": " \n",
        "431": "so what we'll do in the next few videos\n",
        "434": "is develop an algorithm called logistic\n",
        "437": "regression which has the property that\n",
        "439": "the output the predictions of logistic\n",
        "441": "regression are always between zero and\n",
        "443": "one and doesn't become bigger than one\n",
        "446": "or become less than zero and by the way\n",
        "449": "logistic regression is and we will use\n",
        "453": "it as a classification algorithm is some\n",
        "456": "may be sometimes confusing that the term\n",
        "458": "regression appears in this name even\n",
        "460": "though logistic regression is actually a\n",
        "462": "classification algorithm but that's just\n",
        "464": "the name it was given for historical\n",
        "466": "reasons so don't be confused by that\n",
        "468": "logistic regression is actually a\n",
        "470": "classification algorithm that we apply\n",
        "473": "to settings where the label Y is the\n",
        "476": "speed value when is either 0 1 so\n",
        "479": "hopefully you now know why if you have a\n",
        "481": "classification problem using linear\n",
        "484": "regression isn't a good idea in the next\n",
        "485": "video we'll start working out the\n",
        "487": "details of the logistic regression\n"
    },
    "086OcT-5DYI": {
        "0": " \n",
        "2": "and this makes other videos I'd like to\n",
        "3": "tell you about the problem called\n",
        "7": "anomaly detection this is a reasonably\n",
        "8": "commonly used type of machine learning\n",
        "11": "and one of the interesting aspects is\n",
        "13": "that is mainly fold up as an\n",
        "16": "unsupervised learning problem but there\n",
        "17": "are some aspects of it that are also\n",
        "19": "very similar in to so the supervised\n",
        "22": "learning problem so what is anomaly\n",
        "25": "detection to explain it let me use the\n",
        "28": "motivating example of imagine that\n",
        "30": "you're a manufacturer of aircraft\n",
        "33": "engines and let's say that as your\n",
        "35": "aircraft engines roll off the assembly\n",
        "37": "line you're doing you know a QA Quality\n",
        "40": "Assurance testing and that part that\n",
        "43": "testing you measure features of your\n",
        "45": "aircraft engine like maybe you measure\n",
        "47": "the heat generated things are the\n",
        "49": "vibrations and so on I share some\n",
        "50": "friends that worked on this problem a\n",
        "52": "long time ago and these were actually\n",
        "53": "the source of features that they were\n",
        "54": "the source of features that they were\n",
        "56": "collecting off actual aircraft engines\n",
        "60": "and so you now have a data set of a x1\n",
        "62": "through XM if you have manufactured M\n",
        "65": "aircraft engines and if you plot your\n",
        "67": "data or maybe it looks like this\n",
        "70": "so the point across here is one of your\n",
        "73": "unlabeled examples so the anomaly\n",
        "75": "detection problem is the following let's\n",
        "78": "say that on you know the next day you\n",
        "81": "have a new aircraft engine that rolls\n",
        "82": "off the assembly line and your new\n",
        "84": "aircraft engine has some set of features\n",
        "87": "XS what the anomaly detection problem is\n",
        "90": "what to know if this aircraft engine is\n",
        "92": "anomalous in any way in other words we\n",
        "94": "want to know if maybe this engine should\n",
        "98": "undergo further testing because the or\n",
        "100": "even looks like an okay engine and so\n",
        "102": "it's okay to just ship it to a customer\n",
        "105": "without further testing so if your new\n",
        "108": "aircraft engine looks like a point over\n",
        "110": "well you know that looks a lot like the\n",
        "112": "aircraft engines we've seen before and\n",
        "114": "so maybe we'll say that it looks okay\n",
        "117": "whereas if your new aircraft engine if X\n",
        "120": "test you know where a point that were\n",
        "123": "all here so this x1 and x2 of the\n",
        "125": "features of this new example X test\n",
        "128": "we're all the way out there then we will\n",
        "129": "call that an\n",
        "132": "and maybe send that aircraft engine for\n",
        "134": "further testing before we ship it to a\n",
        "136": "customer since it looks nothing like\n",
        "138": "since it was very different than the\n",
        "140": "rest of the aircraft engines would seem\n",
        "142": "to fall more formally in the anomaly\n",
        "145": "detection problem were given some data\n",
        "148": "sets x1 through XM of examples and we\n",
        "150": "usually assume that these M examples are\n",
        "153": "normal or non of non-anomalous examples\n",
        "156": "and we want an algorithm to tell us if\n",
        "159": "some new example X test is anomalous the\n",
        "161": "first we're going to take is that given\n",
        "163": "this training set given the unlabeled\n",
        "165": "training set we're going to build a\n",
        "168": "model for P of X in other words we're\n",
        "169": "going to build a model for the\n",
        "171": "probability of X where X are these\n",
        "175": "features of say aircraft engines and so\n",
        "177": "having built a model of the property of\n",
        "180": "X we're then going to say that if for\n",
        "182": "the new aircraft engine if P of X test\n",
        "188": "is less than some epsilon then we flag\n",
        "192": "this as an anomaly so we see a new\n",
        "193": "engine that you know has very low\n",
        "196": "probability under our model P of X that\n",
        "198": "we estimate from the data then we thank\n",
        "201": "the tsunami whereas if P of X test is\n",
        "204": "say greater than or equal to some small\n",
        "206": "threshold then we say that you know K\n",
        "208": "looks ok and so given the training set\n",
        "211": "like that plotted here if you build a\n",
        "213": "model hopefully you will find that\n",
        "215": "aircraft engines hopefully the model P\n",
        "217": "of X will say that points that lie you\n",
        "219": "know somewhere in the middle that's\n",
        "221": "pretty hard probability less points a\n",
        "222": "little bit further ELLs have lower\n",
        "224": "probability points that are even further\n",
        "226": "I'll have somewhat little probability\n",
        "228": "and the point that's way out here the\n",
        "230": "points that appointment is way out there\n",
        "233": "would be an anomaly\n",
        "235": "whereas four point that's way in there\n",
        "238": "right in the middle this would be okay\n",
        "240": "because P of X right in the middle of\n",
        "241": "that would be very high because obscene\n",
        "244": "of all the points in that region here\n",
        "247": "are some examples of applications of\n",
        "248": "anomaly detection perhaps the most\n",
        "250": "common application of anomaly detection\n",
        "253": "is actually for detection if you have\n",
        "255": "many users and if each of your users\n",
        "257": "take different activities you know maybe\n",
        "259": "on your website or in the physical plant\n",
        "262": "something you can compute features X I\n",
        "264": "of the different users activities and\n",
        "266": "what you can do is build a model to say\n",
        "268": "you know what is the probability of\n",
        "270": "different users behaving different ways\n",
        "272": "what is the probability of a particular\n",
        "275": "vector of features of a user's behavior\n",
        "278": "so you know examples of features of a\n",
        "281": "user's activity may be on the website\n",
        "283": "would be things like maybe x1 is on how\n",
        "286": "often does this user login you know x2\n",
        "289": "maybe the number of webpages visited or\n",
        "292": "the number of transactions maybe x3 is\n",
        "293": "you know the number of posts of the\n",
        "295": "users on the forum a feature export\n",
        "297": "could be what is the typing speed of the\n",
        "299": "user and some websites you measure track\n",
        "301": "that so it was the typing speed of this\n",
        "304": "user in characters per second and so you\n",
        "306": "can model P of X based on this sort of\n",
        "310": "data and finally having multiple X you\n",
        "313": "can try to identify users that behaving\n",
        "314": "very strange to your website by checking\n",
        "317": "which ones have probably of X less than\n",
        "319": "epsilon and maybe I send the profiles of\n",
        "323": "those users for further review or demand\n",
        "324": "the additional identification from those\n",
        "328": "users or somesuch to gather gains you\n",
        "330": "know they strange behavior or fraudulent\n",
        "333": "behavior on your website this sort of\n",
        "336": "technique will tend to flag to users\n",
        "338": "behaving unusually or not just the\n",
        "340": "behavior not just users that may be\n",
        "342": "behaving fraudulently sort of not just a\n",
        "345": "a constantly have been stolen or users\n",
        "346": "they're trying to do funny things or\n",
        "348": "just find unusual users but this is\n",
        "350": "actually the technique that is used by\n",
        "353": "many online websites to sell things to\n",
        "354": "try\n",
        "356": "identify users behaving strangely and\n",
        "358": "then where there might be indicative of\n",
        "360": "either fortune and behavior or of\n",
        "362": "computers accounts that have been stolen\n",
        "366": "another example of anomaly detection is\n",
        "367": "mini Factory\n",
        "368": "so already talked about the aircraft\n",
        "370": "engine thing where we can find unusual\n",
        "373": "see aircraft engines and send those for\n",
        "375": "further review the third application\n",
        "378": "would be monitoring computers in the\n",
        "379": "data center and actually have some\n",
        "381": "friends that work on this too so if you\n",
        "383": "have a lot of machines in a computer\n",
        "385": "cluster or in a data center you can do\n",
        "388": "things like compute features of each\n",
        "391": "machine so maybe some features capturing\n",
        "392": "you know how much memory use number of\n",
        "395": "dis accesses CPU load as well as more\n",
        "397": "complex features like what is the CPU\n",
        "400": "load on this machine divided by the\n",
        "402": "amount of network traffic on this\n",
        "404": "machine then given a data set of how\n",
        "406": "your computers in your data center\n",
        "408": "usually behave you can model the\n",
        "409": "probability of X you can model the\n",
        "412": "probability of these machines having\n",
        "413": "different different amounts of memory\n",
        "415": "use or probably of these machines having\n",
        "417": "different numbers of dis accesses or\n",
        "420": "different CPU knows and so on and if you\n",
        "423": "ever have a machine who is a probably of\n",
        "425": "X P of X is very small then you know\n",
        "427": "that machine is behaving unusually and\n",
        "429": "maybe that machine is about to go down\n",
        "433": "and you can flag that for review by\n",
        "435": "system administrator and this is\n",
        "437": "actually being used today by various\n",
        "440": "data centers to watch out for unusual\n",
        "442": "things happening on their machines\n",
        "446": "so that's anomaly detection in the next\n",
        "448": "video I'll talk a bit about the Gaussian\n",
        "450": "distribution and review properties of\n",
        "452": "the Gaussian probability distribution\n",
        "455": "and in videos after that we'll apply it\n",
        "456": "to develop an anomaly detection\n"
    },
    "0a19YIQgRL4": {
        "0": " \n",
        "1": "in this and the next video I want to\n",
        "3": "work through a detailed example showing\n",
        "6": "how a neural network in computing a\n",
        "8": "complex nonlinear function of the input\n",
        "10": "and hopefully does give you a good sense\n",
        "12": "of why neural networks can be used to\n",
        "16": "learn complex nonlinear hypotheses\n",
        "19": "consider the following problem where we\n",
        "21": "have input features x1 and x2 that are\n",
        "24": "binary values so either 0 or 1 so x1 x2\n",
        "26": "can each take on only one of two\n",
        "29": "possible values in this example I've\n",
        "31": "drawn only 2 positive examples and two\n",
        "33": "negative examples but you can think of\n",
        "36": "this as a simplified version of a more\n",
        "38": "complex learning problem where we may\n",
        "40": "have a bunch of positive examples that\n",
        "41": "you have to write in the lower left and\n",
        "43": "a bunch of negative examples did notify\n",
        "46": "the circles and what we'd like to do is\n",
        "50": "learn a nonlinear decision boundary then\n",
        "51": "maybe this separates the positive and\n",
        "54": "the negative examples so how can any row\n",
        "57": "network do this and rather than use the\n",
        "58": "example they're very going to use this\n",
        "61": "may be easier to examine example on the\n",
        "64": "left concretely what this is is really\n",
        "68": "computing the target label y equals x 1x\n",
        "71": "or X 2 or actually on this is actually\n",
        "75": "the X 1 X nor X 2 function where X nor\n",
        "77": "is the alternative notation for not x1\n",
        "83": "or x2 so x1 x1 x2 that's true only if\n",
        "88": "exactly one of x1 or x2 is equal to 1 it\n",
        "89": "turns out that the specific example on\n",
        "90": "turns out that the specific example on\n",
        "92": "the US works out a little bit better if\n",
        "95": "if we use the X nor examples that these\n",
        "97": "two are the same of course it means not\n",
        "101": "x1 X not X 1 X or X 2 and so we're going\n",
        "103": "to a positive examples that either both\n",
        "106": "are true or both are false and we'll\n",
        "109": "have that's y equals 1 y equals\n",
        "111": "we're going to have y equals zero if\n",
        "114": "only one of them is true and we want to\n",
        "115": "figure out if we can get a neural\n",
        "117": "network to fit to this sort of training\n",
        "121": "set in order to build out to a network\n",
        "125": "that 50 X norm example and we're going\n",
        "127": "to start to a slightly simpler one and\n",
        "129": "show a network that since the end\n",
        "132": "function concretely let's say we have\n",
        "135": "input X 1 and X 2 that are again by no\n",
        "138": "means this either 0 1 and let's say our\n",
        "142": "target labels Y our Y is equal to X 1\n",
        "151": "and X 2 this is a logical end so can we\n",
        "155": "get a 1 unit network to compute this\n",
        "158": "logical end function in order to do so\n",
        "161": "I'm going to actually draw in the bias\n",
        "165": "units as well the plus 1 unit now let me\n",
        "167": "just assign some values to the weights\n",
        "170": "or the parameters of this network I'm\n",
        "172": "going to write down the parameters on\n",
        "177": "this diagram drug minus 30 here plus 20\n",
        "180": "and plus 20 and what this means is that\n",
        "183": "I'm assigning a value of minus 30 to the\n",
        "187": "value associated with X 0 this is plus 1\n",
        "189": "going into this unit and a parameter\n",
        "192": "value of plus 20 that multiplies to X 1\n",
        "194": "and the value of plus 20 for the\n",
        "197": "parameter that multiplies into X 2 so\n",
        "199": "concretely this is saying that my\n",
        "204": "hypothesis H of X is equal to G of minus\n",
        "211": "30 plus 20 X 1 plus 20 X 2 so sometimes\n",
        "213": "it's just convenient to draw these\n",
        "215": "weights that draw these parameters up\n",
        "217": "here you know in the diagram with a\n",
        "218": "neural net\n",
        "220": "and of course this minus 30 this is\n",
        "227": "actually theta 1 of 1 0 this is theta 1\n",
        "233": "of 1 1 and that's theta 1 of 1 2 but\n",
        "234": "it's just easier think about it as\n",
        "237": "you're associating these parameters with\n",
        "241": "the edges of the network let's look at\n",
        "243": "what this little single neuron Network\n",
        "246": "will compute just to remind you the\n",
        "248": "sigmoid activation function G of Z looks\n",
        "251": "like this is class from 0 Rises smoothly\n",
        "254": "across the 0.5 and then the asymptotes\n",
        "257": "at 1 and to give some landmarks if the\n",
        "261": "horizontal axis values Z is equal to 4.6\n",
        "265": "then the sigmoid function is equal to\n",
        "269": "0.99 this is very close to 1 and kind of\n",
        "273": "symmetrically it is negative 4.6 then\n",
        "275": "the sigmoid function there is equal to\n",
        "279": "0.01 this is very close to 0 let's look\n",
        "282": "at the four possible input values for x1\n",
        "284": "and x2 and look at what the hypothesis\n",
        "287": "will output in that case if x1 and x2\n",
        "290": "are both equal to 0 if you look at this\n",
        "292": "if x1 x2 above equal to 0 then the\n",
        "296": "hypothesis output G of negative 30 so\n",
        "298": "sorry very far to the left of this\n",
        "301": "diagram and still be very close to 0 if\n",
        "305": "X 1 equals 0 and x2 equals 1 then this\n",
        "309": "formula here evaluates to G the sigmoid\n",
        "312": "function applied to minus 10 and again\n",
        "314": "that's you know to the far left of this\n",
        "316": "plot and so that's again very close to\n",
        "320": "zero this is also G of minus 10 that is\n",
        "324": "a if x1 is equal to 1 and X to 0 this is\n",
        "327": "minus 30 plus 20 which is minus 10\n",
        "330": "and finally if X 1 equals 1 X 2 equals 1\n",
        "334": "then you have G of minus 30 plus 20 plus\n",
        "336": "20 so that's G a positive 10 which is\n",
        "340": "therefore very close to 1 and if you\n",
        "344": "look in this column this is exactly the\n",
        "346": "logical end function so this is\n",
        "349": "computing H of X is you know\n",
        "355": "approximately X 1 and X 2 in other words\n",
        "359": "it outputs 1 if and only if X 2 X 1 and\n",
        "364": "X 2 above equal to 1 so by writing out\n",
        "367": "our little truth table like this we\n",
        "370": "managed to figure out what's the logical\n",
        "372": "function that our little neural network\n",
        "376": " \n",
        "379": "this network is shown here confuse the\n",
        "381": "old function just to show you how I work\n",
        "384": "that out if you are read out the\n",
        "386": "hypothesis you find that this confusing\n",
        "391": "G of minus 10 plus 20 X 1 plus 20 X 2\n",
        "393": "and so you fill in these values you find\n",
        "396": "that that's G of minus 10 which should\n",
        "399": "also be 0 G of 10 which is approximately\n",
        "401": "1 and so on these are approximately 1\n",
        "405": "and approximately 1 and these numbers is\n",
        "410": "essentially the logical or function so\n",
        "411": "hopefully with this you now understand\n",
        "414": "how single neurons in the neural network\n",
        "416": "can be used to compute logical functions\n",
        "419": "like ant and all and so on in the next\n",
        "421": "video we'll continue building on these\n",
        "423": "examples and work through a more complex\n",
        "426": "example we'll get to show you how a\n",
        "428": "neural network now with multiple layers\n",
        "430": "of units can be used to compute more\n",
        "432": "complex functions like your XOR function\n"
    },
    "0i9OhkbfNwE": {
        "0": " \n",
        "1": "in this video I'd like to keep working\n",
        "4": "through our example to show how a neural\n",
        "7": "network can compute complex nonlinear\n",
        "11": "hypotheses in the last video we saw how\n",
        "13": "a neural network can be used to compute\n",
        "16": "the functions X 1 and X 2 and the\n",
        "19": "function X 1 or X 2 when X 1 and X 2 are\n",
        "22": "binary that is when they take on values\n",
        "25": "in 0 1 we can also have a network to\n",
        "28": "compute negation that is to compute the\n",
        "31": "function not x1 let me just write down\n",
        "33": "the weights associated with this network\n",
        "35": "we have only one input feature X 1 in\n",
        "38": "this case and the bias unit plus 1 and\n",
        "40": "if I associate this with the waste plus\n",
        "44": "10 and minus 20 then my hypothesis is\n",
        "47": "computing this H of x equals sigmoid of\n",
        "51": "10 minus 20 times X 1 so when X 1 is\n",
        "53": "equal to 0 my hypothesis will be\n",
        "54": "equal to 0 my hypothesis will be\n",
        "59": "computing G of 10 minus 20 times 0 which\n",
        "62": "is just 10 and so that's approximately 1\n",
        "65": "and when X is equal 1 this will be G of\n",
        "67": "minus 10 which is therefore\n",
        "70": "approximately equal to 0 and if you look\n",
        "71": "at what these values are that's\n",
        "74": "essentially did not x1 function so to\n",
        "78": "include negations the general idea is to\n",
        "80": "put a large negative weight in front of\n",
        "82": "the variable you want to negate if this\n",
        "85": "minus 20 multiplied by x1 and that you\n",
        "87": "know that that's the general idea of how\n",
        "91": "you end up getting x1 and so in an\n",
        "93": "example that I hope you really figure\n",
        "95": "out yourself if you want to compute a\n",
        "98": "function like this not x1 and not x2 you\n",
        "100": "know what part of that would probably be\n",
        "102": "putting large negative weights in front\n",
        "105": "of x1 and x2 but it should be feasible\n",
        "110": "to get a neural network with just one\n",
        "113": "output unit to compute this as well\n",
        "115": "right so this logical function not x1\n",
        "121": "and not x2 is going to be equal to 1\n",
        "129": "definitely if x1 equals x2 equals zero\n",
        "131": "right so this is a logical function this\n",
        "134": "is not x1 that means X 1 must be 0 and\n",
        "136": "not X 2 that means X 2 must be equal to\n",
        "139": "0 as well so this logical function is\n",
        "142": "equal to 1 if enemy of both X 1 and X 2\n",
        "145": "are equal to 0 and hopefully you should\n",
        "147": "going to figure out how to make a small\n",
        "149": "neural network to compute this logical\n",
        "152": " \n",
        "155": "now taking the three pieces that we have\n",
        "157": "put together that is the network for\n",
        "160": "computing X 1 and X 2 and the network\n",
        "164": "for computing not x1 and not x2 and one\n",
        "167": "last network for computing x1 or x2 we\n",
        "168": "should be able to put these three two\n",
        "172": "pieces together to compute this x1 x0 x2\n",
        "175": "function and just to remind you if this\n",
        "178": "was X 1 X 2 this function that we want\n",
        "181": "to compute would have negative examples\n",
        "183": "here and here and we have positive\n",
        "186": "examples there there so clearly this you\n",
        "188": "know will need a nonlinear decision\n",
        "189": "boundary in order to separate the\n",
        "193": "positive and negative examples let's\n",
        "195": "draw the network I'm going to take my\n",
        "199": "input plus 1 x1 x2 and create my first\n",
        "201": "hidden unit here I'm going to call this\n",
        "203": "a to 1 because that's my first hidden\n",
        "206": "unit and I'm going to copy the weight\n",
        "209": "over from the red Network the x1 and x2\n",
        "216": "networks and minus 30 20 20 next let me\n",
        "218": "create a second hidden unit which I'm\n",
        "221": "going to call a 2 2 that is the second\n",
        "224": "hidden unit of layer 2 I'm going to copy\n",
        "227": "over the cyan network in the middle so I\n",
        "233": "have to wait 10 minus 20 minus 20 and so\n",
        "235": "let's throw in some of the truth table\n",
        "237": "values for the red Network we know that\n",
        "241": "was computing the x1 and x2 and so this\n",
        "244": "will be approximately 0 0 0 1 depending\n",
        "248": "on the values of x1 x2 and for a2 to\n",
        "251": "this design Network whether we know the\n",
        "254": "function not x1 not x2 that outputs 1 0\n",
        "258": "0:04 the four values of x1 and x2\n",
        "260": "finally I'm going to create my output\n",
        "265": "note my output units that is a 3-1 this\n",
        "268": "is what will output H of X and I'm going\n",
        "271": "to copy over the old Network for that\n",
        "274": "and come on either plus 1 bias unit here\n",
        "276": "so draw that in and I'm going to copy\n",
        "278": "over the weights from the green network\n",
        "282": "so that's minus 10 20 20 and we know\n",
        "284": "earlier that this computes the old\n",
        "287": "function so let's throw in the truth\n",
        "291": "table entries for the first entry is 0\n",
        "295": "or 1 with only 1 then X 0 or 0 which is\n",
        "300": "0 0 or 0 2 0 1 or 0 and that was 2 1 and\n",
        "305": "thus H of X is equal to 1 when either\n",
        "309": "both x1 and x2 are 0 or when X 1 and X 2\n",
        "313": "are both 1 and concretely H of X L plus\n",
        "316": "1 exactly at these two locations and it\n",
        "319": "outputs 0 otherwise and thus with this\n",
        "322": "neural network which has a input layer\n",
        "325": "one hidden layer and one output layer we\n",
        "327": "end up with a nonlinear decision\n",
        "330": "boundary that computes this xnor\n",
        "333": "function and the more general intuition\n",
        "335": "is that in the input layer we just have\n",
        "338": "raw inputs then we had a hidden layer\n",
        "340": "which computed some slightly more\n",
        "342": "complex functions of the inputs that is\n",
        "343": "shown here slightly more complex\n",
        "345": "functions and then by adding yet another\n",
        "347": "layer we end up with an even more\n",
        "351": "complex nonlinear function and this is\n",
        "354": "the sort of intuition about why neural\n",
        "355": "networks can compute pretty complicated\n",
        "357": "functions that when you have multiple\n",
        "359": "layers you have your relatively simple\n",
        "361": "function of the inputs to the second\n",
        "363": "layer but the third layer can build on\n",
        "365": "that to compute even more complex\n",
        "367": "functions and then the layer after that\n",
        "369": "can compute even more complex functions\n",
        "372": "to wrap up this video I want to show you\n",
        "374": "a fun example of a application of the\n",
        "375": "neural\n",
        "377": "well that captures this intuition of the\n",
        "379": "deeper layers computing more complex\n",
        "381": "features I want to show you a video of\n",
        "383": "that come from a good friend of mine\n",
        "386": "John McCone yond is a professor at New\n",
        "389": "York University at NYU and he was one of\n",
        "391": "the early pioneers of neural network\n",
        "394": "research and sort of a legend in the\n",
        "396": "field now and his ideas are used in sort\n",
        "399": "of all sorts of problems and\n",
        "401": "applications throughout the world now so\n",
        "403": "I want to show you a video from some of\n",
        "405": "his early work in which he was using a\n",
        "409": "neural network to recognize handwriting\n",
        "410": "to do handwritten digit recognition\n",
        "412": "you might remember earlier this clause\n",
        "415": "spelled as house I said that one of the\n",
        "417": "early successes of neural networks was\n",
        "419": "trying to use it to read zip codes to\n",
        "422": "help us do to help us send mail also in\n",
        "424": "repost buckles so this is one of the\n",
        "427": "attempts one since one of the algorithms\n",
        "429": "used to try to address that problem in\n",
        "431": "the video that I'll show you this area\n",
        "434": "here is the input area that shows a\n",
        "436": "handwritten character shown to the\n",
        "439": "network this column here shows a\n",
        "440": "network this column here shows a\n",
        "442": "visualization of the features computed\n",
        "444": "by some of the first hidden layer of the\n",
        "446": "network and so the first hidden layer\n",
        "448": "you know this visualization shows\n",
        "450": "different features different edges and\n",
        "452": "lines and so on detected this is a\n",
        "453": "lines and so on detected this is a\n",
        "455": "visualization of the next hidden layer\n",
        "457": "it's kind of harder to see how to\n",
        "458": "understand the deeper hidden layers and\n",
        "460": "that's the visualization of what the\n",
        "463": "Knicks in there is computing on the prey\n",
        "465": "of a hard time seeing what's going on\n",
        "466": "you know much beyond the first hit in\n",
        "469": "there but then finally all of these\n",
        "472": "learned features get fed to the output\n",
        "475": "layer and shown over here is the final\n",
        "478": "answer is the final predictive value for\n",
        "480": "what for what have written digit the\n",
        "482": "neural network thinks that is being\n",
        "564": " \n",
        "589": " \n",
        "592": "so I hope you enjoyed the video and that\n",
        "593": "this hopefully gave you some intuition\n",
        "596": "about this also pretty complicated\n",
        "598": "functions neural networks can learn in\n",
        "600": "which it takes as input this image just\n",
        "603": "takes us input the raw pixels and the\n",
        "604": "first thing they're confused some set of\n",
        "606": "features the Nixon they're confused even\n",
        "608": "more complex features and even more\n",
        "610": "complex features and these features can\n",
        "612": "then be used by essentially the final\n",
        "614": "layer of logistic regression classifier\n",
        "617": "to make accurate predictions about what\n"
    },
    "0kns1gXLYg4": {
        "0": " \n",
        "3": "in this video let's delve deeper and get\n",
        "4": "even better intuition about what the\n",
        "7": "cost function is doing this video\n",
        "8": "assumes that you're familiar with\n",
        "11": "contour plots if you're not familiar\n",
        "13": "with contour plots or contour figures\n",
        "16": "some of the illustrations in this video\n",
        "17": "may or may not make sense to you but\n",
        "18": "may or may not make sense to you but\n",
        "20": "it's okay and if you end up skipping\n",
        "21": "this video or if some of it doesn't\n",
        "23": "quite make sense because you haven't\n",
        "25": "seen contour plots before that's okay\n",
        "27": "and you still understand the rest of\n",
        "31": " \n",
        "34": "here's our problem formulation as usual\n",
        "36": "with the hypothesis parameters cost\n",
        "40": "function and our optimization objective\n",
        "42": "unlike before unlike the last video I'm\n",
        "44": "going to keep both of my parameters\n",
        "47": "theta 0 and theta 1 as we generate our\n",
        "50": "visualizations for the cost function so\n",
        "53": "same as last time we want to understand\n",
        "57": "the hypothesis H and the cost function J\n",
        "59": "so here's my training set of housing\n",
        "63": "prices and let's make some hypothesis\n",
        "64": "you know like that one this is not a\n",
        "67": "particularly good hypothesis but if I\n",
        "71": "set theta 0 equals 50 and theta one\n",
        "74": "equals 0.06 then I end up with this\n",
        "76": "hypothesis down here and that\n",
        "79": "corresponds to that straight line now\n",
        "81": "given these values of theta zero and\n",
        "82": "theta one we want to plot the\n",
        "85": "corresponding you know cost function on\n",
        "88": "the right what we did last time was\n",
        "91": "regular where we only had theta one you\n",
        "93": "know we're drawing plots that look like\n",
        "95": "this as a function of theta one but now\n",
        "97": "we have two parameters theta zero and\n",
        "100": "theta one and so the plot gets a little\n",
        "103": "more complicated it turns out that when\n",
        "105": "we have only one parameter the plus we\n",
        "106": "drew had this sort of bow shaped\n",
        "110": "function now when we have two parameters\n",
        "112": "it turns out the cost function also has\n",
        "115": "a similar sort of bow shape and in fact\n",
        "117": "depending on your training set you might\n",
        "118": "get a cost function that maybe looks\n",
        "119": "get a cost function that maybe looks\n",
        "122": "something like this so this is a 3d\n",
        "125": "surface plot where the axes are labeled\n",
        "128": "theta 0 and theta\n",
        "131": "one so as you vary theta zero and theta\n",
        "134": "one to two parameters you get different\n",
        "136": "values of the cost function J of theta\n",
        "139": "zero comma theta one and the height of\n",
        "142": "the surface above a particular point of\n",
        "144": "theta zero theta one right that's the\n",
        "146": "vertical axis the height of the surface\n",
        "149": "at the point indicates the value of J of\n",
        "152": "theta zero comma J of theta one and you\n",
        "154": "can see it sort of has this bow like\n",
        "156": "shape let me show you the same part in\n",
        "164": "3d so here's the same figure in 3d with\n",
        "166": "axis a zero theta one and vertical axis\n",
        "168": "J of theta zero theta one and if I\n",
        "171": "rotate this your plot around you kind of\n",
        "173": "get a sense I hope this bell shaped\n",
        "177": "surface is that's what the cost function\n",
        "180": "J looks like now for the purpose of\n",
        "182": "illustration in the rest of this video\n",
        "183": "I'm not actually going to use these\n",
        "186": "sorts of 3d surfaces to show you the\n",
        "189": "cost function J instead I'm going to use\n",
        "195": "contour plots or what I also call\n",
        "196": "contour figures\n",
        "201": "I guess they mean the same thing to show\n",
        "204": "you these surfaces so here's an example\n",
        "206": "of a contour figure shown on the right\n",
        "210": "where the axes are theta zero and theta\n",
        "214": "one and one of each of these ovals what\n",
        "217": "each of these ellipses shows is a set of\n",
        "220": "points that takes on the same value for\n",
        "224": "J of theta zero theta one so completely\n",
        "227": "for example let's you know take that\n",
        "230": "point at that point at that point all\n",
        "233": "three of these points like just drew a\n",
        "237": "magenta they have the same value for J\n",
        "239": "of theta zero theta one okay where write\n",
        "240": "these this is a theta zero theta one\n",
        "243": "axis but those three have the same value\n",
        "245": "for J of theta zero comma theta one and\n",
        "247": "if you haven't seen console parts much\n",
        "250": "before think of imagine if you will a\n",
        "253": "bowl shape function that's coming out at\n",
        "255": "my screen so that the minimum so the\n",
        "257": "bottom of the bowl is this\n",
        "258": "right there right this middle the middle\n",
        "261": "of these concentric ellipses and imagine\n",
        "263": "a bow shape that's of grows out my\n",
        "266": "screen like this so that each of these\n",
        "268": "ellipses you know has the same height\n",
        "270": "above my screen and the minimum of the\n",
        "273": "bow right is right down there and so the\n",
        "276": "contour figures is a way to is it maybe\n",
        "278": "a more convenient way to visualize my\n",
        "281": "function J so let's look at some\n",
        "286": "examples over here I have a particular\n",
        "290": "point right and so this is a with you\n",
        "292": "know theta zero equals me about eight\n",
        "298": "hundred and feita one equals maybe a\n",
        "302": "minus 0.15 and so this point right is\n",
        "304": "this point in red corresponds to one\n",
        "305": "Pacific pair of values of theta zero\n",
        "308": "comma theta one and they correspond in\n",
        "311": "fact to that hypothesis right with theta\n",
        "313": "zero is about 800 meters intersects the\n",
        "316": "vertical axis around 800 and this is a\n",
        "320": "slope of about minus 0.15 now this line\n",
        "322": "is really not such a good fit to the\n",
        "325": "data right this hypothesis H of X with\n",
        "327": "these values of theta 0 theta 1 3 not\n",
        "330": "such a good fit to the data and so you\n",
        "333": "find that is cost as a value that's out\n",
        "335": "here let's feel pretty far from the\n",
        "338": "minimum right so it's pretty far it's in\n",
        "339": "a pretty high cost because this is just\n",
        "342": "not that good fit to the data let's look\n",
        "344": "at some more examples now here's the\n",
        "346": "different hypothesis that's you know\n",
        "348": "still not a great fit of the data there\n",
        "350": "may be slightly better so here right\n",
        "353": "that's my point\n",
        "355": "that those are my parameters theta zero\n",
        "360": "theta one and so my theta zero value\n",
        "366": "right that's um about 360 and my value\n",
        "370": "for theta one is equal to 0 so you know\n",
        "371": "let's write that out the state of 0\n",
        "377": "equals 360 theta 1 equals 0 and this\n",
        "380": "pair of parameters corresponds to that\n",
        "382": "hypothesis corresponds to a flat line\n",
        "384": "right this is H of x equals\n",
        "388": "360 plus zero times X so that's my\n",
        "391": "hypothesis and this hypothesis again has\n",
        "393": "some cost and that causes you know\n",
        "396": "plotted as the height of the J function\n",
        "399": "at that point let's look at just a\n",
        "402": "couple of examples here's one more you\n",
        "405": "know at this value of theta zero and\n",
        "409": "then that value of theta one we end up\n",
        "412": "with this hypothesis H of X and again\n",
        "415": "not a great fit to the data and just I\n",
        "416": "can move even further out for the\n",
        "419": "mineral last example this is actually\n",
        "421": "not quite at the minimum but it's pretty\n",
        "422": "close to minimum so this is not such a\n",
        "425": "bad fit to the to the data where for a\n",
        "429": "particular value of theta zero which\n",
        "430": "whatever this value is in for a\n",
        "433": "particular value of theta one we get a\n",
        "435": "particular H of X and this is this is\n",
        "436": "not quite at the minimum is pretty close\n",
        "439": "and so the sum of squares errors or some\n",
        "442": "squares distances between my training\n",
        "445": "examples and my hypothesis every that's\n",
        "447": "the sum of squared distances right of\n",
        "450": "all of these errors this is pretty close\n",
        "451": "all of these errors this is pretty close\n",
        "452": "to the minimum even though it's not\n",
        "455": "quite a minimum so with these figures I\n",
        "456": "hope that gives you a better\n",
        "459": "understanding of what values of the cost\n",
        "462": "function J and how they are and how that\n",
        "464": "corresponds to different hypotheses as\n",
        "466": "well as how better hypotheses may\n",
        "469": "correspond to points that closer to the\n",
        "472": "minimum of this cost function J now of\n",
        "474": "course what we really want is an\n",
        "476": "efficient algorithm like your efficient\n",
        "478": "piece of software for automatically\n",
        "480": "finding the value of theta zero and\n",
        "481": "theta one that minimizes the cost\n",
        "484": "function J what we don't want to do is\n",
        "486": "to you know have to write software to\n",
        "488": "plot out this point and then try to\n",
        "490": "manually read off we don't do that this\n",
        "493": "is not a good way to do it and in fact\n",
        "495": "we'll see later that when we look at\n",
        "497": "more complicated examples will have\n",
        "499": "higher dimensional figures with more\n",
        "501": "parameters that that it turns out we'll\n",
        "503": "see if you will see in maintenance cause\n",
        "506": "examples where this figure you cannot\n",
        "508": "really be plotted is becomes much harder\n",
        "512": "to visualize and so what we want is to\n",
        "514": "have software to find the value of theta\n",
        "515": "0 thing\n",
        "518": "then minimizes this function and in the\n",
        "520": "next video we'll start to talk about an\n",
        "522": "algorithm for automatically finding that\n",
        "525": "value of theta zero and theta one that\n"
    },
    "0twSSFZN9Mc": {
        "0": " \n",
        "1": "neural networks are one of the most\n",
        "3": "powerful learning algorithms that we\n",
        "5": "have today in this and in the next few\n",
        "7": "videos I'd like to start talking about a\n",
        "9": "learning algorithm for fitting the\n",
        "12": "parameters of a neural network given a\n",
        "14": "training set as with the discussion of\n",
        "16": "most of our learning algorithms we're\n",
        "18": "going to begin by talking about a cost\n",
        "20": "function for fitting the parameters of\n",
        "23": "the network I'm going to focus on the\n",
        "25": "application of neural networks to\n",
        "28": "classification problems so suppose we\n",
        "30": "have a network like that shown on the\n",
        "32": "left and suppose we have a training set\n",
        "35": "like this of X I comma Y I pairs of M\n",
        "38": "training examples I'm going to use\n",
        "41": "uppercase L to denote the total number\n",
        "44": "of layers in this network so for the\n",
        "45": "network shown on the Left we would have\n",
        "48": "capital L equals 4 and I'm going to use\n",
        "51": "s subscript L to denote the number of\n",
        "54": "units that is the number of neurons not\n",
        "57": "counting the bias unit in layer L of the\n",
        "60": "network so for example we would have a s\n",
        "63": "1 which is the input layer equals has 3\n",
        "66": "units as 2 in my example as 5 units and\n",
        "70": "the output layer as 4 which is also\n",
        "72": "equal to SL because capital L is equal\n",
        "74": "to 4 the output layer in my example the\n",
        "78": "left has 4 units we're going to consider\n",
        "80": "two types of classification problems the\n",
        "83": "first is binary classification where the\n",
        "86": "labels Y are either 0 or 1 in this case\n",
        "89": "we would have one output unit so this\n",
        "91": "neural network on top has four output\n",
        "93": "units but if we have binary\n",
        "95": "classification we would have only one\n",
        "99": "output unit that computes H of X\n",
        "101": "and the outputs of the neural network\n",
        "105": "will be H of X is going to be a real\n",
        "108": "number and in this case the number of\n",
        "111": "output units s L where L is again the\n",
        "114": "index of the final layer because that's\n",
        "115": "the number of layers we have in the\n",
        "118": "network but so the number of units we\n",
        "119": "have in the output layer is going to be\n",
        "122": "equal to 1 in this case to simplify\n",
        "124": "notation later among also going to set K\n",
        "126": "equals 1 so you can think of K is also\n",
        "130": "denoting the number of units in the\n",
        "132": "upper layer the second type of\n",
        "134": "classification problem consider will be\n",
        "136": "multi-class classification problem where\n",
        "139": "we may have K distinct two classes so\n",
        "141": "our earlier example I had this\n",
        "143": "representation for Y if we had four\n",
        "145": "classes and in this case we would have\n",
        "149": "capital K output units and our\n",
        "153": "hypothesis or output vectors that are K\n",
        "156": "dimensional and the number of output\n",
        "159": "units will be equal to K and usually we\n",
        "162": "would have K greater than or equal to 3\n",
        "164": "in this case because if we had two\n",
        "166": "classes then you know we don't need to\n",
        "169": "use the one versus all method we need to\n",
        "171": "use the one versus all method only if we\n",
        "173": "have K greater than or equal to three\n",
        "175": "classes so we had only two classes we\n",
        "177": "will need to use over one output unit\n",
        "180": "now let's define a cost function for our\n",
        "185": "neural network the cost function we use\n",
        "186": "for the neural network is going to be a\n",
        "188": "generalization of the one that we use\n",
        "191": "for which is the regression for logistic\n",
        "193": "regression and we use to minimize the\n",
        "196": "cost function J of theta that was minus\n",
        "198": "1 over M of this cost function and then\n",
        "201": "plus this extra regularization term here\n",
        "204": "where this was a sum from J equals 1\n",
        "207": "through n because we did not regularize\n",
        "211": "the bias term theta 0 for a neural\n",
        "214": "network our cost function is going to be\n",
        "216": "a generalization of this where instead\n",
        "218": "of having basically just one logistic\n",
        "221": "regression output unit we may instead\n",
        "223": "have K of them so here's our cost\n",
        "225": "function our own network now outputs\n",
        "227": "vectors in RK\n",
        "229": "where k might be equal to one if we have\n",
        "231": "a binary classification problem I'm\n",
        "233": "going to use this notation a curve X\n",
        "236": "subscript I to denote the if-- output\n",
        "240": "that is H of X is a K dimensional vector\n",
        "243": "and so this subscript I just selects out\n",
        "246": "the I've element of the vector that is\n",
        "249": "output by my new network my cost\n",
        "251": "function J of theta is now going to be\n",
        "252": "function J of theta is now going to be\n",
        "255": "the following is minus 1 over m of a sum\n",
        "257": "of a similar term to what we have the\n",
        "259": "logistic regression except that we have\n",
        "262": "this sum from k equals 1 through K this\n",
        "264": "summation is basically a sum over my K\n",
        "268": "output unit so if I have four output\n",
        "270": "units that is that the final layer of my\n",
        "272": "neural network has four output units\n",
        "275": "then this sum from this is the sum from\n",
        "279": "k equals one through four of basically\n",
        "282": "the logistic regression algorithms cost\n",
        "284": "function but summing that cost function\n",
        "287": "over each of my four output units in\n",
        "289": "turn and so you notice in particular\n",
        "293": "that this applies to YK HK because we're\n",
        "296": "basically taking the Cape upper unit and\n",
        "299": "comparing that to the value of YK which\n",
        "303": "is a you know that which is that one of\n",
        "304": "those vectors say what class it should\n",
        "308": "be and finally the second term here is\n",
        "311": "the regularization term similar to what\n",
        "314": "we had for logistic regression the\n",
        "315": "summation terms look looks really\n",
        "318": "complicated but always doing is the\n",
        "321": "summing over these terms theta Jil for\n",
        "324": "all values of I J and L except that we\n",
        "327": "don't sum over the terms corresponding\n",
        "329": "to these bias values like we had for\n",
        "332": "logistic regression concretely we don't\n",
        "334": "sum over the terms corresponding to\n",
        "338": "where I is equal to 0 so that is because\n",
        "340": "when we are computing the activation of\n",
        "343": "a neuron we have terms like these theta\n",
        "346": "i 0 plus\n",
        "351": "theta I 1 X 1 plus and so on we're\n",
        "354": "against crema 2 there this is the first\n",
        "357": "hidden there and so the values of a 0\n",
        "359": "there that corresponds to something that\n",
        "362": "multiplies into an X 0 or a 0 and so\n",
        "364": "this is kind of like the bias unit and\n",
        "366": "by analogy to what we were doing for\n",
        "368": "logistic regression we won't sum over\n",
        "369": "those terms in our regularization term\n",
        "370": "those terms in our regularization term\n",
        "372": "because we don't want to regularize that\n",
        "375": "man strange a value 0 but this is just\n",
        "378": "one possible convention and even if you\n",
        "381": "were to sum over you know I equals 0 up\n",
        "384": "to SL will work about the same and it\n",
        "386": "doesn't make a big difference but maybe\n",
        "388": "this convention of not regularizing the\n",
        "392": "bias term is just like the more common\n",
        "394": "so that's the cost function we're going\n",
        "397": "to use the filler neural network in the\n",
        "399": "next video we'll start to talk about an\n",
        "402": "algorithm for trying to optimize the\n"
    },
    "1ZhtwInuOD0": {
        "0": " \n",
        "2": "in this and in the next set of videos\n",
        "4": "I'd like to tell you about a learning\n",
        "7": "algorithm called a neural network we're\n",
        "8": "going to first talk about the\n",
        "10": "representation and then in the next set\n",
        "11": "of videos talk about learning algorithms\n",
        "14": "forward neural networks is actually a\n",
        "16": "pretty old idea but had fallen out of\n",
        "19": "favor for a while but today it is the\n",
        "21": "state of the art technique for many\n",
        "23": "different machine learning problems so\n",
        "25": "why do we need yet another learning\n",
        "27": "algorithm we already have linear\n",
        "28": "regression and we have logistic\n",
        "31": "regressions so why do we need you know\n",
        "34": "neural networks in order to motivate the\n",
        "35": "discussion of neural networks let me\n",
        "36": "discussion of neural networks let me\n",
        "38": "start by showing you a few examples of\n",
        "40": "machine learning problems where we need\n",
        "43": "to learn complex nonlinear hypotheses\n",
        "45": "consider a supervised learning\n",
        "47": "classification problem where you have a\n",
        "50": "training set like this if you want to\n",
        "51": "apply the just a progression to this\n",
        "54": "problem one thing you could do is apply\n",
        "56": "logistic regression with a lot of\n",
        "59": "nonlinear features like that so here G\n",
        "61": "as usual is a sigmoid function and we\n",
        "64": "can include lots of polynomial terms\n",
        "66": "like these and if you include in a\n",
        "69": "polynomial terms then you know maybe you\n",
        "72": "can get a hypothesis that separates the\n",
        "74": "positive and negative examples\n",
        "76": "this particular method works well when\n",
        "79": "you have only say two features x1 and x2\n",
        "81": "because you can then include all those\n",
        "84": "polynomial terms of x1 and x2 but for\n",
        "86": "many interesting machine learning\n",
        "88": "problems would have a lot more features\n",
        "91": "than just two we've been talking for a\n",
        "94": "while about housing prediction and\n",
        "96": "suppose you have a housing\n",
        "97": "suppose you have a housing\n",
        "99": "classification problem rather than a\n",
        "101": "regression problem like maybe if you\n",
        "104": "have different features of a house and\n",
        "106": "you want to predict one of the odds that\n",
        "108": "the house will be sold within the next\n",
        "110": "six months so that would be a\n",
        "111": "classification problem\n",
        "114": "and as we saw we can come up with the\n",
        "116": "quite a lot of features maybe a hundred\n",
        "119": "different features of different houses\n",
        "122": "for a problem like this if you were to\n",
        "124": "include all the quadratic terms all of\n",
        "127": "these are even all the quadratic that is\n",
        "128": "the second-order polynomial terms\n",
        "130": "there'll be a lot of them they'll be\n",
        "136": "terms like X 1 squared X 1 X 2 X 1 X 3\n",
        "141": "you have X 1 X 4 up to X 1 X 1 100 and\n",
        "145": "then you have X 2 squared X 2 X 3 and so\n",
        "148": "on and if you include just the\n",
        "149": "second-order terms that is the terms\n",
        "152": "that are a product of you know two of\n",
        "155": "these terms x1 times x2 and so on then\n",
        "159": "for the case of N equals 100 you end up\n",
        "163": " \n",
        "165": "asymptotically the number of quadratic\n",
        "168": "features grows roughly as order N\n",
        "171": "squared where n is the number of the\n",
        "174": "original features like X 1 3 X 100 that\n",
        "177": "we had and in this actually closer to N\n",
        "181": "squared over 2 so including all the\n",
        "183": "quadratic features doesn't seem like it\n",
        "185": "may be a good idea because that's a lot\n",
        "187": "of features and you might end up\n",
        "190": "overfitting the training set and it can\n",
        "193": "also be computationally expensive to\n",
        "194": "have you know to be working with that\n",
        "197": "many features one thing you could do is\n",
        "199": "include only a subset of these so if you\n",
        "202": "include only the features x1 squared x2\n",
        "206": "squared x3 squared up to maybe X 100\n",
        "209": "squared then the number of features is\n",
        "211": "much smaller here you have only you know\n",
        "214": "100 such quadratic features but this is\n",
        "216": "not enough features and certainly won't\n",
        "218": "let you for the data set like that on\n",
        "219": "the upper left\n",
        "221": "in fact them if you include only these\n",
        "223": "quadratic features together with the\n",
        "225": "original x1\n",
        "227": "so on up to X 100 features then you\n",
        "229": "can't actually fit very interesting\n",
        "231": "hypotheses or you can fit things like\n",
        "233": "you know X's align ellipses like these\n",
        "236": "but you can't you certainly cannot fit a\n",
        "237": "more complex data set like that shown\n",
        "241": "here so 505,000 features seems like a\n",
        "244": "lot if you were to include the cubic or\n",
        "247": "third order polynomial features the X 1\n",
        "253": "X 2 X 3 you know x1 squared x2 X 10 X 11\n",
        "257": "X 17 and so on you can imagine there\n",
        "259": "going to be a lot of these features in\n",
        "260": "fact they're going to be order n cubed\n",
        "264": "such features and if N equals 100 you\n",
        "266": "cannot compute that you end up with on\n",
        "270": "the order of about 170,000 such cubic\n",
        "272": "features and so including these higher\n",
        "275": "order polynomial features when your\n",
        "277": "original feature set n is large this\n",
        "278": "really dramatically blows up your\n",
        "281": "feature space and this doesn't seem like\n",
        "284": "a good way to come up with additional\n",
        "286": "features with which to build nonlinear\n",
        "289": "classifiers when n is large for many\n",
        "292": "machine learning problems and will be\n",
        "295": "pretty large here's an example let's\n",
        "298": "consider the problem of computer vision\n",
        "301": "and suppose you want to use machine\n",
        "303": "learning to train a classifier to\n",
        "306": "examine an image and tell us whether or\n",
        "310": "not it is the image is a car many people\n",
        "311": "wonder why computer vision could be\n",
        "313": "difficult I mean when you and I look at\n",
        "315": "this picture is so obvious what this is\n",
        "317": "you wonder how is it that the learning\n",
        "320": "algorithm could possibly fail to know\n",
        "322": "what this picture is to understand why\n",
        "325": "computer vision is hard and let's zoom\n",
        "327": "into a small part of the image like that\n",
        "329": "area where the little red rectangle is\n",
        "332": "it turns out that where you and I see a\n",
        "335": "car the computer sees that what it sees\n",
        "338": "is this matrix which is a grid of pixel\n",
        "340": "intensity values that tells us the\n",
        "343": "brightness of each pixel in the image so\n",
        "345": "the computer vision problem is to\n",
        "348": "look at this matrix of pixel intensity\n",
        "350": "values and tell us that these numbers\n",
        "353": "represent the door handle of a car\n",
        "356": "concretely when we use machine learning\n",
        "359": "to build a car detector what we do is we\n",
        "362": "come up with a label training set with\n",
        "365": "let's say a few label examples of calls\n",
        "367": "and if you label the examples of things\n",
        "369": "that are not cars then we give our\n",
        "371": "training set to the learning algorithm\n",
        "373": "train a classifier and then you know we\n",
        "375": "may test it and show the new image in us\n",
        "378": "what is this new thing and hopefully\n",
        "381": "recognize that that is a car to\n",
        "385": "understand why we need nonlinear\n",
        "387": "hypotheses let's take a look at some of\n",
        "390": "the images of cars and maybe non cause\n",
        "391": "that we might feed to our learning\n",
        "394": "algorithm let's pick a couple pixel\n",
        "396": "locations in our images so let's pixel\n",
        "398": "one location in pixel two location and\n",
        "402": "let's plot this car you know at the\n",
        "405": "location at a certain point depending on\n",
        "408": "the intensities of pixel 1 and pixel 2\n",
        "411": "and let's do this a few other images so\n",
        "413": "let's take a different example of a car\n",
        "415": "and here look at the same two pixel\n",
        "418": "locations and that image has a different\n",
        "419": "intensity for pixel one and the\n",
        "421": "different intensity for pixel two so it\n",
        "422": "ends up in a different location on the\n",
        "423": "ends up in a different location on the\n",
        "425": "figure and then let's plot some negative\n",
        "426": "examples as well there's an on car\n",
        "431": "that's an on car and if we do this for\n",
        "433": "more and more examples using the pluses\n",
        "436": "to denote calls and minuses to no non\n",
        "439": "cause what we'll find is that the calls\n",
        "441": "are non calls end up lying in different\n",
        "445": "regions of this space and what we need\n",
        "447": "therefore is some sort of nonlinear\n",
        "449": "hypotheses to try to separate out the\n",
        "453": "two causes what is the dimension of the\n",
        "456": "feature space suppose we were to use\n",
        "459": "just 50 by 50 pixel images now that\n",
        "461": "suppose our images were pretty small\n",
        "463": "ones just 50 pixels on the side then we\n",
        "467": "would have 2500 pixels and so the\n",
        "469": "dimension of our feature size will be N\n",
        "472": "equals 2500 where our feature vector X\n",
        "474": "list of all the pixel intensities you\n",
        "477": "know the pixel brightness of a pixel 1\n",
        "479": "the brightness of pixel 2 and so on down\n",
        "481": "to the pixel brightness of the last\n",
        "484": "pixel where you know in a typical\n",
        "485": "computer representation each of these\n",
        "489": "may be values between say 0 to 255 if it\n",
        "491": "gives us maybe grayscale value so we\n",
        "495": "have N equals 2500 if and that's if we\n",
        "498": "were using grayscale images if we were\n",
        "501": "using RGB images with separate red green\n",
        "506": " \n",
        "510": "so if we were to try to learn nonlinear\n",
        "512": "hypotheses by including all the\n",
        "514": "quadratic features that is all the terms\n",
        "518": "of the form Y or X I times XJ well with\n",
        "521": "2500 pixels we will end up with a total\n",
        "523": "of 3 million features and that's just\n",
        "525": "too large to be reasonable will be\n",
        "528": "computationally very expensive to find\n",
        "530": "and to represent all of these three\n",
        "534": "million features per training example\n",
        "537": "so simple logistic regression together\n",
        "540": "with adding in maybe the quadratic or\n",
        "541": "the cubic features that's just not a\n",
        "545": "good way to learn complex nonlinear\n",
        "547": "hypotheses when n is large because you\n",
        "548": "just end up with too many features in\n",
        "549": "just end up with too many features in\n",
        "551": "the next few videos I'd like to tell you\n",
        "553": "about neural networks which turns out to\n",
        "556": "be a much better way to the complex\n",
        "558": "hypotheses complex nonlinear hypotheses\n",
        "561": "even when your input features phase even\n",
        "564": "when n is large and along the way I'll\n",
        "566": "also get to show you a couple of fun\n",
        "569": "videos of historically important\n",
        "571": "applications of neural networks as well\n",
        "573": "that I hope those those videos that\n",
        "575": "we'll see later will be fun for you to\n"
    },
    "39PyhM0LAow": {
        "0": " \n",
        "1": "you now know a bunch about machine\n",
        "3": "learning in this video I'd like to teach\n",
        "6": "you a programming language octave in\n",
        "8": "which you be able to very quickly\n",
        "10": "implement the learning algorithms we've\n",
        "12": "seen already and the learning algorithms\n",
        "14": "we'll see later in this course in the\n",
        "15": "past I've tried to teach machine\n",
        "18": "learning using a live variety of\n",
        "19": "different programming languages\n",
        "24": "including C pluses Java Python numpy R\n",
        "27": "and also octave and what I found is that\n",
        "30": "students were able to learn the most\n",
        "32": "productively learned most quickly and\n",
        "34": "prototype your algorithms most quickly\n",
        "36": "using a relatively high level language\n",
        "40": "like octave in fact um what I often see\n",
        "42": "in Silicon Valley is that if even if you\n",
        "44": "need to build if you want to build a\n",
        "45": "large-scale deployment of a learning\n",
        "48": "algorithm what people will often do is\n",
        "50": "prototype in the language like octave\n",
        "52": "which is a great prototyping language\n",
        "53": "you can sort of get your learning\n",
        "55": "algorithms working quickly and then only\n",
        "57": "if you need to very large scale\n",
        "59": "deployment of it only then spend your\n",
        "62": "time re-implementing the algorithms in\n",
        "64": "C++ or Java or some other language like\n",
        "65": "that because one of the lessons we've\n",
        "67": "learned is that programmer time or\n",
        "69": "developer time that is your time your\n",
        "71": "the machine learning Swiss time is\n",
        "75": "incredibly valuable and if you can get\n",
        "76": "your learning Urban's to work more\n",
        "78": "quickly an octave then overall you have\n",
        "81": "a huge time savings by first developing\n",
        "83": "the algorithms in octave and then\n",
        "84": "implementing and like maybe C versus\n",
        "86": "Java only after we have the ideas\n",
        "89": "working the most common prototyping\n",
        "91": "languages I see people use for machine\n",
        "95": "learning are octave MATLAB Python numpy\n",
        "99": "and our octave is nice since this free\n",
        "102": "open source and MATLAB works well too\n",
        "105": "but it's expensive for too many people\n",
        "107": "but if you have access to coffee in\n",
        "109": "MATLAB you can also use matter for this\n",
        "112": "class if you know Python numpy or if you\n",
        "115": "know R I do see some people use it but\n",
        "117": "what I see is that people usually end up\n",
        "120": "developing somewhat more slowly in you\n",
        "123": "know these languages because the Python\n",
        "125": "numpy syntax is just slightly clunkier\n",
        "129": "than the octave syntax and so because of\n",
        "131": "that and because we're releasing starter\n",
        "132": "code and octave\n",
        "134": "I strongly recommend that you not try to\n",
        "137": "do the toning exercises in this class in\n",
        "139": "the umpire are but that I do recommend\n",
        "140": "that you instead do they're probably\n",
        "142": "exercises for this cause in octave\n",
        "145": "instead what I'm going to do in this\n",
        "147": "video is go through a list of commands\n",
        "149": "and very fairly quickly and the goal is\n",
        "151": "to quickly show you the range of\n",
        "153": "commands and the range of things you can\n",
        "156": "do in octave the course website will\n",
        "159": "have a transcript of everything I do and\n",
        "163": "so after watching this video you can\n",
        "165": "refer to the to the transcript I posted\n",
        "167": "on the course website when you want to\n",
        "169": "find a command completely whatever\n",
        "171": "recommend you do is first watch the\n",
        "174": "tutorial videos and after watching you\n",
        "177": "know to the end then install octave on\n",
        "179": "your computer and finally it goes to the\n",
        "181": "course website download the transcript\n",
        "183": "of the things you see in the session and\n",
        "186": "type in whatever commands seem\n",
        "189": "interesting to you in two octaves of\n",
        "190": "running on your own computer so that you\n",
        "193": "can see it work for yourself and what's\n",
        "196": "that let's get started here's my Windows\n",
        "198": "desktop and I'm gonna start up octave\n",
        "201": "and I'm now in octave and there's my\n",
        "203": "octave prompt let me first show you the\n",
        "205": "elementary operations you can do an\n",
        "207": "octave so you type in 5 + 6 that gives\n",
        "215": "you the answer of 11 z -2 5 x 8 1 / 2 2\n",
        "219": "to the power of 6 is 64 so does the\n",
        "222": "elementary math operations you can also\n",
        "225": "do logical operations so one equals two\n",
        "228": "this evaluates to false the percent come\n",
        "232": "on here is means a comment so one equals\n",
        "234": "two evaluates to false which is\n",
        "236": "represented by zero one not equals to\n",
        "240": "two this is true so that returns one\n",
        "242": "note that the not equal sign is this\n",
        "246": "toter equals symbol and not bang equals\n",
        "249": "which is what some other programming\n",
        "251": "languages use let's see logic operations\n",
        "255": "1 &amp; 0 use a double ampersand sign to\n",
        "259": "denote the logical end and that\n",
        "263": "Falls one all zero is the or operation\n",
        "266": "and that evaluates the true and I can X\n",
        "269": "all one and 0 and that evaluates the one\n",
        "272": "this thing over on the left is octane 3\n",
        "274": "240 X equal 11 this is the default\n",
        "277": "octave prompt it shows the what\n",
        "279": "variation an octave and so on if you\n",
        "280": "don't want that problem this is somewhat\n",
        "284": "cryptic come on PS quote greater greater\n",
        "288": "than and so on that you can use to\n",
        "289": "change the prompt and I guess this\n",
        "291": "quoted string in the middle your quote\n",
        "293": "greater than greater than space that's\n",
        "295": "what I prefer my octave prompt to look\n",
        "297": "like so if I hit enter\n",
        "302": "oops excuse me like so ps1 like so now\n",
        "304": "my octave prompt has changed to their\n",
        "306": "greater than greater than sign which you\n",
        "309": "know looks quite a bit better\n",
        "312": "next let's talk about octave variables I\n",
        "314": "can take the variable a and assign it to\n",
        "319": "3 and hit enter and now is equal to me\n",
        "321": "you want to assign a variable but you\n",
        "322": "don't want to print out the result if\n",
        "323": "don't want to print out the result if\n",
        "326": "you put a semicolon the semicolon\n",
        "330": "suppresses the print the print outputs\n",
        "332": "let me do that until it doesn't print\n",
        "335": "anything whereas a equals 3 you know\n",
        "337": "makes it printed out whereas a equals 3\n",
        "340": "semicolon doesn't print anything I can\n",
        "344": "do string assignment equals hi now if I\n",
        "346": "just enter be a prince of the variable B\n",
        "349": "so B is the string hi C equals 3 grades\n",
        "355": "in equal 1 so now C evaluates to true if\n",
        "358": "you want to print out or display a\n",
        "360": "variable here's how you go about it let\n",
        "364": "me set a equals pi and if I want to\n",
        "367": "print a and just type a like so no\n",
        "369": "printed out for more complex printing\n",
        "371": "there's also the disc command which\n",
        "373": "stands with this place when I display a\n",
        "376": "just prints out a like so you can also\n",
        "380": "display the strings so this is printf\n",
        "383": "two decimals\n",
        "388": "percent 0.2 F comma a like so and this\n",
        "389": "percent 0.2 F comma a like so and this\n",
        "390": "will print out the string two decimals :\n",
        "395": "3.14 this is kind of a old style C\n",
        "397": "syntax for those of you that have\n",
        "399": "permanent C before this is essentially\n",
        "401": "the syntax you use to print string so\n",
        "404": "the printf generates a spirit generates\n",
        "406": "a string that is this you know two\n",
        "409": "decimals 3.14 string this percent 0.2 f\n",
        "412": "means substitute a into here sharing it\n",
        "414": "with you know two digits after the\n",
        "417": "decimal point and this takes the string\n",
        "420": "that's generated by this printf command\n",
        "422": "s printf well the string printf come on\n",
        "425": "and this actually displays a string and\n",
        "428": "it's really another example this printf\n",
        "434": "6 decimals percent 0.6 F comma a and\n",
        "439": "this should print PI with 6 decimal to 6\n",
        "443": "decimal places finally I was displaying\n",
        "445": "a like 7 love like this you can see\n",
        "446": "they're they're useful shortcuts you\n",
        "450": "type format long it causes strings to by\n",
        "453": "default be its way to a lot more decimal\n",
        "455": "places and format short is a command\n",
        "457": "that we stores the default of just\n",
        "460": "printing a small number of digits ok\n",
        "463": "that's how you work with variables now\n",
        "464": "that's how you work with variables now\n",
        "467": "let's look at vectors and matrices let's\n",
        "469": "say I want to assign me a to a matrix\n",
        "471": "I'm sure in example 1 2 semicolon 3 4\n",
        "476": "semicolon 5 6 this generates a 3 by 2\n",
        "479": "matrix a whose first row as 1 2 second\n",
        "482": "row 3 for the third row is 5 6 what the\n",
        "485": "semicolon does is essentially say go to\n",
        "488": "the next row of the matrix there are\n",
        "489": "other ways to type the same type equals\n",
        "494": "1 2 semicolon 3 4 semicolon 5 6 like so\n",
        "497": "and that's another equivalent way of\n",
        "500": "assigning a to be the values of this 3\n",
        "501": "by 2 matrix\n",
        "503": "similarly you can assign vectors so V\n",
        "506": "equals one two three this is actually a\n",
        "508": "row vector or this is a three by one\n",
        "511": "vector right there so that's why vectors\n",
        "515": "excuse me not this is on a one by three\n",
        "520": "matrix right not to me by one if I want\n",
        "524": "to assign this to a column vector what I\n",
        "526": "would do is instead to V equals one\n",
        "529": "semicolon to semicolon three and this\n",
        "532": "will give me a three by one instead of 1\n",
        "535": "by 3 vector so this is a column vector\n",
        "538": "here's some more useful notation I can\n",
        "542": "all set V equals 1 and : 0.1 : to what\n",
        "545": "this does is it sets V to the bunch of\n",
        "548": "elements that start from 1 and\n",
        "552": "increments in steps of 0.1 until you get\n",
        "557": "up to 2 so if I do this V is going to be\n",
        "559": "this you know row vector oh this is a\n",
        "564": "what 1 by 11 matrix already that's 1.1\n",
        "568": "1.1 1.2 1.3 and so on until we get up to\n",
        "574": "2 now um and I can also set B equals 1 :\n",
        "577": "6 and that's set to be to be you know\n",
        "581": "these numbers 1 through 6 okay now\n",
        "583": "here's some other way to generate a\n",
        "587": "matrices once 2 by 3 there's a command\n",
        "589": "that generates a matrix that is a two by\n",
        "591": "three matrix that is the matrix of all\n",
        "596": "ones so if I set C equals 2 times 1 is 2\n",
        "601": "by 3 this generates a 2 by 3 matrix does\n",
        "603": "all tubes and this is you can think of\n",
        "605": "it as a shorter way of writing this and\n",
        "608": "C equals 2 to 2 semicolon 2 to 2 which\n",
        "611": "will also give you the same result let's\n",
        "615": "say W equals 1 1 by 3 so this is going\n",
        "622": "to be a row vector or a row of 3 runs\n",
        "625": "and similarly you can also set W equals\n",
        "629": "rows one by three and this generates a\n",
        "633": "matrix one by three matrix of all zeros\n",
        "636": "just a couple more ways to generate\n",
        "640": "matrices if I do W equals R and 1 by 3\n",
        "644": "this gives me a 1 by 3 matrix of all\n",
        "649": "random numbers if I do R and 3 by 3 this\n",
        "651": "gives me a 3 by 3 matrix of all random\n",
        "653": "numbers drawn from the uniform\n",
        "656": "distribution between 0 &amp; 1 so every time\n",
        "658": "I do this I get a different set of\n",
        "661": "random numbers drawn uniformly between 0\n",
        "664": "&amp; 1 for those of you that know whether\n",
        "665": "Gaussian random variable and so for\n",
        "667": "those who than there was a normal and\n",
        "668": "the variable is you can also set down\n",
        "673": "equals R and N 1 by 3 and so these are\n",
        "675": "going to be three values drawn from a\n",
        "678": "Gaussian distribution with mean 0 and\n",
        "680": "variance and standard deviation equal to\n",
        "683": "1 and you can set more complex things\n",
        "685": "like W equals minus 6 plus square root\n",
        "690": "10 times let's say R and and one by\n",
        "693": "10,000 and I'm gonna put a semicolon at\n",
        "694": "the end because I don't really want this\n",
        "695": "the end because I don't really want this\n",
        "697": "printed out this is going to be a what\n",
        "700": "well it was going to be a vector with a\n",
        "706": "hundred thousand 10,000 elements so well\n",
        "708": "okay you know what let's let's print it\n",
        "710": "out so this will generate a matrix like\n",
        "714": "this right with 10,000 elements so\n",
        "717": "that's what W is and if I now plot a\n",
        "720": "histogram of W what the his command I\n",
        "725": "can now and I'll type in his command it\n",
        "727": "takes a couple seconds to bring this up\n",
        "729": "but this is a histogram of my random\n",
        "732": "variable for W there was minus 6 plus 3\n",
        "734": "over 10 times this Gaussian random\n",
        "737": "variable and I can plot a histogram with\n",
        "740": "more buckets with more bins will say 50\n",
        "743": "bins and this is my histogram of a\n",
        "746": "Gaussian with mean minus 6 because I\n",
        "748": "have a minus 6 there plus square root 10\n",
        "749": "times\n",
        "753": "so the dr variance or ritas of this\n",
        "755": "Gaussian random variable was 10 on the\n",
        "757": "standard deviation this square root of\n",
        "762": "10 which is about what 3.1 finally one\n",
        "764": "special command for generated matrix\n",
        "766": "which is the I command\n",
        "769": "so I stands for this may be a pun on the\n",
        "773": "word identity but survived said I for\n",
        "775": "this is the 4 by 4 identity matrix I\n",
        "779": "said I enclose I for this gives me a 4\n",
        "782": "by 4 identity matrix and I because I v\n",
        "786": "i6 that gives me a 6 by 6 identity\n",
        "789": "matrix and I 3 is the 3 by 3 identity\n",
        "793": "matrix lastly to wrap up this video as\n",
        "794": "one more useful command which is the\n",
        "797": "help command so you can type help I and\n",
        "799": "this brings up the help function for the\n",
        "802": "identity matrix hit Q to quit and you\n",
        "805": "can also type help Rand brings up\n",
        "808": "documentation for the rand or the random\n",
        "810": "number generation function or even\n",
        "812": "though help-help which shows you you\n",
        "816": "know hope on the hope function\n",
        "820": "so those are the basic operations in\n",
        "822": "octave oh and with this you should be\n",
        "825": "able to generate a few matrices multiply\n",
        "828": "add things and and use the basic\n",
        "831": "operations in octave in the next video\n",
        "833": "I'd like to start talking about more\n",
        "836": "sophisticated commands and how to move\n",
        "838": "data around and start to process data in\n"
    },
    "4MKN-JkNGXY": {
        "0": " \n",
        "2": "you've seen how regularization can help\n",
        "5": "prevent overfitting but how does it\n",
        "7": "affect the bias and variance of a\n",
        "9": "learning algorithm in this video I'd\n",
        "11": "like to go deeper into the issue of bias\n",
        "14": "and variance and talk about how it\n",
        "17": "interacts with and is affected by the\n",
        "18": "regularization of your learning\n",
        "23": "algorithm suppose we fit a linear\n",
        "25": "regression model with a very high order\n",
        "28": "polynomial but to prevent overfitting\n",
        "30": "we're going to use regularization as\n",
        "32": "shown here suppose we're fitting a high\n",
        "33": "shown here suppose we're fitting a high\n",
        "35": "order polynomial like that shown here\n",
        "37": "but to prevent overfitting we're going\n",
        "39": "to use the regularization like that\n",
        "40": "shown here so we have this\n",
        "43": "regularization term to try to keep the\n",
        "45": "pair the values of the parameter small\n",
        "47": "and as usual the regularization stems\n",
        "50": "from J equals 1 to M rather than J\n",
        "53": "equals 0 to them let's consider three\n",
        "55": "cases the first is a case of a very\n",
        "57": "large value of the regularization\n",
        "60": "parameter lambda such as if lambda were\n",
        "64": "equal to 10,000 some huge value in this\n",
        "66": "case all of these parameters theta 1\n",
        "68": "theta 2 theta 3 and so on will be\n",
        "71": "heavily penalized and so we'll end up\n",
        "74": "with most of these parameter values\n",
        "76": "being close to zero and the hypothesis\n",
        "79": "will be roughly H of X just equal or\n",
        "81": "approximately equal to theta 0 and so we\n",
        "83": "end up with the hypothesis that more or\n",
        "85": "less looks like that this is more than a\n",
        "88": "flat constant straight line and so this\n",
        "89": "flat constant straight line and so this\n",
        "91": "hypothesis has high bias and the badly\n",
        "93": "under fits this data sets about\n",
        "95": "horizontal straight line is just not a\n",
        "97": "very good model for this data set and\n",
        "100": "the other extreme is if we have a very\n",
        "103": "small value of lambda such as if lambda\n",
        "106": "were equal to zero in that case given\n",
        "107": "that with fitting a high order\n",
        "110": "polynomial this is a usual overfitting\n",
        "114": "setting in that case given that with\n",
        "115": "fitting a high order polynomial\n",
        "118": "basically without regularization or with\n",
        "120": "very minimal verbalization we end up\n",
        "122": "with our usual high variance overfitting\n",
        "124": "setting this basically if long dress\n",
        "125": "equals 0\n",
        "126": "which is fitting it without\n",
        "129": "regularization so that all the fits the\n",
        "131": "hypothesis and there's only if we have\n",
        "133": "some intermediate value of lambda that\n",
        "136": "is neither too large nor too small that\n",
        "139": "we observed with parameters theta that\n",
        "143": "give us a reasonable fit to this data so\n",
        "146": "how can we automatically choose a good\n",
        "147": "value for the regularization parameter\n",
        "150": "lambda just to reiterate here is our\n",
        "152": "model and here is our learning\n",
        "154": "algorithms objective for the setting\n",
        "156": "where we're using regularization let me\n",
        "159": "define J train of theta to be something\n",
        "161": "different to be the optimization\n",
        "164": "objective but without the regularization\n",
        "168": "term previously in an earlier video when\n",
        "170": "we were not using regularization I\n",
        "173": "define J train of theta to be the same\n",
        "175": "as J of theta as the cost function but\n",
        "178": "when we're using regularization when the\n",
        "179": "sixth row lambda term we're going to\n",
        "182": "define J train my training set error to\n",
        "185": "be just my sum of squared errors on the\n",
        "186": "training set or my average squared error\n",
        "189": "on the training set without taking into\n",
        "190": "account that regularization term and\n",
        "193": "similarly I'm then also going to define\n",
        "196": "the cross-validation set error when the\n",
        "198": "test set error as before to be the\n",
        "200": "average sum of squared errors on the\n",
        "203": "cross-validation and the test sets so\n",
        "207": "just to summarize my definitions of J\n",
        "209": "train j cv and j tests are just the\n",
        "212": "average squared error or one half of the\n",
        "213": "average squared error on my training\n",
        "216": "validation and test sets without the\n",
        "219": "extra regularization term so this is how\n",
        "221": "we can automatically choose the\n",
        "224": "regularization parameter lambda what I\n",
        "226": "usually do is maybe have some range of\n",
        "229": "values of lambda I want to try out so I\n",
        "231": "might be considering not using\n",
        "232": "regularization well here are a few\n",
        "234": "values I might try out might be\n",
        "235": "considering long because over there one\n",
        "237": "of my no two or four or four and so on\n",
        "240": "and you know I usually step these up in\n",
        "242": "multiples of two until some some maybe\n",
        "245": "larger value if I were doing this in\n",
        "246": "multiples of two and\n",
        "250": "without 10.24 instead of 10 exactly but\n",
        "252": "you know this is close enough enter the\n",
        "255": "third and fourth decimal places won't\n",
        "260": "won't affect your result that much so\n",
        "263": "this gives me maybe 12 different models\n",
        "265": "I'm trying to select amounts\n",
        "267": "corresponding to tell different values\n",
        "273": " \n",
        "276": "and of course you can also go to stal\n",
        "278": "use less than 0.01 or values larger than\n",
        "281": "10 but I've just truncated it here for\n",
        "287": "convenience given each of these 12\n",
        "289": "models what we can do is then the\n",
        "292": "following we can take this first model\n",
        "296": "what amount equals zero and minimize my\n",
        "298": "cost function J of theta and this will\n",
        "300": "give me some parameter vector theta and\n",
        "302": "similar to the earlier video let me just\n",
        "306": "denote this as say the superscript one\n",
        "311": "and then I can take my second model with\n",
        "314": "lambda is set to 0.01 and minimize my\n",
        "317": "cost function now using lambda equals\n",
        "319": "0.01 of course to get some different\n",
        "320": "parameter vector theta let me denote\n",
        "323": "that theta 2 and for that I end up with\n",
        "325": "theta 3 so if this is right for my third\n",
        "329": "model and so on and itself for my final\n",
        "331": "model was wrong they're set to 10 when I\n",
        "334": "want to go ten on 10.24 I end up with\n",
        "336": "this theta 12\n",
        "339": "next I can take all of these hypotheses\n",
        "342": "or all of these parameters and use my\n",
        "345": "cross-validation set to evaluate them so\n",
        "348": "I can look at my first model my second\n",
        "350": "model fits with these different values\n",
        "352": "of the regularization parameter and\n",
        "354": "evaluate them on my cross-validation\n",
        "357": "sets basically measure the average\n",
        "359": "squared error of each of these parameter\n",
        "362": "vectors theta on my cross validation set\n",
        "365": "and I would then pick whichever one of\n",
        "367": "these 12 models gives me the lowest\n",
        "371": "error on the cross-validation set and\n",
        "372": "let's say for the sake of the example\n",
        "376": "that I end up picking theta 5 the fifth\n",
        "379": "order polynomial because that has the\n",
        "381": "lowest cross-validation error\n",
        "384": "having done that finally what I would do\n",
        "387": "if I want to report a test set error is\n",
        "390": "to take the parameter theta 5 stuff that\n",
        "393": "I've selected and look at how well it\n",
        "396": "does on my test sets and once again here\n",
        "399": "is as if we fit this parameter theta to\n",
        "402": "my cross-validation sets which is why\n",
        "404": "I'm saving aside a separate test set\n",
        "407": "that I'm going to use to get a better\n",
        "410": "estimate of how well my parameter vector\n",
        "412": "theta will generalize to previously\n",
        "413": "unseen examples\n",
        "416": "so that's model selection applied to\n",
        "418": "selecting the regularization parameter\n",
        "421": "lambda the last thing I'd like to do in\n",
        "424": "this video is get a better understanding\n",
        "427": "of how cross validation and training\n",
        "431": "error of the air V as we as we vary the\n",
        "433": "regularization parameter lambda and so\n",
        "435": "just a reminder right that was our\n",
        "437": "original cost function J of theta but\n",
        "439": "for this purpose we're going to define\n",
        "442": "training error without using your\n",
        "444": "organization parameter and cross\n",
        "446": "validation error without using the\n",
        "447": "regularization\n",
        "450": "and what I'd like to do is plot this\n",
        "454": "j-train and plug this GTV meaning just\n",
        "457": "the how well does my hypothesis do for\n",
        "460": "on the training set and how others my\n",
        "461": "hypothesis do on the cross-validation\n",
        "464": "set as I vary my regularization\n",
        "467": " \n",
        "468": " \n",
        "472": "so as we saw earlier if lambda is small\n",
        "474": "then we're not using much regularization\n",
        "479": "and we run a larger risk of overfitting\n",
        "483": "whereas if lambda is large that is if we\n",
        "485": "were on the right part of this\n",
        "488": "horizontal axis then with a large value\n",
        "491": "of lambda we run a higher risk of having\n",
        "494": "a bias problem so if you plot J train\n",
        "499": "and JCV what you find is that for small\n",
        "502": "values of lambda you are you can fit the\n",
        "504": "training set relatively well because\n",
        "506": "you're not regularizing so for small\n",
        "508": "values of lambda the regularization term\n",
        "510": "basic goes away and you're just\n",
        "512": "minimizing pretty much your squared\n",
        "514": "error so when lambda is small you end up\n",
        "516": "with a small value for j-train\n",
        "519": "whereas if lambda is large then you have\n",
        "521": "a high bias problem and you might not\n",
        "523": "fit your training so well so you didn't\n",
        "529": "value up there so J train of theta will\n",
        "532": "tend to increase when lambda increases\n",
        "534": "because a large value of lambda\n",
        "536": "corresponds to high bias where you might\n",
        "538": "not even fit your training set well\n",
        "540": "whereas a small value of lambda\n",
        "543": "corresponds to if you can you know 3d\n",
        "546": "fits a fairly high degree polynomial to\n",
        "547": "your data let's say as for the\n",
        "550": "cross-validation error we end up with a\n",
        "554": "figure like this where we're over here\n",
        "556": "on the right if we have a large value of\n",
        "559": "lambda we may end up under fitting and\n",
        "564": "so this is the bias regime whereas and\n",
        "567": "so the cross-validation error will be\n",
        "569": "high let me just label that so that's\n",
        "573": "JCV of theta because with high bias we\n",
        "575": "won't be fitting we won't be doing well\n",
        "578": "on the cross-validation set whereas here\n",
        "580": "on the left this is the high variance\n",
        "581": "regime\n",
        "584": "where if we have two smaller value of\n",
        "587": "lambda then we may be overfitting the\n",
        "589": "data and so for overfitting the data\n",
        "592": "then the cross-validation error were\n",
        "596": "also behind and so this is what do cross\n",
        "599": "validation error and what the training\n",
        "601": "error may look like on the training sets\n",
        "604": "as we believe different to launder as we\n",
        "606": "vary the randomization so long term and\n",
        "609": "so once again it will often be some\n",
        "612": "intermediate value of lambda that you\n",
        "614": "know this sort of called just rights or\n",
        "616": "that works best in terms of having a\n",
        "618": "small cross-validation error or a small\n",
        "620": "test set and whereas the curls I've\n",
        "623": "drawn here are somewhat cartoonish and\n",
        "625": "somewhat idealized so on a real data set\n",
        "628": "the curves you get may end up looking a\n",
        "629": "little bit more messy and just a little\n",
        "632": "bit more noisy than this for some data\n",
        "634": "says you will really see these poor\n",
        "637": "source of trends and by looking at the\n",
        "639": "plot of the holdout cross-validation\n",
        "642": "error you can either manually or\n",
        "644": "automatically try to select a point that\n",
        "648": "minimizes the the cross-validation error\n",
        "650": "and select a value of lambda\n",
        "652": "corresponding to low cross-validation\n",
        "654": "error when I'm trying to pick the\n",
        "657": "regularization parameter lambda for a\n",
        "659": "learning algorithm often I find that\n",
        "661": "plotting a figure like this one shown\n",
        "664": "here helps me understand better what's\n",
        "667": "going on and helps me verify that I am\n",
        "669": "indeed picking a good value for the\n",
        "671": "regularization parameter lambda so\n",
        "673": "hopefully that gives you more insight\n",
        "676": "into regularization and this effects on\n",
        "678": "the bias and variance of a learning\n",
        "681": "algorithm by now you've seen bias and\n",
        "682": "variance from a lot of different\n",
        "685": "perspectives and what I'd like to do in\n",
        "687": "the next video is think of all of the\n",
        "689": "insights that we've gone through and\n",
        "691": "build on them to put together a\n",
        "694": "diagnostic that's called learning curves\n",
        "697": "which is a tool that I often use to try\n",
        "698": "to diagnose that the learning algorithm\n",
        "702": "may be suffering from a bias problem or\n"
    },
    "4WP6jVGIn7M": {
        "0": " \n",
        "2": "in this video we'll talk about matrix\n",
        "4": "addition and subtraction as well as how\n",
        "7": "to multiply a matrix by number also\n",
        "10": "called scalar multiplication let's start\n",
        "14": "with an example given two matrices like\n",
        "16": "these let's say I want to add them\n",
        "18": "together how do I do that and so what\n",
        "20": "this addition of matrices means it turns\n",
        "22": "out that if you want to add two matrices\n",
        "25": "what you do is you just add up the\n",
        "28": "elements of these matrices one at a time\n",
        "31": "so my result of adding two matrices is\n",
        "33": "going to be itself another matrix and\n",
        "35": "the first element again just by taking\n",
        "37": "one and four and multiplying them and\n",
        "40": "adding them together so I get five the\n",
        "42": "second element I get by taking two and\n",
        "45": "two and adding them so get four three\n",
        "49": "plus three plus zero is three and so on\n",
        "51": "we'll stop changing colors I guess and\n",
        "56": "on the right is 0.5 ten and two and it\n",
        "58": "turns out you can add only two matrices\n",
        "61": "that out of the same dimension so this\n",
        "67": "example is a three by two matrix because\n",
        "70": "this has V rows and two columns so\n",
        "72": "there's three by two this is also a\n",
        "75": "three by two matrix and the result of\n",
        "78": "adding these two matrices is a three by\n",
        "80": "two matrix again so you can only add\n",
        "82": "matrices of the same dimension and the\n",
        "83": "matrices of the same dimension and the\n",
        "85": "result would be another matrix of the\n",
        "87": "same dimension as as the ones you just\n",
        "90": "added whereas in contrast you were to\n",
        "92": "take these two matrices so this one is a\n",
        "94": "three by two matrix so again three rows\n",
        "97": "two columns this here is a two by two\n",
        "100": "matrix and because these two matrices\n",
        "103": "are not of the same dimension you know\n",
        "106": "this this is an error so you cannot add\n",
        "108": "these two matrices and you know their\n",
        "111": "sum is not well-defined\n",
        "114": "so that's matrixes next let's talk about\n",
        "117": "multiplying matrices by a by a scalar\n",
        "120": "number and the scalar is just maybe a\n",
        "122": "overly fancy term for you know a number\n",
        "125": "or a real number right the scalar just\n",
        "128": "means real number so let's take the\n",
        "129": "number 3 and multiply it by this matrix\n",
        "132": "if you do that the result is pretty much\n",
        "134": "what you'd expect you just take your\n",
        "136": "elements of the matrix and multiply them\n",
        "140": "by 3 1 at a time so you know 1 times 3\n",
        "146": "is 3 what 2 times 3 6 3 times 3 is 9 and\n",
        "148": "let's see I'm going to stop changing\n",
        "152": "colors again 0 times 3 is 0 3 times 5 is\n",
        "157": "15 and 3 times 1 is 3 and so this matrix\n",
        "159": "is the result of multiplying that matrix\n",
        "162": "on the left by 2b and you notice again\n",
        "164": "this is a 3 by 2 matrix and the result\n",
        "167": "is a matrix of the same dimension this\n",
        "170": "is a 3 by 2 both of these are 3 by 2\n",
        "174": "dimensional matrices and by the way you\n",
        "176": "can write multiplication you know either\n",
        "179": "way so I had 3 times this matrix I could\n",
        "183": "also have written this matrix 1 0 2 5 3\n",
        "187": "1 right I just copied this matrix over\n",
        "189": "to the right I can also take this matrix\n",
        "191": "and multiply this line 3 so whether it's\n",
        "193": "you know 3 times the matrix or the\n",
        "195": "matrix times 3 is the same thing and\n",
        "197": "this thing here in the middle is the\n",
        "200": "result you can also take a matrix and\n",
        "204": "divide it by a number so turns out\n",
        "205": "taking this matrix and dividing it by 4\n",
        "208": "this is actually the same as taking the\n",
        "211": "number one quarter and multiplying it by\n",
        "216": "this matrix 4 0 6 3 and so you know in\n",
        "218": "very answer the result of this product\n",
        "222": "is you know 1/4 times 4 is 1 100 2 times\n",
        "229": "0 0 1 or 2 times 6 is what 3 has that 6\n",
        "232": "over 4 is 3 arms and 1 whole 2 times 3\n",
        "236": "is equal to and so that's a result of\n",
        "239": "computing this matrix divided by 4 that\n",
        "241": "gives you yourself\n",
        "244": "finally for a slightly more complicated\n",
        "246": "example you can also take these\n",
        "249": "operations and combine them together\n",
        "251": "so in this calculation I have three\n",
        "253": "times the vector plus a vector minus\n",
        "256": "another vector divided by three so just\n",
        "257": "make sure we know what these are right\n",
        "261": "this multiplication this is an example\n",
        "265": "of scalar multiplication because I'm\n",
        "268": "taking three and multiplying it and this\n",
        "271": "is you know another scalar\n",
        "273": "multiplication or going to write scalar\n",
        "274": "division I guess this really just means\n",
        "277": "one three times this and so if we\n",
        "283": "evaluate these two operations first then\n",
        "285": "what we get is this thing is equal to\n",
        "288": "let's see so three times that vector is\n",
        "294": "three twelve six plus my vacuum in the\n",
        "297": "middle which is a zero zero five minus\n",
        "303": "one zero two thirds all right and again\n",
        "305": "just to make sure we understand what's\n",
        "310": "going on here this plus symbol that is\n",
        "316": "matrix addition right already since\n",
        "318": "these are vectors remember vectors are\n",
        "320": "special cases of matrices are just you\n",
        "324": "can also call this vector addition and\n",
        "328": "this minus sign here this is again you\n",
        "331": "know matrix subtraction but because this\n",
        "334": "is a n by one really a 3 by 1 matrix\n",
        "336": "that this is actually a vector so this\n",
        "340": "is also vector just call call this\n",
        "343": "matrix a vector subtraction as well ok\n",
        "346": "and finally to wrap this up this\n",
        "348": "therefore gives me a vector whose first\n",
        "353": "element is going to be 3 plus 0 minus 1\n",
        "356": "so that's 3 minus 1 which is 2 the\n",
        "359": "second element is 12 plus 0 minus 0\n",
        "364": "which is 12 and the third element of\n",
        "369": "this is 6 plus 5 minus 2/3 which is a\n",
        "373": "11 minus 2/3 so that's 10 in 1/3 I'm\n",
        "376": "going to close my square bracket and so\n",
        "381": "this gives me a 3 by 1 matrix which is\n",
        "387": "also just called a 3 dimensional vector\n",
        "390": "right which is the outcome of this\n",
        "394": "calculation over here so that's how you\n",
        "396": "add and subtract matrices and vectors\n",
        "399": "and multiply them by scalars so by real\n",
        "402": "numbers so far I've only talked about\n",
        "405": "how to multiply matrices and vectors by\n",
        "407": "scalars by real numbers in the next\n",
        "409": "video we'll talk about the a much more\n",
        "411": "interesting step of taking two matrices\n"
    },
    "5R1xOJOFRzs": {
        "0": " \n",
        "2": "in the last few videos we talked about a\n",
        "4": "collaborative filtering algorithm in\n",
        "6": "this video I wanna say a little bit\n",
        "8": "about the vectorization implementation\n",
        "10": "of this algorithm and also talk a little\n",
        "12": "bit about other things you can do with\n",
        "14": "this algorithm for example one of the\n",
        "17": "things you do is given one product can\n",
        "19": "you find other products that are related\n",
        "21": "to this so that if for example a user\n",
        "23": "has recently been looking at one product\n",
        "25": "on their other related products that you\n",
        "28": "could recommend to this user so let's\n",
        "30": "see what we can do about that what I'd\n",
        "32": "like to do is work out an alternative\n",
        "35": "way of writing out the predictions of\n",
        "37": "the collaborative filtering algorithm to\n",
        "40": "start here's our data set with our five\n",
        "43": "movies and what I'm going to do is take\n",
        "45": "all the ratings by all the users and\n",
        "49": "group them into a matrix so here we have\n",
        "53": "five movies and four users and so this\n",
        "55": "matrix Y is going to be a five by four\n",
        "57": "matrix let's just you know taking all\n",
        "60": "the elements all of this data including\n",
        "61": "question marks and grouping them into\n",
        "64": "this matrix and of course the elements\n",
        "66": "of this matrix are the IJ element of\n",
        "68": "this matrix is really what we were\n",
        "71": "previously writing as Y superscript I\n",
        "73": "comma jeans the rating given to Ruby I\n",
        "77": "by user J given this matrix Y of all the\n",
        "78": "ratings that we have there's an\n",
        "81": "alternative way of writing out all the\n",
        "83": "predicted ratings of the algorithm and\n",
        "87": "in particular if you look at what a\n",
        "90": "certain user predicts on a certain movie\n",
        "94": "what user J predicts on movie I is given\n",
        "96": "by this formula\n",
        "100": "and so if you have a matrix of the\n",
        "102": "predictive ratings what you would have\n",
        "106": "is the following matrix where the I\n",
        "111": "comma J entry so this corresponds to the\n",
        "114": "rating that we predict user J would give\n",
        "118": "to movie I is exactly equal to that\n",
        "121": "theta J transpose X I and so you know\n",
        "124": "this is a matrix where right this first\n",
        "125": "element the one one element is a\n",
        "128": "predictive rating of user 1 on movie 1\n",
        "131": "and this element this is the 1 2 element\n",
        "134": "is their predictive rating of user 2 on\n",
        "138": "movie 1 and so on and this is the\n",
        "142": "predictive rating of user 1 on the last\n",
        "145": "movie and if you want you know this\n",
        "147": "rating is what we would have predicted\n",
        "152": "for this value and this rating is what\n",
        "154": "were their predicted for that value and\n",
        "158": "so on now given this matrix of predicted\n",
        "160": "ratings there is then a simpler or\n",
        "163": "vectorized way of writing this out in\n",
        "166": "particular if I define the matrix X and\n",
        "168": "this is going to be just like the matrix\n",
        "171": "we had earlier for linear regression to\n",
        "175": "me so if X 1 transpose X 2 transpose\n",
        "181": "down to X of n M\n",
        "183": "Transpo's so I'm going to take all the\n",
        "186": "features for my movies and stack them in\n",
        "189": "rows so if you think of each movie as\n",
        "191": "one example and stack all of the\n",
        "193": "features of the different movies and\n",
        "197": "rules and if we also define a matrix\n",
        "202": "capital theta to be equal to and what\n",
        "204": "I'm going to do is take my per user\n",
        "208": "parameters and stack them in columns so\n",
        "210": "that's by parameter for user 1 for user\n",
        "214": "2 and so on down to my final user and\n",
        "216": "stack them in columns then given this\n",
        "221": "definition of X and theta there is now a\n",
        "223": "much simpler way to write out this\n",
        "226": "matrix of predated ratings in particular\n",
        "230": "this matrix are predicted ratings can be\n",
        "233": "more simply written as just x times\n",
        "236": "theta and so this becomes a vectorized\n",
        "239": "implementation of how to compute all the\n",
        "242": "predicted ratings of all the users on\n",
        "246": "all the movies and in order to give this\n",
        "248": "approach a name the specific\n",
        "250": "collaborative filtering algorithm that\n",
        "253": "we talked about also has a name called\n",
        "256": "low rank matrix factorization and so\n",
        "258": "sometimes we hear a low rank determine\n",
        "260": "or rank matrix factorization that is\n",
        "263": "exactly this algorithm that we worked on\n",
        "265": "and determine the rank matrix\n",
        "266": "factorization comes from the fact that\n",
        "270": "this matrix X data has a mathematical\n",
        "272": "property which in linear algebra is\n",
        "275": "called that just a low rank matrix and\n",
        "278": "so that's what gives this\n",
        "281": "and so this mathematical property of the\n",
        "283": "matrix excavator of the matrix of\n",
        "286": "predictive ratings gives this algorithms\n",
        "288": "name of low rank matrix factorization\n",
        "291": "but if you don't know when the low rank\n",
        "293": "matrix is don't worry about it you\n",
        "294": "really don't need to know that in order\n",
        "297": "to do this algorithm but either way this\n",
        "300": "gives a nice vectorized way to compute\n",
        "303": "all of the users rate predictive ratings\n",
        "306": " \n",
        "309": "finally having run D collaborative\n",
        "310": "filtering algorithm here's one other\n",
        "311": "filtering algorithm here's one other\n",
        "312": "thing you can do which is use the\n",
        "313": "thing you can do which is use the\n",
        "315": "learned features to find related movies\n",
        "318": "specifically for each product I really\n",
        "320": "for niche movie I we've learned a\n",
        "323": "feature vector X I so you know when you\n",
        "325": "learn the set of features you don't\n",
        "326": "really know the advanced what the\n",
        "328": "different features are going to be but\n",
        "330": "if you run the algorithm and probably\n",
        "332": "the features will tend to capture what\n",
        "334": "are the important aspects of these\n",
        "336": "different movies or different projects\n",
        "337": "or what have you one of the important\n",
        "339": "aspects that cause some users to like\n",
        "341": "certain movies and cause some users to\n",
        "343": "like different sets of movies and so\n",
        "345": "maybe you end up learning a feature you\n",
        "347": "know where x1 equals real man's x2\n",
        "350": "equals action similar to an earlier\n",
        "352": "video and maybe you learn it's different\n",
        "354": "feature x3 which is the degree to which\n",
        "357": "this is a comedy the some feature it's\n",
        "360": "Fulgencio or some other thing and you\n",
        "363": "have n features altogether and after\n",
        "365": "you've learnt features there's actually\n",
        "367": "often pretty difficult to go in to the\n",
        "370": "learn features and come up with a human\n",
        "371": "learn features and come up with a human\n",
        "373": "understandable interpretation of what\n",
        "375": "these features really are but in\n",
        "376": "practice the features you know even\n",
        "378": "though these features it can be hard to\n",
        "379": "visualize you want to figure out just\n",
        "383": "what these features are usually it will\n",
        "384": "learn features that are very meaningful\n",
        "386": "for capturing whatever are the most\n",
        "389": "important or the most salient properties\n",
        "392": "of a movie that causes users like or\n",
        "395": "dislike it and so now let's say the one\n",
        "397": "adds the following problem say you have\n",
        "400": "some specific movie I and you want to\n",
        "403": "find other movies J that are related to\n",
        "405": "that movie and so well why would you\n",
        "407": "want to do this right maybe you have a\n",
        "409": "user that's browsing movies and they're\n",
        "411": "currently watching movie J then what's\n",
        "413": "the reasonable movie to recommend to\n",
        "415": "them to watch after the double move\n",
        "416": "J or if someone's recently purchased\n",
        "419": "movie J well what's a different movie\n",
        "420": "that would be reasonable to recommend to\n",
        "422": "them to for them to consider purchasing\n",
        "424": "so now you have learned these picture\n",
        "426": "vectors this gives us a very convenient\n",
        "427": "vectors this gives us a very convenient\n",
        "429": "way to measure how similar two movies\n",
        "432": "are in particular movie I has a feature\n",
        "435": "vector X I and so if you can find a\n",
        "440": "different movie J so that the distance\n",
        "445": "between X I XJ is small then this is a\n",
        "448": "pretty strong indication that you know\n",
        "453": "movies Jay and I are somehow similar at\n",
        "455": "least in the sense that some of the\n",
        "457": "lights movie I may be more likely to\n",
        "460": "like movie J as well so just the recap\n",
        "464": "if your your user is looking at some\n",
        "466": "movie I and if you want to find the five\n",
        "468": "most similar movies through that movie\n",
        "470": "in order to recommend five new movies to\n",
        "472": "them what you do is find the five movies\n",
        "475": "J with the smallest distance between the\n",
        "476": "features between all of these different\n",
        "479": "movies and this could give you a few\n",
        "480": "different movies to recommend to your\n",
        "483": "user so with that hopefully you now know\n",
        "486": "how to use a vectorized implementation\n",
        "489": "to compute all the predicted ratings of\n",
        "491": "all the users on all the movies and also\n",
        "494": "how to do things like use learn features\n",
        "496": "to find what might be movies or what\n",
        "498": "might be products that are related to\n"
    },
    "5T77nG7YJhk": {
        "0": " \n",
        "2": "in the previous video we talked about\n",
        "5": "evaluation metrics in this video I'd\n",
        "7": "like to switch tracks a bit and touch on\n",
        "10": "another important aspect the machine\n",
        "12": "learning system design wolf which will\n",
        "14": "often come up which is the issue of how\n",
        "15": "often come up which is the issue of how\n",
        "17": "much data to train on now in some\n",
        "20": "earlier videos I had cautioned against\n",
        "22": "blindly going out and just spending lots\n",
        "24": "of time collecting lots of data because\n",
        "26": "there's only some times that that were\n",
        "28": "actually called but it turns out that\n",
        "30": "under certain conditions and I'll say in\n",
        "32": "this video what those conditions are\n",
        "35": "getting a lot of data and training on a\n",
        "38": "certain type of learning algorithm can\n",
        "39": "be a very effective way to get a\n",
        "41": "learning algorithm to give very good\n",
        "44": "performance and this arises often enough\n",
        "46": "that those conditions hold true for your\n",
        "48": "problem and if you're able to get a lot\n",
        "51": "of data this could be a very good way to\n",
        "52": "get a very high performance learning\n",
        "56": "algorithm so in this video let's talk\n",
        "59": "more about that let me start with a\n",
        "62": "story many many years ago two\n",
        "64": "researchers that i know michelle banko\n",
        "67": "and eric bro ran the following\n",
        "70": "fascinating study they were interested\n",
        "72": "of studying the effect of using\n",
        "74": "different learning algorithms versus\n",
        "76": "trying them out on different training\n",
        "79": "set sciences they were considering the\n",
        "81": "problem of classifying between\n",
        "83": "confusable words so for example in the\n",
        "85": "sentence for breakfast I ate should it\n",
        "88": "be tea OTW or tea or well for this\n",
        "93": "example for breakfast I ate TW o28 so\n",
        "96": "this is one example of said that\n",
        "97": "confusable words and that's a different\n",
        "99": "set and so they took machine learning\n",
        "101": "problems like these sort of supervised\n",
        "103": "learning problems to try to categorize\n",
        "106": "what is the appropriate word to go into\n",
        "108": "a certain position in an English\n",
        "112": "sentence they took a few different\n",
        "113": "learning algorithms which were you know\n",
        "115": "sort of considered state-of-the-art back\n",
        "117": "in the day when they ran this study in\n",
        "120": "2001 so they took a variance roughly a\n",
        "123": "variant on logistic regression in color\n",
        "125": "perception they also took some of their\n",
        "127": "elements that word say they got back\n",
        "129": "then but are someone that's to use now\n",
        "132": "so that whenever I'm also very similar\n",
        "134": "to logistic regression but different in\n",
        "138": "some ways much use somewhat less you use\n",
        "140": "not too much right now took what's\n",
        "142": "called a memory based learning algorithm\n",
        "144": "we can use somewhat less now so I'll\n",
        "146": "talk a little bit about that later and\n",
        "148": "they use the naive Bayes algorithm which\n",
        "149": "which is something to actually talk\n",
        "152": "about in this course the exact\n",
        "153": "algorithms of these details aren't\n",
        "155": "different aren't important\n",
        "156": "think of this less you know just picking\n",
        "158": "four different classification algorithms\n",
        "160": "and then really the exact algorithms\n",
        "162": "aren't important but what they did was\n",
        "164": "they varied the training set size and\n",
        "166": "tried out these learning algorithms on\n",
        "168": "the range of training set sizes and\n",
        "170": "that's the result they got and the\n",
        "173": "trends are very clear right first most\n",
        "175": "of these algorithms to give remarkably\n",
        "177": "similar performance and second as the\n",
        "179": "training set size increases on the\n",
        "182": "horizontal axis is the training set size\n",
        "185": "in millions as you go from you know a\n",
        "188": "hundred thousand up to a thousand\n",
        "190": "million that is a billion training\n",
        "191": "examples the performance of the\n",
        "194": "algorithms all pretty much monotonically\n",
        "196": "increase and the fact if you pick any\n",
        "200": "algorithm maybe pick a core inferior\n",
        "202": "algorithm but if you give that quote\n",
        "205": "inferior algorithm more data then it\n",
        "207": "from these examples it looks like there\n",
        "209": "will most likely beat even equal\n",
        "212": "superior algorithm so since this\n",
        "214": "original study which is very influential\n",
        "217": "there's been a range of many different\n",
        "219": "studies showing similar results that\n",
        "220": "show that many different learning\n",
        "223": "algorithms you know tend to can\n",
        "225": "sometimes depending on details can do\n",
        "227": "pretty similar ranges of performance but\n",
        "229": "what can really drive performance is if\n",
        "231": "you can give the algorithm\n",
        "235": "data and this is results like these has\n",
        "236": "lets us say in in machine learning that\n",
        "239": "often the machine learning is not who\n",
        "241": "has the best algorithm that wins is who\n",
        "244": "has the most data so when it's just true\n",
        "246": "and when is this not true because we\n",
        "248": "have a learning algorithm for which this\n",
        "251": "is true then getting a lot of data is\n",
        "254": "often maybe the best way to ensure that\n",
        "256": "we have an algorithm who have very high\n",
        "257": "performance rather than you know\n",
        "260": "debating worrying about exactly which of\n",
        "262": "these albums to use let's try to learn a\n",
        "265": "set of assumptions under which having a\n",
        "268": "massive training set we think will be\n",
        "271": "able to help let's assume that in a\n",
        "273": "machine learning problem the features X\n",
        "276": "have sufficient information with which\n",
        "280": "we can use to predict Y accurately for\n",
        "282": "example if we take the confusable words\n",
        "284": "problem that we had in the previous\n",
        "287": "slide let's say that features X capture\n",
        "290": "what are the surrounding words around\n",
        "291": "the blank that we're trying to fill in\n",
        "293": "so the features capture that we want to\n",
        "295": "have with sentence for breakfast I ate\n",
        "298": "black eggs then you know that's pretty\n",
        "300": "much enough information to tell me that\n",
        "302": "the word that one in the middle is T wo\n",
        "305": "and that is nothing worth tio and it's\n",
        "308": "not the word tio\n",
        "311": "so the features capture you know one of\n",
        "313": "these surrounding words then that gives\n",
        "315": "me enough information to pretty\n",
        "317": "unambiguously decide what is the label\n",
        "320": "why or in other words what is the word\n",
        "322": "that I should be using to fill in that\n",
        "324": "blank out of this set of three\n",
        "328": "confusable words so that's an example of\n",
        "329": "where the features X have sufficient\n",
        "332": "information to predict Y for a counter\n",
        "335": "example consider the problem of\n",
        "338": "predicting the price of a house from\n",
        "340": "only the size of the house and from no\n",
        "343": "other features so if you imagine I tell\n",
        "346": "you that houses you know five hundred\n",
        "347": "square feet but I don't give you any\n",
        "349": "other features I don't tell you if the\n",
        "352": "house is in an expensive part of the\n",
        "354": "city or if I don't tell you if the house\n",
        "357": "the number of rooms in the house or how\n",
        "359": "nice the furnish the house is or whether\n",
        "361": "the house is new or old if I don't tell\n",
        "363": "you anything other than that this is a\n",
        "365": "five hundred square foot house well\n",
        "366": "there are so many other factors that\n",
        "368": "will affect the price of a house other\n",
        "371": "than just the size of the holes that if\n",
        "373": "all you know the size is actually very\n",
        "374": "difficult to predict the price\n",
        "377": "accurately so that would be a counter\n",
        "379": "example to this assumption that the\n",
        "381": "features have sufficient information to\n",
        "383": "predict the price to the design level\n",
        "384": "accuracy meaning the way I think about\n",
        "387": "testing this assumption one way I often\n",
        "389": "think about it is I often ask myself\n",
        "392": "given the input features X or given the\n",
        "394": "features given the same information\n",
        "396": "available to a learning algorithm if we\n",
        "398": "were to go to a human expert in this\n",
        "401": "domain can a human experts accurately\n",
        "403": "Orkin human experiment ly predict the\n",
        "406": "value of y for this first example if we\n",
        "409": "go to you know an expert human English\n",
        "411": "speaker that you go to some of them\n",
        "414": "speak English well right then the human\n",
        "416": "experts in English just really most\n",
        "417": "experts in English just really most\n",
        "419": "people like you and me would probably\n",
        "421": "would probably be able to predict what\n",
        "423": "words should go in here so a good\n",
        "426": "English speaker can predict as well and\n",
        "427": "so this gives me confidence that X\n",
        "430": "allows us predict Y accurately but in\n",
        "433": "contrast if we would go to an expert in\n",
        "434": "human prices like maybe an expert\n",
        "437": "realtor right someone who sells houses\n",
        "439": "for a living if I just tell them the\n",
        "440": "size of a house\n",
        "442": "tell them what is what the price is well\n",
        "445": "even an expert in pricing or selling\n",
        "447": "houses wouldn't be able to tell me and\n",
        "449": "so there's a sign that for the housing\n",
        "452": "price example knowing only the size\n",
        "454": "doesn't give me enough information to\n",
        "458": "predict the price of the house so let's\n",
        "462": "say this assumption holds let's see then\n",
        "464": "when having a lot of data it could help\n",
        "466": "suppose the features have enough\n",
        "468": "information to predict the value of my\n",
        "470": "and let's suppose we use a learning\n",
        "473": "algorithm with a large number of\n",
        "475": "parameters so maybe logistic regression\n",
        "477": "or linear regression with a large number\n",
        "479": "of features or one thing that I\n",
        "480": "sometimes do one thing that often\n",
        "482": "directly is used a neural network with\n",
        "485": "many hidden units there'll be another\n",
        "487": "learning algorithm with a lot of\n",
        "490": "parameters so these are all powerful\n",
        "491": "learning algorithms with a lot of\n",
        "494": "parameters that can fit very complex\n",
        "498": "functions so I'm gonna call these we're\n",
        "500": "going to think of these as low bias\n",
        "502": "algorithms because you know they can fit\n",
        "506": "very complex functions and because we\n",
        "509": "have a very powerful learning algorithm\n",
        "511": "that can fit very complex functions\n",
        "515": "chances are if we run these algorithms\n",
        "518": "on the data set it will be able to fit\n",
        "521": "the training set well and so hopefully\n",
        "524": "the training error will be small now\n",
        "526": "let's say we use a massive massive\n",
        "529": "training set in that case and we have a\n",
        "532": "huge training set then hopefully even\n",
        "533": "though we have a lot of parameters but\n",
        "536": "if the training set is sort of even much\n",
        "538": "larger than the number of parameters\n",
        "540": "then hopefully these algorithms will be\n",
        "543": "unlikely to overfit right because we\n",
        "545": "have such a massive training set and by\n",
        "548": "unlikely to overfit what that means is\n",
        "550": "that the training error will hopefully\n",
        "554": "be close to the test error finally\n",
        "556": "putting these two together if the\n",
        "559": "training set error is small and the test\n",
        "561": "set error is close to the training error\n",
        "564": "what these two together imply is that\n",
        "567": "hopefully the test set\n",
        "572": "we'll also be small another way to think\n",
        "575": "about this is that in order to have high\n",
        "578": "performance learning algorithm we wanted\n",
        "580": "not to have high buyers and not to have\n",
        "583": "high variance so the bias problem we're\n",
        "585": "going to address by making sure we have\n",
        "586": "a learning algorithm with many\n",
        "588": "parameters and so that gives us a little\n",
        "591": "bias algorithm and by using a very large\n",
        "594": "training set this ensures that we don't\n",
        "596": "have a variance problem either and so\n",
        "597": "hopefully our algorithm will have low\n",
        "600": "variance and so it's by putting these\n",
        "602": "two together that we end up with a\n",
        "605": "little bias and a low variance learning\n",
        "608": "algorithm and this allows us to do well\n",
        "611": "on the test set and fundamentally it is\n",
        "614": "the key ingredients of assuming that the\n",
        "616": "features have enough information and we\n",
        "618": "have a rich class of functions that's\n",
        "621": "why guarantees low bias and then it's\n",
        "622": "having a massive training set that\n",
        "626": "that's what guarantees no variance\n",
        "629": "so this gives us a set of conditions\n",
        "631": "wrong hopefully some understanding of\n",
        "634": "what's the sort of problem where if you\n",
        "635": "have a lot of data and you train a\n",
        "637": "learning algorithm with a moment\n",
        "639": "parameters that might be a good way to\n",
        "641": "give a high performance learning\n",
        "643": "algorithm and really I think the key\n",
        "646": "test that I often ask myself are first\n",
        "648": "can a human expert look at the features\n",
        "650": "X and confidently predict the value of y\n",
        "652": "because there's sort of a certification\n",
        "654": "that y can indeed be predicted\n",
        "656": "accurately from the features X and\n",
        "658": "second can we actually get a large\n",
        "660": "training set and train the learning\n",
        "662": "algorithm for long time selection sets\n",
        "665": "and if you can do both then that will\n",
        "667": "often give you a very high performance\n"
    },
    "5aHWplWElcc": {
        "0": " \n",
        "2": "in the PCA algorithm we take n\n",
        "4": "dimensional features and reduce them to\n",
        "6": "some K dimensional feature\n",
        "8": "representation this number K is a\n",
        "11": "parameter of the PCA algorithm this\n",
        "14": "number K is also called the number of\n",
        "16": "principal components or the number of\n",
        "18": "principal components that we retain and\n",
        "20": "in this video I'd like to give you some\n",
        "22": "guidelines to sort of tell you about how\n",
        "24": "people tend to think about how to choose\n",
        "29": "this parameter K for PCA in order to\n",
        "31": "choose K that is to choose the number of\n",
        "33": "principal components here are a couple\n",
        "37": "of useful concepts what PCA tries to do\n",
        "40": "is it tries to minimize the average\n",
        "42": "square projection error so it tries to\n",
        "45": "minimize this quantity which I'm writing\n",
        "47": "down which is a the difference between\n",
        "51": "the original data X and the projected\n",
        "54": "version X aprox I which was defined in\n",
        "55": "the last video so it tries to minimize\n",
        "57": "the squared distance between X and this\n",
        "59": "projection onto that lower dimensional\n",
        "62": "surface so that's the average squared\n",
        "65": "projection error also let me define the\n",
        "68": "total variation in the data to be the\n",
        "73": "average length squared of these examples\n",
        "75": "excise so the total variation in the\n",
        "78": "data is you know the average of my\n",
        "80": "training set of the length of each of my\n",
        "82": "training examples and this Allah says on\n",
        "85": "average how far are my training examples\n",
        "87": "from the vector from from just being all\n",
        "91": "zeros so how far is half out on average\n",
        "93": "on my training examples from the origin\n",
        "96": "well we're trying to choose K pretty\n",
        "98": "common rule of thumb for choosing K is\n",
        "101": "to choose the smallest value so that the\n",
        "105": "ratio between these is less than 0.01 so\n",
        "107": "in other words pretty common way to\n",
        "109": "think about how we choose K is we want\n",
        "111": "the average square projection error that\n",
        "116": "is the average distance between X and s\n",
        "119": "projections divided by you know the\n",
        "121": "total variation of the data that is how\n",
        "123": "much the data varies we want this ratio\n",
        "124": "to be less\n",
        "128": "let's say 0.01 all to be on less than 1%\n",
        "129": "would be a which is another way of\n",
        "131": "thinking about it and the way most\n",
        "133": "people think about choosing K is rather\n",
        "136": "than choosing K directly the way most\n",
        "139": "people talk about it is as what this\n",
        "141": "number is whether this is 0.01 or some\n",
        "144": "other number and this is 0.01 another\n",
        "147": "way to say this to use the language of\n",
        "151": "PCA is that 99% of the variance is\n",
        "153": "maintained I don't really want to don't\n",
        "155": "don't worry about what this phrase\n",
        "157": "really means technically but this rate\n",
        "159": "99% of variance retain just means that\n",
        "161": "this quantity on the left is less than\n",
        "166": "0.01 and so if you are using PCA and if\n",
        "168": "you want to tell someone you know how\n",
        "169": "many your principle components you\n",
        "172": "retained there'd be more Commons and say\n",
        "174": "well I chose K so that 99% of the\n",
        "176": "variance was retained and that's kind of\n",
        "178": "a useful thing to know it means that you\n",
        "180": "know the average square projection error\n",
        "182": "divided by the total variation that was\n",
        "184": "at most one percent that's kind of an\n",
        "185": "insightful thing to think about\n",
        "188": "whereas if you tell someone that well\n",
        "190": "I've written I had 100 principal\n",
        "193": "components or K was equal to 100 in\n",
        "196": "1,000 data is this sort of harder for\n",
        "199": "people to interpret that so this number\n",
        "201": "oh point oh one is you know what people\n",
        "204": "often use other common values will be\n",
        "208": "0.05 and so to speak 5% and if you do\n",
        "211": "that then you go and say well 95% of the\n",
        "214": "variance is retained and you know other\n",
        "218": "numbers may be in maybe around 90% of\n",
        "220": "the variance is retained maybe as low as\n",
        "222": "85% so that 90% would correspond to say\n",
        "223": "85% so that 90% would correspond to say\n",
        "225": "existence you open one Oh\n",
        "228": "kind of parens 10% and so range of\n",
        "231": "values Ramiro 1990 $5.99 maybe as low as\n",
        "233": "85% of the variance retail would be a\n",
        "236": "fairly typical range of values and maybe\n",
        "239": "a 95 to 99 it's really the most common\n",
        "242": "range of values that people use and for\n",
        "243": "many data sets you'd be surprised in\n",
        "246": "order to retain 99% of the variance\n",
        "248": "you can often reduce the dimension of\n",
        "250": "the data significantly still and still\n",
        "252": "retain most of the variance because for\n",
        "253": "retain most of the variance because for\n",
        "255": "most real life data says many features\n",
        "257": "are just highly correlated and so it\n",
        "259": "turns out to be possible to compress the\n",
        "261": "data a lot and still retain you know 99%\n",
        "264": "of the variance or 95% of the variance\n",
        "267": "so how do you implement this well here's\n",
        "269": "one algorithm that you might use we\n",
        "270": "might start off if you want to choose\n",
        "273": "the value of K we might start off with K\n",
        "275": "plus 1 and then we run through PCA you\n",
        "277": "know so we computed you reduce compute\n",
        "281": "z1 z2 up to ZM compete all those x1 and\n",
        "283": "rots and so on up to XM approach so then\n",
        "285": "we check if 99% of the variance is\n",
        "288": "retained then we're good and we use K\n",
        "290": "equals 1 but if it isn't then what we'll\n",
        "293": "do is we'll next try T equals 2 and then\n",
        "295": "go again run through this entire\n",
        "297": "procedure and check you know business\n",
        "299": "expression satisfied is this less than\n",
        "301": "0.01 and it's not let me do this again\n",
        "304": "let's try K equals 3 then try K equals 4\n",
        "307": "and so on until maybe we get up to K\n",
        "310": "equals 17 and we find that 99% of the\n",
        "312": "variance abstain and then we use K\n",
        "314": "equals 17 right so that's one way to\n",
        "317": "choose the the choose the value of\n",
        "319": "choose the smallest value of K so that\n",
        "322": "99% of the variance is retained but as\n",
        "324": "you can imagine this this procedure\n",
        "326": "seems horribly inefficient for triangles\n",
        "328": "1 K equals 2 we're doing all these\n",
        "330": "calculations fortunately when you\n",
        "334": "implement PCA it actually in this inner\n",
        "336": "step it actually gives us a quantity\n",
        "339": "that makes it much easier to compute\n",
        "342": "these things as well specifically when\n",
        "344": "you're calling SVD to get\n",
        "346": "these matrices us and D we're calling us\n",
        "348": "video on the covariance matrix Sigma it\n",
        "351": "also gives us back this matrix s and\n",
        "354": "what s is is going to be a square matrix\n",
        "357": "and n by n matrix in fact that is\n",
        "360": "diagonal so it's diagonal entries s 1 1\n",
        "365": "s 2 to s 3 3 down to s n n are going to\n",
        "367": "be the only nonzero elements of this\n",
        "369": "matrix and everything off the diagonals\n",
        "372": "is going to be 0 ok so those big O's not\n",
        "374": "drawing by that what I mean is that you\n",
        "377": "know everything off the diagonal of this\n",
        "379": "matrix all of those entries there are\n",
        "383": "going to be zeros and so what is\n",
        "385": "possible to show and we'll go improve\n",
        "387": "this here but it turns out that for a\n",
        "395": " \n",
        "398": "can be computed much more simply and\n",
        "401": "that quantity compute can be computed as\n",
        "406": "1 minus sum from I equals 1 to K of SII\n",
        "410": "divided by sum from I equals 1 through n\n",
        "415": "of s III so just to say that inwards so\n",
        "416": "just to take another view of how to\n",
        "420": "explain that if K equals 3 let's say\n",
        "422": "what we're going to do to compute the\n",
        "424": "numerator it's tan from 1 R equals 1\n",
        "426": "through 3 of S is you just compute the\n",
        "429": "sum of these first three elements so\n",
        "431": "that's the numerator and then for the\n",
        "433": "denominator well that's the sum of all\n",
        "436": "of these diagonal entries and 1 minus\n",
        "439": "the ratio of that that gives me this\n",
        "442": "quantity over here the circle thank you\n",
        "445": "and so what we can do is just test that\n",
        "448": "this is less than or equal to open\n",
        "451": "one or equivalently we can test if you\n",
        "453": "know something I equals 1 through K si R\n",
        "457": "/ central I equals 1 through n SII if\n",
        "460": "this is greater than or equal to 0.99 if\n",
        "463": "you want to make sure that 99% of the\n",
        "465": "variance is retained and so what you can\n",
        "468": "do is just slowly increase K you know\n",
        "470": "set K equals 1 so k equals 2 second\n",
        "473": "equals 3 and so on and just test this\n",
        "476": "quantity to see what is the smallest\n",
        "479": "value of K that ensures that 99% of the\n",
        "482": "variance is retained and if you do this\n",
        "483": "then you need to call the SVD function\n",
        "486": "only once because that gives you the S\n",
        "487": "matrix and once you have the S matrix\n",
        "489": "you can then yeah just keep on doing\n",
        "492": "this calculation by increasing the value\n",
        "494": "of K in the numerator and so you don't\n",
        "496": "need to keep on calling SVD over and\n",
        "498": "over again to test out different values\n",
        "500": "of K so this procedure is much more\n",
        "503": "efficient and this can allow you to\n",
        "506": "select the value of K without needing to\n",
        "508": "run PCA from scratch over and over you\n",
        "511": "just run SVD once this gives you all of\n",
        "513": "these diagonal numbers all of these\n",
        "516": "numbers s 1 1 s 2 2 Delta s n n and then\n",
        "519": "you can just you know very K in this\n",
        "521": "expression to find the smallest value of\n",
        "524": "K so the 99% of the variance is retained\n",
        "527": "so to summarize the way that I often use\n",
        "529": "the way that I often choose K when I'm\n",
        "531": "using PCA for compression is I would\n",
        "533": "call SBT once and the covariance matrix\n",
        "535": "and then I will use this formula and\n",
        "538": "pick the value of smallest value of K\n",
        "540": "for which this expression is satisfied\n",
        "543": "and by the way even if you were to pick\n",
        "545": "some different value of K even we were\n",
        "546": "to pick the value of K manually you know\n",
        "548": "maybe have a thousand dimensional data\n",
        "551": "and I just want to choose K equals 100\n",
        "554": "then if you want to explain to others\n",
        "556": "what you just did a good way to explain\n",
        "558": "the performance of your implementation\n",
        "560": "of PCA to them is actually to take this\n",
        "562": "quantity and compute what this is and\n",
        "564": "that will tell you what was the\n",
        "565": "percentage of variance\n",
        "568": "and if you report that number then you\n",
        "570": "know people that are familiar with PTA\n",
        "572": "or people can use this to try to get a\n",
        "575": "good understanding of how well your\n",
        "577": "hundred dimensional representation is\n",
        "579": "approximating your original data set\n",
        "581": "because it's 99% of variants rotate\n",
        "584": "that's really a measure of your square\n",
        "586": "reconstruction error you know that ratio\n",
        "588": "of being at most 0.01 this gives people\n",
        "591": "a good intuitive sense of whether your\n",
        "594": "implementation of PCA is finding a good\n",
        "597": "approximation of your original data set\n",
        "600": "so hopefully that gives you an efficient\n",
        "602": "procedure for choosing the number K for\n",
        "605": "choosing what dimension to reduce your\n",
        "607": "data to and if you apply PCA it's a very\n",
        "609": "high dimensional data set you know so\n",
        "611": "like a thousand dimensional data very\n",
        "613": "often just because data sets tend to\n",
        "616": "have highly correlated features just as\n",
        "617": "a property or most of the data says you\n",
        "620": "see you often find that PCA will be able\n",
        "623": "to retain 99% of the variance or attain\n",
        "625": "95-99 some high fraction of the variance\n",
        "628": "even while compressing the data there by\n"
    },
    "5yK4rC_xGaY": {
        "0": " \n",
        "1": "in this video I want to just quickly\n",
        "3": "step you through the logistics of how to\n",
        "6": "work on homeworks in this process and\n",
        "8": "how to use the submission system which\n",
        "10": "will let you verify right away that you\n",
        "12": "got the right answer for your machine\n",
        "16": "learning programming exercise here's my\n",
        "18": "octave window and let's first go to my\n",
        "19": "desktop\n",
        "23": "I saved the files for my first exercise\n",
        "25": "some of the files on my desktop in this\n",
        "29": "directory Emma class II x1 and we\n",
        "31": "provide the number of files and ask you\n",
        "34": "to edit some of them so the first file\n",
        "38": "you should meet the detailed PDF file\n",
        "40": "for this programming exercise to get the\n",
        "42": "details but one of the files we also\n",
        "44": "edit is this file called warm-up\n",
        "46": "exercise where you know the exercise is\n",
        "48": "really just to make sure that you're\n",
        "50": "familiar with the submission system and\n",
        "52": "all you need to do is return the 5x5\n",
        "55": "identity matrix so you know the solution\n",
        "57": "to this exercise just show you is to\n",
        "61": "write a equals I five right so that\n",
        "63": "modifies this function to generate the\n",
        "66": "5x5 identity matrix and this function\n",
        "69": "warmup exercise now returns you know the\n",
        "72": "5x5 identity matrix and I'm just gonna\n",
        "74": "save it so I've done the first part of\n",
        "77": "this homework going back to my octave\n",
        "81": "window let's now go to my directory user\n",
        "84": "slash ng slash desktop slash and I'll\n",
        "88": "class - x1 and if I want to make sure\n",
        "90": "that I've you know implemented in some\n",
        "94": "type warm-up exercise like so and the\n",
        "96": "update returns to desta five by five\n",
        "98": "entity matrix that we just wrote the\n",
        "101": "code to create and I can now submit the\n",
        "103": "code as follow us on the type submit'\n",
        "106": "from this directory and I'm ready to\n",
        "108": "submit part 1 so when they enter your\n",
        "111": "choice one so let me find email address\n",
        "112": "so I'm going to go to the course website\n",
        "115": "and you know this list Maya this is an\n",
        "117": "internal testing site so your your\n",
        "119": "version of the website maybe look a\n",
        "120": "little bit different but there's my\n",
        "123": "email address and this is my submission\n",
        "127": "password and I'm just type them in here\n",
        "128": "so\n",
        "132": "NGH siesta centered on you and my\n",
        "139": "submission password is 975 u.s. sgf\n",
        "142": "enter it connects to the server and\n",
        "144": "submits it and right away it tells you\n",
        "146": "you know this is congratulations you've\n",
        "147": "successfully complete over in part one\n",
        "150": "and this gives you a verification that\n",
        "153": "you you you got this part right and if\n",
        "154": "you don't submit a right answer then you\n",
        "155": "know it will give you a message\n",
        "157": "indicating that you haven't quite gotten\n",
        "161": "it right yet and by the way um you can\n",
        "163": "also call you can use this submission\n",
        "165": "password and you can generate new\n",
        "168": "passwords it doesn't matter but you can\n",
        "170": "also use your regular website login\n",
        "172": "password but because you know this\n",
        "175": "password here is typed in clear text you\n",
        "178": "know on your monitor we gave you this\n",
        "179": "extra submission password in case you\n",
        "180": "extra submission password in case you\n",
        "181": "don't want to type in your website's\n",
        "183": "normal password onto a window that\n",
        "186": "depending on your operating system this\n",
        "189": "password may or may not appear as text\n",
        "193": "when you type it into the octaves of\n",
        "196": "submission script so that's how you\n",
        "198": "submit the homeworks after you've done\n",
        "200": "it good luck and I hope you get when you\n",
        "202": "get around to homeworks I hope you get\n",
        "205": "all of them right and finally in the\n",
        "207": "next and final octave tutorial video I\n",
        "209": "want to tell you about vectorization\n",
        "211": "which is a way to get your octave code\n"
    },
    "6vO3DVJlsK4": {
        "0": " \n",
        "2": "in the last video we talked about\n",
        "4": "gradient descent for minimizing the cost\n",
        "6": "function J of theta for logistic\n",
        "9": "regression in this video I'd like to\n",
        "10": "tell you about some advanced\n",
        "12": "optimization algorithms and some\n",
        "15": "advanced optimization concepts using\n",
        "17": "some of these ideas we'll be able to get\n",
        "19": "logistic regression to run much more\n",
        "22": "quickly than is possible with gradient\n",
        "24": "descent and this will also let the\n",
        "27": "algorithms scale much better to very\n",
        "28": "large machine learning problems such as\n",
        "30": "if we have a very large number of\n",
        "33": "features here's an alternative view of\n",
        "35": "what gradient descent is doing we have\n",
        "37": "some cost function J and we want to\n",
        "40": "minimize it so what we need to do is we\n",
        "42": "need to write code they can take as\n",
        "45": "input the parameters theta and they can\n",
        "48": "compute two things J of theta and these\n",
        "50": "partial derivative terms for your J\n",
        "53": "equals 0 1 up to n given code they can\n",
        "54": "equals 0 1 up to n given code they can\n",
        "55": "do these two things what gradient\n",
        "58": "descent does is it repeatedly performs\n",
        "60": "the following update right so given the\n",
        "61": "code that we wrote to compute these\n",
        "63": "partial derivatives gradient descent\n",
        "66": "plugs in here and uses that to update\n",
        "69": "our parameters theta so another way of\n",
        "71": "thinking about gradient descent is that\n",
        "73": "we need to supply code to compute J of\n",
        "75": "theta and these derivatives and then\n",
        "77": "these get plugged into gradient descents\n",
        "79": "which can then try to minimize the\n",
        "82": "function for us for gradient descent I\n",
        "83": "guess technically you don't actually\n",
        "86": "need code to compute the cost function J\n",
        "88": "you only need code to compute the\n",
        "90": "derivative terms but if you think of\n",
        "92": "your code as also monitoring convergence\n",
        "94": "of some such we'll think of this we'll\n",
        "96": "just think of ourselves as providing\n",
        "98": "code to probe up to compute both the\n",
        "101": "cost function and the derivative terms\n",
        "104": "so having written code to compute these\n",
        "107": "two things one algorithm we can use is\n",
        "110": "gradient descent but gradient is descent\n",
        "112": "isn't the only algorithm we can use and\n",
        "114": "there are other algorithms more advanced\n",
        "117": "more sophisticated ones that if we only\n",
        "120": "provide them a way to compute these two\n",
        "121": "things then these are different\n",
        "123": "approaches to optimize the cost function\n",
        "126": "for us so conjugate gradient bfgs and\n",
        "129": "l-bfgs are examples of more\n",
        "131": "sophisticated optimization algorithms\n",
        "132": "that need a way to compute J\n",
        "134": "they turn either way to compute the\n",
        "136": "derivatives and can then use more\n",
        "138": "sophisticated strategies than gradient\n",
        "140": "descent to minimize the cost function\n",
        "143": "the details of exactly what these three\n",
        "145": "algorithms that do is well beyond the\n",
        "148": "scope of this course and in fact you\n",
        "150": "often end up spending you know many days\n",
        "152": "or small number of weeks studying these\n",
        "154": "algorithms if you take a class if you\n",
        "155": "take a class in the advanced numerical\n",
        "157": "computing but let me just tell you about\n",
        "160": "some of their properties these three\n",
        "163": "algorithms have number of advantages one\n",
        "165": "is that with any of these algorithms you\n",
        "168": "usually do not need to manually pick the\n",
        "171": "learning rate alpha so one way to think\n",
        "174": "of these algorithms is that given these\n",
        "176": "the way to compute the derivative of the\n",
        "178": "cost function you can think of these\n",
        "180": "algorithms as having a clever inter-loop\n",
        "182": "and in fact they have a clever\n",
        "185": "called a line search algorithm that\n",
        "187": "automatically tries out different values\n",
        "189": "for the learning rate alpha and\n",
        "190": "automatically picks a good learning rate\n",
        "193": "alpha so that it can even pick a\n",
        "194": "different learning rate for every\n",
        "197": "iteration and so then you don't need to\n",
        "201": "choose it yourself these algorithms\n",
        "203": "actually do more sophisticated things\n",
        "205": "than just pick a good learning rate and\n",
        "208": "so they often end up converging much\n",
        "211": "faster than rate than gradient descent\n",
        "213": "these algorithms actually do more\n",
        "215": "sophisticated things than just pick a\n",
        "217": "good learning rate and so they often end\n",
        "219": "up converging much faster than gradient\n",
        "222": "descent but a detailed discussion of\n",
        "224": "exactly what they do is beyond the scope\n",
        "227": "in fact them I actually used to have\n",
        "229": "used these algorithms for a long time\n",
        "231": "like maybe over a decade quite\n",
        "234": "frequently and it was only you know a\n",
        "236": "few years ago that I actually figured\n",
        "238": "out for myself the details and what\n",
        "241": "conjugate gradient bfgs and l-bfgs do so\n",
        "243": "it is actually entirely possible to use\n",
        "245": "these algorithms successfully and\n",
        "246": "applied to lots of different learning\n",
        "249": "problems without actually understanding\n",
        "251": "the inner loop of what these algorithms\n",
        "253": "do if these algorithms have a\n",
        "255": "disadvantage I'll say that the main\n",
        "257": "disadvantage is that they're quite a lot\n",
        "259": "more complex than gradient descent and\n",
        "262": "in particular you probably should not\n",
        "264": "implement these algorithms concha\n",
        "265": "gradient lbf\n",
        "268": "yes BFGS yourself unless you're an\n",
        "271": "expert in numerical computing instead\n",
        "273": "just as you know I wouldn't recommend\n",
        "275": "that you write your own code to compute\n",
        "277": "square roots of numbers or to compute\n",
        "279": "inverses of matrices for these\n",
        "280": "algorithms also what I would recommend\n",
        "282": "you do is just use the software library\n",
        "284": "so you know to take a square root what\n",
        "287": "all of us do is use some function that\n",
        "289": "some someone else has written to compute\n",
        "291": "the square root of all numbers and\n",
        "294": "fortunately octave and the closely\n",
        "295": "related language MATLAB will be using\n",
        "298": "that octave has a very good it's a\n",
        "300": "pretty reasonable library implementing\n",
        "302": "some of these advanced optimization\n",
        "304": "algorithms and so if you just use the\n",
        "305": "built-in library\n",
        "307": "you know you get pretty good results I\n",
        "310": "should say that there is a difference\n",
        "312": "between good and bad implementations of\n",
        "314": "these algorithms and so if you're using\n",
        "316": "a different language for your machine\n",
        "317": "learning application that you're using\n",
        "321": "like C C++ Java and so on you might want\n",
        "324": "to try out a different couple different\n",
        "325": "libraries to make sure that you find a\n",
        "327": "good library for implementing these\n",
        "329": "algorithms because there is a difference\n",
        "330": "in performance between a good\n",
        "332": "implementation of you know constant\n",
        "335": "gradient or l-bfgs versus a less good\n",
        "337": "implementation of contra gradient or\n",
        "344": "l-bfgs so now let's explain how to use\n",
        "346": "these algorithms I'm going to do so with\n",
        "347": "these algorithms I'm going to do so with\n",
        "350": "an example let's say that you have a\n",
        "353": "problem with two parameters or theta\n",
        "355": "equals theta zero and theta one and\n",
        "358": "let's say your cost function is J of\n",
        "359": "theta equals theta one minus five\n",
        "361": "squared plus theta two minus five\n",
        "362": "squared\n",
        "365": "so with this cost function you know the\n",
        "367": "value for theta one and theta two if you\n",
        "368": "want to minimize J of theta as a\n",
        "370": "function of theta the value that\n",
        "372": "minimizes it is going to be theta one\n",
        "376": "equals 5 theta 2 equals 5 now again I\n",
        "378": "know some of you will know more calculus\n",
        "380": "than others but the derivatives of the\n",
        "383": "cost function J turn out to be these two\n",
        "385": "expressions down here and I've done the\n",
        "387": "calculus so if you want to apply one of\n",
        "389": "the advanced optimization algorithms to\n",
        "392": "minimize this cost function J so if you\n",
        "394": "know if we didn't know the minimum was\n",
        "396": "at 5 5 but if you want to have a\n",
        "397": "cost function find the minimum\n",
        "399": "numerically right using something like\n",
        "401": "gradient descent but preferably more\n",
        "403": "advanced than gradient descent what you\n",
        "406": "would do is implement an octave function\n",
        "408": "like this so we implement a cost\n",
        "411": "function cost function theta function\n",
        "413": "like that and what this does is it\n",
        "417": "returns two arguments the first j-val is\n",
        "420": "a how we would compute the cost function\n",
        "423": "J so this says J Val equals you know\n",
        "425": "theta 1 minus 5 squared plus de 2 theta\n",
        "427": "2 minus 5 squared so it's just computing\n",
        "430": "this cost function over here and the\n",
        "432": "second argument that this function\n",
        "435": "returns is gradient so gradient is going\n",
        "439": "to be a 2 by 1 vector and the two\n",
        "440": "elements of the gradient vector\n",
        "444": "correspond to the two partial derivative\n",
        "448": "terms over here having implemented this\n",
        "450": "cost function you would you can then\n",
        "454": "call the advanced optimization function\n",
        "456": "called the F min uncle it stands for\n",
        "459": "function minimization unconstrained in\n",
        "461": "octave and the way you call this as\n",
        "463": "follows you set a few options this is a\n",
        "465": "options it's a data structure that\n",
        "468": "stores the options you want so grant up\n",
        "470": "on this sense the gradient objective\n",
        "473": "parameter on it just means that you are\n",
        "475": "indeed going to provide a gradient to\n",
        "476": "this algorithm and we're going to set\n",
        "478": "the maximum number of iterations to\n",
        "481": "let's say 100 we're going to give it an\n",
        "483": "initial guess for theta there's a 2 by 1\n",
        "487": "vector and then this command calls F min\n",
        "490": "UNK this add symbol represents a pointer\n",
        "493": "to the cost function of function that we\n",
        "496": "just defined up there and if you call\n",
        "497": "this this will compute you know we'll\n",
        "499": "use one of the more advanced\n",
        "501": "optimization algorithms and if you if\n",
        "502": "you want to think of it as just like\n",
        "504": "gradient descent but automatically\n",
        "506": "choosing the learning rate alpha for you\n",
        "508": "so you don't have to do so yourself but\n",
        "510": "it will then attempt to use these sort\n",
        "512": "of advanced optimization algorithms like\n",
        "514": "gradient descent on steroids to try to\n",
        "516": "find the optimal value of theta for you\n",
        "518": "let me actually show you what this looks\n",
        "521": "like in octave so I've written there's a\n",
        "524": "cost function of theta function exactly\n",
        "526": "as we had it on their previous line it\n",
        "527": "computes j-val\n",
        "528": "which is the\n",
        "530": "cost function and it computes the\n",
        "533": "gradient with the two elements being the\n",
        "534": "partial derivatives of the cost function\n",
        "536": "with respect to you know the two\n",
        "539": "parameters theta 1 and theta 2 now let's\n",
        "541": "switch to my octave window I'm going to\n",
        "543": "type in those commands I had just now\n",
        "546": "so options equals optim set this is a\n",
        "550": "the notation for setting my parameters\n",
        "552": "or my options for my optimization\n",
        "557": "algorithm grad object on max enter 100\n",
        "559": "so that says 100 iterations and I am\n",
        "562": "going to provide the gradient to my\n",
        "566": "algorithm let's say initial theta equals\n",
        "569": "0 2 by 1 so that's my initial guess for\n",
        "573": "theta and now I have updater function\n",
        "574": "Val\n",
        "580": "exit flag equals F min unconstrained a\n",
        "584": "pointer to the cost function and provide\n",
        "589": "my initial guess and the options like so\n",
        "592": "and if I hit enter this will run the\n",
        "594": "optimization algorithm and the returns\n",
        "596": "pretty quickly so this funny formatting\n",
        "598": "that's because my line you know Maya\n",
        "602": "might come wrapped around but what this\n",
        "603": "funny thing is just because my command\n",
        "605": "line had wrapped around but what this\n",
        "607": "says is that it numerically renders you\n",
        "609": "know think of as gradient descent on\n",
        "611": "steroids they found the optimal value of\n",
        "614": "a theta is Theta y equals 5 theta 2\n",
        "616": "equals 5 exactly as we're hoping for the\n",
        "619": "function value at the optimum is\n",
        "622": "essentially 10 to the minus 30 so that's\n",
        "624": "essentially 0 which is also what we're\n",
        "629": "and the exit flag is 1 and this shows\n",
        "631": "what the convergence status of this end\n",
        "635": "if you want you can do help F min um to\n",
        "636": "read the documentation for how to\n",
        "638": "interpret the exit flag but the exit\n",
        "640": "flag lets you verify whether or not this\n",
        "644": "algorithm things that has converged so\n",
        "646": "that's how you run these algorithms in\n",
        "648": "octave I should mention by the way that\n",
        "651": "for the octave implementation this value\n",
        "653": "of theta your parameter vector theta\n",
        "657": "must be in Rd for D greater than or\n",
        "659": "equal to 2 so if theta\n",
        "661": "is just a real number so if it's not if\n",
        "663": "it's not at least a two-dimensional\n",
        "664": "vector or some higher than\n",
        "667": "two-dimensional vector this fminunc may\n",
        "670": "not work so if in case you have a\n",
        "671": "one-dimensional function that you need\n",
        "673": "to optimize you can look in the octave\n",
        "675": "documentation for F min uncle for\n",
        "678": "additional details so that's how we\n",
        "682": "optimize our toy example of this simple\n",
        "685": "quadratic cost function how do we apply\n",
        "688": "this to logistic regression in logistic\n",
        "690": "regression we have a parameter vector\n",
        "692": "theta and I'm going to use a mix of\n",
        "695": "octave notation and select math notation\n",
        "697": "but I hope this explanation will be\n",
        "699": "clear but our parameter vector theta\n",
        "701": "comprises these parameters theta 0\n",
        "705": "through theta n because octave indexes\n",
        "708": "vectors using indexing from 1 you know\n",
        "711": "theta 0 is actually written theta 1 in\n",
        "713": "octave theta 1 is going to be written\n",
        "715": "sort of theta 2 in octave and that can\n",
        "718": "be written theta n plus 1 right and\n",
        "721": "that's because octave indexes is vectors\n",
        "724": "starting from index of 1 instead of an\n",
        "728": "index from 0 so what we need to do then\n",
        "732": "is write a cost function to there\n",
        "733": "captures the cost function for logistic\n",
        "736": "regression concretely the cost function\n",
        "739": "needs to return j-val which is you know\n",
        "740": "j Vallance you need some code to compute\n",
        "743": "J of theta and we also need to give it\n",
        "746": "the gradient so gradient 1 is going to\n",
        "747": "be some code to compute the partial\n",
        "750": "derivative respect to theta 0 the next\n",
        "753": "partial derivative respect to theta 1\n",
        "755": "and so on and once again this is\n",
        "758": "gradient 0 this gradient 1 gradient 2\n",
        "760": "and so on rather than gradient 0\n",
        "763": "gradient 1 because octave indexes is\n",
        "766": "vectors starting from 1 rather than from\n",
        "768": "0 but the main concept I hope you take\n",
        "770": "away from the slide is that what you\n",
        "773": "need to do is write a function that\n",
        "776": "returns the cost function and returns a\n",
        "780": "gradient and so in order to apply this\n",
        "782": "to logistic regression or even to linear\n",
        "783": "to logistic regression or even to linear\n",
        "784": "regression if you want to use these\n",
        "786": "optimization algorithms for linear\n",
        "788": "regression what you need to do is plug\n",
        "790": "the appropriate code to compute these\n",
        "794": "things over here\n",
        "796": "so now you know how to use these\n",
        "799": "advanced optimization algorithms because\n",
        "801": "I'm using because for these algorithms\n",
        "803": "you are using a sophisticated\n",
        "805": "optimization library it makes the code\n",
        "808": "just a little bit more opaque and so\n",
        "810": "just maybe a little bit harder to debug\n",
        "813": "but because these algorithms often run\n",
        "815": "much faster than gradient descent often\n",
        "817": "quite typically whenever I have a large\n",
        "819": "machine learning problem I will use\n",
        "821": "these algorithms instead of using\n",
        "824": "gradient descent and with these ideas\n",
        "826": "hopefully you'll be able to get logistic\n",
        "828": "regression and also linear regression to\n",
        "832": "work on much larger problems so that's\n",
        "834": "it for advanced optimization concepts\n",
        "837": "and in the next and final video on\n",
        "839": "logistic regression I want to tell you\n",
        "840": "how to take the logistic regression\n",
        "842": "algorithm that you already know about\n",
        "844": "and make it work also on multi-class\n"
    },
    "7snro4M6ukk": {
        "0": " \n",
        "2": "in this video we'll tell you about a\n",
        "4": "couple of special matrix operations\n",
        "6": "called the matrix inverse and the matrix\n",
        "9": "transpose operation that's not by\n",
        "12": "talking about matrix inverse and as\n",
        "14": "usual we'll start by thinking about how\n",
        "17": "relates to real numbers in the last\n",
        "20": "video I said that the number one plays a\n",
        "24": "row of sort of the identity right in the\n",
        "26": "space of real numbers because one times\n",
        "29": "anything is equal to itself it turns out\n",
        "31": "the row numbers have this property that\n",
        "34": "every number have and each number has an\n",
        "36": "inverse for example given the number\n",
        "39": "three there exists some number which\n",
        "41": "happens to be three inverse so that that\n",
        "44": "number of times to me gives you back the\n",
        "46": "identity element one right and so you\n",
        "48": "know three inverse of course this is\n",
        "51": "just one third right and given some\n",
        "55": "other number maybe 12 there's some\n",
        "58": "number which is the inverse of 12\n",
        "61": "written as 12 to the minus 1 or really\n",
        "64": "this is just 1 12 so that when you\n",
        "67": "multiply these two things together right\n",
        "69": "the product is equal to the identity\n",
        "73": "element 1 again now it turns out that in\n",
        "76": "the space of real numbers not everything\n",
        "78": "has an inverse for example the number 0\n",
        "80": "does not have an inverse right because\n",
        "84": "years so 0 inverse is 1 over 0 that's\n",
        "86": "that's undefined right this 1 over 0 is\n",
        "90": "it's not it's not well-defined and what\n",
        "92": "we want to do in the rest of the slide\n",
        "94": "is figure out what does it mean to you\n",
        "98": "know compute the inverse of a matrix\n",
        "103": "here's the idea um if a is an M by M\n",
        "106": "matrix and if has an inverse I'll say a\n",
        "107": "little bit more about that later then\n",
        "111": "the inverse is going to be written a to\n",
        "115": "the minus 1 and a time since inverse a\n",
        "117": "to the minus 1 is going to equal to a\n",
        "121": "inverse times a is going to give us back\n",
        "126": "the identity matrix okay only matrices\n",
        "129": "that are M by M for some value of M have\n",
        "132": "so it matrixes M by M this is also\n",
        "137": "called a square matrix and it's called\n",
        "141": "square because the number of rows is\n",
        "145": "equal to the number of columns right and\n",
        "148": "it turns out only square matrices have\n",
        "152": "inverses so a is a square matrix is n by\n",
        "154": "M and if it is inverse then it satisfies\n",
        "157": "this equation over here let's look at\n",
        "161": "the concrete example so let's say I have\n",
        "169": "a matrix 3 4 2 16 so this is a 2 by 2\n",
        "172": "matrix so it's a square matrix and so\n",
        "175": "this matrix could have an inverse and it\n",
        "178": "turns out that I happen to know the\n",
        "183": "inverse of this matrix is 0.4 minus 0.1\n",
        "189": "minus 0.05 0.075 and if I take this\n",
        "192": "matrix and multiply these together it\n",
        "195": "turns out what I get is the 2 by 2\n",
        "199": "identity matrix I can write this as 5 2\n",
        "203": "by 2 okay and so on the slide you know\n",
        "205": "this matrix is the matrix a and this\n",
        "208": "matrix the matrix a inverse and it turns\n",
        "210": "out that here I've computed a times a\n",
        "212": "inverse it turns out if you compute 8\n",
        "214": "inverse times a you also get back the\n",
        "219": "identity matrix so how did I find this\n",
        "221": "inverse so how did I come up with this\n",
        "223": "inverse over here it turns out that\n",
        "226": "sometimes you can compute inverses by\n",
        "228": "hand but almost no one does that these\n",
        "230": "days and it turns out there's very good\n",
        "233": "numerical software for taking a matrix\n",
        "235": "and computing is inference so again this\n",
        "237": "is one of those things where their loss\n",
        "239": "of open source libraries that you can\n",
        "241": "link to from any of the popular\n",
        "243": "programming languages to compute\n",
        "245": "inverses of matrices let me show you a\n",
        "248": "quick example how I actually computed\n",
        "249": "this inverse and what I did was I used\n",
        "253": "software called octave so let me bring\n",
        "255": "that up we'll see a lot more about\n",
        "257": "octave later so but let me just quickly\n",
        "259": "show you an example and I set my eight\n",
        "260": "matrix a\n",
        "262": "to be equal that matrix on the Left Type\n",
        "267": "3 4 to 16 so that's my matrix a right as\n",
        "271": "this matrix 3 4 to 16 that I have down\n",
        "273": "here on the left and the software lets\n",
        "276": "me computes the inverse of a very easily\n",
        "278": "and it's like P inva a equals this and\n",
        "282": "so this is right this matrix here 0.4\n",
        "284": "minus 0.1 and so on this has given me a\n",
        "286": "numerical solution towards the inverse\n",
        "289": "of a second let me just write inverse of\n",
        "294": "a equals P of a so inverse available\n",
        "297": "that I can now just verify that a times\n",
        "299": "a inverse the identity is inside a times\n",
        "304": "inverse of a and the result of that is\n",
        "307": "this matrix and this is one one on the\n",
        "309": "diagonal and you know essentially what\n",
        "312": "10 to the minus 17 10 to the minus 16 so\n",
        "315": "up to numerical precision up to a load\n",
        "317": "of the round off error that my computer\n",
        "319": "had in multiplying these two matrices so\n",
        "321": "absolutely right around are these are\n",
        "323": "these numbers after diagonals are\n",
        "325": "essentially zero and so eight times the\n",
        "327": "inverse is essentially the identity\n",
        "330": "matrix and also verify inverse of a\n",
        "334": "times a there's also is also equal to\n",
        "337": "the identity with ones of diagonals and\n",
        "339": "you know values essentially zero except\n",
        "341": "for a little bit of round off ever on\n",
        "346": "the off diagonals in my definition of\n",
        "348": "the inverse of a matrix I had this\n",
        "350": "caveat right first it must always be a\n",
        "353": "square matrix and I had this caveat so\n",
        "357": "if a has an inverse exactly what\n",
        "359": "matrices have an inverse is beyond the\n",
        "361": "scope of this linear algebra review but\n",
        "363": "one intuition you might take away is\n",
        "366": "that just as this number 0 doesn't have\n",
        "369": "an inverse it turns out that if a is say\n",
        "373": "the matrix of all zeros then this matrix\n",
        "375": "a also does not have an inverse because\n",
        "377": "you know there's no matrix sorry there's\n",
        "379": "no a inverse matrix so that this matrix\n",
        "381": "times some other matrix would give you\n",
        "383": "the identity matrix of this matrix of\n",
        "385": "all zeros and there are few other\n",
        "387": "matrices were probably similar to this\n",
        "389": "that also don't have an inverse\n",
        "392": "but it turns out that I don't in this\n",
        "393": "but it turns out that I don't in this\n",
        "394": "review I don't want to go too deeply\n",
        "396": "into what it means for a matrix to have\n",
        "399": "an inverse but it turns out that for all\n",
        "401": "machine learning applications this\n",
        "403": "shouldn't be an issue or more precisely\n",
        "406": "for the learning algorithms where this\n",
        "408": "may be an issue may be whether or not an\n",
        "411": "inverse appears I'll tell you when we\n",
        "412": "get to those learning algorithms just\n",
        "414": "what it means for an algorithm to have\n",
        "416": "or not have an inversion and how to fix\n",
        "418": "it in case we end up working with\n",
        "420": "matrices that don't have inverses but\n",
        "423": "the intuition if you want is that you\n",
        "426": "can think of matrices as not having an\n",
        "427": "inverse if there's somehow too close to\n",
        "432": "0 in some sense and to give just to wrap\n",
        "435": "up the terminology our matrix that don't\n",
        "436": "have an inverse are sometimes called the\n",
        "439": "singular matrix or degenerate matrix and\n",
        "442": "so you know this matrix over here is an\n",
        "445": "example the 0 0 0 matrix as an example\n",
        "447": "of a matrix as singular or matrix as\n",
        "451": "degenerate finally the last special\n",
        "452": "matrix operation I want to tell you\n",
        "456": "about is d matrix transpose so suppose\n",
        "458": "that matrix a if I compute the transpose\n",
        "461": "of a that's what I get here on the right\n",
        "463": "this is a transpose which is written a\n",
        "467": "superscript T and the way you compute\n",
        "469": "the transpose of a matrix is as follows\n",
        "470": "to get a transpose I'm going to first\n",
        "475": "take the first row of a 1 to 0 that\n",
        "477": "becomes this first column of the\n",
        "479": "transpose and then I'm going to take the\n",
        "482": "second row of a 3 5 9 and that becomes\n",
        "485": "the second column of the matrix a\n",
        "488": "transpose and another way of thinking\n",
        "490": "about how to compute the transpose is is\n",
        "492": "as if you're taking this sort of 45\n",
        "494": "degree you know axis and you're\n",
        "497": "mirroring or you are flipping the matrix\n",
        "502": "along that 45 degree axis okay so here's\n",
        "504": "the more formal definition of a matrix\n",
        "508": "transpose let's say a is an M by n\n",
        "513": "matrix and let's let B equal a transpose\n",
        "517": "B equals a transpose like so then B is\n",
        "519": "going to be an N by M matrix with the\n",
        "523": "dimensions reversed so here we have a 2\n",
        "527": "by 3 matrix and so the transpose becomes\n",
        "532": "a 3 by 2 matrix and moreover bij is\n",
        "536": "equal to AJ I so the IJ element of this\n",
        "537": "matrix being is going to be the ji\n",
        "538": "matrix being is going to be the ji\n",
        "542": "element of that earlier matrix a so for\n",
        "548": "example b12 is going to be equal to look\n",
        "550": "in this nephews be b12 0 equal to this\n",
        "552": "element 3 right first row second column\n",
        "557": "and that's equal to this which is a 2 1\n",
        "561": "second row first column right and well\n",
        "563": "which is equal to 2 and it's not an\n",
        "569": "example B 3 2 right that's B 3 2 who is\n",
        "574": "this element 9 and that's equal to a 2 3\n",
        "578": "which is this element up here nine and\n",
        "581": "so that wraps up the definition of what\n",
        "583": "it means to take the transpose of a\n",
        "586": "matrix and in fact that concludes our\n",
        "589": "linear algebra review so by now\n",
        "591": "hopefully you know how to add and\n",
        "594": "subtract matrices as well as multiply\n",
        "597": "them and you also know how what are the\n",
        "598": "definitions of the inverses and\n",
        "602": "transposes of a matrix and these are the\n",
        "604": "main operations you need in linear\n",
        "606": "algebra for this course in case this is\n",
        "608": "the first time you're seeing this\n",
        "610": "material I know this was this was a lot\n",
        "612": "of linear algebra material all presented\n",
        "613": "of linear algebra material all presented\n",
        "615": "very quickly and this is a lot absol but\n",
        "618": "if you but there's no need to memorize\n",
        "620": "all the definitions we just went through\n",
        "623": "and if you download a copy of either\n",
        "625": "these slides or of the lecture notes\n",
        "626": "from the website and from the website\n",
        "628": "from the course website and use either\n",
        "630": "the slides and lecture notes as a\n",
        "632": "reference then you can always refer back\n",
        "634": "to the definitions to figure out you\n",
        "635": "know what are these multiplication\n",
        "638": "transpose and so on definitions and the\n",
        "640": "lecture notes on the course website also\n",
        "643": "has pointers to additional resources on\n",
        "644": "linear algebra we\n",
        "646": "you can use to learn more about linear\n",
        "652": "algebra by yourself and and mix what\n",
        "655": "these new tools will be able in the next\n",
        "657": "few videos to develop more powerful\n",
        "659": "forms of linear regression they can view\n",
        "660": "with a lot more detail a lot more\n",
        "662": "features and a lot more training\n",
        "664": "examples and later on after the new\n",
        "666": "regression will actually continue using\n",
        "669": "these linear algebra tools to derive\n",
        "671": "more powerful than learning algorithms\n"
    },
    "8ClrEqsvPP0": {
        "0": " \n",
        "2": "when developing learning algorithms very\n",
        "4": "often a few simple plots can give you a\n",
        "6": "better sense of what the album is doing\n",
        "8": "and just sanity check that everything's\n",
        "10": "going okay and the algorithm is doing\n",
        "13": "what is supposed to for example in an\n",
        "14": "earlier video I talked about how\n",
        "17": "plotting the cost function J of theta\n",
        "19": "can help you make sure that gradient\n",
        "22": "descent is converging often plus of the\n",
        "24": "data or of the learning album outputs\n",
        "26": "will also give you ideas for how to\n",
        "28": "improve your learning algorithm\n",
        "31": "fortunately octave has a very simple\n",
        "33": "tools to generate lots of different\n",
        "35": "plots and when I use learning algorithms\n",
        "38": "I find that plotting the data plotting\n",
        "40": "the learning our album and so on are\n",
        "43": "often an important part of how I get\n",
        "45": "ideas for improving the algorithms and\n",
        "48": "in this video I'd like to show you some\n",
        "50": "of these octave tools for plotting and\n",
        "54": "visualizing your data here's my octave\n",
        "56": "window let's quickly generate some data\n",
        "59": "for us the plot so I'm going to set T to\n",
        "61": "be equal to you know this array of\n",
        "65": "numbers est set of numbers going from 0\n",
        "68": "up to a point 9 8 let's set y1 equals\n",
        "73": "sine of 2 pi for T and if I want to plot\n",
        "76": "the sine function is very easy I just\n",
        "80": "type plot T comma y1 and hit enter and\n",
        "82": "up come this plot where the horizontal\n",
        "85": "axis is the T variable and the vertical\n",
        "87": "axis is y1 which is this sort of\n",
        "88": "sinusoidal function that we just\n",
        "92": "computed let's set Y to its be equal to\n",
        "99": "the cosine of 2 pi for T like so and if\n",
        "104": "I plot T comma y2 what octave will do is\n",
        "106": "they'll take my sinusoid plot and it\n",
        "108": "will replace it with this cosine\n",
        "110": "function there now you know cosine of X\n",
        "114": "is a 1 now what if I want to have both\n",
        "116": "the sine and the cosine plots on top of\n",
        "117": "each other what I'm going to do is I'm\n",
        "121": "going to type plot T comma y1 so here's\n",
        "123": "my sine function and then I'm going to\n",
        "127": "use the function hold on and what hold\n",
        "129": "on does is it causes octave to now plot\n",
        "131": "new figures on top\n",
        "135": "of the old one and let me now plot T y2\n",
        "137": "and I plot the cosine function in a\n",
        "141": "different color and so let me put there\n",
        "144": "are in quotation marks there and instead\n",
        "146": "of replacing the current figure I'll\n",
        "148": "plot the cosine function on top and the\n",
        "151": "R indicates that went is an event color\n",
        "155": "and here are additional commands xlabel\n",
        "157": "time say to label the x-axis or\n",
        "161": "horizontal axis and y label value say to\n",
        "166": "label the vertical axis value and I can\n",
        "177": "also and I can also label my two lines\n",
        "181": "with this command legend sine cosine and\n",
        "184": "this Plus this legend up on the upper\n",
        "186": "right showing what the two lines are and\n",
        "190": "finally title my plot for the title at\n",
        "193": "the top of this figure lastly if you\n",
        "194": "want to save this figure you type\n",
        "200": "printing the attach DP and G my plot dot\n",
        "204": "PNG so PNG is a graphics file format and\n",
        "206": "if you do this this will actually save\n",
        "209": "this as a file if I do that let me\n",
        "214": "actually change the rectory to let's see\n",
        "218": "like that and then I will print that out\n",
        "223": "so this will take a while depending on\n",
        "226": "how your octave conversion is set up\n",
        "228": "this may take a few seconds but change\n",
        "230": "the rectory to my desktop and octave is\n",
        "234": "now taking a few seconds to save this if\n",
        "237": "I now go to my desktop let's hide these\n",
        "239": "windows here's my plot dot PNG with\n",
        "241": "Octavia saying you know there's the\n",
        "243": "figure I save it as a PNG file also can\n",
        "245": "save files in other formats as well so\n",
        "248": "you can type help plot if you want to\n",
        "251": "see the other file formats rather than\n",
        "253": "the PNG that time we can say figures it\n",
        "257": "and lastly if you want to get rid of a\n",
        "258": "plot\n",
        "263": "the close command causes a figure to go\n",
        "264": "away so that's that figure but type\n",
        "266": "close you know that figure just\n",
        "268": "disappeared from my just disappeared\n",
        "271": "from my desktop octave also lets you\n",
        "273": "specify a figure number so you type\n",
        "279": "figure one plots T y1 that thoughts up\n",
        "282": "first figure and that plus T y1 and then\n",
        "283": "if you want the second figure you\n",
        "285": "specify a different figure number so\n",
        "290": "figure two plot T y2 like so and now on\n",
        "292": "my desktop I actually have two figures\n",
        "295": "figure 1 and figure 2 that's one\n",
        "297": "plotting the sine function one plotting\n",
        "299": "the cosine function here's one other\n",
        "301": "neat command that I often use which is\n",
        "303": "the subplot command so we'll use subplot\n",
        "306": "one two one what that does is it device\n",
        "312": "subdivides the plot into a one by two\n",
        "314": "grid that's what the first two\n",
        "317": "parameters are and it starts to access\n",
        "319": "the first element that's what the final\n",
        "322": "parameter one is right so this divided\n",
        "324": "my figure into one by two grid and I\n",
        "327": "want to access the first element right\n",
        "330": "now and so if I type that in this\n",
        "332": "brought up this figure that is on the\n",
        "337": "left and if I plot T y1 it now fills up\n",
        "341": "this you know first element and if I now\n",
        "344": "do subpart 1 to 2 I'm going to start to\n",
        "348": "access the second element and plot T y2\n",
        "352": "well so in y2 in the right hand side or\n",
        "355": "in the second element and last command\n",
        "358": "you can also change the axis scale so\n",
        "362": "you change X is to 0.5 1 minus 1 1 and\n",
        "367": "the sets dear x range and Y range for\n",
        "370": "the figure on on the right and\n",
        "372": "concretely assess the horizontal range\n",
        "374": "of values and figure on the right to\n",
        "376": "range from 0.5 to 1 and the vertical\n",
        "378": "axis values to range from minus 1/2\n",
        "380": "and you know you don't need to memorize\n",
        "383": "all these commands if you ever need to\n",
        "385": "change the axis all you need to know is\n",
        "386": "that you know there's an axis command\n",
        "388": "then you can really get the details from\n",
        "391": "the usual octave helpful come and find\n",
        "392": "me\n",
        "394": "just a couple last commands COF clear is\n",
        "398": "a figure and yes one more neat trick\n",
        "402": "let's set a to be equal to a 5x5 magic\n",
        "405": "square say so a is now this 5x5 matrix\n",
        "408": "doesn't mean trick that I sometimes use\n",
        "411": "to visualize the matrix which is I can\n",
        "416": "use image SC of a and what this will do\n",
        "419": "is this will plot a five by five matrix\n",
        "421": "so take my matrix and plot this as a\n",
        "424": "five by five grid of colors where the\n",
        "425": "different colors correspond to the\n",
        "429": "different values in the a matrix so\n",
        "434": "concretely I can also do color bar let\n",
        "436": "me use a more sophisticated command in\n",
        "442": "which SC a color bar color map gray this\n",
        "443": "which SC a color bar color map gray this\n",
        "444": "is actually running three commands at\n",
        "445": "the time we're running a rich SC then\n",
        "447": "running color bar then running color map\n",
        "450": "gray and what this does is it sets the\n",
        "452": "color map to a grey color map and on the\n",
        "455": "right it also puts in this color bar and\n",
        "458": "so this color bar shows what the\n",
        "460": "different shades of color correspond to\n",
        "463": "concretely the upper-left element of the\n",
        "465": "a matrix of seventeen and so here that\n",
        "468": "corresponds to kind of a mint shade of\n",
        "471": "gray whereas in contrast the second\n",
        "474": "element of a sort of the one two element\n",
        "478": "of a is 24 right so it's a 1 2 is 24 so\n",
        "480": "that corresponds to this square up here\n",
        "482": "which is a you know nearly a shade of\n",
        "487": "white and the small value say a what is\n",
        "491": "that a four five you know it's the value\n",
        "494": "3 over here that corresponds you can see\n",
        "496": "on my color bar that corresponds to a\n",
        "499": "much darker shade in this image so\n",
        "501": "here's another example I can plot a\n",
        "504": "larger you know here's a magic fifteen\n",
        "506": "that gives me a fifteen by fifteen Magic\n",
        "509": "and this gives me a plot of what my\n",
        "510": "fifteen by fifteen\n",
        "513": "magic squares values look like and\n",
        "516": "finally to wrap up this video what\n",
        "519": "you've seen me do here is use a comma\n",
        "522": "chaining of function calls here's how\n",
        "524": "you actually do this if I type equals 1\n",
        "528": "B equals 2 C equals 3 and hit enter then\n",
        "530": "this this is actually carrying out three\n",
        "532": "commands at the same time or really\n",
        "534": "carrying out three commands one after\n",
        "537": "another and it prints out all three\n",
        "538": "results and this is a lot like equals 1\n",
        "542": "B equals 2 T equals 3 except that if I\n",
        "544": "use semicolons instead of a comma it\n",
        "547": "doesn't print out anything so this you\n",
        "548": "know this thing here we call comma\n",
        "551": "chaining of command so comma chaining of\n",
        "553": "function calls and it's just another\n",
        "556": "convenient way in octave to put multiple\n",
        "559": "commands like image SC color bar color\n",
        "561": "map to put multi commands on the same\n",
        "564": "line so that's it you now know how to\n",
        "567": "plot different figures in octave and in\n",
        "570": "the next video the next main P is going\n",
        "572": "to tell you about is how to write\n",
        "574": "control statements like if while for\n",
        "576": "statements in octave as well as how to\n"
    },
    "8DfXJUDjx64": {
        "0": " \n",
        "1": "in the last video we developed an\n",
        "4": "anomaly detection algorithm in this\n",
        "6": "video I'd like to talk about the process\n",
        "9": "of how to go about developing a specific\n",
        "11": "application of anomaly detection to a\n",
        "13": "problem and in particular there's a\n",
        "16": "focus on the problem of how to evaluate\n",
        "18": "an anomaly detection algorithm in\n",
        "20": "previous videos we had already talked\n",
        "22": "about the importance of real number\n",
        "25": "evaluation and just captures the idea\n",
        "27": "that when you're trying to develop a\n",
        "28": "learning algorithm for a specific\n",
        "31": "application you need to often make a lot\n",
        "32": "of choices when you're choosing what\n",
        "34": "features to use and then and so on and\n",
        "37": "making decisions about all of these\n",
        "40": "choices is often much easier if you have\n",
        "41": "a way to evaluate your learning\n",
        "43": "algorithm that just gives you back a\n",
        "45": "number so if you're trying to decide you\n",
        "47": "know I have an idea for one extra\n",
        "50": "feature do I include this feature or not\n",
        "52": "if you can run the algorithm with the\n",
        "53": "feature and run the applet without the\n",
        "56": "feature and just get back a number that\n",
        "58": "tells you you know did it improve or\n",
        "60": "worsen performance to add the speaker\n",
        "61": "then it gives you a much better way a\n",
        "64": "much simpler way to just soon would wish\n",
        "66": "to decide whether or not to include that\n",
        "69": "feature so in order to be able to\n",
        "72": "develop an anomaly detection system\n",
        "75": "quickly we it really helpful to have a\n",
        "77": "way of evaluating an anomaly detection\n",
        "81": "system in order to do this in order to\n",
        "83": "evaluate an anomaly detection system\n",
        "85": "we're actually going to assume that we\n",
        "88": "have some labeled data so so far we'll\n",
        "89": "be treating anomaly detection as an\n",
        "90": "be treating anomaly detection as an\n",
        "91": "unsupervised learning problem using\n",
        "92": "unsupervised learning problem using\n",
        "95": "unlabeled data but if you have some\n",
        "97": "label data that specifies what are some\n",
        "99": "anomalous examples and what are some\n",
        "103": "non-anomalous examples then this is how\n",
        "104": "we actually think of as the standard way\n",
        "106": "of evaluating an anomaly detection\n",
        "109": "algorithm so taking the aircraft\n",
        "111": "engineer example game let's say that you\n",
        "114": "know you have some label data of a lot\n",
        "117": "of just a few anomalous examples of some\n",
        "119": "aircraft engines that were manufacture\n",
        "120": "in the past that turns out to be\n",
        "122": "anomalous if it turns out to be floral\n",
        "124": "to strange in some way that's how we\n",
        "126": "usually we also have some non-anomalous\n",
        "130": "examples of some perfectly okay examples\n",
        "132": "I'm going to use y equals zero to denote\n",
        "134": "the normal or the non-anomalous examples\n",
        "136": "and y equals wanted to know the\n",
        "140": "anomalous examples the process of\n",
        "144": "developing and evaluating an anomaly\n",
        "146": "detection algorithm is as follows\n",
        "148": "we're going to think of it as the\n",
        "150": "training set to talk about the cross\n",
        "152": "validation and test sets later but the\n",
        "153": "training is that we you should think of\n",
        "155": "this as though the unlabeled training\n",
        "158": "set and so this is a large collection of\n",
        "161": "normal non-anomalous and not anomalous\n",
        "164": "examples and usually we think of this as\n",
        "165": "being non anomalous but it is actually\n",
        "168": "okay even if a few anomalies slip into\n",
        "171": "your unlabeled training set and next\n",
        "173": "we're going to define a cross-validation\n",
        "175": "set and the test set with which to\n",
        "178": "evaluate a particular anomaly detection\n",
        "180": "algorithm so specifically for both the\n",
        "182": "cross validation and test sets we're\n",
        "184": "going to assume that you know we can\n",
        "185": "include a few examples in the\n",
        "187": "cross-validation set and the test set\n",
        "189": "that contain examples that are known to\n",
        "192": "be anomalous so the test set say will\n",
        "195": "have a few examples with y equals 1 that\n",
        "197": "corresponds to anomalous aircraft\n",
        "200": "engines so here's a specific example\n",
        "203": "let's say that altogether this isn't\n",
        "206": "data that we have we have manufactured\n",
        "209": "10,000 examples of engines that as far\n",
        "210": "as we know were perfectly normal\n",
        "213": "perfectly good aircraft engines and\n",
        "215": "again it turns out to be okay even if a\n",
        "219": "few flawed engine slips into this set of\n",
        "220": "10,000 is\n",
        "222": "okay but kind of assumed that the vast\n",
        "225": "majority of these 10,000 examples are\n",
        "226": "you know good normal\n",
        "228": "non-anomalous engines and let's say that\n",
        "231": "you know historically in however long\n",
        "232": "we've been running our manufacturing\n",
        "235": "plants let's say that we end up getting\n",
        "238": "features giving twenty floors or two the\n",
        "241": "anomalous engines as well and for pretty\n",
        "243": "typical application of anomaly detection\n",
        "245": "you know the number of known anomalies\n",
        "249": "example on status with y equals one we\n",
        "251": "may have anywhere from you know 20 to 50\n",
        "253": "it would be a pretty typical range of\n",
        "255": "examples number of examples that we have\n",
        "257": "with y plus one and usually we'll have a\n",
        "261": "much larger number of good examples\n",
        "264": "so given this data set a fairly typical\n",
        "267": "way to split it into the training set\n",
        "268": "cross-validation set and the test set\n",
        "271": "would be as follows let's take off 10000\n",
        "274": "goods aircraft engines and put 6,000 of\n",
        "277": "that into the unlabeled training set so\n",
        "278": "this is I'm calling this an unlabeled\n",
        "280": "training set but all of these examples\n",
        "282": "are really ones that correspond to y\n",
        "285": "equals 0 as far as we know and so we\n",
        "288": "will use this to fit P of X right so we\n",
        "290": "will use these 6,000 inches to fit P of\n",
        "292": "X which was that to a P of x1\n",
        "296": "parametrized by mu 1 Sigma squared 1 up\n",
        "298": "to P of X and parametrized by mu and\n",
        "302": "Sigma squared n and so it would be these\n",
        "304": "6,000 examples that we were used to\n",
        "306": "estimate the parameters mu 1 Sigma\n",
        "309": "squared 1 up to mu and Sigma square n so\n",
        "311": "that's a training set of all you know\n",
        "313": "good or the vast majority of good\n",
        "317": "examples next we will take our good\n",
        "318": "aircraft engines and put some number of\n",
        "319": "aircraft engines and put some number of\n",
        "321": "them in the cross-validation sets put\n",
        "322": "some number of them in tested six\n",
        "323": "thousand plus two thousand four two\n",
        "325": "thousand that's how we split up about\n",
        "326": "10,000\n",
        "329": "good aircraft engines and then we also\n",
        "333": "have 20 flawed aircraft engines we'll\n",
        "335": "take that and maybe split it up you know\n",
        "336": "put ten of them and across\n",
        "340": "pretended them in the attest set and in\n",
        "342": "the next I will talk about how to\n",
        "345": "actually use this to evaluate to the\n",
        "348": "anomaly detection algorithm so what I've\n",
        "351": "just described here is your parada\n",
        "353": "recommended or pretty good way of\n",
        "355": "splitting the label and on the apron\n",
        "357": "example of the good end of fluid\n",
        "359": "aircraft engines where we use like a\n",
        "361": "sixty twenty twenty percent split for\n",
        "363": "the good engines and we take the flawed\n",
        "365": "engines and we put them just in the\n",
        "367": "cross validation set and just in the\n",
        "368": "test at the most feared in the explain\n",
        "371": "why that's the case just as an aside uh\n",
        "373": "if you look at how people apply anomaly\n",
        "375": "detection algorithm sometimes you see\n",
        "377": "other people split the data differently\n",
        "379": "as well so another alternative this is\n",
        "381": "really not recommended alternatives but\n",
        "383": "some people will take of your ten\n",
        "384": "thousand to the engines may put six\n",
        "386": "thousand of them in the training set and\n",
        "388": "they put the same four thousand in the\n",
        "390": "cross-validation set in the test set and\n",
        "392": "so you know we like to think of the\n",
        "394": "cross-validation set and the test set as\n",
        "396": "being completely different data sets in\n",
        "399": "each other but anomaly detection you\n",
        "401": "know for sometimes you see people it's\n",
        "403": "like use the same set of good engines\n",
        "404": "and the cross-validation set and the\n",
        "406": "test set and sometimes you see people\n",
        "410": "use exactly the same sets of anomalous\n",
        "414": "engines the cross-validation set and the\n",
        "415": "test set and so all of these are\n",
        "417": "consider you know less good practices\n",
        "419": "and definitely less recommended\n",
        "422": "certainly right using the same data and\n",
        "423": "the cross-validation in the set and the\n",
        "424": "test set\n",
        "426": "that's not constantly good machine\n",
        "428": "learning practice but sometimes you see\n",
        "430": "people do this too so given the training\n",
        "433": "cross-validation and test sets\n",
        "435": "here's how you evaluate so here's how\n",
        "437": "you develop and evaluate an algorithm\n",
        "440": "first we take the training set and we\n",
        "441": "fit the model P of X so we fit new all\n",
        "445": "those gaussians to my M unlabeled\n",
        "447": "examples of aircraft engines and these\n",
        "448": "are I'm calling them unlabeled examples\n",
        "451": "but these are ready examples that we're\n",
        "453": "assuming are good or the normal\n",
        "456": "revengeance then imagine that your\n",
        "458": "anomaly detection algorithm is actually\n",
        "460": "making predictions so on the cross\n",
        "463": "validation test set given a say test\n",
        "465": "example X think of the algorithm as\n",
        "468": "predicting that Y is equal to 1 the P of\n",
        "471": "X is less than Epsilon was predicting 0\n",
        "473": "the P of X is greater than equal Epsilon\n",
        "476": "so it's trying to predict given X is\n",
        "478": "trying to predict what is the label you\n",
        "480": "know that y equals 1 corresponding to an\n",
        "482": "anomaly or is it y equals 0\n",
        "486": "corresponding to a normal to a normal\n",
        "488": "example so few minute training\n",
        "490": "cross-validation and test sets how do\n",
        "491": "you develop an algorithm and more\n",
        "494": "specifically how do you evaluate an\n",
        "497": "anomaly detection algorithm well to do\n",
        "499": "so the first step is to take the\n",
        "502": "unlabeled training set and to fit the\n",
        "504": "model P of X to a training data so you\n",
        "506": "take this you know I'm calling unlabeled\n",
        "507": "training set but really these are\n",
        "510": "examples that we were assuming the vast\n",
        "512": "majority of which are normal aircraft\n",
        "515": "engines not as they're not anomalies and\n",
        "517": "will fit the model P of X so fit all\n",
        "518": "those parameters for all the gaussians\n",
        "523": "on this data nix on the cross-validation\n",
        "525": "on the test set we're going to think of\n",
        "527": "the anomaly detection algorithm as\n",
        "531": "trying to predict the value of y so in\n",
        "534": "each of my say test examples we have\n",
        "539": "these x i tests why i test where y is\n",
        "541": "going to be equal to 1 or 0 depending on\n",
        "544": "whether this was an anomalous example so\n",
        "546": "given any products in my test set my\n",
        "548": "anomaly detection algorithm think of\n",
        "551": "that predicting the Y is 1 if P of X is\n",
        "553": "less than epsilon so predicting that is\n",
        "555": "an anomaly that it probably is very low\n",
        "556": "and we think of the algorithm as\n",
        "559": "predicting that Y is equal to 0 if P of\n",
        "561": "X was greater than or equal to Epsilon\n",
        "562": "so predicting does in\n",
        "565": "all example if the P of X is this\n",
        "568": "reasonably large and so we can now think\n",
        "570": "of the anomaly detection algorithm as\n",
        "572": "making predictions for what are the\n",
        "575": "values of these Y labels in the test set\n",
        "577": "or on the cross validation set and this\n",
        "579": "puts us somewhat most similar to the\n",
        "581": "supervised learning setting right where\n",
        "583": "we have like a label test set and our\n",
        "585": "algorithm is making predictions on these\n",
        "588": "labels and so we can evaluate it you\n",
        "590": "know by seeing how often they get these\n",
        "593": "labels right of course these labels are\n",
        "596": "alone will be very skewed because y\n",
        "599": "equals 0 that is normal examples will\n",
        "602": "you'd be much more common than Y equals\n",
        "604": "1 then anomalous examples but you know\n",
        "606": "this is much closer to the source of\n",
        "608": "evaluation metrics we can use in\n",
        "609": "evaluation metrics we can use in\n",
        "611": "supervised learning\n",
        "614": "so what's a good evaluation metric to\n",
        "617": "use well because the data because\n",
        "619": "because the data is very skewed because\n",
        "621": "y equals zero was much more common\n",
        "624": "classification accuracy would not be a\n",
        "625": "good evaluation metric so if we talked\n",
        "628": "about this in the earlier video so if\n",
        "630": "you have a very skewed data set then\n",
        "633": "predicting y equals 0 all the time we\n",
        "635": "have very high classification accuracy\n",
        "637": "instead we should use evaluation metrics\n",
        "639": "like computing the fraction of true\n",
        "641": "positives false positives false\n",
        "642": "negatives true negatives a few things\n",
        "644": "about while computer the precision and\n",
        "645": "about while computer the precision and\n",
        "647": "recall of this algorithm to things like\n",
        "650": "compute e f1 score right what you they\n",
        "651": "say single real number way of\n",
        "653": "summarizing the position and the recall\n",
        "655": "members and so these would be ways to\n",
        "657": "evaluate an anomaly detection algorithm\n",
        "659": "on your cross-validation sets or on your\n",
        "663": "test phase finally earlier in in the\n",
        "665": "anomaly detection algorithm we also had\n",
        "667": "this parameter epsilon and so epsilon\n",
        "671": "was this threshold that we would use to\n",
        "673": "decide when to flag something as an\n",
        "677": "anomaly and so if you have a cross\n",
        "679": "validation set another way that one way\n",
        "681": "to choose this parameter epsilon would\n",
        "684": "be to try a different try many different\n",
        "686": "values of epsilon and then pick the\n",
        "688": "value of epsilon that let's say\n",
        "691": "maximizes f1 score that\n",
        "693": "or that otherwise does well on your\n",
        "696": "cross-validation set and more generally\n",
        "698": "the way to use the training testing and\n",
        "702": "cross-validation sets is that when we're\n",
        "703": "trying to make decisions like what\n",
        "704": "trying to make decisions like what\n",
        "706": "features to include or trying to you\n",
        "708": "know tune the parameter epsilon we would\n",
        "711": "then continually evaluate the algorithm\n",
        "713": "on the cross-validation set to make all\n",
        "714": "those decisions like what features to\n",
        "718": "use on the set epsilon use that evaluate\n",
        "719": "the algorithm on the cross-validation\n",
        "721": "set and then when we've picked a set of\n",
        "723": "features where we've found the value of\n",
        "725": "epsilon they were happy work we can then\n",
        "728": "take the final model and evaluate it you\n",
        "730": "know do the final evaluation of the\n",
        "733": "algorithm the test set so in this video\n",
        "735": "we talked about the process of how to\n",
        "738": "evaluate an anomaly detection algorithm\n",
        "741": "and again having been able to evaluate\n",
        "743": "an algorithm you know with a single real\n",
        "745": "number evaluation or the number like an\n",
        "747": "f1 school that often allows you to make\n",
        "748": "f1 school that often allows you to make\n",
        "750": "much much more efficient use of your\n",
        "752": "time when you're trying to develop an\n",
        "754": "anomaly detection system when we're\n",
        "755": "trying to make these sorts of decisions\n",
        "757": "that constitutes epsilon what features\n",
        "759": "to include and so on in this video we\n",
        "762": "started to use a bit of label data in\n",
        "764": "order to evaluate the anomaly detection\n",
        "766": "algorithm this takes us a little bit\n",
        "768": "closer to a supervised learning setting\n",
        "771": "in the next video I'm going to say a bit\n",
        "773": "more about that and in particular we'll\n",
        "775": "talk about when should you be using an\n",
        "777": "anomaly detection algorithm when should\n",
        "779": "we be thinking about using supervised\n",
        "780": "learning instead what are the\n"
    },
    "8fx1nSFQqkk": {
        "0": " \n",
        "1": "in earlier videos that said over and\n",
        "3": "over that when you're developing a\n",
        "5": "machine learning system one of the most\n",
        "7": "valuable resources is your time as the\n",
        "10": "developer in terms of picking what to\n",
        "12": "work onyx or if you have a team of\n",
        "14": "developers or a team of engineers\n",
        "15": "working together on a machine learning\n",
        "17": "system again one of the most valuable\n",
        "20": "resources is the time of the engineers\n",
        "21": "or the developers working on the system\n",
        "24": "and what you really want to avoid is if\n",
        "26": "you know you or your colleagues your\n",
        "27": "friends spend a lot of time working on\n",
        "30": "some component only to realize after\n",
        "33": "weeks or months of time spent that all\n",
        "35": "that work you know just doesn't make a\n",
        "37": "huge difference on the performance of\n",
        "40": "the final system in this video what I'd\n",
        "42": "like to do is talk about something\n",
        "45": "called ceiling analysis when you when\n",
        "47": "you when you have a team working on the\n",
        "48": "pipeline machine learning system this\n",
        "50": "can sometimes give you a very strong\n",
        "53": "statement or very strong guidance on\n",
        "55": "what parts of the pipeline might be the\n",
        "60": "best use of your time to work on to talk\n",
        "62": "about ceiling analysis I'm going to keep\n",
        "65": "on using the example of the photo OCR\n",
        "68": "pipeline and said earlier each of these\n",
        "70": "boxes text detection interactive\n",
        "72": "segmentation character recognition each\n",
        "74": "of these boxes can have even a small\n",
        "77": "engineering team working on it or maybe\n",
        "79": "the entire system is just bought by you\n",
        "81": "either way but the question is where\n",
        "83": "should you allocate resources of which\n",
        "86": "of these boxes is most worth your effort\n",
        "89": "trying to improve the performance of in\n",
        "91": "order to explain the idea of ceiling\n",
        "93": "analysis I'm going to keep using the\n",
        "97": "example of our photo OCR pipeline as I\n",
        "98": "mentioned earlier each of these boxes\n",
        "100": "here you should be as machine learning\n",
        "103": "components could be the work of even a\n",
        "105": "small team of engineers or maybe it\n",
        "107": "could be the whole system could be built\n",
        "109": "by just one person but the question is\n",
        "111": "where should you allocate scarce\n",
        "113": "resources and status which of these\n",
        "115": "components on which one or two or maybe\n",
        "117": "all three of these components this most\n",
        "120": "worth your time to try to improve the\n",
        "122": "performance of so here's that their\n",
        "123": "ceiling analysis\n",
        "126": "as in the development process for other\n",
        "128": "machine learning systems as well in\n",
        "129": "order to make decisions on what to do\n",
        "132": "for developing the system is going to be\n",
        "135": "very helpful to have a single row number\n",
        "137": "evaluation metric for this learning\n",
        "139": "system so let's say we pick character\n",
        "141": "level accuracy so if you're given a test\n",
        "143": "set image what is the fraction of\n",
        "145": "alphabets of characters in the tested\n",
        "149": "image that we recognize correctly or you\n",
        "151": "can pick some other single real number\n",
        "152": "evaluation metric if you want but you\n",
        "155": "know the let's say that for whatever the\n",
        "157": "evaluation metric we pick we get that we\n",
        "159": "find that the overall system currently\n",
        "162": "has 72% accuracy so in other words we\n",
        "164": "have some set of test set images and\n",
        "166": "from each test set images we run it\n",
        "168": "through text section then character\n",
        "169": "segmentation and interactive recognition\n",
        "172": "and we find that on our test set the\n",
        "174": "overall accuracy of the entire system\n",
        "178": "was 72% on whatever metric you chose now\n",
        "181": "here's the idea behind ceiling analysis\n",
        "183": "which is that we're going to go to let's\n",
        "185": "say the first module of a machine\n",
        "188": "learning pipeline say text detection and\n",
        "189": "what we're going to do is we're going to\n",
        "190": "monkey around the test we're going to go\n",
        "194": "to test set and in for every test\n",
        "196": "example we're just gonna provide it the\n",
        "199": "correct text detection outputs rights in\n",
        "200": "other words we're going to go to the\n",
        "203": "test set and just manually tell the\n",
        "207": "algorithm where the text is in each of\n",
        "210": "the test examples so in other words it's\n",
        "211": "going to you know simulate what happens\n",
        "214": "if we have a text detection system with\n",
        "217": "a hundred percent accuracy for the\n",
        "221": "purpose of detecting text in an image\n",
        "223": "and really the way you do that is pretty\n",
        "225": "simple right instead of letting your\n",
        "227": "learning algorithm detect the text in\n",
        "229": "the images you want to say go to the\n",
        "231": "images and just manually label what is\n",
        "233": "the location of the text in my test set\n",
        "235": "image and you would then let these\n",
        "237": "correct or let these ground truth labels\n",
        "240": "of whereas the text be part of your test\n",
        "242": "set and just use these ground truth\n",
        "244": "labels as what you feed in to the next\n",
        "245": "stage of the pipeline so the character\n",
        "247": "segmentation pipeline\n",
        "250": "yeah so just said again by putting a\n",
        "252": "checkmark over here what I mean is I'm\n",
        "254": "going to go to my test set and just give\n",
        "255": "it the correct answers give it the\n",
        "257": "correct labels for the text detection\n",
        "260": "part of the pipeline so that is as if I\n",
        "262": "have a perfect text detection system on\n",
        "264": "my test set what we're going to do then\n",
        "267": "is run this data through the rest of the\n",
        "268": "pipeline through the character\n",
        "270": "segmentation and character recognition\n",
        "273": "and then use the same evaluation metric\n",
        "275": "as before to measure what is the overall\n",
        "278": "accuracy of the entire system and with\n",
        "279": "perfect text detection hopefully the\n",
        "281": "performance go up and in this example\n",
        "284": "let's say it goes up 89% and then we're\n",
        "286": "going to keep going next let's go to the\n",
        "288": "next stage of the pipeline to character\n",
        "290": "segmentation so again I'm going to go to\n",
        "292": "my test set and now I'm going to give it\n",
        "295": "the correct text detection output and\n",
        "296": "give it the correct character\n",
        "298": "segmentation output so I'm just go to\n",
        "300": "the test set and manually label three\n",
        "303": "correct segmentations of the text into\n",
        "305": "individual characters and see how much\n",
        "307": "that helps and nothing goes up to 90\n",
        "309": "percent accuracy for the overall system\n",
        "312": "right so as always the accuracy is the\n",
        "314": "accuracy of the overall systems and so\n",
        "315": "whatever the final output of the\n",
        "317": "character recognition system is whatever\n",
        "319": "the final output of the overall pipeline\n",
        "321": "is going to measure the accuracy of that\n",
        "323": "and then finally I'm going to go to the\n",
        "325": "character recognition system and give\n",
        "326": "that the correct labels as well and if I\n",
        "327": "that the correct labels as well and if I\n",
        "328": "do that too then you know no surprise I\n",
        "330": "should get a hundred percent accuracy\n",
        "333": "now the nice thing about having done\n",
        "335": "this analysis is we can now understand\n",
        "338": "what is the potential upside of the\n",
        "340": "upside potential for improving each of\n",
        "342": "these components so we see that if we\n",
        "345": "get perfect text detection our\n",
        "348": "performance went up from 72 to 89% so\n",
        "351": "that's a 17% performance gain so this\n",
        "353": "means that if we take a current system\n",
        "355": "and we spend a lot of time improving\n",
        "358": "text detection that means that we could\n",
        "359": "potentially improve our systems\n",
        "362": "performance by 17% this seems to seems\n",
        "364": "like is well worth our while whereas in\n",
        "367": "contrast when you know going from text\n",
        "369": "detection when we gave it a perfect\n",
        "371": "character segmentation performance went\n",
        "374": "up only by 1% so that's a more sobering\n",
        "375": "message you mean\n",
        "377": "that you know no matter how much time\n",
        "379": "you spend on character segmentation may\n",
        "381": "be the upside potential is going to be\n",
        "383": "pretty small and maybe you do not want\n",
        "384": "to have a large team of engineers\n",
        "387": "working on character segmentation if the\n",
        "388": "sort of analysis shows that you know\n",
        "389": "sort of analysis shows that you know\n",
        "390": "even when you give it the perfect\n",
        "393": "character segmentations your performance\n",
        "395": "goes up by only 1% that is really\n",
        "398": "estimates what is the ceiling or what's\n",
        "399": "an upper bound on how much you can\n",
        "401": "improve the performance your system by\n",
        "403": "working on one of these components and\n",
        "406": "finally going from characters when we\n",
        "408": "get better character recognition the\n",
        "411": "performance went up by 10% so you know\n",
        "413": "again you decide there's a 10%\n",
        "415": "improvement how much does that work your\n",
        "416": "well but this tells you that maybe with\n",
        "419": "more efforts bent on the last stage of\n",
        "422": "the pipeline you can improve the system\n",
        "423": "you can improve the performance of the\n",
        "426": "systems as well another way of thinking\n",
        "428": "about this is that by going through this\n",
        "429": "sort of analysis you trying to figure\n",
        "430": "out you know what is the upside\n",
        "433": "potential D of improving each of these\n",
        "435": "components or how much could you\n",
        "438": "possibly gain if one of these components\n",
        "440": "became absolutely perfect and this\n",
        "442": "really places an upper bound on the\n",
        "444": "performance of that system so the idea\n",
        "446": "of ceiling analysis is pretty important\n",
        "448": "let me just illustrate this idea again\n",
        "450": "but with a different example of a more\n",
        "453": "complex one let's say that you want to\n",
        "455": "do face recognition from images so\n",
        "456": "unless you want to look at the picture\n",
        "457": "and recognize you know whether or not\n",
        "459": "the person in this picture is a\n",
        "461": "particular friend of yours I would try\n",
        "463": "to recognize the person shown in this\n",
        "466": "image this is a slightly artificial\n",
        "468": "example this isn't actually how face\n",
        "470": "recognition is done in the practice but\n",
        "472": "another step through an example of a\n",
        "474": "sort of what a pipeline might look like\n",
        "476": "to give you another example of how a\n",
        "478": "ceiling analysis process might might\n",
        "482": "look so we have a camera image and let's\n",
        "484": "say that we design a pipeline as almost\n",
        "485": "you know let's say the first thing you\n",
        "487": "want to do is do pre-processing of the\n",
        "489": "image so let's take this image like that\n",
        "490": "have shown on the upper right unless\n",
        "493": "they want to remove the background so do\n",
        "494": "people see that\n",
        "498": "Pierce Nix we want to say detect the\n",
        "499": "face of the person that's usually done\n",
        "500": "face of the person that's usually done\n",
        "501": "with a learning algorithm to run the\n",
        "503": "sliding windows classifier to you know\n",
        "505": "draw a box around the person's face\n",
        "507": "having detected the fades it turns out\n",
        "509": "that if you want to recognize people it\n",
        "510": "turns out that the eyes is a highly\n",
        "513": "useful to you we have a way actually in\n",
        "515": "terms of you know recognizing your\n",
        "516": "friends the appearance of their eyes is\n"
    },
    "9AP-DgFBNP4": {
        "0": " \n",
        "2": "in this video we'll talk about an\n",
        "4": "approach to building a recommender\n",
        "6": "system that's called collaborative\n",
        "8": "filtering the algorithm that we're going\n",
        "10": "to talk about has a very interesting\n",
        "12": "property that it does what is called\n",
        "14": "peek alerting and by that I mean that\n",
        "16": "this would be an algorithm they can\n",
        "18": "start to learn for itself what features\n",
        "22": "to use here was the data set that we had\n",
        "25": "and we had assumed that for each movie\n",
        "28": "someone had come and told us how\n",
        "29": "romantic that movie was and how much\n",
        "32": "action there was in that movie but as\n",
        "33": "you can imagine it can be very difficult\n",
        "35": "and time-consuming and expensive to\n",
        "37": "actually try to get someone to get a\n",
        "40": "watch each movie and tell you how\n",
        "42": "romantic rejection paths in each movie\n",
        "45": "and often you want even more features\n",
        "47": "than than just these tools and so where\n",
        "50": "do you get these features from so let's\n",
        "53": "change the problem a bit and suppose\n",
        "56": "that we have a data set where we do not\n",
        "58": "know the values of these features so\n",
        "61": "we're given a data set of movies and of\n",
        "64": "how the users rated them but we have no\n",
        "66": "idea how romantic each movie is and we\n",
        "67": "have no idea how action-packed each\n",
        "69": "movie is so it replace all of these\n",
        "71": "things with question ones but now let's\n",
        "73": "make a slightly different assumption\n",
        "75": "let's think that we've gone to each of\n",
        "77": "our users and each of our users has told\n",
        "79": "us how much they like romantic movies\n",
        "81": "and how much they like action-packed\n",
        "84": "movies so Alice has associated a\n",
        "87": "parameter vector theta one Bob theta two\n",
        "90": "Carol three to three days later four and\n",
        "91": "let's say that we also use this you know\n",
        "95": "let's say Alice tells us that um she\n",
        "97": "really likes romantic movies and so\n",
        "99": "there's a five there which is the\n",
        "101": "multiplier associated with x1 and let's\n",
        "103": "say Alice tells us she really doesn't\n",
        "105": "like actually movies so this is zero\n",
        "107": "there and Bob tells us something similar\n",
        "110": "so we have theta two over here\n",
        "114": "whereas Carol tells us that she really\n",
        "115": "likes action movies which is why there's\n",
        "117": "a five there that's the multiplier\n",
        "119": "associated with x2\n",
        "121": "remember there's also a\n",
        "124": "zero equals one and let's say that Carol\n",
        "126": "you know tells us she doesn't like\n",
        "129": "romantic movies and so on similarly for\n",
        "131": "Dave so so let's assume that somehow we\n",
        "134": "can go to our users and each user J just\n",
        "138": "tells us what is the value of theta J\n",
        "140": "for them and so basically specifies to\n",
        "142": "us how much they like different types of\n",
        "145": "movies if we can get these parameters\n",
        "148": "theta for my users then it turns out\n",
        "150": "that it becomes possible to try to infer\n",
        "153": "what are the values of x1 and x2 for\n",
        "155": "each movie let's look at an example\n",
        "157": "let's look at the movie let's look at\n",
        "159": "movie one so that movie one has\n",
        "162": "associated with it a feature vector x1\n",
        "164": "and you know this movie is called love\n",
        "165": "at last but let's ignore that so let's\n",
        "166": "at last but let's ignore that so let's\n",
        "167": "pretend we don't know what this movie is\n",
        "168": "from right so let's ignore the title of\n",
        "171": "this movie all we know is that Alice\n",
        "173": "loved this movie Bob loved this movie\n",
        "177": "Carol and Dave hated this movie so what\n",
        "179": "from you infer well we know from the\n",
        "182": "feature vectors that Alice and Bob love\n",
        "184": "romantic movies because they told us\n",
        "186": "that one is of the five year whereas\n",
        "190": "Carol and Dave we know that they hate\n",
        "192": "romantic movies and that they love\n",
        "195": "action movies so because those are the\n",
        "197": "parameter vectors that you know user\n",
        "199": "three and four Carol and Dave gave us\n",
        "202": "and so based on the fact that movie one\n",
        "204": "is loved by Alice and Bob and hated by\n",
        "207": "Carol and Dave we might reasonably\n",
        "209": "conclude that you know this is probably\n",
        "212": "a romantic movie and is probably not\n",
        "216": "much of an action range this example is\n",
        "217": "a little bit mathematically simplified\n",
        "220": "but what we're really asking is what\n",
        "223": "feature vectors should x1 be so that\n",
        "228": "theta 1 transpose x1 is approximately\n",
        "229": "equal to 5\n",
        "233": "that Alice's rating and theta 2\n",
        "236": "transpose x1 is also approximately equal\n",
        "241": "to 5 and theta 3 transpose x1 is\n",
        "243": "approximately equal to 0 so this would\n",
        "248": "be a careless rating and theta 4\n",
        "252": "transpose x1 is approximately equal to 0\n",
        "255": "and from this it looks like you know x1\n",
        "258": "equals 1 destined set ramen at one point\n",
        "260": "zero zero point zero\n",
        "262": "that makes sense given what we know of\n",
        "264": "Alice Bob Carol and Dave's preferences\n",
        "266": "for movies in the way they rated this\n",
        "269": "movie and so more generally we can go\n",
        "271": "down this list and try to figure out\n",
        "273": "what might be reasonable features for\n",
        "274": "what might be reasonable features for\n",
        "278": " \n",
        "280": "let's formalize this problem of learning\n",
        "284": "the features X I let's think that our\n",
        "286": "users have given us their preferences so\n",
        "288": "let's say that our users have come in\n",
        "290": "you know told us these values for theta\n",
        "293": "1 to theta of Nu and we want to learn\n",
        "296": "the feature vector X I for movie number\n",
        "299": "I what we can do is therefore pose the\n",
        "301": "following optimization problem so we\n",
        "304": "want to sum over all the indices J for\n",
        "308": "which we have a rating for movie I\n",
        "310": "because we're trying to learn the\n",
        "312": "features for movie I then is this\n",
        "315": "feature vector X I so and then what we\n",
        "318": "want to do is minimize this squared\n",
        "321": "error so we want to choose features X I\n",
        "324": "so that you know the predictive value of\n",
        "328": "how user J Ray's movie I will be similar\n",
        "330": "will be not too far in the squared error\n",
        "334": "sense of the actual value Y IJ that we\n",
        "337": "actually observe in the rating of user J\n",
        "342": "on movie I so just summarize what this\n",
        "344": "term does is it tries to choose features\n",
        "348": "X I so that for all the users J that\n",
        "351": "have rated that movie the our algorithm\n",
        "354": "also predicts a value for how that user\n",
        "356": "would rate to that movie that is not too\n",
        "359": "far in the squared error sense from the\n",
        "361": "actual value that the user had rated\n",
        "364": "that movie so that's the squared error\n",
        "368": "term and as usual we can also add this\n",
        "370": "sort of regularization term to prevent\n",
        "372": "the features from becoming too big\n",
        "376": "so this is how we would learn the\n",
        "379": "features so one specific movie but what\n",
        "381": "we want to do is learn all the features\n",
        "384": "for all the movies and so what I'm going\n",
        "386": "to do is add this extra summation here\n",
        "389": "so I'm going to sum over all n M movies\n",
        "393": "and subscript n movies and minimize this\n",
        "395": "objective on top but somes over our\n",
        "398": "movies and if you do that you end up\n",
        "400": "with the following optimization problem\n",
        "403": "and if you minimize this you have\n",
        "406": "hopefully a reasonable set of features\n",
        "409": "for all of your movies so putting\n",
        "411": "everything together what we do we talked\n",
        "412": "about in the previous video and the\n",
        "414": "algorithm that we just talked about in\n",
        "417": "this video in the previous video what we\n",
        "419": "show was that you know if you have a set\n",
        "421": "of movie ratings so if you have the data\n",
        "425": "the RI J's and if you have the Y IJ\n",
        "427": "trance you've had the movie ratings\n",
        "429": "then given features for your different\n",
        "431": "movies we can learn these parameters\n",
        "434": "data so if you need the features you can\n",
        "436": "learn the parameters theta for your\n",
        "439": "different users and what we show earlier\n",
        "442": "in this video is that if your users are\n",
        "445": "willing to give you parameters then you\n",
        "448": "can estimate features for the different\n",
        "451": "movies so this is kind of a chicken in a\n",
        "452": "problem right which comes first you know\n",
        "455": "do we want to if we get the Thetas you\n",
        "456": "can learn the Exodus\n",
        "458": "you can if we have the XS with a test\n",
        "462": "and what you can do in and then this\n",
        "463": "actually works what you can do is in\n",
        "466": "fact randomly gets some value of the\n",
        "469": "Thetas now based on your initial\n",
        "470": "rambling guests for the Thetas you can\n",
        "473": "then go ahead and use the procedure that\n",
        "476": "we just talked about in order to learn\n",
        "478": "features for your different movies now\n",
        "480": "given some initial set of features for\n",
        "482": "movies you can then use you know this\n",
        "484": "first method that we talked about the\n",
        "486": "previous video to try to get an even\n",
        "488": "better estimate for your parameters\n",
        "490": "theta now they have a better setting of\n",
        "492": "the parameters theta for your users we\n",
        "494": "can use that to maybe get an even better\n",
        "497": "set of features and so on we can sort of\n",
        "499": "keep iterating going back and forth and\n",
        "503": "optimizing theta X theta X theta X and\n",
        "505": "this actually works and if you do this\n",
        "507": "this will actually cause your algorithm\n",
        "510": "to converge to a reasonable set of\n",
        "512": "features for your movies and the\n",
        "514": "reasonable sets of parameters for your\n",
        "517": "different users so this is a basic\n",
        "519": "collaborative filtering algorithm this\n",
        "521": "isn't actually define your algorithm\n",
        "522": "that we're going to use in the next\n",
        "524": "video we're going to be able to improve\n",
        "526": "on this algorithm and make it quite a\n",
        "528": "bit more computationally efficient but\n",
        "530": "hopefully this gives you a sense of how\n",
        "532": "you can formulate a problem where you\n",
        "534": "can simultaneously learn the parameters\n",
        "537": "and simultaneously learn B features from\n",
        "539": "the different movies and for this\n",
        "540": "problem for the recommender system\n",
        "542": "problem this is possible only because\n",
        "544": "the user base multiple movies and\n",
        "546": "hopefully each movie is rated by\n",
        "549": "multiple users and so you can do this\n",
        "551": "back and forth process to this\n",
        "555": "index so to summarize in this video\n",
        "558": "we've seen an initial collaborative\n",
        "560": "filtering algorithm the term\n",
        "562": "collaborative filtering refers to the\n",
        "564": "observation that when you run this out\n",
        "566": "room with a large set of users what all\n",
        "568": "of these users are effectively doing are\n",
        "571": "sort of collaboratively or collaborating\n",
        "573": "to get better movie ratings for everyone\n",
        "575": "because with every user rating some\n",
        "578": "subset of the movies every user is\n",
        "580": "helping the algorithm a little bit to\n",
        "583": "learn better features and then by\n",
        "585": "helping you know by rating a few movies\n",
        "588": "myself I would be hoping the system\n",
        "589": "learn better features and then these\n",
        "591": "features can be used by the system to\n",
        "593": "make better movie predictions for\n",
        "595": "everyone else and so there's a sense of\n",
        "597": "collaboration where every user is\n",
        "599": "helping the system learn better features\n",
        "601": "sort of for the common good and since\n",
        "604": "just collaborative filtering and in the\n",
        "605": "next video what we're going to do is\n",
        "608": "take the ideas that we've worked out and\n",
        "611": "try to develop an even better algorithm\n",
        "613": "slightly better technique for\n"
    },
    "9CIYT72UDHw": {
        "0": " \n",
        "1": "in this video I'd like to tell you about\n",
        "5": "the idea of vectorization so whether\n",
        "6": "you're using octave or a similar\n",
        "8": "language like MATLAB or whether you're\n",
        "11": "using Python numpy are Java or C C++\n",
        "12": "using Python numpy are Java or C C++\n",
        "14": "although these languages have either\n",
        "17": "built into them or have readily and\n",
        "20": "easily accessible different to numerical\n",
        "21": "linear algebra libraries that are\n",
        "23": "usually very well written highly\n",
        "25": "optimized often sort of developed by\n",
        "28": "people that you know have PhDs in\n",
        "29": "numerical computing or they're really\n",
        "31": "specialized in numerical computing and\n",
        "33": "when you're implementing machine\n",
        "35": "learning algorithms if you're able to\n",
        "38": "take advantage of these linear algebra\n",
        "39": "libraries or these numerical linear\n",
        "42": "algebra libraries and make subroutine\n",
        "44": "calls to them rather than sort of write\n",
        "46": "code yourself to do things that these\n",
        "48": "libraries could be doing if you do that\n",
        "51": "then often you get code that first is\n",
        "52": "more efficient so just run more quickly\n",
        "55": "and take better advantage of and your\n",
        "56": "parallel hardware your computer may have\n",
        "57": "parallel hardware your computer may have\n",
        "60": "and so on and second it also means that\n",
        "62": "you end up with less code that you need\n",
        "64": "to run sort of a simpler implementation\n",
        "66": "that is therefore maybe also more likely\n",
        "70": "to be bug free and as a concrete example\n",
        "73": "rather than writing code yourself to\n",
        "75": "multiply matrices if you let octave do\n",
        "78": "it by typing a times B that will use a\n",
        "80": "very efficient routine to multiply the\n",
        "82": "two matrices and there's a bunch of\n",
        "85": "examples like these where if you use\n",
        "87": "appropriate vectorized implementations\n",
        "88": "you get much simpler code and much more\n",
        "90": "efficient code let's look at some\n",
        "94": "examples here's our usual hypothesis for\n",
        "96": "linear regression and if you want to\n",
        "98": "compute a key of X notice that there's a\n",
        "100": "sum on the right and so one thing you\n",
        "103": "could do is compute the sum from J\n",
        "106": "equals 0 to J equals n yourself another\n",
        "109": "way to think of this is to think of H of\n",
        "112": "X as theta transpose X and what you can\n",
        "114": "do is think of this as you know\n",
        "116": "computing this inner product between two\n",
        "120": "vectors where theta is you know your\n",
        "123": "vector say theta 0 theta 1 theta 2 if\n",
        "125": "you have two features if N equals two\n",
        "128": "and if you think of X as this vector X 0\n",
        "134": "x1 x2 and these two views can give you\n",
        "137": "two different implementations here's\n",
        "139": "what I mean here's an unvectorized\n",
        "142": "implementation for how to compute H of X\n",
        "144": "and by unvectorized I mean without\n",
        "147": "vectorization we might first initialize\n",
        "149": "your prediction this to be zero point\n",
        "152": "zero this is going to eventually be on\n",
        "154": "predictions going to eventually be H of\n",
        "157": "X and then I'm going to have a for loop\n",
        "158": "for J equals 1 through n plus one\n",
        "161": "prediction gets incremented by theta J\n",
        "163": "times XJ so it's kind of this expression\n",
        "166": "over here by the way I should mention in\n",
        "168": "these vectors I've rolled over here I\n",
        "171": "had these vectors being 0 index so has\n",
        "173": "theta 0 theta 1 theta 2 but because\n",
        "177": "MATLAB is one index theta 0 in MATLAB we\n",
        "180": "might end up representing as theta 1 and\n",
        "184": "the second element ends up as theta 2\n",
        "186": "and this third element might end up as\n",
        "188": "theta 3\n",
        "190": "just because vectors in MATLAB are\n",
        "192": "indexed starting from one even though\n",
        "194": "you know I wrote theta and X here\n",
        "197": "stopping indexing from 0 which is why\n",
        "199": "here I have a for loop J goes from 1\n",
        "201": "through n plus 1 rather than J goes\n",
        "207": "through 0 up to n right but so this is\n",
        "209": "an unvectorized implementation in that\n",
        "211": "we have a for loop that you know summing\n",
        "214": "up the n elements of the sum in contrast\n",
        "216": "here's how you write a vectorized\n",
        "219": "implementation which is that you would\n",
        "223": "think of X and theta as vectors and you\n",
        "225": "just set prediction equals theta\n",
        "227": "transpose times X and just computing\n",
        "230": "like so so you instead of writing you\n",
        "232": "know all these lines of code with a for\n",
        "234": "loop use they're just at one line of\n",
        "236": "code and what this what what this line\n",
        "238": "of code on the right will do is it will\n",
        "241": "use octaves highly optimized numerical\n",
        "243": "linear algebra routines to compute this\n",
        "245": "inner product between the two vectors\n",
        "248": "theta and X and not only is the\n",
        "251": "vectorized implementation simpler it\n",
        "255": "will also run much more efficiently so\n",
        "258": "that was octave but the issue of\n",
        "260": "vectorization applies to other\n",
        "262": "programming languages as well\n",
        "265": "let's look at an example in C++ here's\n",
        "266": "what an unvectorized implementation\n",
        "269": "might look like we again initialize you\n",
        "271": "know prediction 20.0 and then we now\n",
        "275": "have a for loop for J equals 0 up to n\n",
        "278": "prediction plus equals theta J times XJ\n",
        "280": "where again you have this explicit for\n",
        "283": "loop that you write yourself in contrast\n",
        "285": "using a good numerical linear algebra\n",
        "289": "library in C++ you could use write the\n",
        "295": "function like or rather in contrast\n",
        "297": "using a good numerical linear algebra\n",
        "300": "library in C++ you can instead write\n",
        "302": "code that might look like this so\n",
        "304": "depending on the details of your\n",
        "306": "numerical linear algebra library you\n",
        "308": "might really have an object this is a\n",
        "310": "c++ object which is vector theta and the\n",
        "313": "c++ object which is a vector X and you\n",
        "316": "just take theta dot transpose times X\n",
        "320": "where this x becomes a C++ sort of\n",
        "322": "overload the operator so that you can\n",
        "325": "just multiply these two vectors in C++\n",
        "328": "and depending on you know the details of\n",
        "329": "your numerical linear algebra library\n",
        "331": "you might end up using a slightly\n",
        "333": "different syntax but by relying on the\n",
        "335": "library to do this in the product you\n",
        "337": "can get a much simpler piece of code and\n",
        "341": "a much more efficient one let's now look\n",
        "343": "at a more sophisticated example just to\n",
        "345": "remind you here's our update rule for\n",
        "347": "gradient descent for linear regression\n",
        "350": "and so we update theta J using this rule\n",
        "354": "for all values of J equals 0 1 2 and so\n",
        "357": "on and if I just write out these\n",
        "359": "equations for theta 0 theta 1 theta 2\n",
        "362": "assuming we have two features so N\n",
        "364": "equals 2 then these are the updates we\n",
        "367": "perform to theta 0 theta 1 theta 2 where\n",
        "369": "you might remember am i saying in an\n",
        "371": "earlier video that these should be\n",
        "376": "simultaneous updates so next thing we\n",
        "377": "can come up with a vectorized\n",
        "381": "implementation of this here are my same\n",
        "382": "three equations written in a slightly\n",
        "384": "smaller font and you can imagine that\n",
        "385": "smaller font and you can imagine that\n",
        "386": "one way to implement these three lines\n",
        "388": "of code is to have a for loop that says\n",
        "392": "you know for J equals 0 1 through 2 to\n",
        "394": "update theta J\n",
        "396": "like that but instead let's come up with\n",
        "399": "a vectorized implementation and see if\n",
        "401": "we can have a simpler way to basically\n",
        "403": "compress these three lines of code or\n",
        "406": "for loop they you know effectively does\n",
        "408": "these three sets one set at a time let's\n",
        "410": "say you can take these three steps and\n",
        "412": "compress them into one line of a\n",
        "415": "vectorized code here's the idea what I'm\n",
        "417": "going to do is I'm going to think of\n",
        "420": "theta as a vector and I'm going to\n",
        "426": "update theta as theta minus alpha times\n",
        "433": "some other vector Delta where Delta is\n",
        "437": "going to be equal to one over m sum from\n",
        "443": "I equals 1 through m and then this term\n",
        "448": "over on the right okay so let me explain\n",
        "452": "what's going on here here I'm going to\n",
        "456": "treat theta as a vector so there's an n\n",
        "458": "plus 1 dimensional vector I'm singing\n",
        "460": "that theta gets your updated as that's a\n",
        "465": "vector or n plus 1 alpha is a real\n",
        "470": "number and Delta here is a vector so\n",
        "473": "this subtraction operation that's a\n",
        "475": "vector subtraction because uh alpha\n",
        "478": "times Delta is a vector and so I'm\n",
        "480": "saying theta gets you know this vector\n",
        "484": "alpha times Delta subtracted from it so\n",
        "487": "what is the vector Delta well this\n",
        "491": "vector Delta looks like this and what is\n",
        "494": "meant to be is really meant to be this\n",
        "496": "thing over here\n",
        "500": "concretely Delta will be a n plus 1\n",
        "502": "dimensional vector and the very first\n",
        "506": "element of the vector Delta is going to\n",
        "509": "be equal to that so if we have that\n",
        "511": "Delta you know if we index it from zero\n",
        "515": "as Delta 0 Delta 1 Delta 2 what I want\n",
        "520": "is that Delta 0 is equal to you know\n",
        "522": "this first boxing green up above and\n",
        "525": "indeed you might be able to convince\n",
        "528": "yourself that Delta 0 is this 1 over m\n",
        "534": "sum of you know H of X X I minus y I\n",
        "539": "times X I 0 so let's just make sure that\n",
        "542": "we're on the same page about how Delta\n",
        "545": "really is computed Delta is 1 over m\n",
        "548": "times this sum over here and you know\n",
        "551": "what is this sum well this term over\n",
        "558": "here that's a real number and the second\n",
        "562": "term over here X I this term over there\n",
        "566": "is a vector right because X I you know\n",
        "569": "may be a vector that would be you say\n",
        "577": "xi0 xi1 xi2 right and what is the\n",
        "579": "summation well what the summation is\n",
        "585": "saying is that this term that is this\n",
        "591": "term over here this is equal to H of x1\n",
        "601": "minus y1 times x1 plus h of x2 minus y 2\n",
        "606": "times x2 plus you know and so on okay\n",
        "608": "because this is a summation over I so as\n",
        "611": "I ranges from I equals 1 through m you\n",
        "612": "get these different terms and you're\n",
        "615": "summing up these terms here and the\n",
        "616": "meaning of each of these terms you know\n",
        "618": "this is a lot like if you remember\n",
        "620": "actually from the from the earlier quiz\n",
        "622": "in this right you you solve this\n",
        "625": "equation we said that in order to\n",
        "627": "vectorize this code we will instead set\n",
        "631": "u equals to V plus 5w so we're saying\n",
        "633": "that the vector u is equal to 2 times\n",
        "636": "the vector V plus 5 times the vector W\n",
        "639": "so this is an example of how to add\n",
        "642": "different vectors and this summation is\n",
        "644": "the same thing this is saying that the\n",
        "648": "summation over here is just some real\n",
        "650": "number right that's kind of like the\n",
        "653": "number 2 or some other number times it\n",
        "654": "back to x1\n",
        "656": "you know two times V and staying with\n",
        "659": "some other number at times x1 and then\n",
        "662": "plus you know instead of five times W we\n",
        "664": "instead have some other real number plus\n",
        "667": "some other vector and then you add on\n",
        "669": "other vectors you know plus dot dot dot\n",
        "672": "plus the other vectors which is why\n",
        "676": "overall this thing over here that whole\n",
        "680": "quantity that doubter is just some\n",
        "684": "vector and concretely the three elements\n",
        "688": "of delta correspond if N equals two the\n",
        "689": "three elements of delta correspond\n",
        "693": "exactly to this thing to the second\n",
        "696": "thing and this third thing which is why\n",
        "698": "when you update theta according to theta\n",
        "701": "minus Alpha Delta we end up having\n",
        "704": "exactly the same simultaneous updates as\n",
        "707": "as the update rules that we had on top\n",
        "709": "so I know that there was a lot that\n",
        "712": "happens on the slides but again feel\n",
        "714": "free to pause the video and data\n",
        "717": "encourage you to sort of step through\n",
        "718": "the differences if you aren't sure what\n",
        "721": "just happened I'll encourage you to step\n",
        "722": "through this line to make sure you\n",
        "725": "understand why is it that this update\n",
        "729": "here with this definition of Delta right\n",
        "731": "why is it that that's equal to this\n",
        "733": "update on top and it's still not clear\n",
        "735": "one one one insight is that you know\n",
        "740": "this thing over here that's exactly the\n",
        "742": "vector X and so we're just taking you\n",
        "744": "know all three of these computations and\n",
        "747": "compressing them into one step with this\n",
        "751": "vector Delta which is why we can come up\n",
        "753": "with a vectorized implementation of this\n",
        "755": "of the step of linear regression this\n",
        "759": "way so I hope this step makes sense and\n",
        "760": "way so I hope this step makes sense and\n",
        "761": "do do look at the video and make sure\n",
        "764": "and see if you can understand it in case\n",
        "766": "you don't understand quite the\n",
        "768": "equivalence of this math if you\n",
        "769": "implement this this turns out that the\n",
        "771": "right answer anyway so even if you\n",
        "773": "didn't quite understand the equivalence\n",
        "776": "if you just implemented this way you you\n",
        "779": "really get linear regression to work but\n",
        "781": "if you are if you're able to figure out\n",
        "783": "why these two steps are equivalent then\n",
        "784": "hopefully that will give you a better\n",
        "785": "understand\n",
        "787": "vectorization as well\n",
        "791": "and finally if you are implementing\n",
        "794": "linear regression using more than one or\n",
        "795": "two features so sometimes we'll use\n",
        "798": "linear regression with tens or hundreds\n",
        "800": "or thousands of features but if you use\n",
        "802": "the vectorized implementation of linear\n",
        "804": "regression usually that will run much\n",
        "806": "faster than if you had say your own\n",
        "808": "volume there was you know updating theta\n",
        "811": "0 then theta 1 and theta 2 yourself so\n",
        "813": "using a vectorized implementation\n",
        "814": "usually people are getting a much more\n",
        "815": "usually people are getting a much more\n",
        "816": "efficient implementation of linear\n",
        "819": "regression and when you vectorize later\n",
        "820": "algorithms that we'll see in this class\n",
        "823": "is a good trick whether in octave or\n",
        "824": "some of the language they see pluses\n",
        "827": "Java for getting your code to run more\n"
    },
    "9siFuMMHNIA": {
        "0": " \n",
        "3": "in the last video we talked about the\n",
        "5": "recommender system problem where for\n",
        "7": "example you may have a set of movies and\n",
        "10": "you may have a set of users each of whom\n",
        "13": "has rated some subsets of the movies\n",
        "14": "that rated the movies one to five stars\n",
        "17": "or 0 to 5 stars and what we would like\n",
        "19": "to do is look at these users and predict\n",
        "21": "how they would have rated other movies\n",
        "23": "that they have not yet rated in this\n",
        "25": "video I'd like to talk about our first\n",
        "27": "approach to building a recommender\n",
        "28": "system this approach is called\n",
        "31": "content-based recommendations here's our\n",
        "34": "data set from the fall and just remind\n",
        "36": "you of a bit of notation I was using and\n",
        "38": "you to denote the number of users and so\n",
        "43": "that's equal to 4 and nm to denote the\n",
        "47": "number of movies I have 5 movies so how\n",
        "50": "do I predict what these missing values\n",
        "53": "would be let's suppose that for each of\n",
        "56": "these movies I have a set of features\n",
        "59": "folder in particular let's say that 20\n",
        "61": "the movies I have 2 features which on\n",
        "63": "which I'm going to denote X 1 and X 2\n",
        "66": "where X 1 measures the degree to which a\n",
        "69": "movie is a romantic movie and X 2\n",
        "71": "measures the degree to which a movie is\n",
        "73": "an action movie so if you take movie\n",
        "77": "love that laughs you knows 0.9 or rating\n",
        "79": "on the romance fields of highly romantic\n",
        "81": "movie but 0 on the action skills there\n",
        "83": "are almost no action in that movie\n",
        "87": "romance forever is a 1.0 lot of romance\n",
        "90": "and 0.01 action I don't know maybe oh\n",
        "92": "there's a minor contraction that movie\n",
        "93": "or something so it's a little bit of\n",
        "95": "action\n",
        "99": "skipping 1 let's do a source vs. karate\n",
        "101": "maybe that as a 0 romance rating no\n",
        "103": "romance at all on that that plenty of\n",
        "106": "action and you know non-stop car chases\n",
        "107": "maybe again there's a tiny bit of\n",
        "110": "romance in that movie but maybe\n",
        "111": "and then you have two puppies of love\n",
        "113": "again maybe a romance movie with no\n",
        "114": "again maybe a romance movie with no\n",
        "117": "action though so if we have features\n",
        "119": "like these then each movie can be\n",
        "122": "represented with a feature vector\n",
        "124": "let's take movie one so let's call these\n",
        "126": "movies you know movies one two three\n",
        "128": "four and five but my first movie love at\n",
        "132": "last I have my two features 0.9 and 0\n",
        "135": "and so these are features x1 and x2 and\n",
        "138": "let's add an extra feature as usual\n",
        "140": "which is Mario interceptor feature x0\n",
        "143": "which is equal to 1 and so putting these\n",
        "146": "together I would then have a feature x1\n",
        "148": "the superscript 1 denotes is the feature\n",
        "150": "vector for my first movie and this\n",
        "153": "feature vector is equal to 1 the first\n",
        "155": "one there is this intercept term and\n",
        "160": "then my two features 0.90 like so so if\n",
        "162": "I love at last I will have a feature\n",
        "165": "vector x1 for the movie romance forever\n",
        "168": "I may have a separate feature vector x2\n",
        "170": "and so on and for souls vs. karate I\n",
        "172": "would have you know a different feature\n",
        "177": "vector X superscript 5 also consistent\n",
        "179": "with our earlier notation that we were\n",
        "181": "using going to set n to be the number of\n",
        "184": "features not counting this x0 - septum\n",
        "186": "and so n is equal to 2 because we have\n",
        "189": "two features x1 and x2 capturing the\n",
        "191": "degree of romance and the degree of\n",
        "194": "action in this movie now in order to\n",
        "197": "make predictions here's one thing to do\n",
        "200": "which is that we can treat predicting\n",
        "201": "which is that we can treat predicting\n",
        "203": "the ratings of each user as a separate\n",
        "206": "linear regression problem so\n",
        "207": "specifically let's say that for each\n",
        "210": "user J we're going to learn a parameter\n",
        "212": "vector theta J which would be an r3 in\n",
        "215": "this case more generally theta J would\n",
        "216": "be\n",
        "219": "our n plus 1 where n is the number of\n",
        "222": "features not counting the etc and we're\n",
        "224": "going to predict user J as rating movie\n",
        "226": "I with just the inner product between\n",
        "229": "yield parameter vector theta and the\n",
        "233": "features X I so let's take a specific\n",
        "239": "example let's take a user one so that\n",
        "240": "example let's take a user one so that\n",
        "242": "would be Alice and associating with\n",
        "244": "Alice would be some parameter vector\n",
        "247": "theta one and our second user Bob will\n",
        "249": "be associated with a different parameter\n",
        "251": "vector theta two Carol we associated to\n",
        "253": "a different parameter vector theta three\n",
        "255": "and Dave a different parameter vector\n",
        "258": "theta four so let's say we want to make\n",
        "261": "a prediction for what Alice will think\n",
        "264": "of the movie cute puppies of love well\n",
        "266": "that movie is going to have some\n",
        "270": "parameter vector X 3 where we have that\n",
        "273": "x3 is going to be equal to 1 which is 4\n",
        "277": "intercept term and then 0.99 and then 0\n",
        "280": "and let's say for this example let's say\n",
        "282": "that you know we've somehow already\n",
        "285": "gotten a parameter vector theta 1 for\n",
        "287": "Alice we will say later exactly how we\n",
        "290": "come up with this parameter vector but\n",
        "292": "let's just say for now that you know\n",
        "294": "some unspecified learning algorithm has\n",
        "295": "learned the parameter vector theta 1 and\n",
        "296": "learned the parameter vector theta 1 and\n",
        "300": "is equal to the 0 5 0 so our prediction\n",
        "305": "for this entry is going to be equal to\n",
        "309": "theta 1 that is Alice's parameter vector\n",
        "313": "transpose X 3 that is the feature vector\n",
        "316": "for the Q puppies of love movie number 3\n",
        "318": "and so the inner product between these\n",
        "320": "two vectors is going to be you know 5\n",
        "323": "times 0.9\n",
        "327": "which is equal to your 4.95 and so my\n",
        "329": "prediction for this value over here is\n",
        "332": "going to be four point nine five maybe\n",
        "334": "that seems like a reasonable value if\n",
        "335": "indeed to them\n",
        "339": "this is my parameter vector theta one so\n",
        "341": "all we're doing here is we're applying a\n",
        "343": "different copy of essentially linear\n",
        "345": "regression for each user and we're\n",
        "348": "saying that what Alice does is Alice has\n",
        "350": "some parameter vector theta one that she\n",
        "352": "uses you know that that we use to\n",
        "355": "predict her ratings as a function of how\n",
        "357": "romantic and how action-packed movie is\n",
        "360": "and Bob in Cameron Dave each of them\n",
        "362": "have a different linear function of the\n",
        "365": "romantic nodes and action ysidro Mans\n",
        "368": "and the degree of action in in movie and\n",
        "370": "that that's how we're going to predict\n",
        "373": " \n",
        "377": "more formally here's how we can write\n",
        "380": "down the problem a notation is that our\n",
        "382": "our J is equal to 1 that user J has\n",
        "385": "rated movie I and Y IJ is the rating of\n",
        "388": "that movie if that if that rating exists\n",
        "391": "that is a that user has actually rated\n",
        "394": "that movie and on the previous line we\n",
        "396": "also defined these theta J which is a\n",
        "399": "parameter of a user X I which is a\n",
        "402": "feature vector for specific movie every\n",
        "404": "user and each movie we predict that\n",
        "409": "rating as follows so let me introduce\n",
        "412": "just temporarily introduce one extra bit\n",
        "414": "of notation MJ going to use MJ to denote\n",
        "416": "the number of users rated by the movie\n",
        "418": "Jane going to need this notation only\n",
        "420": "for this line now in order to learn the\n",
        "423": "parameter vector for theta J well how we\n",
        "425": "do so this is basically a linear\n",
        "428": "regression problem so what we can do is\n",
        "430": "just choose a parameter vector theta J\n",
        "433": "so that the predicted values here are as\n",
        "436": "close as possible to the values that we\n",
        "437": "observed in our training set to the\n",
        "440": "values we observed in our data so let's\n",
        "443": "write that down in order to learn the\n",
        "446": "parameter vector theta J let's minimize\n",
        "450": "over my parameter vector theta J of some\n",
        "454": "and I want to sum over all movies that\n",
        "457": "user J has rated so we write this as sum\n",
        "462": "over all values of AI that's the : r I J\n",
        "465": "equals 1 so the way to read this your\n",
        "467": "summation index is this is summation\n",
        "469": "over all the values of I so there are i\n",
        "472": "j is equal to 1 since to be summing over\n",
        "475": "all the movies that user J has rated and\n",
        "481": "then I'm going to compute theta J\n",
        "485": "transpose X I so that's the prediction\n",
        "491": "of user J's rating on Ruby I minus y I J\n",
        "493": "so that's the actual observed rating\n",
        "497": "squared and then let me just divide by\n",
        "500": "the number of movies that user J has\n",
        "503": "actually rated it as divided by 1 over 2\n",
        "507": "MJ and so this is just like the these\n",
        "509": "squares regressions it's just like you\n",
        "511": "know linear regression where we want to\n",
        "513": "choose the parameter vector theta J to\n",
        "515": "minimize this type of squared error term\n",
        "518": "and if you want we can also add in a\n",
        "520": "regularization term so it's a plus\n",
        "524": "lambda over 2 m and this is really 2 MJ\n",
        "526": "because there's a zip we have MJ\n",
        "529": "examples right because if user J has\n",
        "531": "rated that many movies the sort of like\n",
        "533": "we have that many data points on which\n",
        "535": "to fit the parameters theta J and then\n",
        "538": "let me add in my usual regularization\n",
        "543": "term here of theta J k squared as usual\n",
        "546": "this sum this room k equals 1 through n\n",
        "549": "so here theta J is going to be an N plus\n",
        "552": "1 dimensional vector where in our\n",
        "555": "earlier example n was equal to 2 the\n",
        "557": "more broadly more generally n is the\n",
        "559": "number of features we have per movie and\n",
        "561": "so as usual we don't regularize over\n",
        "563": "theta 0 we don't rewrite over the bias\n",
        "565": "terms those sums from k equals 1 through\n",
        "570": "n so if you minimize this as a function\n",
        "572": "of theta J you get a good solution you\n",
        "574": "get a pretty good estimate of a\n",
        "577": "parameter vector theta J with which to\n",
        "579": "make predictions for user J's movie\n",
        "582": "ratings for recommender systems I'm\n",
        "584": "going to change this notation a little\n",
        "586": "bit so to simplify the subsequent math\n",
        "587": "I'm actually going to get rid of this\n",
        "590": "term MJ so that's just a constant right\n",
        "593": "so I can delete it without changing the\n",
        "595": "value of theta J I get out of this\n",
        "596": "optimization so if you imagine taking\n",
        "599": "this whole equation taking this whole\n",
        "601": "expression the x MJ get rid of that\n",
        "602": "Const\n",
        "604": "and whether minimizes I should still get\n",
        "606": "the same value of theta J as before so\n",
        "609": "just to repeat what we wrote on the\n",
        "611": "previous slide here's our optimization\n",
        "613": "objective in order to learn theta J\n",
        "615": "which is the parameter for user J we're\n",
        "618": "going to minimize over theta J of this\n",
        "620": "optimization objectives so this is our\n",
        "623": "usual squared error term and then this\n",
        "626": "is our regularization term now of course\n",
        "628": "in building a recommender system we\n",
        "630": "don't just want to learn parameters for\n",
        "631": "a single user we want to learn\n",
        "633": "parameters for all of the all about\n",
        "636": "users so I have n subscript new users so\n",
        "637": "I want to learn all of these parameters\n",
        "641": "and so what I'm going to do is take this\n",
        "643": "minimal take this optimization objective\n",
        "645": "and just add an extra summation there so\n",
        "648": "you know this expression here with the\n",
        "650": "one half on top against it's exactly the\n",
        "653": "same as what we have on top except there\n",
        "654": "now instead of just doing this for a\n",
        "657": "specific user theta J I'm going to sum\n",
        "660": "my objective over all of my users and\n",
        "663": "then minimize this overall optimization\n",
        "665": "objective minimizes overall cost\n",
        "669": "function and when I minimize this as a\n",
        "672": "function of theta 1 theta 2 up to theta\n",
        "675": "n you I will get a separate parameter\n",
        "678": "vector for each user and I can then use\n",
        "680": "that to make predictions for all of my\n",
        "683": "users but all of my n subscript new uses\n",
        "685": "so putting everything together\n",
        "688": "this was our optimization objective on\n",
        "691": "top and to give this thing a name or\n",
        "694": "just call this J of theta 1 dot dot\n",
        "697": "theta and user J as usual as my\n",
        "699": "optimization objective which I'm trying\n",
        "703": "to minimize next in order to actually do\n",
        "705": "the minimization if you were to derive\n",
        "706": "the gradient descent up\n",
        "708": "is these are the equations that you will\n",
        "712": "get so you take theta J K and subtract\n",
        "714": "from an alpha which is the learning rate\n",
        "716": "times these terms over here on the right\n",
        "718": "so we're slightly different cases when K\n",
        "721": "equals zero in one case no to 0 because\n",
        "723": "our regularization term here regular\n",
        "726": "Rises only the values of theta J K for K\n",
        "728": "not equal 0 so we don't regularize theta\n",
        "732": "0 so the slightly different updates for\n",
        "735": "k equals 0 k naught equals 0 and this\n",
        "738": "term over here for example is just the\n",
        "740": "partial derivative with respect to your\n",
        "746": "parameter that of your optimization\n",
        "750": "objective right and so you know this is\n",
        "753": "just gradient descent but um and I've\n",
        "755": "already computed the derivatives applied\n",
        "760": "them into here and if this and of these\n",
        "762": "gradient descent updates look a lot like\n",
        "764": "what we had for linear regression that's\n",
        "766": "because these are essentially the same\n",
        "768": "as linear regression the only minor\n",
        "770": "difference is that for linear regression\n",
        "772": "you know we had these 1 over m terms\n",
        "777": "it's really 1 would have been 1 over m j\n",
        "779": "but because earlier when we are deriving\n",
        "781": "the optimization objective we got rid of\n",
        "783": "dis that's why we don't have this 1 over\n",
        "786": "m term but otherwise it's really some of\n",
        "788": "my training examples of\n",
        "792": "vo ever times XK plus that\n",
        "794": "regularization term plus that term that\n",
        "796": "the regularization term contributes to\n",
        "799": "the derivative and so if you're using\n",
        "801": "gradient descent here's how you can\n",
        "803": "minimize the cost function J to learn\n",
        "805": "all the parameters and using these\n",
        "807": "formulas for the derivatives if you want\n",
        "809": "you can also plug them into a more\n",
        "810": "advanced optimization algorithm like\n",
        "812": "conjugate gradients or l-bfgs or what\n",
        "814": "have you and use that to try to minimize\n",
        "818": "the cost function J as well so hopefully\n",
        "819": "you now know how you can apply\n",
        "821": "essentially a variation on linear\n",
        "824": "regression in order to predict different\n",
        "826": "movie ratings by different users this\n",
        "828": "particular algorithm is called a\n",
        "830": "content-based recommendations or a\n",
        "832": "content-based approach because we assume\n",
        "834": "that we have available to us features\n",
        "836": "for the different movies and so we're\n",
        "838": "features that capture what is the\n",
        "840": "content of these movies of how\n",
        "842": "romantical this movie how much action in\n",
        "843": "this movie and we're really using\n",
        "846": "features of the content of the movies to\n",
        "849": "make our predictions but for many movies\n",
        "851": "we don't actually have such features or\n",
        "854": "maybe very difficult to get such\n",
        "856": "features for all of our movies or for\n",
        "857": "all of whatever items we're trying to\n",
        "860": "sell and so in the next video we'll\n",
        "862": "start to talk about an approach to\n",
        "863": "recommender systems that isn't\n",
        "865": "content-based and does not assume that\n",
        "868": "we have someone else giving us all of\n",
        "869": "these features for all of the movies in\n"
    },
    "9tbxsbQ2Z0A": {
        "0": " \n",
        "1": "in the previous video we talked about\n",
        "4": "the photo OCR pipeline and how that\n",
        "6": "works in which we would take an image\n",
        "9": "and pass the image through a sequence of\n",
        "11": "machine learning components in order to\n",
        "13": "try to read the text that appears in an\n",
        "16": "image in this video I'd like to tell you\n",
        "18": "a bit more about how the individual\n",
        "20": "components of the photo OCR pipeline\n",
        "22": "works in particular most of this video\n",
        "24": "will center around the discussion of\n",
        "26": "what's called a sliding windows\n",
        "32": "classifier the first stage of the photo\n",
        "34": "OCR pipeline was text detection where we\n",
        "36": "would look at an image like this and\n",
        "38": "tried to find the regions of text that\n",
        "40": "appear in this image text detection is\n",
        "42": "an unusual problem in computer vision\n",
        "45": "because depending on the length of the\n",
        "47": "text you trying to find these rectangles\n",
        "49": "that you try to find can have different\n",
        "52": "aspect ratios so in order to talk about\n",
        "54": "detecting things and images let's start\n",
        "57": "with a simpler example of a pedestrian\n",
        "59": "detection and we'll then later go back\n",
        "62": "to apply the ideas from that would\n",
        "63": "develop in pedestrian detection and\n",
        "66": "apply them to text attention so in\n",
        "68": "pedestrian detection you want to take an\n",
        "71": "image that looks like this and find the\n",
        "72": "individual pedestrians that appear in\n",
        "74": "the image so that's one pedestrian that\n",
        "76": "we found there's a second one a third\n",
        "78": "one the fourth one the fifth one and the\n",
        "80": "sixth one and this problem is maybe\n",
        "82": "slightly simpler than text detection\n",
        "84": "just for the reason that the aspect\n",
        "87": "ratio of most pedestrians are pretty\n",
        "89": "similar and so we can just use a fixed\n",
        "91": "aspect ratio for these rectangles that\n",
        "93": "we try to find so by aspect ratio I mean\n",
        "95": "the ratio between the height and the\n",
        "97": "width of these rectangles is look\n",
        "98": "they're all the same for different\n",
        "101": "pedestrians but for text detection you\n",
        "104": "know the height to width ratio is this\n",
        "106": "difference for different lines of text\n",
        "108": "although for pedestrian detection the\n",
        "109": "pedestrians can be different distances\n",
        "111": "away from the camera and so the height\n",
        "114": "of these rectangles can be different\n",
        "116": "depending on how far away they are but\n",
        "118": "the aspect ratio is the same in order to\n",
        "120": "build a pedestrian detection system\n",
        "122": "here's how you can go about it let's say\n",
        "125": "that we decide to standardize on this\n",
        "128": "sort of aspirational 82 by 3rd\n",
        "131": "six and we could have chosen some\n",
        "133": "rounded number like 80 by 40 or\n",
        "136": "something but 82 by 36 seems over what\n",
        "138": "we would do is then go and collect a\n",
        "140": "large training set of positive and\n",
        "141": "negative examples so here are examples\n",
        "144": "of 82 by 36 image packages that do\n",
        "146": "contain pedestrians and here are\n",
        "149": "examples of images that do not on this\n",
        "152": "slide I've shown 12 positive examples of\n",
        "154": "y equals 1 and 12 negative examples with\n",
        "157": "y equals 0 in a more typical pedestrian\n",
        "159": "detection application we may have\n",
        "161": "anywhere from a thousand training\n",
        "163": "examples up to maybe 10,000 training\n",
        "165": "examples or even more if you can get\n",
        "167": "even larger training sets and what you\n",
        "170": "can do is then train a neural network or\n",
        "173": "some other learning algorithm to take as\n",
        "176": "input and an image patch of dimension a\n",
        "180": "2 by 36 and to classify Y and to\n",
        "181": "classify that image patch as either\n",
        "185": "containing a pedestrian or not so this\n",
        "187": "gives you a way of applying supervised\n",
        "189": "learning in order to take an image patch\n",
        "191": "and determine whether or not a\n",
        "193": "pedestrian appears in that image patch\n",
        "196": "now let's say we get a new image a test\n",
        "198": "set image like this and we want to try\n",
        "200": "to find the pedestrians to the pier in\n",
        "202": "this image what we will do is start by\n",
        "205": "taking a rectangular patch of this image\n",
        "207": "like that shown up here so that's maybe\n",
        "210": "a 82 by 36 patch of this image and we'll\n",
        "212": "run that image patch it through our\n",
        "214": "classifier to determine whether or not\n",
        "216": "there is a pedestrian in that image\n",
        "218": "patch and hopefully our classifier will\n",
        "220": "return y equals 0 for that patch this is\n",
        "222": "no pedestrian magus we didn't take that\n",
        "224": "green rectangle and we slide it over a\n",
        "227": "bit and then run that new image patch to\n",
        "229": "our classifier to decide if there's a\n",
        "231": "pedestrian there and having done that we\n",
        "233": "then slide the window further direct and\n",
        "235": "run that patch through the classifier\n",
        "238": "again the amount by which you ship the\n",
        "241": "rectangle over each time is a parameter\n",
        "243": "that sometimes called the step size of\n",
        "244": "the parameter\n",
        "246": "sometimes also called the stride\n",
        "250": "parameter and if you step this over one\n",
        "252": "pixel at a time so you use a step size\n",
        "254": "or stride of one that usually performs\n",
        "257": "best that is more competition expensive\n",
        "259": "and so using a step size of maybe four\n",
        "261": "pixels at a time or eight pixels at a\n",
        "262": "time or some large number of pixels\n",
        "264": "might be more common since you're\n",
        "266": "they're moving the rectangle a little\n",
        "269": "bit more each time but so using this\n",
        "271": "process you continue stepping with the\n",
        "273": "rectangle over to the right a bit at a\n",
        "274": "time and running each of these patches\n",
        "277": "through the classifier until eventually\n",
        "282": "as you slide this window over the\n",
        "283": "different locations in the image first\n",
        "285": "starting with the first row and then\n",
        "289": "with a further rows in the image you\n",
        "291": "would then run all of these different\n",
        "293": "image patches at some step sizes um\n",
        "297": "stride through your classifier now that\n",
        "300": "was a pretty small rectangle that all we\n",
        "301": "don't need to take pedestrians up as one\n",
        "304": "specific size what we do Nix is then\n",
        "306": "start to look at larger image patches so\n",
        "309": "now let's take larger image patches like\n",
        "311": "those shown here and run those through\n",
        "313": "the classifier as well and by the way\n",
        "314": "the classifier as well and by the way\n",
        "315": "when I say take a larger image patch\n",
        "318": "what I really mean is well when you take\n",
        "319": "an image patch like this what you're\n",
        "321": "really doing is taking that image patch\n",
        "325": "and resizing it down to 82 by 36 a so\n",
        "328": "take this larger patch and resize it to\n",
        "330": "be a smaller image and then there's this\n",
        "332": "smaller resize image that is what you\n",
        "334": "would pass through your crossfire to try\n",
        "335": "to decide that there's a pedestrian in\n",
        "338": "that patch and finally you can do this\n",
        "342": "at even larger skills and run that\n",
        "345": "sliding windows to the end and after\n",
        "347": "this whole process hopefully your\n",
        "349": "algorithm will detect what are the\n",
        "351": "pedestrians didn't appear in this image\n",
        "354": "so that's how you train a supervised\n",
        "356": "learning classifier and then use a\n",
        "358": "sliding windows classifier use a sliding\n",
        "360": "windows detector in order to find\n",
        "363": "pedestrians in an image let's now return\n",
        "366": "to the text detection example and talk\n",
        "368": "about that stage in our photo OCR\n",
        "370": "pipeline where our goals define the text\n",
        "372": "regions in the image\n",
        "375": "similar to pedestrian detection we can\n",
        "377": "come up with a labeled training set with\n",
        "379": "positive examples and negative examples\n",
        "382": "with positive examples corresponding to\n",
        "384": "regions where text appeared so instead\n",
        "386": "of trying to detect pedestrians we're\n",
        "388": "now trying to detect text and so the\n",
        "390": "positive examples are going to be\n",
        "391": "patches of images where there is text\n",
        "393": "and negative example is going to be\n",
        "395": "patches of images where there isn't text\n",
        "398": "having trained this classifier we can\n",
        "401": "now apply it to a new image into a test\n",
        "404": "set image so here's the image that we've\n",
        "406": "been using as an example now let's say\n",
        "409": "we run for this example going to run a\n",
        "411": "sliding windows cross bar at just one\n",
        "412": "fixed skill just for purpose of\n",
        "414": "illustration meaning that I'm going to\n",
        "417": "use just one rectangle size but let's\n",
        "419": "say I run my little sliding windows\n",
        "421": "classifier on here lots of little image\n",
        "425": "patches like this if I do that what I\n",
        "427": "end up with is a result like this where\n",
        "431": "the white regions show where my text\n",
        "432": "detection system thinks it has found\n",
        "435": "text and so the axis of these two\n",
        "437": "figures are the same so you know there's\n",
        "439": "a region up here corresponds the region\n",
        "441": "up here and so the fact that this is\n",
        "444": "black up here this represents that the\n",
        "445": "classifier does not think has found any\n",
        "446": "classifier does not think has found any\n",
        "448": "text up there whereas the fact that\n",
        "450": "there's a lot of white stuff here that\n",
        "452": "reflects that the classifier things has\n",
        "454": "found a bunch of text over there on the\n",
        "456": "image well I've done on this image on\n",
        "459": "the lower left is actually use white to\n",
        "460": "show where the classify things that has\n",
        "463": "found text and the different shades of\n",
        "465": "grey correspond to the probability that\n",
        "467": "was output by the classifier so lighter\n"
    },
    "Am9fhp2Q91o": {
        "0": " \n",
        "2": "by now you've seen all of the main\n",
        "4": "pieces of the recommender system\n",
        "6": "algorithm or the collaborative filtering\n",
        "9": "algorithm in this video I want to just\n",
        "11": "share one loss implementational detail\n",
        "14": "namely mean normalization which can\n",
        "16": "sometimes just make the algorithm work a\n",
        "19": "little bit better to motivate the idea\n",
        "23": "of mean normalization let's consider an\n",
        "26": "example of where there's a user that has\n",
        "29": "not written any movies so in addition to\n",
        "31": "our four users Alice Bob Carol and Dave\n",
        "34": "have added a fifth user Eve who has\n",
        "37": "invaded any movies let's see what our\n",
        "38": "collaborative filtering algorithm will\n",
        "41": "do on this user let's say that n is\n",
        "44": "equal to two and so we're going to learn\n",
        "46": "two features and we're going to have to\n",
        "49": "learn a parameter vector theta five\n",
        "52": "which is going to be in R to remember\n",
        "54": "this now vectors in the heart and not\n",
        "55": "our n plus one when we learn the\n",
        "57": "parameter vector theta five for our user\n",
        "61": "number five Eve so if we look in the\n",
        "62": "first term in this optimization\n",
        "65": "objective well the user Eve hasn't\n",
        "67": "raised in any movies and so you know\n",
        "70": "there are no movies there are no movies\n",
        "73": "for which R IJ is equal to one for the\n",
        "76": "user Eve and so this first term plays no\n",
        "77": "role at all in determining theta 5\n",
        "79": "because there are no movies that Eve is\n",
        "82": "rated and so the only term that affects\n",
        "85": "theta 5 is this term and so we're saying\n",
        "89": "that we want to choose vector theta 5 so\n",
        "91": "that the last regularization term is as\n",
        "92": "long as possible\n",
        "94": "in other words we want to minimize this\n",
        "99": "lambda over 2 theta 5 subscript 1\n",
        "102": "squared plus\n",
        "106": "theta 5 subscript 2 squared so that's\n",
        "108": "the component of the regularization term\n",
        "111": "that corresponds to use a 5 and of\n",
        "114": "course if your goal is to minimize this\n",
        "116": "term then what you're going to end up\n",
        "120": "with is just theta 5 equals 0 0 because\n",
        "122": "the regularization term is encouraging\n",
        "127": "us to set parameters close to 0 and if\n",
        "129": "there is no data to try to pull the\n",
        "131": "parameters away from 0 because this\n",
        "134": "first term is it doesn't affect theta 5\n",
        "136": "groups end up with theta 5 equals the\n",
        "139": "vector of all zeros and so when we go to\n",
        "142": "predict how user 5 of a tiny movie we\n",
        "147": "have that theta 5 transpose x I for any\n",
        "149": "I that's just going to be equal to 0 and\n",
        "152": "so because theta 5 is 0 for any value of\n",
        "154": "x this inner product is going to equal 0\n",
        "156": "and what we're going to have therefore\n",
        "159": "is that when I predict that Eve is going\n",
        "162": "to rate every single movie with zero\n",
        "165": "stars but this doesn't seem very useful\n",
        "166": "does it I mean if you look at the\n",
        "169": "different movies you know love that laws\n",
        "171": "this first movie a couple people rated\n",
        "173": "it five stars and four you know the even\n",
        "176": "those dear Souls vs. karate is someone\n",
        "178": "rated it 5 stars so some people do like\n",
        "181": "some movies it seems kind of not useful\n",
        "182": "to just predict that Eve was going to\n",
        "185": "bring everything zero stars and in fact\n",
        "187": "if we're predicting that Eve is going to\n",
        "189": "rate everything zero stars we also don't\n",
        "191": "have any good way of recommending any\n",
        "193": "movies to her because you know all of\n",
        "195": "these movies are getting exactly the\n",
        "197": "same predicted rating for Eve so there's\n",
        "199": "no one movie with a higher predicted\n",
        "202": "rating that we could recommend to her so\n",
        "203": "that's not very good\n",
        "206": "the idea of me normalization will let us\n",
        "209": "fix this problem so here's how it works\n",
        "212": "as before let me group all of my movie\n",
        "214": "ratings into this matrix wise we'll just\n",
        "216": "take all of these ratings and group them\n",
        "219": "into this matrix Y and just call them\n",
        "220": "over here of all question marks\n",
        "222": "corresponds to Eve\n",
        "225": "not having read any movies now to\n",
        "226": "perform mean normalization what I'm\n",
        "228": "going to do is compute the average\n",
        "231": "rating that each Ruby obtains and I'm\n",
        "233": "going to store that in a vector that we\n",
        "236": "so the first movie got two five star and\n",
        "238": "two zero star ratings so the average of\n",
        "240": "that is a two point five star rating you\n",
        "242": "know second movie and an average of two\n",
        "244": "point five star send so on then the\n",
        "247": "final movie has zero zero five zero and\n",
        "250": "the average of zero zero five zero that\n",
        "252": "averages out to an average of one point\n",
        "254": "two five rating and then what I'm going\n",
        "256": "to do is look at all the movie ratings\n",
        "259": "and I'm going to subtract off the mean\n",
        "262": "rating so this first element five I'm\n",
        "264": "going to subtract off of two point five\n",
        "267": "Omega zero two point five and the second\n",
        "269": "element five subtract off it 2.5 unit\n",
        "272": "2.5 and then B is zero zero subtract off\n",
        "273": "two point five then we get minus two\n",
        "276": "point five minus two point five in other\n",
        "278": "words what I'm going to do is take my\n",
        "280": "matrix that movie ratings they take this\n",
        "284": "Y matrix and subtract from each row the\n",
        "287": "average rating for that movie so what\n",
        "289": "I'm doing is I'm just normalizing each\n",
        "291": "movie to have an average rating of zero\n",
        "292": "movie to have an average rating of zero\n",
        "294": "and so just one last example if you look\n",
        "297": "at this last row these zero zero five\n",
        "299": "zero we're going to subtract one point\n",
        "301": "two five and so I end up with these\n",
        "304": "values over here okay so now and of\n",
        "305": "course the question marks there question\n",
        "310": "mark and so each movie in this new\n",
        "313": "matrix Y has an average rating of zero\n",
        "315": "what I'm going to do then is take this\n",
        "316": "what I'm going to do then is take this\n",
        "318": "set of ratings and use it with my\n",
        "320": "collaborative filtering algorithm so I'm\n",
        "322": "going to pretend that this was the data\n",
        "325": "that I had gotten through my users will\n",
        "326": "pretend that these were the actual\n",
        "328": "ratings I had gotten from the users and\n",
        "330": "I'm going to use this as my data set\n",
        "334": "with wish to learn my parameters theta J\n",
        "338": "and my features X I from these mean\n",
        "340": "normalized review\n",
        "343": "when I want to make predictions of movie\n",
        "345": "ratings what I'm going to do is the\n",
        "347": "for a user J on movie I I'm gonna\n",
        "355": "predict theta J transpose X i where X\n",
        "356": "and theta are the parameters have\n",
        "358": "learned from this me normalize data set\n",
        "361": "but because on the data set I had\n",
        "363": "subtracted off the means in order to\n",
        "365": "make prediction on movie I I'm going to\n",
        "368": "need to add back in the mean and so I'm\n",
        "371": "going to add back in mu I and so that's\n",
        "373": "going to be our prediction where my\n",
        "374": "training data are subtracted up all the\n",
        "377": "means and so when I make predictions I\n",
        "379": "need to add back in these means we rifle\n",
        "383": "movie I and so specifically for a user\n",
        "386": "of five which is Eve the same argument\n",
        "388": "as the previous slide store applies in\n",
        "390": "the sense that Eve had not rated any\n",
        "393": "movies and so the learn parameter for\n",
        "395": "user five is still going to be equal to\n",
        "400": "zero zero and and so what we're going to\n",
        "402": "get then is that on a particular movie I\n",
        "406": "we're going to predict for Eve theta v\n",
        "412": "transpose X I plus add back in mu I and\n",
        "414": "so this first component is going to\n",
        "417": "equal zero if theta v is equal to zero\n",
        "419": "and so a movie I we're gonna end up\n",
        "422": "predicting mu I and this this actually\n",
        "425": "make sense and use it on Ruby one we're\n",
        "427": "going to protect Eve rates in 2.5 on\n",
        "429": "movie to we're going to predict Eve\n",
        "432": "racer 2.5 on movie three we're putting\n",
        "434": "greater than 2 and so on and this\n",
        "436": "actually makes sense because it says\n",
        "438": "that if Eve hasn't written any movies we\n",
        "440": "just don't know anything about this new\n",
        "442": "user Eve what we're going to do is just\n",
        "445": "predict for each of the movies whatever\n",
        "449": "the average rating that those movie got\n",
        "452": "finally as an aside in this video we\n",
        "454": "talked about mean normalization where we\n",
        "455": "normalized\n",
        "459": "each row of the matrix Y to be mean 0 in\n",
        "461": "case you have some movies with no\n",
        "463": "ratings so there's analogous to a user\n",
        "465": "who hasn't written anything but in case\n",
        "467": "you have some movies with no ratings you\n",
        "469": "can also play with their versions of the\n",
        "471": "algorithm where you are normalized to\n",
        "473": "different columns to have mean 0 instead\n",
        "475": "of normalizing the rows that means 0\n",
        "477": "although that's maybe less important\n",
        "479": "because if you really have a movie with\n",
        "481": "no ratings maybe you just shouldn't\n",
        "484": "recommend that movie to anyone anyway\n",
        "489": "and so no taking taking care of the kids\n",
        "491": "of a user who hasn't written anything\n",
        "493": "might be more important than taking care\n",
        "495": "of the case of a movie that hasn't\n",
        "498": "gotten a single rating\n",
        "500": "so to summarize that's how you can do\n",
        "503": "mean normalization as a solid\n",
        "504": "pre-processing step for collaborative\n",
        "506": "filtering depending on your data set\n",
        "508": "this might sometimes make your\n",
        "510": "implementation work just a little bit\n"
    },
    "B-Ks01zR4HY": {
        "0": " \n",
        "1": "in this video we'll talk about the\n",
        "4": "normal equation which for some linear\n",
        "6": "regression problems will give us a much\n",
        "8": "better way to solve for the optimal\n",
        "11": "value of the parameters theta concretely\n",
        "13": "so far the algorithm that we've been\n",
        "15": "using for linear regression is gradient\n",
        "18": "descent where in order to minimize the\n",
        "21": "cost function J of theta we would take\n",
        "23": "this iterative algorithm that takes many\n",
        "26": "steps so that multiple iterations of\n",
        "28": "gradient descent to converge to the\n",
        "29": "global minimum\n",
        "32": "in contrast the normal equation will\n",
        "35": "give us a method to solve for theta\n",
        "37": "analytically so there rather than\n",
        "39": "needing to run this iterative algorithm\n",
        "41": "we can instead just solve for the\n",
        "43": "optimal value for theta all in one go so\n",
        "46": "that in basically one step you get to\n",
        "50": "the optimal value right there it turns\n",
        "53": "out the normal equation method has some\n",
        "55": "advantages and some disadvantages but\n",
        "57": "before we get to that and talk about\n",
        "59": "when you should use it let's get some\n",
        "61": "intuition about what this method does\n",
        "64": "for this explanatory example let's\n",
        "66": "imagine let's take a very simplified\n",
        "68": "cost function J of theta that's just a\n",
        "71": "function of a real number theta so for\n",
        "74": "now imagine that theta is just a scalar\n",
        "76": "value or that theta is just a real value\n",
        "78": "is just a number rather than a vector\n",
        "80": "imagine that we have a cost function J\n",
        "82": "there's a quadratic function of this\n",
        "85": "real valued parameter theta so J of\n",
        "86": "real valued parameter theta so J of\n",
        "88": "theta looks like that well how do you\n",
        "90": "minimize a quadratic function for those\n",
        "91": "of you that know a little bit of\n",
        "94": "calculus you may know that the way to\n",
        "96": "minimize a function is to take\n",
        "100": "derivatives and to set diverters equal\n",
        "102": "to 0 so if you take the derivative of J\n",
        "104": "with respect to the parameter theta you\n",
        "106": "get some formula which I'm not going to\n",
        "108": "derive and then you set that derivative\n",
        "112": "equal to 0 and this allows you to solve\n",
        "115": "for the value of theta that minimizes J\n",
        "118": "of theta that was the simpler case of\n",
        "122": "when theta is just a real number in the\n",
        "124": "problem that we're interested in theta\n",
        "126": "is no longer just a real number but\n",
        "129": "instead is this n plus 1 dimensional\n",
        "133": "ramit er vector and a cost function J is\n",
        "136": "a function of this vector value of theta\n",
        "138": "0 through theta M and the cost function\n",
        "140": "looks like this sum square cost function\n",
        "143": "on the right how do we minimize this\n",
        "146": "cost function J calculus actually tells\n",
        "149": "us that if you that one way to do so is\n",
        "152": "to take the partial derivative of J with\n",
        "155": "respect to every parameter theta J in\n",
        "159": "turn and then to set all of these 0 if\n",
        "162": "you do that and you solve for the values\n",
        "165": "of theta 0 theta 1 up to theta n then\n",
        "167": "this will give you the values of theta\n",
        "170": "that minimize the cost function J well\n",
        "171": "if you actually work through the\n",
        "173": "calculus and work through the solution\n",
        "175": "to the parameters theta 0 through theta\n",
        "179": "n the derivation ends up being someone\n",
        "181": "involved and what I'm going to do in\n",
        "184": "this video is actually do not go through\n",
        "186": "the derivation which is kind of long and\n",
        "188": "kind of involved but what I want to do\n",
        "190": "is just tell you what you need to know\n",
        "192": "in order to implement this process so\n",
        "194": "that you can solve for the values of the\n",
        "196": "Thetas that corresponds to where the\n",
        "199": "partial derivative is equal to 0 or\n",
        "202": "alternatively or equivalently the values\n",
        "203": "of Thetas that minimize the cost\n",
        "206": "function J of theta and I realize some\n",
        "208": "of the comments I made may have made\n",
        "209": "more sense only to those of you that\n",
        "212": "know more familiar calculus so but if\n",
        "214": "you don't know if you're less familiar\n",
        "216": "of calculus don't worry about it I'm\n",
        "217": "just going to tell you what you need to\n",
        "219": "know in order to implement this\n",
        "221": "algorithm and get it to work for the\n",
        "223": "example that I want to use as a running\n",
        "227": "example let's say that I have M equals 4\n",
        "231": "training examples in order to implement\n",
        "234": "this normal equation method what I'm\n",
        "235": "going to do is the following\n",
        "236": "going to do is the following\n",
        "238": "I'm going to take my data set so here\n",
        "240": "here are my four training examples in\n",
        "241": "this case let's assume that you know\n",
        "243": "these four examples is all the data I\n",
        "247": "have ok what I'm going to do is take my\n",
        "250": "data set and add an extra column that\n",
        "254": "corresponds to my extra feature x0 that\n",
        "256": "still always takes on this value of 1\n",
        "259": "what I'm going to do is I'm then going\n",
        "261": "to construct a matrix called capital X\n",
        "262": "that's a major\n",
        "265": "exactly contains all of the features for\n",
        "269": "my training data so completely here is\n",
        "273": "my here all my features I'm going to\n",
        "275": "take all those numbers and put them into\n",
        "278": "this matrix X ok so just you know copy\n",
        "282": "the data over one column at a time and\n",
        "284": "then I'm going to do something similar\n",
        "286": "for for Y so I'm going to take the\n",
        "287": "values I'm trying to predict and\n",
        "292": "construct now a vector like so and call\n",
        "299": "that a vector Y so X is going to be a M\n",
        "306": "by n plus 1 dimensional matrix and Y is\n",
        "314": "going to be a M dimensional vector where\n",
        "316": "m is the number of training examples and\n",
        "319": "n is n is the number of features I have\n",
        "321": "n plus 1 because of this extra feature\n",
        "326": "x0 that I had finally if you take your\n",
        "327": "matrix action you take your vector Y and\n",
        "330": "if you just compute this and set theta\n",
        "333": "to be equal to X transpose X inverse\n",
        "336": "times X transpose Y this will give you\n",
        "339": "the value of theta that minimizes your\n",
        "342": "cost function there was a lot that\n",
        "344": "happened on the slide and I work through\n",
        "346": "it using one specific example of one\n",
        "348": "data set let me just write this out in a\n",
        "351": "slightly more general form and then let\n",
        "353": "me just and later on in this video let\n",
        "355": "me explain this equation a little bit\n",
        "356": "me explain this equation a little bit\n",
        "357": "more in case um is not yet entirely\n",
        "361": "clear how to do this in the general case\n",
        "363": "let's say we have M training examples so\n",
        "369": "it's 1 y 1 up to XM ym and n features so\n",
        "371": "each of my training examples X I may\n",
        "373": "look like vector like this there's an N\n",
        "377": "plus 1 dimensional feature vector the\n",
        "379": "way I'm going to construct the matrix X\n",
        "384": "this is also called the design matrix is\n",
        "388": "as follows each training example gives\n",
        "390": "me a feature vector like this that say\n",
        "393": "this sort of n plus 1 dimensional vector\n",
        "395": "the way we're going to construct my\n",
        "397": "design matrix X is only construct a\n",
        "400": "matrix like this and what I'm going to\n",
        "403": "do is take the first training example so\n",
        "406": "that's a vector take us transpose so it\n",
        "408": "ends up being this you know long flat\n",
        "412": "thing and make X 1 transpose the first\n",
        "415": "row of my design matrix X then I'm going\n",
        "417": "to take my second training example x2\n",
        "420": "take the transpose of that and put that\n",
        "423": "as a second row of X and so on down\n",
        "427": "until my last training example take the\n",
        "429": "transpose of that and that's my last row\n",
        "432": "of my matrix X and so this makes my\n",
        "437": "matrix X and M by n plus 1 dimensional\n",
        "441": "matrix as a concrete example let's say I\n",
        "444": "have only one feature really only one\n",
        "446": "feature other than X 0 which is always\n",
        "449": "equal to 1 so if my features feature\n",
        "452": "vectors X I are equal to this 1 which is\n",
        "454": "x0 and then you know some real feature\n",
        "457": "like maybe the size of the house then my\n",
        "460": "design matrix X would be equal to this\n",
        "463": "for the first row I'm going to you know\n",
        "465": "basically take this and take this\n",
        "467": "transpose so I'm going to end up with 1\n",
        "472": "and then I'm x11 for the second row go\n",
        "477": "negative 1 and then X 1 2 and so on down\n",
        "481": "to 1 and then X 1m and thus this will be\n",
        "487": "a M by 2 dimensional matrix so that's\n",
        "491": "how I construct the matrix X and the\n",
        "493": "vector Y maybe and sometimes I might\n",
        "495": "read and everyone talk did you know that\n",
        "497": "as a vector there very often I'll just\n",
        "500": "write this as Y either way the vector Y\n",
        "503": "is obtained by taking all the labels all\n",
        "505": "the correct prices of houses in my\n",
        "507": "training set and just stacking them up\n",
        "511": "into an M dimensional vector and that's\n",
        "515": "why finally having constructed the\n",
        "517": "matrix X and the vector Y we then just\n",
        "521": "compute theta as X transpose X inverse\n",
        "523": "times\n",
        "525": "Transpo's lie\n",
        "528": "I just wanna make sure that this\n",
        "530": "equation makes sense to you and that you\n",
        "531": "know how to implement it so you know\n",
        "533": "what we'll let concretely what is this X\n",
        "537": "transpose X inverse well X transpose X\n",
        "540": "inverse is the inverse of the matrix X\n",
        "544": "transpose X concretely if you set if you\n",
        "549": "were to say set a to be equal to X\n",
        "551": "transpose times X so X transpose is a\n",
        "553": "matrix X transpose times X gives you\n",
        "555": "another matrix and we call that matrix a\n",
        "559": "then you know X transpose X inverse is\n",
        "561": "just you take this matrix a and you\n",
        "564": "invert it right let's equal to let's say\n",
        "566": "a inverse and so that's how you compute\n",
        "569": "this thing you compute X transpose X and\n",
        "572": "then you compute this inverse we haven't\n",
        "574": "yet talked about octave we'll do so in a\n",
        "577": "later set of videos but in the octave\n",
        "579": "programming language or similarly and\n",
        "581": "also the MATLAB programming language is\n",
        "584": "very similar the command to compute this\n",
        "588": "quantity I guess really a disk quantity\n",
        "590": "X transpose X inverse times X times y is\n",
        "594": "as follows in octave X prime is the\n",
        "596": "notation that you use to denote X\n",
        "600": "transpose and so this expression that's\n",
        "603": "boxing and read that's computing X\n",
        "607": "transpose times X P in is a function for\n",
        "609": "computing the inverse of a matrix so\n",
        "613": "this computes X transpose X inverse and\n",
        "616": "then you multiply that by X transpose\n",
        "619": "and you multiply that by Y so you end up\n",
        "623": "computing that formula which I didn't\n",
        "625": "prove but it is possible to show\n",
        "627": "mathematically even though I'm not gonna\n",
        "630": "do so here it that this formula gives\n",
        "632": "you the optimal value of theta in the\n",
        "634": "sense that this if you set theta equal\n",
        "637": "to this that's the value of theta that\n",
        "639": "minimizes the cost function J of theta\n",
        "642": "for linear regression one last detail in\n",
        "645": "an earlier video I talked about feature\n",
        "648": "scaling and the idea of getting features\n",
        "652": "to be on similar ranges of scales of\n",
        "654": "similar ranges of values of each other\n",
        "656": "if you're using this normal equation\n",
        "660": "method then feature scaling isn't\n",
        "663": "actually necessary and is actually okay\n",
        "666": "if say some feature x1 is between 0 and\n",
        "668": "1 and some feature exteriors between\n",
        "671": "your ranges from 0 to 1000 and some\n",
        "674": "feature x three ranges from 0 to 10 to\n",
        "677": "minus 5 and if you're using the normal\n",
        "679": "equation method this is okay and there's\n",
        "682": "no need to do feature scaling although\n",
        "684": "of course if you were using gradient\n",
        "686": "descent then feature scaling is still\n",
        "687": "important\n",
        "689": "finally when should you use gradient\n",
        "691": "descent and when should you use the\n",
        "692": "normal equation method\n",
        "694": "here are some of their advantages and\n",
        "697": "disadvantages let's say you have M\n",
        "700": "training examples and n features one\n",
        "702": "disadvantage of gradient descent is that\n",
        "704": "you need to choose the learning rate\n",
        "707": "alpha and often this means running up a\n",
        "708": "few times with different learning rate\n",
        "710": "alpha and then see what works best and\n",
        "712": "so that's a little extra work and extra\n",
        "715": "hassle another disadvantage of n descent\n",
        "717": "is that new it needs many iterations and\n",
        "719": "so depending on the details that could\n",
        "722": "make it slower although there's more to\n",
        "723": "the story as we'll see in a second as\n",
        "726": "for the normal equation you don't need\n",
        "728": "to choose any learning rate alpha so\n",
        "729": "that it makes it really convenient makes\n",
        "731": "it simpler to implement you just run it\n",
        "734": "and it just usually just works and you\n",
        "735": "don't need to iterate so you don't need\n",
        "737": "to plot J of theta or check the\n",
        "739": "conversion sort of take all those extra\n",
        "741": "steps so far the balance seems to favor\n",
        "744": "a normal equation the normal equation\n",
        "746": "here are some disadvantages of the\n",
        "748": "normal equation and some advantages of\n",
        "750": "grading descent gradient descent works\n",
        "752": "pretty well even when you have a very\n",
        "755": "large number of features so you know\n",
        "757": "even if you have millions of features\n",
        "759": "you can run gradient descent and they'll\n",
        "761": "be reasonably efficient and it will do\n",
        "766": "in contrast the normal equation in order\n",
        "767": "to solve for the parameters theta we\n",
        "770": "need to solve for this term we need to\n",
        "772": "compute this term x transpose x inverse\n",
        "777": "this matrix x transpose x that's an N by\n",
        "781": "n matrix if you have n features because\n",
        "783": "if you look at the dimensions of X\n",
        "785": "transpose dimension of X you multiply\n",
        "787": "and figure out what the dimensional\n",
        "790": "of the product is the matrix X transpose\n",
        "793": "X is an N by n matrix where n is the\n",
        "797": "number of features and phone on most\n",
        "799": "computer implementations the cost of\n",
        "803": "inverting a matrix grows roughly as the\n",
        "805": "cube of the dimension of the matrix so\n",
        "808": "computing this inverse costs roughly\n",
        "810": "order n cube times sometimes is slightly\n",
        "811": "faster than thank you but design you\n",
        "813": "know close enough to fall for our\n",
        "816": "purposes so if n is the number of\n",
        "818": "features is very large then computing\n",
        "820": "this quantity can be slow and the normal\n",
        "822": "equation that that can actually be much\n",
        "827": "slower so if n is large then I might\n",
        "829": "usually use gradient descent because we\n",
        "831": "don't want to pay this all the NQ time\n",
        "833": "but the Zen is relatively small then the\n",
        "834": "normal equation might give you a better\n",
        "837": "way to solve the parameters what a small\n",
        "840": "and large mean well if n is on the order\n",
        "842": "of a hundred then inverting one hundred\n",
        "844": "one hundred matrix is no problem by\n",
        "846": "modern computing standards if n is a\n",
        "849": "thousand I will still use a normal\n",
        "851": "equation method inverting a thousand by\n",
        "852": "thousand matrix is actually a really\n",
        "855": "fast in the modern computer if n is ten\n",
        "857": "thousand then I might start to wonder\n",
        "860": "inverting a 10,000 by 10,000 matrix thus\n",
        "862": "we get kind of slow and it might then\n",
        "865": "start to maybe lean in the direction of\n",
        "866": "gradient descent but maybe not quite N\n",
        "868": "equals 10,000 you can sort of inverted\n",
        "871": "10,000 by 10,000 matrix but it gets much\n",
        "873": "bigger than that then I would probably\n",
        "875": "use gradient descent so if N equals 10\n",
        "877": "to the 6 or even million features then\n",
        "880": "inverting a million by million matrix is\n",
        "882": "going to be very expensive and I would\n",
        "884": "definitely favor gradient descent of if\n",
        "887": "that many features so exactly how washio\n",
        "888": "set of features has to be before you\n",
        "891": "convert to greatness N and it was hard\n",
        "893": "to give a strict number but for me it's\n",
        "895": "usually around 10,000 but I might start\n",
        "898": "start to consider switching over to our\n",
        "900": "gradient descent or maybe some other\n",
        "902": "algorithms that I will talk about later\n",
        "905": "in the sauce to summarize so long as the\n",
        "907": "number of features is not to launch the\n",
        "909": "normal equation gives us a great\n",
        "911": "alternative method to solve for the\n",
        "914": "concretely so long as the number of\n",
        "916": "features is less than 1000 you know I\n",
        "918": "would use I would usually just\n",
        "919": "the normal equation method rather than\n",
        "922": "gradient descent to preview some ideas\n",
        "924": "that we'll talk about later in this\n",
        "926": "course as we get to the more complex\n",
        "928": "learning algorithm for example when we\n",
        "930": "talk about classification algorithms\n",
        "932": "like logistic regression algorithm we'll\n",
        "934": "see that those algorithms actually the\n",
        "936": "normal equation method actually do not\n",
        "939": "work for those more sophisticated\n",
        "941": "learning algorithms and we will have to\n",
        "942": "resort to gradient descent for those\n",
        "944": "algorithms so so gradient descent is a\n",
        "947": "very useful algorithm to know both for\n",
        "949": "linear regression when we have a large\n",
        "950": "number of features and for some of the\n",
        "953": "other algorithms that we'll see in this\n",
        "955": "course because for them the normal\n",
        "957": "equation method just doesn't apply it\n",
        "959": "doesn't work but for the specific model\n",
        "962": "of linear regression the normal equation\n",
        "966": "can give you a alternative then can be\n",
        "968": "much faster than gradient descent and so\n",
        "971": "depending on the details of your\n",
        "973": "algorithm it depend on the details of\n",
        "974": "your problem and how many features you\n",
        "976": "have both of these algorithms are well\n"
    },
    "BpgnnS7mKKU": {
        "0": " \n",
        "2": "in this video I'd like to talk about how\n",
        "5": "to evaluate a hypothesis that has been\n",
        "7": "learned by your algorithm in later\n",
        "9": "videos we'll build on this to talk about\n",
        "11": "how to prevent the problems of\n",
        "15": "overfitting and underfitting as well\n",
        "17": "when we fit the parameters of our\n",
        "19": "learning algorithm we think about\n",
        "22": "choosing the parameters to minimize the\n",
        "24": "training error one might think that\n",
        "26": "getting a really low value of training\n",
        "28": "error might be a good thing though we've\n",
        "30": "already seen that just because the\n",
        "32": "hypothesis has low training error that\n",
        "34": "doesn't mean it's necessarily a good\n",
        "36": "hypothesis and we've already seen the\n",
        "39": "example of how a hypothesis can over fit\n",
        "43": "and therefore fail to generalize to new\n",
        "45": "examples that are not in the training\n",
        "48": "set so how do you tell if the hypothesis\n",
        "49": "might be overfitting\n",
        "52": "in this simple example we could plot the\n",
        "55": "hypothesis H of X and just see what's\n",
        "58": "going on but in general for problems\n",
        "60": "with more features than just one feature\n",
        "62": "for problems with a large number of\n",
        "64": "features like these it becomes hard or\n",
        "67": "maybe impossible to plot what the\n",
        "70": "hypothesis function looks like and so we\n",
        "72": "need some other way to evaluate a\n",
        "74": "hypothesis the standard way to evaluate\n",
        "76": "a learned hypothesis is as follows\n",
        "78": "suppose we have a data set like this\n",
        "81": "here I've just shown 10 training\n",
        "83": "examples but of course usually we may\n",
        "85": "have dozens or hundreds or maybe\n",
        "87": "thousands of training examples in order\n",
        "89": "to make sure we can evaluate our\n",
        "91": "hypothesis what we're going to do is\n",
        "94": "split the data we have into two portions\n",
        "98": "the first portion is going to be our\n",
        "103": "usual training set and the second\n",
        "107": "portion is going to be our test set and\n",
        "110": "a pretty typical split of this of all\n",
        "112": "the data we have into a training set and\n",
        "116": "test set might be around say a 70% 30%\n",
        "119": "slit with more of the data going to\n",
        "122": "and relatively less the test set and so\n",
        "126": "now if we have some data set we might\n",
        "129": "assign only say 70% of the data to be\n",
        "132": "our training set where here M is as\n",
        "134": "usual on number of training examples and\n",
        "137": "the remainder of our data might then be\n",
        "139": "assigned to become our test sets and\n",
        "141": "here I'm going to use the notation M\n",
        "145": "subscript test denote to denote the\n",
        "149": "number of test examples and so in\n",
        "152": "general this subscript test is going to\n",
        "154": "denote examples they come from my test\n",
        "158": "set so that x1 subscript test comma y1\n",
        "162": "subscript test is my first test example\n",
        "164": "which I guess in this example might be\n",
        "167": "this example over here\n",
        "170": "so a fairly typical procedure for\n",
        "172": "training and testing a learning\n",
        "174": "algorithm may be linear regression would\n",
        "175": "be to first learn the parameter vector\n",
        "178": "from the training data here the training\n",
        "181": "data is only that first you know 70% of\n",
        "184": "the data set finally one last detail\n",
        "187": "whereas here I've drawn des as though\n",
        "189": "the first 70% goes to the training set\n",
        "192": "and the last 30% to the test set if\n",
        "194": "there is any sort of ordering to the\n",
        "197": "data it actually better to send a random\n",
        "200": "70% of your data than the training set\n",
        "202": "and a random 30% of your data to the\n",
        "205": "test set so if your data were already\n",
        "207": "randomly sorted you could just take the\n",
        "210": "first 70% and last 30% but if your data\n",
        "213": "were not random the order may be better\n",
        "215": "to randomly shuffle or to randomly\n",
        "217": "reorder the examples in your training\n",
        "219": "set before you know sending the first\n",
        "221": "studies 70% of the training set and the\n",
        "224": "last 30% of the test set here then is a\n",
        "227": "fairly typical procedure for how you\n",
        "229": "will train and test a learning algorithm\n",
        "231": "maybe linear regression first you learn\n",
        "234": "the parameters theta from the training\n",
        "236": "set so you minimize the usual training\n",
        "238": "error objective J of theta where J of\n",
        "242": "theta here was defined using that 70% of\n",
        "244": "all the data you have says only the\n",
        "247": "training data and then you will compute\n",
        "249": "the test error and I'm going to denote\n",
        "252": "the test error as J subscript test and\n",
        "254": "so what you do is you take your\n",
        "256": "parameter theta that you've learned from\n",
        "258": "the training set and plug it in here and\n",
        "262": "compute your test set error which I'm\n",
        "266": "going to write as follows so this is\n",
        "267": "basically\n",
        "272": "the average square error has measured it\n",
        "275": "on your test set it's pretty much what\n",
        "277": "you'd expect so I've run every test\n",
        "280": "example through your hypothesis with\n",
        "282": "parameter theta and just measure the\n",
        "286": "squared error the your hypothesis has on\n",
        "290": "your M subscript test test examples and\n",
        "292": "of course this is the definition of the\n",
        "296": "test set error if we are using linear\n",
        "299": "regression and using this squared error\n",
        "308": "metric how about if we were doing a\n",
        "310": "classification problem and say using\n",
        "313": "logistic regression instead in that case\n",
        "316": "the procedure for training and testing\n",
        "318": "say logistic regression is pretty\n",
        "320": "similar first we will learn the\n",
        "322": "parameters from the training data that\n",
        "324": "first 70% of the data and then they will\n",
        "327": "compute the test error as follows is the\n",
        "330": "same objective function as we always use\n",
        "332": "for logistic regression except that now\n",
        "335": "is defined using our M subscript test\n",
        "338": "test examples while this definition of\n",
        "340": "the test set error J subscript test is\n",
        "343": "perfectly reasonable sometimes there's\n",
        "345": "an alternative test set metric that\n",
        "348": "might be easier to interpret and that's\n",
        "350": "the miss classification error it's also\n",
        "352": "called the zero one miss classification\n",
        "355": "error with zero one denoting that you\n",
        "357": "either get an example right or you get\n",
        "360": "an example wrong here's what I mean let\n",
        "363": "me define the error of a prediction that\n",
        "370": "is H of X and given the label Y as equal\n",
        "374": "to one if my hypothesis outputs a value\n",
        "377": "greater than equal to five and Y is\n",
        "378": "equal to zero\n",
        "381": "or\n",
        "384": "if my hypothesis outputs a value less\n",
        "387": "than 0.5 and y is equal to 1 right so\n",
        "389": "both of these cases basically correspond\n",
        "393": "to if your hypothesis mislabeled the\n",
        "396": "example assuming you threshold it at 0.5\n",
        "398": "so either thought it was more likely to\n",
        "401": "be 1 but there was actually zero or your\n",
        "402": "hypothesis thought that was more likely\n",
        "404": "with zero but that Labour was actually 1\n",
        "408": "and otherwise we define this error\n",
        "412": "function to be 0 if only you know\n",
        "415": "hypothesis basically classified example\n",
        "419": "Y correctly we could then define the\n",
        "421": "test error using the misclassification\n",
        "427": "error metric to be one of M tests of sum\n",
        "431": "from I equals 1 to M subscript test of\n",
        "439": "the error of each of my tests comma Y\n",
        "442": "and so that's just some my way of\n",
        "444": "writing out that this is exactly the\n",
        "447": "fraction of the examples in my test set\n",
        "451": "that my hypothesis has mislabeled and so\n",
        "453": "that's the definition of the test set\n",
        "455": "error using the misclassification error\n",
        "457": "or the zero one misclassification error\n",
        "458": "or the zero one misclassification error\n",
        "460": "metric so that's the standard technique\n",
        "463": "for evaluating how good a learn\n",
        "466": "hypothesis in the next video we'll adapt\n",
        "469": "these ideas to helping us do things like\n",
        "471": "choose what features like degree of\n",
        "472": "polynomial to use with a learning\n",
        "473": "polynomial to use with a learning\n",
        "474": "algorithm or choose the regularization\n"
    },
    "CYlR9oYhYuY": {
        "0": " \n",
        "2": "in this video I want to give you more\n",
        "3": "practical tips for getting gradient\n",
        "6": "descent to work the ideas in this video\n",
        "8": "will center around the learning rate\n",
        "11": "alpha concretely here's the gradient\n",
        "13": "descent update rule and what I want to\n",
        "16": "do in this video is tell you about what\n",
        "18": "I think of as debugging and some tips\n",
        "20": "for making sure that gradient descent is\n",
        "23": "working correctly and ii want to tell\n",
        "25": "you how to choose the learning rate\n",
        "28": "alpha at least how i go about choosing\n",
        "30": "it here's something that i often do to\n",
        "32": "make sure the gradient descent is\n",
        "35": "working correctly the job of gradient\n",
        "37": "descent is to find the value of theta\n",
        "38": "for you that you know hopefully\n",
        "41": "minimizes the cost function J of theta\n",
        "45": "what I often do is therefore plot the\n",
        "48": "cost function J of theta as creating the\n",
        "51": "center runs so the x-axis here is the\n",
        "52": "number of iterations of gradient descent\n",
        "54": "and is creating descent runs you\n",
        "57": "hopefully get a plot that maybe look\n",
        "59": "like this\n",
        "61": "notice that the x-axis is number of\n",
        "64": "iterations previously we were looking at\n",
        "67": "plots of J of theta where the x-axis\n",
        "69": "where the horizontal axis was the\n",
        "71": "parameter vector theta but this is not\n",
        "74": "what this is concretely what this point\n",
        "78": "is is I'm going to run gradient descent\n",
        "81": "for 100 iterations and whatever value I\n",
        "85": "get for theta after 100 iterations I'm\n",
        "86": "going to get you know some value of\n",
        "87": "theta\n",
        "89": "after 100 iterations and I'm going to\n",
        "91": "evaluate the cost function J of theta\n",
        "94": "fold the value of theta I get after\n",
        "96": "hundred iterations and this vertical\n",
        "99": "height is the value of J of theta for\n",
        "101": "the value of theta I got after 100\n",
        "103": "iterations of green descent and this\n",
        "104": "iterations of green descent and this\n",
        "106": "point here that corresponds to the value\n",
        "110": "of J of theta for the theta that I get\n",
        "113": "after I've run gradient descent for 200\n",
        "116": "iterations so what this plot is showing\n",
        "118": "is assuring the value of your cost\n",
        "120": "function after each iteration of\n",
        "123": "gradient descent and a gradient descent\n",
        "125": "is working properly then J of theta\n",
        "134": " \n",
        "140": "and one useful thing that this sorta\n",
        "142": "plug can tell you also is that if you\n",
        "144": "look at the specific figure that I've\n",
        "146": "drawn you know it looks like by the time\n",
        "148": "you've gotten out two maybe three\n",
        "151": "hundred iterations between 300 and 400\n",
        "153": "iterations in this segment it looks like\n",
        "155": "J of theta hasn't gone down much more so\n",
        "158": "by the time you get to 400 iterations it\n",
        "160": "looks like this curve has flattened out\n",
        "163": "here and so way out here for one\n",
        "165": "generations it looks like gradient\n",
        "166": "descent has more or less converged\n",
        "169": "because your cost function isn't going\n",
        "171": "down much more so looking at this figure\n",
        "173": "can also help you judge whether or not\n",
        "177": "gradient descent has converged by the\n",
        "179": "way the number of iterations that\n",
        "181": "gradient descent takes to converge for a\n",
        "184": "particular application can vary a lot so\n",
        "186": "maybe for one application gradient\n",
        "188": "descent be conversion after just thirty\n",
        "190": "iterations for a different in for your\n",
        "192": "different application gradient descent\n",
        "195": "may take 3,000 iterations for another\n",
        "197": "algorithm for another learning algorithm\n",
        "200": "it may take three million iterations it\n",
        "202": "turns out to be very difficult to tell\n",
        "204": "in advance how many iterations gradient\n",
        "206": "descent needs to converge and it's\n",
        "208": "usually by plotting this sort of plot\n",
        "211": "plotting the cost function as as being\n",
        "212": "increasing number of iterations is\n",
        "214": "usually by looking at these parts like\n",
        "216": "that I try to tell if gradient descent\n",
        "219": "has converged it is also possible to\n",
        "221": "come up with automatic convergence test\n",
        "224": "namely to have an algorithm try to tell\n",
        "227": "you if gradient descent has converged\n",
        "229": "and here's that may be a pretty typical\n",
        "231": "example of an automatic convergence test\n",
        "234": "and such a test may declare convergence\n",
        "236": "if your cost function J of theta\n",
        "239": "decreases by less than some small value\n",
        "241": "some small value epsilon some small\n",
        "243": "value ten to the minus three in one\n",
        "247": "iteration but I find that usually\n",
        "249": "choosing what this threshold is is\n",
        "251": "pretty difficult and so in order to\n",
        "252": "check your gradient descent this\n",
        "255": "converge I actually tend to look at\n",
        "257": "plots like these like this figure on the\n",
        "259": "left rather than rely on and what\n",
        "262": "Mattek convergence test looking at this\n",
        "264": "older figure can also tell you or give\n",
        "266": "you an advance warning if maybe gradient\n",
        "268": "descent is not working correctly\n",
        "271": "concretely if you plot J of theta as a\n",
        "272": "function of the number of iterations\n",
        "275": "then and if you see a figure like this\n",
        "277": "where J of theta is actually increasing\n",
        "280": "then that gives you a clear sign that\n",
        "283": "gradient descent is not working and I\n",
        "285": "figured like this usually means that you\n",
        "288": "should be using a learning rate alpha if\n",
        "290": "J of theta is actually increasing the\n",
        "291": "J of theta is actually increasing the\n",
        "293": "most common cause for that is if you are\n",
        "295": "maybe trying to minimize the function\n",
        "299": "that Tom maybe looks like this but if\n",
        "301": "your learning rate is too big then if\n",
        "303": "you start off their grading descent may\n",
        "305": "overshoots the minimum and send you\n",
        "307": "there and if the learner is too big you\n",
        "309": "may overshoot again and then send you\n",
        "312": "there and so on so that you know well\n",
        "313": "what you really wanted was really start\n",
        "316": "here and for it to slowly go downhill\n",
        "319": "right but the learning rate is too big\n",
        "321": "then gradient descent can instead you\n",
        "324": "know keep on overshooting the minimum so\n",
        "326": "that you actually end up getting worse\n",
        "328": "and worse sort of getting to higher\n",
        "329": "values of the cost function J of theta\n",
        "332": "so do you end up with a plot like this\n",
        "334": "and if you see a plot like this the fix\n",
        "337": "usually is to just use a smaller value\n",
        "339": "of alpha oh and also of course make sure\n",
        "341": "that your code does not have a bug in it\n",
        "343": "but usually to watch a value of alpha\n",
        "346": "this is the most common but it could be\n",
        "350": "a common problem similarly sometimes you\n",
        "352": "may also see J of theta do something\n",
        "353": "like this and we go down for a while\n",
        "355": "then go up then go down for a while and\n",
        "358": "go up go down for a while go up and so\n",
        "360": "on and the fix for something like this\n",
        "363": "is also to use a smaller value of alpha\n",
        "365": "I'm not going to prove it here but under\n",
        "367": "mouth assumptions about the cost\n",
        "369": "function J which that's how true for\n",
        "371": "linear regression you can show what\n",
        "373": "mathematicians have shown that if your\n",
        "375": "learning rate alpha is small enough then\n",
        "378": "J of theta should decrease on every\n",
        "379": "single iteration so if this doesn't\n",
        "382": "happen probably means alpha is too big\n",
        "384": "mean you should send a smaller but of\n",
        "385": "course you also don't want your learning\n",
        "387": "rate to be too small because if you do\n",
        "388": "that if you were to do that then\n",
        "389": "gradient\n",
        "392": "can be slow to converge and if alpha\n",
        "395": "were too small you might end up starting\n",
        "397": "out hero say and you know end up taking\n",
        "399": "just minuscule minuscule baby steps\n",
        "402": "right and just taking a lot a lot of\n",
        "405": "taking a lot of iterations before you\n",
        "407": "finally get to the minimum and sort of\n",
        "409": "alpha is too small green descent can\n",
        "411": "make very slow progress and be slow to\n",
        "413": "converge to summarize if the learning\n",
        "414": "rate is too small\n",
        "417": "you can have a slow convergence problem\n",
        "420": "and then learning rate is too large J of\n",
        "421": "theta may not decrease on every\n",
        "424": "iteration and they may not even converge\n",
        "427": "in some cases the learning rate is too\n",
        "430": "large slow convergence is also possible\n",
        "434": "but the more common problem you see is\n",
        "436": "that just that J of theta may not\n",
        "439": "decrease on every iteration and in order\n",
        "441": "to debug all of these things often\n",
        "444": "plotting that J of theta as a function\n",
        "446": "of the number of innovations can help\n",
        "447": "you figure out what's going on\n",
        "450": "concretely what I actually do when I run\n",
        "452": "gradient descent is I would try a range\n",
        "454": "of values so just try running gradient\n",
        "456": "descent with a range of values for alpha\n",
        "459": "at like four point zero one 0.01 so\n",
        "461": "these are factor of ten differences and\n",
        "463": "for these different values of alpha just\n",
        "465": "plot J of theta as a function of number\n",
        "468": "of iterations and then pick the value of\n",
        "470": "alpha that you know seems to be causing\n",
        "474": "J of theta to decrease rapidly in fact\n",
        "476": "what I do actually isn't these steps of\n",
        "478": "ten so this is zero this is a scale\n",
        "481": "factor of ten of each step up what I\n",
        "484": "actually do is try this range of values\n",
        "489": "and so on where this is you know 0.001\n",
        "491": "I'll then increase the learning rate\n",
        "494": "threefold to get 0.03 and then this step\n",
        "496": "up this is another roughly three-fold\n",
        "500": "increase point from 0.03 to 0.01 and and\n",
        "504": "so these are roughly you know trying out\n",
        "507": "gradient descents with each value I try\n",
        "509": "being about 3x bigger than the previous\n",
        "512": "value so what I'll do is try range of\n",
        "513": "values until I have made sure that I\n",
        "515": "found one value that's too small and\n",
        "516": "make sure I found one value that's too\n",
        "519": "vomiting they're not sort of try to pick\n",
        "521": "the largest possible value or just\n",
        "523": "something slightly too small\n",
        "525": "then the largest reasonable value to\n",
        "528": "that found and when I do that usually\n",
        "530": "disgust me a good learning rate for my\n",
        "533": "problem and if you do this to hopefully\n",
        "535": "you'll be able to choose a good learning\n",
        "537": "rate for your implementation of gradient\n"
    },
    "Ccje1EzrXBU": {
        "0": " \n",
        "2": "sometimes people talk about support\n",
        "4": "vector machines as large margin\n",
        "7": "classifiers in this video I'd like to\n",
        "9": "tell you what that means and this will\n",
        "12": "also give us a useful picture of what an\n",
        "17": "SVM hypothesis may look like here's my\n",
        "19": "cost function for the support vector\n",
        "22": "machine we're here on the Left I've\n",
        "26": "plotted my costs one of Z function that\n",
        "28": "I use for positive examples and on the\n",
        "30": "right are plotted my cost zero of Z\n",
        "32": "function where I have here Z on the\n",
        "35": "horizontal axis now let's think about\n",
        "37": "what it takes to make these cost\n",
        "40": "functions small if you have a positive\n",
        "43": "example so if Y is equal to one then\n",
        "48": "cost one of Z is zero only when Z is\n",
        "50": "greater than or equal to one so in other\n",
        "52": "words if you have a positive example we\n",
        "54": "really want theta transpose X to be\n",
        "57": "greater than equal to one and conversely\n",
        "60": "if Y is equal to zero notice cos zero of\n",
        "63": "Z function then is only in this region\n",
        "66": "where a Z is less than equal to one that\n",
        "68": "we have that cost zero of Z is equal to\n",
        "71": "zero and this is an interesting property\n",
        "73": "of the support vector machine right\n",
        "75": "which is that if you have a positive\n",
        "78": "example so if Y is equal to one then all\n",
        "81": "we really need is that theta transpose X\n",
        "83": "is greater than equal to zero and that\n",
        "84": "will mean that we classified correctly\n",
        "86": "because you know if theta transpose X is\n",
        "88": "greater than zero or hypothesis will\n",
        "91": "predict zero and similarly if you have a\n",
        "92": "negative example then really all we want\n",
        "95": "is that theta transpose X is less than\n",
        "96": "zero and now make sure we got the\n",
        "98": "example right but a support vector\n",
        "100": "machine wants a bit more than that it\n",
        "102": "says you know don't just barely get the\n",
        "104": "example of our ignorance so then um\n",
        "107": "don't just have it just a little bit\n",
        "108": "bigger than zero what I really want is\n",
        "110": "for this to be quite a lot bigger than\n",
        "112": "zero say maybe greater than equal to 1\n",
        "114": "and then want this to be much less than\n",
        "116": "zero maybe I want it less than or equal\n",
        "120": "to minus one and so this builds in an\n",
        "122": "extra safety factor or safety margin\n",
        "124": "factor into the support vector machine\n",
        "126": "logistic regression does something\n",
        "129": "similar to of course but let's see what\n",
        "130": "happens so let's see what the\n",
        "132": "consequences of this are in the context\n",
        "135": "of a support vector machine concretely\n",
        "138": "what life is UNIX is consider a case\n",
        "141": "where we set this constant C to be a\n",
        "144": "very large value so let's imagine we set\n",
        "146": "C to be a very large value maybe a\n",
        "149": "hundred thousand some huge number let's\n",
        "151": "see what a support vector machine will\n",
        "155": "do if C is very very large then when\n",
        "157": "minimizing this optimization objective\n",
        "159": "we're going to be highly motivated to\n",
        "161": "choose a value so that this first term\n",
        "166": "is equal to zero so let's try to\n",
        "168": "understand the optimization problem in\n",
        "170": "the context of what would it take to\n",
        "172": "make this first term in the objective\n",
        "175": "equal to zero because you know maybe\n",
        "178": "we'll set C to some huge constant and\n",
        "180": "this will hope this should give us\n",
        "183": "additional intuition about what sort of\n",
        "185": "hypotheses a support vector machine\n",
        "188": "learns so we saw already that whenever\n",
        "191": "you have a training example with a label\n",
        "194": "of y equals 1 if you want to make that\n",
        "196": "first term 0 what you need is to find a\n",
        "199": "value of theta so that theta transpose X\n",
        "201": "I is greater than or equal to 1 and\n",
        "204": "similarly whenever we have an example\n",
        "208": "with label 0 in order to make sure that\n",
        "210": "it costs called 0 of Z in order to make\n",
        "213": "sure that causes 0 you need that theta\n",
        "216": "transpose X only is less than or equal\n",
        "221": "to minus 1 so if we think of our\n",
        "224": "optimization problem as now really\n",
        "226": "choosing parameters ensure that this\n",
        "229": "first term is equal to 0 what we're left\n",
        "230": "with is the following optimization\n",
        "233": "problem we're going to minimize that\n",
        "236": "first term zeros of C times 0 because\n",
        "237": "we're going to choose parameters so that\n",
        "241": "is equal to 0 plus 1/2 and then you know\n",
        "243": "that second term\n",
        "247": "and this first term that's C times zero\n",
        "248": "so let's just cross that up because I\n",
        "251": "know that's going to 0 p 0 and this will\n",
        "254": "be subject to the constraint that theta\n",
        "257": "transpose X I is greater than or equal\n",
        "264": "to 1 if Y I is equal to 1 and theta\n",
        "267": "transpose X I is less than or equal to\n",
        "272": "minus 1 whenever you have a negative\n",
        "275": "example and it turns out that when you\n",
        "277": "solve this optimization problem what you\n",
        "279": "minimize is as a function of the\n",
        "281": "parameters theta you get a very\n",
        "284": "interesting decision boundary completely\n",
        "287": "if you look at a data set like this with\n",
        "290": "positive and negative examples this data\n",
        "293": "is linearly separable by that I mean\n",
        "295": "that there exists you know a straight\n",
        "296": "line where there is as many different\n",
        "297": "straight lines they can separate the\n",
        "300": "positive and negative examples perfectly\n",
        "303": "for example here's one decision boundary\n",
        "305": "that separates the positive and negative\n",
        "307": "examples but somehow that doesn't look\n",
        "309": "like a very natural one right over by\n",
        "311": "drawing even worse one you know here's\n",
        "314": "another decision boundary that separates\n",
        "315": "the positive and negative examples were\n",
        "317": "just barely but neither of those seem\n",
        "320": "like particularly good choices the\n",
        "322": "support vector machine will instead\n",
        "325": "choose this decision boundary which I'm\n",
        "329": "drawing in black and that seems like a\n",
        "331": "much better decision boundary than\n",
        "333": "either of the ones that I drew in\n",
        "335": "magenta or in green the black line seems\n",
        "337": "like a more robust separator that no\n",
        "339": "just does a better job separating the\n",
        "342": "positive and negative examples and\n",
        "344": "mathematically what that does is this\n",
        "347": "black decision boundary has a larger\n",
        "349": "distance that distance is called the\n",
        "352": "margin but if I draw these two extra\n",
        "355": "blue lines we see that the black\n",
        "357": "decision boundary it has some you know\n",
        "360": "larger minimum distance from any of my\n",
        "362": "training examples whereas the magenta\n",
        "364": "and the green lines they come awfully\n",
        "365": "close to the training examples and it\n",
        "367": "that seems to do a less good job\n",
        "369": "separating the positive and negative\n",
        "372": "is that my plank line and so this\n",
        "376": "distance is called the margin of the\n",
        "381": "support vector machine and this gives\n",
        "384": "the SVM a certain robustness because it\n",
        "385": "tries to separate the data with as large\n",
        "389": "a margin as possible so the support\n",
        "391": "vector machine is sometimes also called\n",
        "394": "a large margin classifier and this is\n",
        "396": "actually a consequence of the\n",
        "398": "optimization problem we wrote down on\n",
        "400": "the previous slide I know that you might\n",
        "402": "be wondering how is it that the\n",
        "403": "optimization problem\n",
        "405": "I wrote down on the previous slide how\n",
        "407": "does that lead to this large margin\n",
        "409": "classifier I know I haven't explained\n",
        "412": "that yet and in the next video I'm going\n",
        "414": "to sketch a little bit of the intuition\n",
        "416": "about why that optimization problem\n",
        "419": "gives us this large margin classifier\n",
        "421": "but this is a useful picture to keep in\n",
        "422": "mind if you're trying to understand what\n",
        "425": "are the source of hypotheses that an SVM\n",
        "427": "will choose that is trying to separate\n",
        "429": "the positive and negative examples with\n",
        "432": "as big a margin as possible\n",
        "434": "want to say one last thing about large\n",
        "437": "margin classifiers and this intuition so\n",
        "439": "we worked out this large margin\n",
        "441": "classification setting in the case of\n",
        "444": "when ste that regularization constant\n",
        "445": "was very large right I think I said that\n",
        "448": "to a hundred thousand something so given\n",
        "450": "an a given a data set like this here\n",
        "453": "maybe we'll use that decision boundary\n",
        "455": "that separates the positive and negative\n",
        "456": "examples of large margin\n",
        "459": "now the SVM is actually slightly more\n",
        "461": "sophisticated than this large margin\n",
        "464": "view might suggest and in particular if\n",
        "466": "all you're doing is use a large margin\n",
        "469": "classifier then your learning algorithms\n",
        "472": "can be sensitive to outliers so let's\n",
        "474": "say add an extra positive example like\n",
        "476": "that shown you know on the on the screen\n",
        "479": "if you add one example then it seems as\n",
        "481": "if to separate the data above a large\n",
        "483": "margin you know maybe I'll end up\n",
        "486": "learning a decision boundary like that\n",
        "489": "right that is the magenta line and it's\n",
        "490": "really not clear that based on the\n",
        "492": "single outlier based on like a single\n",
        "494": "example it's really not clear that the\n",
        "496": "such a good idea to change my decision\n",
        "499": "boundary from the black one over to the\n",
        "503": "magenta one so if see if the\n",
        "505": "regularization parameter C were very\n",
        "508": "large then this is actually what the SVM\n",
        "510": "will do or change the decision boundary\n",
        "513": "from the black to the magenta one but if\n",
        "515": "C were reasonably small or if you were\n",
        "519": "to use the OC not too large then you\n",
        "521": "still end up with this black decision\n",
        "524": "boundary and of course if the data were\n",
        "526": "not linearly separable so you had some\n",
        "529": "positive examples in here or if you had\n",
        "532": "some negative examples in here then the\n",
        "534": "SVM will also do the right thing and so\n",
        "536": "this picture of a large margin\n",
        "539": "classifier that's really that's really\n",
        "541": "the picture that gives better intuition\n",
        "543": "only for the case of when the\n",
        "545": "regularization program to C is very\n",
        "548": "and then just remind you this\n",
        "551": "corresponds C plays a role similar to 1\n",
        "555": "over lambda where lambda is a mysterious\n",
        "557": "parameter we have previously and so is\n",
        "558": "parameter we have previously and so is\n",
        "559": "only of one over lambda is very large or\n",
        "563": "equivalently lambda is very small that\n",
        "565": "you end up with things like this magenta\n",
        "568": "decision boundary but in practice you\n",
        "570": "know when applying support vector\n",
        "573": "machines when C is not very very large\n",
        "575": "like that it can ignore of you can do a\n",
        "577": "better job ignoring a few outliers like\n",
        "579": "here and they'll also do fine and do\n",
        "581": "reasonable things even if your data is\n",
        "582": "more than the reciprocal\n",
        "585": "but tell when we talk about bias and\n",
        "587": "variance in the context of support\n",
        "588": "vector machines which will do a little\n",
        "590": "bit later hopefully all of these\n",
        "592": "trade-offs involving the regularization\n",
        "593": "parameter will become clearer at that\n",
        "596": "time so I hope that gives some intuition\n",
        "598": "about how these support vector machine\n",
        "601": "functions as a large margin classifier\n",
        "603": "that tries to separate the data with a\n",
        "605": "large margin technically this you know\n",
        "607": "picture of this view is true only when\n",
        "610": "the parameter C is very large which is a\n",
        "611": "useful way to think about support vector\n",
        "612": "useful way to think about support vector\n",
        "614": "machines there was one missing step in\n",
        "616": "this video which is there why is it that\n",
        "618": "that optimization problem we were wrote\n",
        "620": "down on these slides how does that\n",
        "621": "actually lead to the large margin\n",
        "623": "classifier I didn't do that in this\n",
        "626": "video in the next video I will sketch a\n",
        "628": "little bit more of the math behind that\n",
        "630": "to explain that that set of the\n",
        "632": "reasoning of how the optimization\n",
        "634": "problem we wrote out results in a large\n"
    },
    "CykIW9hFK24": {
        "0": " \n",
        "1": "in this in the next few videos I wanted\n",
        "3": "to tell you about a machine learning\n",
        "5": "application example or a machine\n",
        "7": "learning application case study centered\n",
        "10": "around an application called photo OCR\n",
        "12": "there are three reasons why I want to do\n",
        "14": "this first I wanted to show you an\n",
        "16": "example of how a complex machine\n",
        "18": "learning system to be pulled together\n",
        "21": "second let's talk about the concept of a\n",
        "23": "machine learning pipeline and how to\n",
        "25": "allocate resources when you try to\n",
        "27": "decide what to do next and this can be\n",
        "29": "either in the context of you working by\n",
        "31": "yourself on the big application or it\n",
        "33": "can be in the context of a team of\n",
        "35": "developers trying to build a complex\n",
        "37": "application together and third and\n",
        "40": "finally the photo OCR problem also gives\n",
        "42": "me an excuse to tell you about just a\n",
        "44": "couple more interesting ideas for\n",
        "46": "machine learning one is some apps some\n",
        "48": "ideas on how to apply machine learning\n",
        "50": "to computer vision problems and second\n",
        "52": "is the idea of artificial data synthesis\n",
        "54": "which we'll see in a couple videos so\n",
        "56": "let's start by talking about what is the\n",
        "59": "photo OCR problem\n",
        "63": "photo OCR stands for photo optical\n",
        "65": "character recognition with a group of\n",
        "68": "digital photography and more recently\n",
        "70": "the growth of cameras in our cell phones\n",
        "72": "we now have tons of digital pictures\n",
        "75": "that we take all over the place and one\n",
        "77": "of the things that has interested many\n",
        "79": "developers is how to get our computers\n",
        "81": "to understand the content of these\n",
        "83": "pictures a little bit better the photo\n",
        "86": "OCR problem focuses on how to get\n",
        "87": "computers to meet the text that appears\n",
        "91": "in images that we take given an image\n",
        "92": "like this it might be nice if the\n",
        "94": "computer can read the text in this image\n",
        "96": "so that if you're trying to look for\n",
        "98": "this picture again you could type in the\n",
        "99": "this picture again you could type in the\n",
        "101": "words you're Lulu bees and Teemo and\n",
        "103": "have it automatically pull up this\n",
        "105": "picture so that you're not spending lots\n",
        "107": "of time digging through your photo\n",
        "109": "collection of maybe hundreds or\n",
        "110": "thousands of pictures and all the\n",
        "113": "finder's so the photo will see our\n",
        "115": "problem does exactly this and it does so\n",
        "117": "in several steps for us given the\n",
        "119": "picture it has to look through the image\n",
        "121": "and detect where is their text in the\n",
        "124": "picture and after it has done that or if\n",
        "126": "it successfully does that it then has to\n",
        "127": "look at these\n",
        "130": "texts medians and actually meet the text\n",
        "132": "in those regions and hopefully it\n",
        "134": "release it correctly or come up with\n",
        "137": "these transcriptions of when this\n",
        "138": "detects that appears in the image\n",
        "141": "whereas OCR or optical character\n",
        "144": "recognition of scanned documents there's\n",
        "147": "a relatively easier problem doing OCR\n",
        "149": "from photographs today is still\n",
        "150": "considered a very difficult machine\n",
        "152": "learning problem and if you can do this\n",
        "155": "not only can this help our computers to\n",
        "158": "understand the content about outdoor\n",
        "160": "images whether there are also\n",
        "162": "applications like helping blind people\n",
        "164": "or for example if you could provide to a\n",
        "167": "blind person a camera they can look at\n",
        "169": "what's in front of them and just tell\n",
        "170": "them whether the words that may be on\n",
        "173": "the street sign in front of them and\n",
        "174": "there are also researchers today working\n",
        "177": "on photo OCR to help with car navigation\n",
        "180": "systems for example if magic if your car\n",
        "182": "could meet the street signs and help you\n",
        "185": "navigate to your destination in order to\n",
        "187": "perform photo OCR here's what we can do\n",
        "189": "first when go through the image and find\n",
        "191": "the regions where there's text in the\n",
        "195": "image so so here is one example of text\n",
        "197": "in the image that the photo OCR system\n",
        "201": "may find second given the rectangle\n",
        "204": "around that text region we can then do\n",
        "206": "character segmentation where we might\n",
        "209": "take this text box that says do antique\n",
        "212": "mall and try to segment it out into the\n",
        "214": "locations of the individual characters\n",
        "217": "and finally having segmented out into\n",
        "220": "individual characters we can then run a\n",
        "222": "classifier which looks at the images of\n",
        "224": "the individual characters and tries to\n",
        "226": "figure out that the first characters in\n",
        "227": "a the second characters an end the third\n",
        "230": "character as a t and so on so that up by\n",
        "232": "doing all that hopefully you can then\n",
        "235": "figure out that this phrase is Lulu B's\n",
        "237": "antique mall and similarly for some of\n",
        "239": "the other words that appear in that\n",
        "241": "image I should say that there's some\n",
        "243": "photo OCR systems that do even more\n",
        "245": "complex things like a bit of spelling\n",
        "247": "correction at the end so if for example\n",
        "250": "your character segmentation and\n",
        "252": "character classification system tells\n",
        "255": "you that the season worth c1\n",
        "259": "a-and I energy then you know a so the\n",
        "262": "spelling correction system might tell\n",
        "263": "you that this is probably the word\n",
        "266": "cleaning and your character\n",
        "267": "classification algorithm had just\n",
        "270": "mistaken the one for the L for one but\n",
        "272": "for the purpose of what we want to do in\n",
        "274": "this video let's ignore this last step\n",
        "277": "and just focus on the system that does\n",
        "279": "these three steps of text attention\n",
        "281": "interactive segmentation and parenting\n",
        "283": "classification a system like this is\n",
        "286": "what we call a machine learning pipeline\n",
        "290": "in particular here's a picture showing\n",
        "293": "the photo OCR pipeline we have an image\n",
        "295": "which is then fed to the text detection\n",
        "298": "system and having detected the text\n",
        "299": "regions we didn't segment out the\n",
        "301": "characters the individual characters in\n",
        "303": "the text then finally we recognize the\n",
        "306": "individual characters in many complex\n",
        "309": "machine learning systems these sorts of\n",
        "312": "pipelines are common where you can have\n",
        "314": "multiple modules in this example the\n",
        "315": "text affection character segmentation\n",
        "317": "character recognition modules each of\n",
        "319": "which may be a machine learning\n",
        "321": "component or sometimes it may not be a\n",
        "323": "machine learning component but to have a\n",
        "325": "set of modules that act one after\n",
        "327": "another on some piece of data in order\n",
        "330": "to produce the output you want which in\n",
        "332": "the photo OCR example is the final\n",
        "334": "transcription of the text appeared in\n",
        "337": "the image if you're designing a machine\n",
        "339": "learning system one of the most\n",
        "341": "important decisions will often be\n",
        "343": "exactly what is the pipeline that you\n",
        "345": "want to put together in other words\n",
        "347": "given the photo OCR problem how do you\n",
        "350": "break this problem down into a sequence\n",
        "352": "of different modules and you designed\n",
        "354": "that the pipeline and the performance of\n",
        "356": "each of the modules in your pipeline\n",
        "358": "will often have a big impact on define\n",
        "360": "the performance of your overall\n",
        "362": "algorithm if you have a team of\n",
        "364": "engineers working on the problem like\n",
        "366": "this is also very common to have\n",
        "368": "different individuals work on different\n",
        "371": "modules so I could easily imagine text\n",
        "373": "detection easily being the toss of\n",
        "374": "anywhere from one to five engineers\n",
        "377": "character segmentation maybe\n",
        "379": "one two five engineers and character\n",
        "380": "recognition being another one two five\n",
        "381": "recognition being another one two five\n",
        "383": "engineers and so having a pipeline like\n",
        "385": "this often offers a natural way to\n",
        "388": "divide up the workload amongst different\n",
        "390": "members of an engineering team as well\n",
        "392": "although of course all of this work\n",
        "394": "could also be done by just one person if\n",
        "398": " \n",
        "401": "in complex machine learning systems the\n",
        "403": "idea of a pipeline of a machine learning\n",
        "406": "pipeline is pretty pervasive and what\n",
        "408": "you just saw is a specific example of\n",
        "409": "you just saw is a specific example of\n",
        "411": "how a photo OCR pipeline might work in\n",
        "414": "the next few videos they'll tell you a\n",
        "415": "little bit more about this pipeline and\n",
        "417": "we'll continue to use this as an example\n",
        "420": "to illustrate I think a few more key\n"
    },
    "Dft1cqjwlXE": {
        "0": " \n",
        "1": "let's get started with a linear algebra\n",
        "4": "review in this video I want to tell you\n",
        "8": " \n",
        "12": "matrix is a rectangular array of numbers\n",
        "16": "written between square brackets so for\n",
        "19": "example here is a matrix I'm going to\n",
        "22": "write a left square bracket and then\n",
        "27": "write in a bunch of numbers and you know\n",
        "29": "these could be features for machine\n",
        "31": "learning problem or it could be data\n",
        "35": "from somewhere else but for example the\n",
        "37": "specific values don't matter and then\n",
        "39": "I'm going to close it with another right\n",
        "41": "bracket on the right and so that's one\n",
        "42": "matrix and you know here's another\n",
        "44": "example the matrix mr right\n",
        "48": "1 2 3 4 5 6 so matrix is just another\n",
        "50": "way for saying is a 2d or a 2\n",
        "55": "dimensional array and the other piece\n",
        "57": "terminology we need is that the\n",
        "59": "dimensional matrix is going to be\n",
        "63": "written as the number of rows times the\n",
        "65": "number of columns and matrix so\n",
        "68": "concretely this example on the left this\n",
        "73": "has 1 2 3 4 rows and it has 2 columns\n",
        "76": "and so this example on the left I'm\n",
        "81": "going to say this is a 4 by 2 matrix ok\n",
        "83": "because this number of rows by number of\n",
        "85": "columns or four rows two columns this\n",
        "87": "one on the right this matrix has 2 rows\n",
        "90": "so that's the first row that's the\n",
        "93": "second row and it has three columns\n",
        "96": "right that's the first column that's the\n",
        "100": "second column that's the third column so\n",
        "104": "this second Matrix we say it is a 2 by 3\n",
        "106": "matrix so we say that the dimension of\n",
        "111": "this matrix is 2 by 3 and sometimes you\n",
        "113": "also see this written out as in the case\n",
        "115": "of left you see this written out as our\n",
        "118": "4 by 2 or concretely what people will\n",
        "121": "sometimes say is that this matrix is an\n",
        "123": "element of the set our 4 by 2 so this\n",
        "125": "this thing here this just means the set\n",
        "127": "of all matrices that are of dimension 4\n",
        "129": "by 2 and this thing on the right\n",
        "131": "sometimes is written out as\n",
        "133": "Trix that's in our 2x3 so if you ever\n",
        "137": "see us use be 2 by 3 so if you ever see\n",
        "139": "something like this near our 4 by 2 or\n",
        "141": "r2 by 3 people are just referring to\n",
        "147": "matrices of a specific dimension next\n",
        "149": "let's talk about how to refer to\n",
        "152": "specific elements of the matrix and by\n",
        "154": "matrix elements are those matrix I just\n",
        "155": "mean the entries or the numbers inside\n",
        "158": "the matrix so in the standard notation\n",
        "161": "if a is this matrix here then a\n",
        "164": "subscript IJ is going to refer to the I\n",
        "168": "comma J entry meaning the entry in the\n",
        "170": "matrix is in the I've row and the J\n",
        "174": "column so for example a11 is we're going\n",
        "177": "to refer to the entry in the first row\n",
        "179": "and the first column so that's the first\n",
        "182": "row in the first column and so you know\n",
        "185": "a11 is going to be equal to 1 4 0 2\n",
        "189": "another example a 1 2 is going to refer\n",
        "193": "to the entry in the first row and the\n",
        "197": "second column and so a 1 2 is going to\n",
        "201": "be equal to 1 9 1 just comfortable quick\n",
        "205": "examples let's see a oh let's say a 3 2\n",
        "208": "is going to refer to the entry in the\n",
        "214": "third row and the second column right\n",
        "216": "because that's V 2 so that's equal to 1\n",
        "224": "4 3 7 and finally a 4 1 is going to\n",
        "226": "refer to you know this one right fourth\n",
        "232": "row first column is equal to 1 4 7\n",
        "234": "and if hopefully you won't believe you\n",
        "237": "were to write say what is a 4 3\n",
        "240": "well that refers to the fourth row and\n",
        "243": "the third column but you know this\n",
        "245": "matrix has no third column so this is\n",
        "247": "undefined you know or you can think of\n",
        "249": "this in an error so the\n",
        "252": "there's no such element as a a43 so you\n",
        "254": "know shouldn't be referring to a four\n",
        "258": "three so the matrix gives you a way of\n",
        "261": "letting you quickly organize index and\n",
        "263": "access lots of data and in case they\n",
        "265": "seem to be tossing up a lot of concepts\n",
        "267": "a lot of new notation very rapidly you\n",
        "269": "don't need to memorize all this but on\n",
        "272": "the course website where we have posted\n",
        "274": "the lecture notes we also have all of\n",
        "276": "these definitions written down so you\n",
        "279": "can always refer back you know either to\n",
        "280": "the slides post on the course website go\n",
        "282": "to the lecture notes if you forget right\n",
        "285": "a 4-1 is that which row which column is\n",
        "286": "that don't worry about memorizing\n",
        "288": "everything now you can always refer back\n",
        "290": "to the written materials on the course\n",
        "292": "website use that as a reference so\n",
        "294": "that's what the matrix is next let's\n",
        "297": "talk about what is a vector a vector\n",
        "299": "turns out to be a special case of a\n",
        "302": "matrix a vector is a matrix that has\n",
        "304": "only one column so if you've an N by 1\n",
        "308": "matrix then that's a remember right n is\n",
        "311": "the number of rows and 1 here is number\n",
        "313": "of columns so so matrix with just one\n",
        "316": "column is what we call a vector so\n",
        "319": "here's an example of a vector right\n",
        "322": "whether I guess I have N equals four\n",
        "325": "elements here so we also called this\n",
        "327": "thing now the term for this is that this\n",
        "331": "is a four dimensional vector just means\n",
        "335": "that this is a vector with four elements\n",
        "338": "with four numbers in it and just as knew\n",
        "339": "earlier four matrices you saw this\n",
        "341": "notation they are 3 by 2 through 4 into\n",
        "343": "3 by 2 matrices right for this vector\n",
        "346": "we're going to refer this to to this as\n",
        "349": "a vector in the set R 4 so this R 4\n",
        "352": "again just needs a set of all four\n",
        "357": "dimensional vectors next let's talk\n",
        "359": "about how to refer to elements of the\n",
        "363": "vector we're going to use the notation Y\n",
        "367": "I to refer to the I've element of the\n",
        "370": "vector Y so Y is this vector Y subscript\n",
        "373": "is the I'd element so y1 is the first\n",
        "377": "element is equal to 460 you know y2\n",
        "380": "to the second element right 2:32 there's\n",
        "383": "the first there's a second why three is\n",
        "386": "equal to three one five and so on and\n",
        "388": "only y1 through wife or are defined\n",
        "390": "because this is a four dimensional\n",
        "394": "vector also it turns out that they're\n",
        "396": "actually two conventions for how to\n",
        "399": "index into a vector and here they are\n",
        "402": "sometimes people will use one index and\n",
        "405": "sometimes zero index vectors so this\n",
        "408": "example on the left is a one index\n",
        "410": "vector where the elements we write as y\n",
        "414": "1 y 2 y 3 y 4 and this example of the\n",
        "416": "right is an example of a zero index\n",
        "419": "vector where we start the indexing of\n",
        "422": "the elements from zero so the elements\n",
        "426": "go from y 0 up to Y 3 and this is a\n",
        "428": "little bit like the arrays of some\n",
        "430": "programming languages right where the\n",
        "432": "arrays can either be indexed starting\n",
        "434": "from 1 so the first element of an array\n",
        "436": "is sometimes a y1 this one sequences\n",
        "438": "notation I guess and sometimes you know\n",
        "441": "this is 0 0 index depending on what\n",
        "444": "programming language you use so it turns\n",
        "447": "out that in most of math the one index\n",
        "450": "version is more common but for a lot of\n",
        "453": "machine learning applications zero index\n",
        "455": "vectors gives us a more convenient\n",
        "458": "notation so what you should usually do\n",
        "460": "is unless otherwise specified you should\n",
        "461": "is unless otherwise specified you should\n",
        "462": "assume that we're using one index\n",
        "465": "vectors in fact throughout the rest of\n",
        "467": "these videos on linear algebra review I\n",
        "470": "will be using one index vectors but just\n",
        "472": "be aware that when we talk about machine\n",
        "474": "learning applications sometimes you know\n",
        "475": "I'll explicitly say when we need to\n",
        "478": "switch to when we need to use zero index\n",
        "482": "vectors as well finally by convention\n",
        "484": "usually when writing matrices and\n",
        "487": "vectors most people will use uppercase\n",
        "490": "to refer to matrices so we reuse capital\n",
        "493": "makes capital letters like a b c you\n",
        "497": "know x to refer to matrices and usually\n",
        "501": "we'll use lowercase like a b XY to refer\n",
        "504": "to either numbers just real numbers or\n",
        "506": "scalars or two vectors but this isn't\n",
        "509": "always true but the system\n",
        "511": "common notation where we use your\n",
        "513": "lowercase Y to refer to vector and we\n",
        "515": "usually use uppercase to refer to a\n",
        "519": "matrix so you now know what are matrices\n",
        "522": "and vectors next we'll talk about some\n"
    },
    "EVeqrPGfuCY": {
        "0": " \n",
        "2": "in this video I want to start telling\n",
        "5": "you about how we represent neural\n",
        "6": "networks in other words how we represent\n",
        "9": "our hypothesis or how we represent our\n",
        "12": "model when using neural networks neural\n",
        "15": "networks were developed as simulating\n",
        "17": "neurons or networks of neurons in the\n",
        "20": "brain so to explain the hypothesis\n",
        "23": "representation let's start by looking at\n",
        "25": "what a single neuron in the brain looks\n",
        "27": "like your brain and mind is jam-packed\n",
        "30": "full of neurons like these and neurons\n",
        "32": "are cells in the brain and the two\n",
        "33": "are cells in the brain and the two\n",
        "34": "things they draw to draw attention to\n",
        "37": "are that first that the neuron has a\n",
        "40": "cell body like so and moreover the\n",
        "42": "neuron has number of input wires and\n",
        "44": "these are called the dendrites you think\n",
        "48": "of them as input wires and these receive\n",
        "51": "inputs from other locations and the\n",
        "53": "neuron also has an output wire called\n",
        "57": "the axon and this output wire is what it\n",
        "61": "uses to send signals to other neurons or\n",
        "65": "to send messages to other neurons so at\n",
        "68": "a simplistic level what a neuron is is a\n",
        "70": "computational unit that gets a number of\n",
        "73": "inputs through its input wires does some\n",
        "75": "computation and then it sends outputs\n",
        "78": "virus axon to other nodes or to other\n",
        "79": "neurons in the brain\n",
        "83": "here's an illustration of a group of\n",
        "85": "neurons the way that neurons communicate\n",
        "88": "with each other is with little pulses of\n",
        "90": "electricity they're also called spikes\n",
        "91": "but that just means a lower pulse of\n",
        "94": "electricity so here's one neuron and\n",
        "98": "what it does is if it wants to send a\n",
        "99": "message what it does is it sends the\n",
        "102": "little pulse of electricity various axon\n",
        "105": "to the to some difference neuron and\n",
        "108": "here this axon that is this open wire\n",
        "111": "connects to the input wire connects to\n",
        "113": "the dendrite of this second neuron over\n",
        "115": "here which then accepts this incoming\n",
        "118": "message that some computation\n",
        "120": "may in turn decide to send out its own\n",
        "122": "messages on its axon to other neurons\n",
        "126": "and this is the process by which all\n",
        "128": "human thoughts\n",
        "129": "happens is these neurons doing\n",
        "131": "computations and pulsing messages to\n",
        "134": "other neurons as a result of what other\n",
        "137": "inputs they got and by the way this is\n",
        "140": "how our senses and our muscles work as\n",
        "143": "well if you want to remove one you've\n",
        "144": "one of your muscles the way that works\n",
        "147": "is that the neuron may send these pulses\n",
        "149": "of electricity to your muscle and that\n",
        "152": "causes your muscles to contract and your\n",
        "155": "eyes it just if some sensor like your\n",
        "157": "eye wants to send a message to your\n",
        "159": "brain what it does is it senses pulses\n",
        "162": "of electricity to a neuron in your brain\n",
        "165": "like so in a neuro in a neural network\n",
        "167": "or rather in an artificial neural\n",
        "170": "network that we implement in a computer\n",
        "172": "we're going to use a very simple model\n",
        "174": "of what a neuron does we're going to\n",
        "177": "model a neuron as just a logistic unit\n",
        "180": "so when I draw a yellow circle like that\n",
        "183": "think of that as playing a role\n",
        "185": "analogous to maybe the body of a neuron\n",
        "190": "and we then feed the neuron a few inputs\n",
        "195": "various dendrites or input wires and the\n",
        "197": "neuron does some computation and output\n",
        "201": "some value on this output wire or\n",
        "204": "biological neuron to source the axon and\n",
        "206": "whenever I draw a diagram like this what\n",
        "208": "this means is that this represents a\n",
        "213": "computation of your H of x equals one\n",
        "215": "one plus e to the negative theta\n",
        "219": "transpose X where as usual X and theta\n",
        "221": "are our parameter vectors like certainly\n",
        "225": "so this is a very simple maybe vastly\n",
        "227": "oversimplified model of the computation\n",
        "229": "that the neuron does where it gets a\n",
        "232": "number of inputs x1 x2 x3 and it outputs\n",
        "239": " \n",
        "242": "when I draw a neural network usually\n",
        "245": "I'll draw only the input notes x1 x2 x3\n",
        "248": "sometimes one is useful to do so I'll\n",
        "252": "draw an extra node for x0 this x0 node\n",
        "255": "is sometimes called the bias unit or the\n",
        "259": "bias neuron but because x0 is always\n",
        "262": "equal to one sometimes I draw it and\n",
        "263": "sometimes I won't just depending on\n",
        "266": "whatever is more notationally convenient\n",
        "271": " \n",
        "274": "finally one last bit of terminology when\n",
        "277": "we talk about neural networks sometimes\n",
        "280": "we'll say that this is a neuron or an\n",
        "282": "artificial neuron with a sigmoid or a\n",
        "285": "logistic activation function so this\n",
        "287": "activation function in the neural\n",
        "289": "network terminology this is just another\n",
        "292": "term for that function for that\n",
        "295": "non-linearity g of z equals 1 over 1\n",
        "298": "plus e to the negative Z and whereas so\n",
        "300": "far up in calling theta the parameters\n",
        "303": "of the model I'll mostly continue to use\n",
        "305": "that terminology to calculate the\n",
        "307": "parameters but in neural networks in the\n",
        "309": "neural network literature sometimes you\n",
        "312": "might hear people talk about weights of\n",
        "314": "the model and weights just means exactly\n",
        "316": "the same thing as parameters of a model\n",
        "318": "but are mostly considered to use the\n",
        "321": "terminology parameters in these videos\n",
        "323": "but sometimes you may hear others use\n",
        "329": "wayde's terminology so this little\n",
        "334": "diagram represents a single neuron what\n",
        "338": "a neural network is is just a group of\n",
        "341": "these different neurons strung together\n",
        "344": "concretely here we have input units X 1\n",
        "348": "X 2 X 3 and once again sometimes we can\n",
        "351": "draw this extra node X 0 and sometimes\n",
        "354": "not so just draw that in here and here\n",
        "357": "we have 3 neurons which have written you\n",
        "359": "know 81 82 83 you have talked about\n",
        "362": "those indices later and once again you\n",
        "366": "know we can we want add in this a 0 and\n",
        "370": "add an extra bias unit there there's\n",
        "372": "always outputs the value of 1 and then\n",
        "374": "finally we have this third node at the\n",
        "377": "final layer and is the sterno that\n",
        "380": "outputs the value that our hypothesis H\n",
        "383": "of X computes to introduce a bit more\n",
        "386": "terminology in a neural network the\n",
        "388": "first layer this is also called the\n",
        "390": "input layer because this is where we\n",
        "394": "input our features X 1 X 2 X 3 the final\n",
        "395": "input our features X 1 X 2 X 3 the final\n",
        "397": "layer is also called the output layer\n",
        "400": "because that layer has the neuron this\n",
        "402": "one over here that outputs the final\n",
        "404": "value computed by our hypothesis and\n",
        "408": "then layer 2 in between this is called\n",
        "410": "the hidden layer the term hidden layer\n",
        "414": "isn't a great terminology but this India\n",
        "416": "tuition is that you know in supervised\n",
        "418": "learning what you get to see the inputs\n",
        "420": "you get to see the correct outputs\n",
        "422": "whereas the hidden layer are values you\n",
        "424": "don't get to observe in the training set\n",
        "426": "it's not X and it's not Y and so we call\n",
        "429": "those hidden and later on we'll see\n",
        "431": "neural networks with more than one\n",
        "434": "hidden layer but in this example we have\n",
        "436": "one input layer they are one one hidden\n",
        "438": "layer layer two and one output there\n",
        "440": "they are three but basically anything\n",
        "442": "that isn't an input layer and isn't an\n",
        "449": "upper layer is called a hidden layer so\n",
        "451": "I want to be really clear about what\n",
        "454": "this neural network is doing let's step\n",
        "456": "through the computational steps that are\n",
        "460": "embodied by this represented by this\n",
        "462": "diagram to explain the specific\n",
        "464": "computation is represented by a neural\n",
        "466": "network here's a little bit more\n",
        "468": "notation I'm going to use a superscript\n",
        "471": "J subscript Y to denote the activation\n",
        "472": "J subscript Y to denote the activation\n",
        "477": "of neuron I or of Units I in layer J so\n",
        "480": "concretely this a superscript to\n",
        "483": "subscript one that's the activation of\n",
        "486": "the first unit in layer two in our\n",
        "488": "hidden layer and by activation I just\n",
        "490": "mean you know the value that is computed\n",
        "493": "by and that is output by a specific in\n",
        "495": "addition on your network is parameterize\n",
        "498": "by these matrices theta a superscript j\n",
        "501": "where theta j is going to be a matrix of\n",
        "503": "ways controlling the function mapping\n",
        "506": "from one layer we do the first layer to\n",
        "507": "the second layer or for the second layer\n",
        "509": "to the\n",
        "511": "so here are the computations that are\n",
        "515": "represented by this diagram this first\n",
        "518": "hidden unit here passes value computed\n",
        "521": "as follows as a a21 is equal to the\n",
        "523": "sigmoid function of the sigmoid\n",
        "525": "activation function also called the\n",
        "528": "logistic activation function apply to\n",
        "532": "this sort of a linear combination of as\n",
        "536": "inputs and then this second hidden unit\n",
        "540": "has this activation value computed as\n",
        "544": "sigmoid of this and similarly for this\n",
        "546": "third hidden unit is computed by that\n",
        "551": "formula so here we have three input\n",
        "558": "units and three hidden units and so the\n",
        "561": "dimension of theta one which is the\n",
        "563": "matrix of parameters governing our\n",
        "565": "mapping from all three input units the\n",
        "568": "three hidden units theta one it is going\n",
        "574": " \n",
        "579": "theta one is going to be a three by four\n",
        "583": "dimensional matrix and more generally if\n",
        "587": "a network has SJ units in layer J and SJ\n",
        "590": "plus one units in layer J plus one then\n",
        "592": "a matrix theta J which governs the\n",
        "594": "function mapping from layer J 2 layer J\n",
        "597": "plus 1 that will have dimension SJ plus\n",
        "601": "1 by SJ plus 1 just be clear about this\n",
        "604": "notation right this is s subscript J\n",
        "607": "plus 1 and that's s subscript J and then\n",
        "610": "just whole thing plus 1 out of this\n",
        "612": "whole thing SJ plus 1 J so that's s\n",
        "623": "subscript J plus 1 plus bi so that's s\n",
        "629": "subscript J plus 1 by SJ plus 1 where\n",
        "631": "this plus 1 is not part of the subscript\n",
        "633": "ok so we've talked about what the three\n",
        "636": "hidden units do to compute their values\n",
        "640": "finally there's a loss of this final in\n",
        "642": "the upper layer we have one more unit\n",
        "645": "which confuse H of X and that's equal\n",
        "649": "can also be written as a 3-1 and that's\n",
        "653": "equal to this and you notice that I've\n",
        "655": "written this with a superscript 2 here\n",
        "657": "because theta superscript 2 is the\n",
        "660": "matrix of parameters or the matrix of\n",
        "662": "weights that controls the function that\n",
        "665": "map's from the hidden units that is the\n",
        "669": "layer 2 units to the one layer 3 unit\n",
        "672": "that is the output unit to summarize\n",
        "675": "what we've done is shown how a picture\n",
        "677": "like this over here defines an\n",
        "680": "artificial neural network which defines\n",
        "683": "a function H that maps from X's\n",
        "685": "input values to hopefully to some space\n",
        "688": "of predictions why and these hypotheses\n",
        "691": "are parametrized by parameters that I'm\n",
        "695": "denoting with a capital theta so that as\n",
        "696": "we vary theta we get different\n",
        "698": "hypotheses or get different functions\n",
        "701": "mapping say from X to Y\n",
        "704": "so this gives us a mathematical\n",
        "706": "definition of how to represent the\n",
        "709": "hypothesis in a neural network into the\n",
        "711": "next few videos what I'd like to do is\n",
        "713": "give you more intuition about what these\n",
        "716": "hypothesis representations do as well as\n",
        "719": "go through a few examples and talk about\n"
    },
    "Ev8YbxPu_bQ": {
        "0": " \n",
        "2": "in this video I'd like to start to talk\n",
        "4": "about clustering this will be exciting\n",
        "6": "because this is offers unsupervised\n",
        "8": "learning algorithm where we learn from\n",
        "10": "unlabeled data instead of from labelled\n",
        "13": "data so what is unsupervised learning I\n",
        "16": "briefly talked about unsupervised\n",
        "17": "learning at the beginning of the class\n",
        "20": "but it's useful to contrast it with\n",
        "22": "supervised learning so here's a typical\n",
        "24": "supervised learning problem where we're\n",
        "26": "given a labeled training set and the\n",
        "28": "goal is to find the decision boundary\n",
        "30": "that separates the positive labeled\n",
        "32": "examples and the negative label examples\n",
        "34": "so the supervised learning problem in\n",
        "36": "this case is given a set of labels to\n",
        "40": "fit a hypothesis to it in contrast in\n",
        "42": "the unsupervised learning problem we're\n",
        "44": "given data that does not have any labels\n",
        "47": "associated with it so we're given data\n",
        "49": "that looks like this here's a set a\n",
        "52": "point and then no labels and so our\n",
        "55": "training set is written just X 1 X 2 and\n",
        "58": "so on up to X M and we don't get any\n",
        "60": "labels Y and that's why the points\n",
        "62": "plotted up on the figure don't have any\n",
        "65": "labels of them so in unsupervised\n",
        "67": "learning what we do is we give this sort\n",
        "69": "of unlabeled training set to an\n",
        "71": "algorithm and we just ask the algorithm\n",
        "74": "find some structure in the data for us\n",
        "76": "given this data set one type of\n",
        "79": "structure we might have an algorithm is\n",
        "81": "that it looks like this data set has\n",
        "84": "points grouped into two separate\n",
        "88": "clusters and so an algorithm data finds\n",
        "89": "the clusters like the ones I've just\n",
        "92": "circled is called a clustering algorithm\n",
        "94": "and this will be our first type of\n",
        "96": "unsupervised learning although there\n",
        "98": "will be other types of unsupervised\n",
        "100": "learning algorithms that we'll talk\n",
        "102": "about later that finds other types of\n",
        "105": "structure or other types of patterns in\n",
        "107": "the data other than clusters so talk\n",
        "108": "about this afterwards we'll talk about\n",
        "111": "clustering so what is clustering good\n",
        "113": "for early in this class I already\n",
        "115": "mentioned a few applications one is\n",
        "117": "market segmentation where you may have a\n",
        "118": "database of customers and want to group\n",
        "121": "them into different market segments so\n",
        "124": "you can sell to them separately or serve\n",
        "126": "your different market segments better\n",
        "129": "social network analysis directly you\n",
        "130": "know groups have done this things like\n",
        "134": "looking at a group of people's social\n",
        "136": "network so things like Facebook Google+\n",
        "138": "or maybe information about who are the\n",
        "140": "people people that you email the most\n",
        "141": "frequently and who are the people that\n",
        "143": "they email the most frequently and to\n",
        "146": "find coherent to groups of people so\n",
        "147": "this would be another may be clustering\n",
        "149": "algorithm where you want to find other\n",
        "152": "coherent groups of friends in the social\n",
        "154": "network here's something that one of my\n",
        "155": "friends actually worked on which is um\n",
        "157": "use clustering to organize compute\n",
        "159": "clusters or to organize data centers\n",
        "161": "better because if you know which\n",
        "163": "computers in the data center and the\n",
        "165": "cluster tend to work together you can\n",
        "167": "use that to reorganize your resources\n",
        "170": "and how you layout into networks and how\n",
        "171": "you design your data center and\n",
        "174": "communications and lastly something that\n",
        "176": "actually another firm worked on using\n",
        "178": "clustering algorithms to understand\n",
        "180": "galaxy formation and using that to\n",
        "182": "understand how to understand\n",
        "186": "astronomical the example\n",
        "189": "so that's clustering which is our first\n",
        "191": "example of an unsupervised learning\n",
        "193": "algorithm in the next video we'll start\n",
        "196": "to talk about a specific clustering\n"
    },
    "F6GSRDoB-Cg": {
        "0": " \n",
        "1": "we previously defined the cost function\n",
        "4": "J in this video I want to tell you about\n",
        "6": "an algorithm called gradient descent for\n",
        "9": "minimizing the cost function J it turns\n",
        "11": "out gradient descent is a more general\n",
        "14": "algorithm and is used not only in linear\n",
        "16": "regression is actually used all over the\n",
        "18": "place in machine learning and later in\n",
        "20": "the class we'll use gradient descent to\n",
        "22": "minimize other functions as well not\n",
        "24": "just the cost function J for linear\n",
        "27": "regression so in this video I'm going to\n",
        "28": "talk about gradient descent for\n",
        "31": "minimizing some arbitrary function J and\n",
        "33": "then in later videos we'll take those\n",
        "36": "algorithm and apply it specifically to\n",
        "38": "the cost function J that we had defined\n",
        "42": "for linear regression so here's the\n",
        "44": "problem setup going to assume that we\n",
        "47": "have some function J of theta 0 comma\n",
        "49": "theta 1 maybe as a cost function from\n",
        "51": "linear regression maybe it's some other\n",
        "53": "function we want to minimize and we want\n",
        "55": "to come over now algorithm for\n",
        "57": "minimizing that as a function of j of\n",
        "61": "theta0 theta1 just as an aside it turns\n",
        "63": "out that gradient descent actually\n",
        "65": "applies to more general functions so\n",
        "67": "imagine if you have a function that's a\n",
        "70": "function of j OS theta 0 theta 1 theta 2\n",
        "74": "up to say something that N and you want\n",
        "77": "to minimize theta 0 you minimize over\n",
        "81": "theta0 up to theta n of this J of theta\n",
        "84": "0 up to theta n it turns out gradient\n",
        "86": "descent is an algorithm for solving of\n",
        "88": "this more general problem but for the\n",
        "91": "sake of brevity on for the sake of you\n",
        "93": "know succinct ins of notation I'm just\n",
        "94": "going to pretend I have only two\n",
        "97": "parameters throughout the rest of this\n",
        "99": "video here's the idea for gradients then\n",
        "101": "what we're going to do is we're going to\n",
        "105": "start off with some initial guesses for\n",
        "107": "theta 0 and theta 1 doesn't really\n",
        "109": "matter what they are but a common choice\n",
        "112": "will be we set theta 0 to state 0 and\n",
        "115": "set theta 1 to 0 just initialize them to\n",
        "117": "0 what we're going to do in gradient\n",
        "120": "descent is we'll keep changing theta 0\n",
        "122": "and say the 1 a little bit they try to\n",
        "125": "reduce J of theta 0 theta 1 until\n",
        "127": "hopefully we wind up at a minimum or\n",
        "129": "maybe a local\n",
        "132": "so let's see what let's see in pictures\n",
        "135": "what gradient descent does let's say you\n",
        "137": "try to minimize this function so notice\n",
        "139": "the axes this is a theta 0 theta 1 on\n",
        "141": "the horizontal axis and J is the\n",
        "143": "vertical axis and so the height of the\n",
        "146": "surface shows J and we want to minimize\n",
        "148": "this function so we're going to start\n",
        "151": "off with theta 0 theta 1 at some point\n",
        "154": "so imagine making some value for theta 0\n",
        "156": "theta 1 and that corresponds to starting\n",
        "159": "at some points on the surface of this\n",
        "160": "function okay so whatever value of theta\n",
        "162": "0 theta 1 gives you some point here\n",
        "165": "I didn't initialize them to 0 0 but you\n",
        "166": "know sometimes you neutralize it to\n",
        "170": "other values as well now I want you to\n",
        "173": "imagine that this figure shows a hole\n",
        "175": "imagine this is like the landscape of\n",
        "178": "some draw c-pop with you know two Hills\n",
        "181": "like so and I want you to imagine that\n",
        "183": "you are physically standing at that\n",
        "185": "point on the hill right on this little\n",
        "188": "red node on your Park in gradient\n",
        "189": "descent what we're going to do is we're\n",
        "192": "going to spin 360 degrees around just\n",
        "195": "look all around us and also if I were to\n",
        "197": "take a little baby step in some\n",
        "200": "direction and I want to go downhill as\n",
        "203": "quickly as possible what direction do I\n",
        "205": "take that little baby step in if I want\n",
        "206": "to go down if I sort of want to\n",
        "208": "physically walk down this hill as\n",
        "212": "rapidly as possible turns out that if\n",
        "213": "you're standing at that point on the\n",
        "215": "hill you look all around you find that\n",
        "216": "the best direction to take a little\n",
        "217": "the best direction to take a little\n",
        "219": "little step downhill is roughly that\n",
        "223": "direction okay and now you have this new\n",
        "225": "point on your hill you're going to again\n",
        "227": "look all around and then say what\n",
        "229": "direction should I step in order to take\n",
        "232": "a little baby step downhill and if you\n",
        "235": "do that and take another step you take a\n",
        "237": "step in that direction and then you keep\n",
        "239": "going you know from this new point you\n",
        "241": "look around taking decide what direction\n",
        "243": "will take you down hill most quickly\n",
        "246": "take another step another step and so on\n",
        "249": "until you converge to this a local\n",
        "252": "minimum down here here the dissenter is\n",
        "255": "an interesting property this first time\n",
        "257": "we ran gradient descent we were starting\n",
        "260": "at this point over here right starts it\n",
        "261": "at that point over here\n",
        "264": "now imagine we have initialized gradient\n",
        "266": "descent just a couple steps to the right\n",
        "268": "imagine with a nationalized gradient\n",
        "269": "descent with that points on the upper\n",
        "272": "right if you were to repeat this process\n",
        "273": "so stop at that point\n",
        "275": "look all around take a little step in\n",
        "277": "the direction of steepest descent you\n",
        "279": "would do that then look around take\n",
        "284": "another step and so on and if you\n",
        "286": "started just a couple steps to the right\n",
        "289": "gradient descent would have taken you to\n",
        "292": "this second local optimum over on the\n",
        "296": "right so if you have started this first\n",
        "297": "point you would have wondered about this\n",
        "299": "local optima but you started just a\n",
        "301": "little bit slightly different location\n",
        "303": "you would have wound up at a very\n",
        "306": "different local optimum and this is a\n",
        "308": "property of gradients in that will stay\n",
        "311": "a little bit more about later so that's\n",
        "315": "the intuition in pictures let's look at\n",
        "319": "the map this is the definition of the\n",
        "321": "gradient descent algorithm we're going\n",
        "325": "to just repeat the Li do this until\n",
        "328": "converging so we're going to update my\n",
        "331": "parameter theta J by you know taking\n",
        "333": "theta J and subtracting from it alpha at\n",
        "336": "times this term over here okay so let's\n",
        "338": "see a lot of details in this equation so\n",
        "342": "let me unpack some of it first this\n",
        "345": "notation here R colon equals going to\n",
        "348": "use colon equals to denote assignment so\n",
        "352": "the assignment operator so concretely if\n",
        "355": "I write a colon equals B what this means\n",
        "358": "is it means eel in a computer this means\n",
        "361": "take the value in B and use it to\n",
        "362": "overwrite whatever value is in so this\n",
        "364": "means zero set a to be equal to the\n",
        "367": "value of B it's its assignment and I can\n",
        "370": "also do a colon equals a plus one this\n",
        "372": "means take a and increase this value by\n",
        "376": "one whereas in contrast if I use the\n",
        "379": "equal sign then I write a equals B then\n",
        "385": "this is a truth assertion okay so if I\n",
        "388": "write a equals B then I'm asserting that\n",
        "390": "the value of a equals to the value of B\n",
        "392": "right so the left hand side that's the\n",
        "394": "computer operation\n",
        "395": "where you set the value of a to new\n",
        "397": "value the right hand side this is\n",
        "399": "asserting I'm just making a claim that\n",
        "401": "the values of a and B are the same and\n",
        "404": "so whereas I can write a colon equals a\n",
        "406": "plus 1/10 is increment a by 1 hopefully\n",
        "409": "I won't ever write a equals a plus 1\n",
        "411": "because this is wrong write a and a plus\n",
        "413": "1 can never be equal to the same values\n",
        "416": "okay so there's the first part of the\n",
        "418": "definition um\n",
        "425": "this alpha here is a is a number that is\n",
        "429": "called the learning rate and what alpha\n",
        "432": "does is it basically controls how big a\n",
        "434": "step we take downhill with creating\n",
        "437": "descent so if alpha is very large then\n",
        "438": "that corresponds to a very aggressive\n",
        "440": "gradient descent procedure when we're\n",
        "442": "trying to take huge steps downhill and\n",
        "444": "if alpha is very small then we're taking\n",
        "447": "little baby steps downhill and I'll come\n",
        "449": "back and say more about this later about\n",
        "452": "how to set alpha and so on and finally\n",
        "456": "this term here that's a derivative term\n",
        "458": "I don't want to talk about it right now\n",
        "461": "but I will derive this derivative term\n",
        "464": "and tell you exactly what this is later\n",
        "466": "okay and some of you will be more\n",
        "468": "familiar with calculus in others but\n",
        "470": "even if you aren't familiar with\n",
        "471": "calculus don't worry about it I'll tell\n",
        "473": "you what you need to know about this\n",
        "477": "term here now there's one more subtlety\n",
        "479": "about gradient descent which is\n",
        "481": "ingrained in descent we're going to\n",
        "483": "update you know theta zero and theta one\n",
        "485": "right so this update takes place for J\n",
        "487": "equals zero and J equals ones you can\n",
        "491": "update J theta zero and update theta one\n",
        "494": "and the subtlety of how you implement\n",
        "496": "gradient descent is for this expression\n",
        "500": "right for this update equation you want\n",
        "508": "to simultaneously update\n",
        "513": "theta 0 and theta 1 what I mean by that\n",
        "514": "is that you know in this equation we're\n",
        "516": "going to update theta 0 co 2 equals\n",
        "518": "theta 0 minus something and update theta\n",
        "520": "1 code equals theta 1 minus something\n",
        "523": "and the way to implement is is you\n",
        "526": "should compute the right hand side right\n",
        "529": "compute that thing for theta 0 and theta\n",
        "533": "1 and then simultaneously at the same\n",
        "536": "time update theta 0 and theta 1 okay so\n",
        "539": "let me say what I mean by that this is a\n",
        "541": "correct implementation of written\n",
        "543": "meaning on simultaneous updates I'm\n",
        "545": "going to set 10 0 equals at set 10 1\n",
        "546": "going to set 10 0 equals at set 10 1\n",
        "547": "equals that so the basic computer right\n",
        "549": "hand sides and then having computed the\n",
        "551": "right hand sides and store them in two\n",
        "553": "variables x 0 and temp 1 I'm going to\n",
        "554": "year update theta 0 and theta 1\n",
        "556": "simultaneously that's a correct\n",
        "559": "implementation in contrast here's an\n",
        "562": "incorrect implementation that does not\n",
        "565": "do a simultaneous update so in this\n",
        "568": "incorrect implementation we compute 10 0\n",
        "571": "and then we update theta 0 let me\n",
        "573": "compute tenth one and then we update 10\n",
        "576": "1 and the difference between the right\n",
        "577": "hand side and the left hand side\n",
        "579": "implementations is that if you look down\n",
        "582": "here you look at this step if by this\n",
        "584": "time you've already updated theta zero\n",
        "588": "then you would be using the new value of\n",
        "590": "theta 0 you know to compute this\n",
        "593": "derivative term and so this gives you a\n",
        "596": "different value of 10 1 then the left\n",
        "599": "hand side right because you've now\n",
        "601": "plugged in the new value of theta 0 into\n",
        "603": "this equation and so this on the right\n",
        "604": "hand side is not a correct\n",
        "607": "implementation of gradient descent ok so\n",
        "609": "I don't want to say why you need to do\n",
        "611": "the simultaneous updates so it turns out\n",
        "614": "that you know the way gradient descent\n",
        "616": "is usually implemented we say more but\n",
        "618": "later it actually turns out to be more\n",
        "620": "natural to implement the simultaneous\n",
        "622": "updates and when people talk about\n",
        "624": "gradient descent they always mean\n",
        "626": "simultaneous update if you implement the\n",
        "629": "non simultaneous update it turns out it\n",
        "631": "will probably work anyway but this is\n",
        "633": "this album on the right is not what\n",
        "635": "people refer to as free ascent and this\n",
        "636": "is some other algorithm with different\n",
        "638": "properties and for various reasons this\n",
        "639": "properties and for various reasons this\n",
        "640": "behaves\n",
        "641": "can behave and slightly stranger ways\n",
        "643": "and so you know what you should do is\n",
        "645": "really implement the simultaneous update\n",
        "648": "of gradient descent so that's the\n",
        "650": "outline of the gradient descent\n",
        "652": "algorithm in the next video we're going\n",
        "655": "to go into the details of the derivative\n",
        "657": "term which I wrote I'll back didn't\n",
        "659": "really define and if you take the\n",
        "661": "calculus class before and if you're\n",
        "663": "familiar with partial derivatives and\n",
        "665": "derivatives it turns out that's exactly\n",
        "669": "what that derivative term is but in case\n",
        "671": "you aren't familiar of calculus don't\n",
        "672": "worry about it the next video will give\n",
        "674": "you all the intuitions and will tell you\n",
        "676": "everything you need to know to compute\n",
        "678": "that derivative term even if you haven't\n",
        "680": "seen calculus or even you haven't seen\n",
        "683": "partial derivatives before and with that\n",
        "685": "with the next video hopefully we'll\n",
        "687": "really give you all the intuitions you\n"
    },
    "FCUBwP-JTsA": {
        "0": " \n",
        "2": "so far we've been talking about SVM's in\n",
        "4": "a fairly abstract level in this video\n",
        "6": "I'd like to talk about what you actually\n",
        "9": "need to do in order to run or to use an\n",
        "12": "SVM the support vector machine algorithm\n",
        "15": "poses a particular optimization problem\n",
        "17": "but as I briefly mentioned in an earlier\n",
        "19": "video I really do not recommend use\n",
        "21": "writing your own software to solve for\n",
        "24": "the parameters data yourself so just as\n",
        "27": "today very few of us or maybe almost\n",
        "29": "essentially none of us would think of\n",
        "31": "writing code ourselves to invert the\n",
        "33": "matrix or take a square root of a number\n",
        "35": "and so on we just you know call some\n",
        "37": "library function to do that in the same\n",
        "40": "way the software for solving the SVM\n",
        "43": "optimization problem is very complex and\n",
        "45": "there be researchers they've been doing\n",
        "46": "essentially numerical optimization\n",
        "48": "research for many years to come up with\n",
        "50": "good software libraries and good\n",
        "52": "software packages to do this and I\n",
        "54": "strongly recommend just using one of the\n",
        "56": "highly optimized software libraries\n",
        "57": "rather than trying to implement\n",
        "59": "something yourself and there are lots of\n",
        "61": "good software libraries out there the\n",
        "63": "tools I happen to use the most often on\n",
        "64": "the linear in live SVM but they're\n",
        "66": "really lots of good software libraries\n",
        "68": "for doing this that you know you can\n",
        "71": "link to many of the major programming\n",
        "73": "languages that you may be using to code\n",
        "75": "up a learning algorithm even though you\n",
        "77": "shouldn't be writing your own SVM\n",
        "79": "optimization software there are a few\n",
        "82": "things you need to do though first is to\n",
        "84": "come up with some choice of the\n",
        "86": "parameter C we talked a little bit about\n",
        "88": "the bias-variance properties of this in\n",
        "91": "an earlier video second you also need to\n",
        "93": "choose the kernel or the similarity\n",
        "96": "function that you want to use so one\n",
        "98": "choice might be we decide not to use any\n",
        "102": "kernel and the idea of no kernel is also\n",
        "105": "called a linear kernel so someone says I\n",
        "107": "use an SVM with a linear kernel what\n",
        "108": "that means is you know they use an SVM\n",
        "112": "without using without using a kernel and\n",
        "114": "it was the version of the SVM that just\n",
        "116": "uses theta transpose X right there\n",
        "119": "predicts one if they - 0 plus theta 1 x1\n",
        "122": "plus so on plus theta n\n",
        "125": "X n is greater than equal to 0 and this\n",
        "127": "term linear kernel you can think of this\n",
        "129": "as you know this is the version of the\n",
        "131": "SVM that just gives you a standard\n",
        "134": "linear classifier so that would be one\n",
        "136": "reasonable choice for some problems and\n",
        "138": "you know there are many software\n",
        "141": "libraries like lib mean linear with one\n",
        "143": "example all the menus one example of a\n",
        "145": "software library that can train an SVM\n",
        "148": "without using a kernel also called a\n",
        "150": "linear kernel so when would you want to\n",
        "152": "do this well if you have a large number\n",
        "157": "of features if n is large and M the\n",
        "160": "number of training examples is small\n",
        "162": "then you know you have a huge number of\n",
        "164": "features that X differ this is an X in\n",
        "167": "RN RN plus 1 so you have a huge number\n",
        "169": "of features already but the small\n",
        "170": "training set you know maybe you want to\n",
        "172": "just fit a linear decision boundary and\n",
        "173": "not try to fit a very complicated\n",
        "176": "nonlinear function because you might not\n",
        "178": "have enough data you might risk\n",
        "179": "overfitting maybe trying to fit a very\n",
        "182": "complicated function in a very high\n",
        "184": "dimensional feature space but if your\n",
        "187": "training set is soft this is small so\n",
        "188": "this would be one reasonable setting\n",
        "190": "where you might decide to not just use\n",
        "192": "to just not use a kernel or equivalent\n",
        "194": "to use what's called a linear kernel a\n",
        "197": "second choice for the kernel of the UN\n",
        "199": "make is this Gaussian kernel and this is\n",
        "201": "what we had previously and if you do\n",
        "203": "this then the other choice you need to\n",
        "206": "make is to choose this parameter Sigma\n",
        "207": "squared when we also talked a little bit\n",
        "210": "about the bias variance trade-offs of\n",
        "213": "how if Sigma squared is large then you\n",
        "215": "tend to have a higher bias lower\n",
        "218": "variance classifier but if Sigma squared\n",
        "221": "is small then you have a higher variance\n",
        "224": "lower bias classifier so when might you\n",
        "227": "choose the Gaussian kernel well if you\n",
        "229": "if your original features X Arbonne are\n",
        "232": "N and if n is small\n",
        "236": "oh and ideally you know if n\n",
        "240": "is large right so that's if you know we\n",
        "242": "have say a two dimensional training set\n",
        "244": "like you're not like the example I drew\n",
        "247": "earlier so then it's equal to two but we\n",
        "249": "have a pretty large training set so you\n",
        "250": "know I've drawn in the fairly large\n",
        "251": "number of training examples then maybe\n",
        "253": "you want to use the kernel to fit a more\n",
        "256": "complex nonlinear decision boundary and\n",
        "258": "the Gaussian kernel would be a fine way\n",
        "260": "to do this I'll say more towards the end\n",
        "262": "of video a little bit more about when\n",
        "263": "of video a little bit more about when\n",
        "264": "you might choose the linear kernel or a\n",
        "268": "Gaussian kernel and so on but if\n",
        "271": "concretely if you decide to use a\n",
        "273": "Gaussian kernel then here's what you\n",
        "276": "need to do depending on what support\n",
        "278": "vector machine software package you use\n",
        "280": "it may ask you to implement a kernel\n",
        "283": "function or to implement the similarity\n",
        "287": "function so if you are using an octave\n",
        "290": "or MATLAB implementation of an SVM it\n",
        "292": "may ask you to provide a function to\n",
        "294": "compute a particular feature of the\n",
        "296": "kernel so this is really computing f\n",
        "298": "subscript I for one particular value of\n",
        "302": "I where F here is just a single real\n",
        "304": "number so maybe I should make this\n",
        "307": "better written FY but what you need to\n",
        "309": "do is write a kernel function that takes\n",
        "311": "this input you know a training example\n",
        "313": "or a test example whatever it takes in\n",
        "316": "some vector X and takes this input one\n",
        "319": "of the landmarks and but only up calling\n",
        "321": "them x1 and x2 here because the\n",
        "323": "landmarks are really training examples\n",
        "327": "as well but what you need to do is write\n",
        "328": "software that takes as input you know x1\n",
        "331": "x2 and confuse this sort of similarity\n",
        "333": "function between them and return a real\n",
        "337": "number and so what some support vector\n",
        "339": "machine packages will do is expect you\n",
        "341": "to provide this kernel function that\n",
        "343": "takes us input you know x1 x2 and\n",
        "346": "returns a real number and then it will\n",
        "347": "take it from there and it will\n",
        "349": "automatically generate all the features\n",
        "351": "so automatically you know take X and map\n",
        "354": "it into f1 f2 down to FM using this\n",
        "356": "function that you write and generate all\n",
        "358": "the features and train the support\n",
        "360": "vector machine from there but sometimes\n",
        "361": "you do need to provide this function\n",
        "362": "yourself\n",
        "364": "although if you are using the Gaussian\n",
        "367": "kernel some some SVM implementations\n",
        "369": "will also include the Gaussian kernel\n",
        "371": "and a few other kernels as well since\n",
        "372": "the Gaussian kernel is probably the most\n",
        "375": "common kernel Gaussian and linear\n",
        "377": "kernels are related to most popular\n",
        "378": "kernels by far\n",
        "381": "just one implementation note if you have\n",
        "383": "features are very different scales it is\n",
        "385": "important to perform feature scaling\n",
        "386": "important to perform feature scaling\n",
        "388": "before using the Gaussian kernel and\n",
        "392": "here's why if you imagine computing the\n",
        "394": "norm between X and L right so this term\n",
        "396": "here instead like the numerator term\n",
        "400": "over there what this is doing the norm\n",
        "401": "between X and now that's really saying\n",
        "402": "you know let's compute the vector V\n",
        "405": "which is equal to X minus L and then\n",
        "408": "let's compute write the norm of this\n",
        "409": "vector V which is the difference Union\n",
        "411": "exit and so the norm of V is really\n",
        "415": "equal to v1 squared plus v2 squared plus\n",
        "419": "dot dot plus VN squared because here X\n",
        "423": "is in R and no RN plus 1 but I'm going\n",
        "427": "to ignore you know X 0 because let's\n",
        "430": "pretend X is an RN for square square on\n",
        "432": "the left side so what it means is\n",
        "434": "correct and so you know this is equal to\n",
        "438": "that's right and so written differently\n",
        "441": "this is going to be x1 minus l1 squared\n",
        "445": "plus x2 minus l2 squared plus 1 divided\n",
        "450": "by X n minus Ln squared and now if your\n",
        "452": "features take on very different ranges\n",
        "455": "of values so take a housing prediction\n",
        "458": "example when you try to if your data is\n",
        "462": "some data about houses and if X is in\n",
        "464": "the range of you know thousands of\n",
        "468": "square feet right to put for the\n",
        "470": "first few x1 but if your second feature\n",
        "472": "x2 is the number of bedrooms so if this\n",
        "473": "x2 is the number of bedrooms so if this\n",
        "474": "is you know in the range of one to five\n",
        "479": "bedrooms then x1 minus l1 is going to be\n",
        "480": "huge this could be like a thousand\n",
        "483": "squared whereas x2 minus l2 is going to\n",
        "485": "be much smaller and if that's the case\n",
        "486": "then\n",
        "489": "in this term those distances will be\n",
        "491": "almost essentially dominated by two\n",
        "494": "sizes by the sizes of the houses and the\n",
        "495": "number of bedrooms will be largely\n",
        "498": "ignored and so to avoid this in order to\n",
        "501": "make an NCO work well doing perform do\n",
        "503": "perform feature scaling and that will\n",
        "505": "make sure that the SVM gives you no\n",
        "506": "make sure that the SVM gives you no\n",
        "508": "comparable amount of attention to all of\n",
        "510": "your different features and not just two\n",
        "512": "in this example the size of houses while\n",
        "514": "big norming the other features when\n",
        "516": "you're applying a support vector machine\n",
        "520": "chances are by far the two most common\n",
        "522": "kernels you use will be the linear\n",
        "524": "kernel meaning no kernel or the Gaussian\n",
        "527": "kernel that we talked about and just one\n",
        "529": "note of warning which is that not all\n",
        "530": "similarity functions you might come up\n",
        "533": "with are valid kernels and the Gaussian\n",
        "535": "kernel and the linear kernel and other\n",
        "537": "kernels that you sometimes others will\n",
        "539": "use all of them need to satisfy a\n",
        "541": "technical condition it's called vs.\n",
        "543": "theorem and the reason you need to do\n",
        "545": "this is because support vector machine\n",
        "548": "of algorithms or implementations of the\n",
        "550": "SVM have lots of clever numerical\n",
        "553": "optimization tricks in order to solve\n",
        "555": "for the parameters data efficiently and\n",
        "559": "in the original design of SDS there was\n",
        "561": "a there was a decision made to restrict\n",
        "563": "our attention only to kernels that\n",
        "565": "satisfy this technical condition called\n",
        "567": "Mercer's theorem and what that does is\n",
        "568": "that make sure that all of these SVM\n",
        "570": "packages all of these SVM software\n",
        "572": "packages can use a large class of\n",
        "576": "optimizations and get the parameter beta\n",
        "580": "theta very quickly so what most people\n",
        "582": "end up doing is using either the linear\n",
        "584": "the Gaussian kernel but there are a few\n",
        "586": "other kernels that also satisfy Mercer's\n",
        "588": "theorem and that you may run across\n",
        "590": "other people using although I Percy end\n",
        "592": "up using other kernels you know very\n",
        "595": "very rarely if at all just to mention\n",
        "596": "some of the other kernels that you may\n",
        "600": "run across one is the polynomial kernel\n",
        "603": "and for that the similarity between X\n",
        "607": "and L is defined as there lot of options\n",
        "610": "I guess you can take X transpose L\n",
        "612": "squared so you know here's one measure\n",
        "613": "of how similar X\n",
        "615": "right if X and L are very close to each\n",
        "617": "other then the inner product will tend\n",
        "622": "to be large and and so you know this is\n",
        "624": "a slightly unusual kernel that's not\n",
        "626": "used that often but it works but you may\n",
        "630": "run across some people using it this is\n",
        "632": "one version of polynomial kernel and\n",
        "636": "other is an X transpose L cubed or these\n",
        "638": "are all examples of the polynomial\n",
        "641": "kernel which runs develop last one cubed\n",
        "644": "x transpose L plus may be number\n",
        "646": "different than 1 by 5 and n notes in\n",
        "648": "power of 4 and so the polynomial kernel\n",
        "651": "actually has two parameters one is what\n",
        "653": "number do you add over here it could be\n",
        "657": "0 this is really at 1 0 over there as\n",
        "658": "well as what's the degree of the\n",
        "661": "polynomial so the degree the power and\n",
        "663": "this number is in the more general form\n",
        "665": "with this of the polynomial kernel is X\n",
        "671": "transpose L plus some constant and then\n",
        "676": "to some degree in the exponent and so\n",
        "679": "both of these are parameters for the\n",
        "680": "polynomial kernel so the polynomial\n",
        "684": "kernel almost always or usually performs\n",
        "685": "worse than the Gaussian kernel and is\n",
        "687": "not used that much but it's just\n",
        "688": "something that you may run across\n",
        "691": "usually is used only for data where X\n",
        "693": "and L are all strictly non-negative and\n",
        "695": "so that ensures that these inner\n",
        "698": "products are never negative but this is\n",
        "700": "the captaincies intuition that if X and\n",
        "702": "O are very similar to each other that\n",
        "703": "maybe the inner product between them\n",
        "704": "will be large and then has some other\n",
        "707": "properties as well but people then tend\n",
        "709": "not to use it much and then depending on\n",
        "710": "not to use it much and then depending on\n",
        "712": "what you're doing there are other more\n",
        "714": "esoteric kernels as well that you may\n",
        "716": "come across so there's a string kernel\n",
        "719": "this is sometimes use if your input data\n",
        "721": "is you know text strings or other types\n",
        "723": "of strings there are things like the\n",
        "725": "chi-square kernel the histogram section\n",
        "727": "kernel and so on there are sort of more\n",
        "729": "esoteric kernels that you can use to\n",
        "731": "measure similarity between different\n",
        "733": "objects so for example if you're trying\n",
        "735": "to do some sort of text classification\n",
        "738": "problem we were XD\n",
        "740": "xs/s string then maybe you want to\n",
        "742": "define the similarity between two\n",
        "744": "strings using the strengths kernel but I\n",
        "746": "personally you know end up very rarely\n",
        "749": "if at all using these more esoteric\n",
        "750": "kernels I think I might have used the\n",
        "752": "chi-square kernel may be once in my life\n",
        "754": "in the histogram kernel maybe once or\n",
        "756": "twice in my life and I've actually never\n",
        "759": "used a sweet kernel myself but in case\n",
        "761": "you run across this in other\n",
        "763": "applications you know if you do a quick\n",
        "765": "web search we do a quick Google search\n",
        "767": "or quick bing search you should find\n",
        "768": "definitions of these other kernels as\n",
        "770": "well\n",
        "773": "so just two last details I want to talk\n",
        "774": "about in this video\n",
        "776": "one is multi-class classification so if\n",
        "778": "you have four classes or more general UK\n",
        "781": "classes how do you get an SEM to output\n",
        "783": "some appropriate decision boundary\n",
        "786": "between your multiple classes most SVM\n",
        "788": "well many SVM packages already have\n",
        "789": "built-in multi-class classification\n",
        "792": "functionality so you're using a package\n",
        "793": "like that you just use that built-in\n",
        "796": "functionality and that you'll find\n",
        "797": "otherwise\n",
        "799": "one way to do this is to use the one\n",
        "801": "versus all method that we talked about\n",
        "803": "when we're developing logistic\n",
        "805": "regression so what you do is you train\n",
        "809": "KS VMs if you have K classes want to\n",
        "810": "distinguish each of the classes from the\n",
        "813": "rest and this would give you K parameter\n",
        "815": "vectors so this will give you you know\n",
        "817": "theta one which is trying to distinguish\n",
        "820": "class y equals one from all the other\n",
        "821": "classes then you get a separate\n",
        "824": "parameter vector theta two which is what\n",
        "826": "you get when you you know have y equals\n",
        "827": "two as the positive class and all the\n",
        "830": "others as negative parts and so on up to\n",
        "833": "a parameter vector theta K which is the\n",
        "836": "parameter vector for distinguishing the\n",
        "838": "final class of class K for everything\n",
        "841": "else and then lastly this is are exactly\n",
        "843": "the same as a one versus all method we\n",
        "844": "have for logistic regression really you\n",
        "847": "just predict the class I with the\n",
        "849": "largest theater translator R transpose X\n",
        "851": "so let's multi-class classification\n",
        "854": "Bayes nets for the more common cases\n",
        "856": "that there's a good chance that whatever\n",
        "857": "software package you use you know\n",
        "859": "there'll be a reasonable chance that\n",
        "861": "will already have built in multi-class\n",
        "863": "classification functionality and so you\n",
        "864": "don't even need to worry about this\n",
        "866": "result finally we developed support\n",
        "868": "vector machines starting off with\n",
        "870": "logistic regression and then modifying\n",
        "872": "the cost function a little bit so the\n",
        "873": "last thing I want to do in this video is\n",
        "875": "just say a little bit about when you\n",
        "878": "would use one of these two algorithms so\n",
        "880": "let's say n is the number of features\n",
        "882": "and M is the number of training examples\n",
        "884": "so when should we use one algorithm\n",
        "888": "versus the other well if n is large\n",
        "891": "relative to your training set size so\n",
        "894": "for example if you think of this as if\n",
        "895": "the number of features is much larger\n",
        "896": "than M\n",
        "898": "and this might be for example if you\n",
        "900": "have a text classification problem where\n",
        "902": "you know the dimension of your feature\n",
        "904": "vectors don't know maybe ten thousand\n",
        "908": "and if your training set size is maybe\n",
        "911": "ten you know maybe up to one thousand to\n",
        "913": "imagine the spam classification problem\n",
        "915": "where email spam where you have ten\n",
        "917": "thousand features corresponding to ten\n",
        "919": "thousand words but you have you know\n",
        "921": "maybe 10 training examples or maybe up\n",
        "923": "to a thousand examples so if n is large\n",
        "925": "relative to M then what I would usually\n",
        "928": "do is use logistic regression or use an\n",
        "930": "SVM without a kernel or use it with a\n",
        "932": "linear kernel because if you have so\n",
        "934": "many features but smaller training set\n",
        "936": "you know a linear function will probably\n",
        "938": "do fine and you don't have really enough\n",
        "940": "data to fit a very complicated nonlinear\n",
        "944": "function now if n is small and M is\n",
        "946": "intermediate so what I mean by this is\n",
        "948": "if you know if n is it may be anywhere\n",
        "951": "from one to a thousand one would be very\n",
        "953": "small but me up to a thousand features\n",
        "956": "and if the number of training examples\n",
        "959": "it's may be anywhere from you know 10 10\n",
        "961": "to maybe up to ten thousand examples\n",
        "963": "maybe up to 50 thousand examples so if M\n",
        "965": "is pretty big like maybe ten thousand\n",
        "968": "but not a million active so if M is\n",
        "971": "intermediate size then often an SVM with\n",
        "973": "a linear current won't work well we\n",
        "975": "talked about this earlier as well of a\n",
        "976": "one country the example of this would be\n",
        "978": "if you have a two dimensional training\n",
        "981": "set so if n is equal to two but you have\n",
        "983": "you know drawing in a pretty large\n",
        "984": "number of training examples\n",
        "986": "so Gaussian kernel would do a pretty\n",
        "988": "good job separating positive a negative\n",
        "990": "horses one third setting does of\n",
        "991": "horses one third setting does of\n",
        "994": "interest is if n is small but M is large\n",
        "997": "so if n is you know gain maybe one to a\n",
        "1001": "thousand could be larger but ever M was\n",
        "1006": "on you know maybe 50,000 and greater to\n",
        "1009": "millions it's a fifty thousand a hundred\n",
        "1011": "thousand million Julianne\n",
        "1013": "and have very very large training set\n",
        "1017": "sizes right so this is the case then an\n",
        "1018": "SVM with a Gaussian kernel will be\n",
        "1020": "somewhat slow to run today's SVM\n",
        "1023": "packages if you're using a Gaussian\n",
        "1025": "kernel tend to struggle a bit if you\n",
        "1027": "have you know maybe 50,000 is okay but\n",
        "1029": "if you have a million training examples\n",
        "1032": "maybe even a hundred thousand with your\n",
        "1034": "massive value of em today's SVM packages\n",
        "1037": "are very good but they can still\n",
        "1038": "struggle a little bit when you have a\n",
        "1040": "massive massive training set size and\n",
        "1042": "using a thousand kernel so in that case\n",
        "1045": "what I would usually do is try to just\n",
        "1048": "manually create or add more features and\n",
        "1050": "then use logistic regression or an SVM\n",
        "1054": "without a kernel and in case you look at\n",
        "1055": "this slide and you see logistic\n",
        "1058": "regression or SVM without a kernel in\n",
        "1060": "both of these places I kind of paired\n",
        "1062": "them together well does the reason for\n",
        "1065": "that is that logistic regression and svm\n",
        "1067": "without a kernel those are really pretty\n",
        "1069": "similar algorithms and you know either\n",
        "1071": "logistic regression or SVM without a\n",
        "1074": "kernel we usually do pretty similar\n",
        "1075": "things and give pretty similar\n",
        "1077": "performance but depending on your\n",
        "1079": "implementational details one may be more\n",
        "1082": "efficient than the other but where one\n",
        "1083": "of these algorithms applies they're just\n",
        "1085": "regression well as we would all occur\n",
        "1086": "and all the other one is likely to work\n",
        "1089": "pretty well as well but a lot of the\n",
        "1092": "power of the SVM is when you use\n",
        "1094": "different kernels to learn complex\n",
        "1097": "nonlinear functions and then this this\n",
        "1099": "regime you know when you have not when\n",
        "1101": "you have maybe up to 10,000 examples\n",
        "1104": "maybe up to 50,000 and and and your\n",
        "1107": "number of features this is reasonably\n",
        "1109": "large maybe that's a very common regime\n",
        "1111": "and maybe that's a regime where a\n",
        "1113": "support vector machine with a Gaussian\n",
        "1115": "kernel will shine you can do things the\n",
        "1117": "harder to do that\n",
        "1120": "rushon and finally whether neural\n",
        "1122": "networks fit in well for all of these\n",
        "1124": "problems both of all of these different\n",
        "1126": "regimes a neural network or\n",
        "1128": "well-designed neural network is likely\n",
        "1130": "to work well as well the one\n",
        "1132": "disadvantage one reason that might not\n",
        "1134": "sometimes use the neural network is that\n",
        "1136": "for some of these problems the neural\n",
        "1138": "network might be slower to train but if\n",
        "1140": "you have a very good SVM implementation\n",
        "1142": "package that could run faster quite a\n",
        "1144": "bit faster of the new neural network and\n",
        "1146": "although we didn't show this earlier it\n",
        "1147": "although we didn't show this earlier it\n",
        "1149": "turns out that the optimization problem\n",
        "1151": "that the SVM has is a convex\n",
        "1154": "optimization problem and so the good SVM\n",
        "1157": "optimization software packages will\n",
        "1160": "always find a global minimum or\n",
        "1162": "something close to it and so for the SVM\n",
        "1164": "you don't need to worry about local\n",
        "1166": "optima in practice local Opterons a huge\n",
        "1168": "problem for new networks but they all\n",
        "1170": "also use one less thing to worry about\n",
        "1173": "if you're using an SVM and depending on\n",
        "1175": "your problems in your network may be\n",
        "1179": "slower especially in this sort of regime\n",
        "1182": "than the SVM in case the guidelines I\n",
        "1184": "gave here seem a little bit again if you\n",
        "1185": "look at some problems and you see if you\n",
        "1188": "like you know the guidelines of the day\n",
        "1190": "I'm still not entirely sure should I use\n",
        "1192": "this algorithm or that algorithm that's\n",
        "1193": "that's actually ok when I face the\n",
        "1194": "machine learning problem you know\n",
        "1196": "sometimes it's actually just not clear\n",
        "1198": "what is the best algorithm to use but so\n",
        "1201": "as you saw in earlier videos really you\n",
        "1203": "know the algorithm does matter but what\n",
        "1205": "often matters even more is things like\n",
        "1207": "how much data do you have and how\n",
        "1209": "skilled are you how good are you doing\n",
        "1211": "error analysis and debugging learning\n",
        "1213": "algorithms figuring out how to design\n",
        "1215": "new features or figure out what other\n",
        "1217": "features to give you learning error and\n",
        "1219": "so on and often those things will matter\n",
        "1221": "more than you're using a logistic\n",
        "1224": "regression or an SVM but having said\n",
        "1227": "that the SVM is the widely perceived as\n",
        "1228": "one of the most powerful learning\n",
        "1230": "algorithms and there is this regime\n",
        "1232": "when there's a very effective way to\n",
        "1235": "learn complex nonlinear functions and so\n",
        "1237": "I actually together with logistic\n",
        "1239": "regression neural networks SVM's\n",
        "1241": "using those to be learning algorithms\n",
        "1243": "you're I think very well positioned to\n",
        "1245": "state-of-the-art you know machine\n",
        "1247": "learning systems but wide range of\n",
        "1250": "applications and this is another very\n",
        "1252": "powerful tool to have in your arsenal\n",
        "1255": "one that is used all over the place in\n",
        "1258": "Silicon Valley or in industry and in\n",
        "1260": "academia to build many high-performance\n"
    },
    "FXLy8D2dnio": {
        "0": " \n",
        "3": "in the second tutorial video on octave\n",
        "4": "I'd like to start to tell you how to\n",
        "7": "move data around in octave so if you\n",
        "9": "have data for a machine learning problem\n",
        "12": "how do you load that data in octave how\n",
        "13": "do you present a matrix how do you\n",
        "14": "do you present a matrix how do you\n",
        "15": "manipulate these matrices how do you\n",
        "18": "save the results how do you move data\n",
        "23": "around and operate with data here's my\n",
        "26": "octave window as before picking up from\n",
        "29": "where we left off in the last video if I\n",
        "32": "type a that's the matrix so we generate\n",
        "34": "it right with this command equals one\n",
        "38": "two three four five six and this is a\n",
        "41": "three by two matrix the size command in\n",
        "45": "octave lets you tells you what is the\n",
        "47": "size of a matrix so size a returns three\n",
        "50": "two it turns out that the size command\n",
        "53": "itself is actually returning a one by\n",
        "55": "two matrix so you can actually set SZ\n",
        "59": "equals size of a and SZ is now a one by\n",
        "61": "two matrix where the first element of\n",
        "63": "this is three and the second element of\n",
        "65": "this is to search as you type size of as\n",
        "70": "Z this Z is a one by two matrix whose\n",
        "72": "two elements contain the dimensions of\n",
        "76": "the matrix a you can also type size a\n",
        "79": "one to give you back the first dimension\n",
        "82": "of a outside to the first dimension of a\n",
        "85": "so that's the number of rows and size a\n",
        "88": "two to give you back two which is the\n",
        "91": "number of columns in the matrix a if you\n",
        "94": "have a vector V so let's say V equals\n",
        "99": "one two three four and you type length V\n",
        "102": "what this does is it gives you the size\n",
        "105": "of the longest dimension so you can also\n",
        "109": "type length a and because a is a three\n",
        "113": "by two matrix the longer dimension is of\n",
        "115": "size three so this should print out\n",
        "117": "three but usually we apply length only\n",
        "120": "to a vector so you know length one two\n",
        "122": "three four five rather than apply\n",
        "124": "lengths to matrices because better\n",
        "126": "that's a little more confusing\n",
        "131": "now let's look at how to load data and\n",
        "134": "find data on a thought system when we\n",
        "136": "start up octave where usually we're\n",
        "139": "often in a path that is near the\n",
        "141": "location of where the octave program is\n",
        "144": "so the PWD command shows the current\n",
        "147": "directory or the current path that\n",
        "149": "octave is in so right now we're in this\n",
        "150": "somewhat maybe somewhat obscure\n",
        "154": "directory the CD command stands for\n",
        "156": "change directory so i can go to c colon\n",
        "159": "slash user slash ang slash desktop and\n",
        "164": "now i'm in you know my desktop and if i\n",
        "167": "type LS LS since whoever comes from a\n",
        "170": "UNIX or Linux command but OS will list\n",
        "172": "the directories on my desktop and so you\n",
        "174": "know these are the files that are on my\n",
        "194": " \n",
        "198": "in fact on my desktop are two vowels\n",
        "201": "features X and price Y that maybe come\n",
        "203": "from a machine learning problem and want\n",
        "205": "to solve so here's my desktop here's\n",
        "211": "some features X and features X is this\n",
        "213": "window excuse me is this file with two\n",
        "214": "columns of data\n",
        "216": "this is actually my housing prices data\n",
        "219": "so I think you know I think I have 47\n",
        "221": "rows in this data set and so the first\n",
        "224": "house had size 204 square feet has three\n",
        "227": "bedrooms second house at 1600 square\n",
        "229": "feet has three bedrooms and so on and\n",
        "234": "price why is this file that has the\n",
        "238": "prices of the data in my training set so\n",
        "241": "features X and price Y I just text files\n",
        "243": "with my data how do I load this data\n",
        "244": "into octave\n",
        "248": "well I pick my load features X dot dat\n",
        "251": "and if I do that and load the features X\n",
        "255": "I can load price y dot that and by the\n",
        "257": "way there multiple ways to do this this\n",
        "259": "command if you are put features X\n",
        "262": "thought that in strings and loaded like\n",
        "265": "so this is a typo there this is a\n",
        "269": "equivalent command so you can but this\n",
        "271": "way I'm just putting the file name of\n",
        "274": "the string in the file name in a string\n",
        "277": "and in octave you use single quotes to\n",
        "282": "to represent strings like so so that's a\n",
        "285": "string and I can load the file whose\n",
        "289": "name is given by that string now the\n",
        "292": "hukum and now shows me what variables I\n",
        "295": "have in my octave workspace so who shows\n",
        "298": "me whether the variables that octave has\n",
        "300": "a memory currently features X and price\n",
        "303": "Y are among them as well as the\n",
        "305": "variables that nearly created earlier in\n",
        "308": "this session so you can type features X\n",
        "311": "to display features X and there's my\n",
        "316": "data and I can type size features X and\n",
        "321": "that's my 47 by 2 matrix and some of the\n",
        "323": "size price Y that\n",
        "329": "by 47 by one vector for this is a 47\n",
        "331": "dimensional vector this tall column\n",
        "333": "vector that has all the prices Y in my\n",
        "334": "training set\n",
        "338": "now the who function shows you what are\n",
        "339": "the variables that in the current\n",
        "341": "workspace there's also the who X\n",
        "344": "variable that gives you the detail view\n",
        "347": "and so this also with an S at the end\n",
        "349": "there's also lists my variables except\n",
        "352": "that it now listed sizes as well so a is\n",
        "355": "a three by two matrix and features X is\n",
        "358": "a 47 by 2 matrix price price Y is a 47\n",
        "360": "by 1 matrix meaning this is just a\n",
        "362": "vector and it shows you know how many\n",
        "364": "bytes and memory is taking up as well as\n",
        "366": "a white type of data this is double\n",
        "368": "means double precision floating point so\n",
        "370": "that just means these are real values or\n",
        "373": "fourteen point numbers now if you want\n",
        "376": "to get rid of a variable you can use the\n",
        "380": "clear command so clear features X and\n",
        "381": "type whose again you notice that the\n",
        "385": "features X therefore has now disappeared\n",
        "389": "and how do you save data let's see let's\n",
        "391": "take the variable V and set its a price\n",
        "396": "Y 1 : 10 this sets V to be the first 10\n",
        "400": "elements of the vector Y so let's type\n",
        "405": "who or whose whereas Y was a 47 by 1\n",
        "408": "vector V is now 10 by 1 because in\n",
        "412": "equals price Y 1 : 10 the sets into the\n",
        "415": "just the first 10 elements of Y let's\n",
        "417": "say I want to save this to day to disk\n",
        "420": "the command save hello.mat\n",
        "424": "V this will save the variable V into a\n",
        "427": "file called hello.mat so let's do that\n",
        "431": "and now a fire has appeared on my\n",
        "434": "desktop you know called hello dot net I\n",
        "436": "happen to have MATLAB installed in this\n",
        "438": "windows which is why you notice this\n",
        "441": "icon looks like this because Windows has\n",
        "443": "recognizes it somehow that file but\n",
        "445": "don't worry about it if this file looks\n",
        "446": "like it has a different icon\n",
        "450": "your machine and let's say I clear all\n",
        "452": "my variables if I give you type clear\n",
        "453": "without anything then just actually do\n",
        "455": "these all the variables in your world\n",
        "457": "space so that whose there's now nothing\n",
        "459": "left in the workspace and if I load\n",
        "464": "hello that I can now load back my\n",
        "467": "variable V which is my data that\n",
        "470": "previously saved into the Holloman file\n",
        "473": "so hello don't matter what we did just\n",
        "475": "now the save hello.mat to begin this\n",
        "477": "saves the data in a binary format and\n",
        "479": "there's somewhat more compressed binary\n",
        "481": "format so that if V is a lot of data\n",
        "483": "this you know will be somewhat more\n",
        "484": "compressed and will take up less the\n",
        "487": "space if you want to save your data in a\n",
        "490": "human readable format then you type save\n",
        "494": "hello Tex the variable V and then -\n",
        "498": "ASCII so this will save it as a text or\n",
        "502": "as ASCII formatted text and now once\n",
        "504": "I've done that I have this file\n",
        "506": "hello Texas just appeared on my desktop\n",
        "507": "hello Texas just appeared on my desktop\n",
        "510": "and if I open this up you see that this\n",
        "512": "is a text file with my with my data\n",
        "515": "saved away so that's how you load and\n",
        "517": "save data now let's talk a bit about how\n",
        "520": "to manipulate data let's set a equals to\n",
        "524": "that matrix again so it's my three by\n",
        "527": "two matrix I saw quite indexing so if I\n",
        "530": "type a three comma two this indexes into\n",
        "533": "the three comma two elements of the\n",
        "537": "matrix a so this is what you normally we\n",
        "542": "write this as a subscript 3 2 or a\n",
        "546": "subscript you know 3 2 and so that's the\n",
        "548": "element in the third row and second\n",
        "551": "column of a which is the element six I\n",
        "556": "can also type a 2 comma colon to fetch\n",
        "559": "everything in the second row so the\n",
        "563": "colon means every element\n",
        "569": "along that row or column so a of 2 comma\n",
        "572": "colon is this second row of a right and\n",
        "575": "similarly if I do a colon comma 2 then\n",
        "579": "this means get everything in the second\n",
        "582": "column of a so this gives me 2 4 6 red\n",
        "585": "this means of a everything calmer second\n",
        "587": "column so this is my second column of a\n",
        "591": "which is 2 4 6 now you can also use\n",
        "593": "somewhat more sophisticated indexing\n",
        "595": "operations so just quickly show you an\n",
        "598": "example you do this maybe less often but\n",
        "602": "let me do a 1 3 comma colon this means\n",
        "605": "get all the elements of a whose first\n",
        "608": "index is 1 or 3 this means I get\n",
        "609": "everything from the first and third rows\n",
        "616": "of a and from all columns so this was\n",
        "620": "the matrix a and so a 1 3 comma colon\n",
        "623": "means get everything from the first row\n",
        "625": "and from the second row and from the\n",
        "627": "third row and colon means you know I\n",
        "629": "want both the first and the second\n",
        "632": "columns and so this gives me this 1 2 5\n",
        "634": "6 although you you use these sort of a\n",
        "637": "more sophisticated indexing operations\n",
        "640": "may be somewhat less often to show you\n",
        "642": "what else we can do here's the a matrix\n",
        "645": "and that this was a colon comma 2 gave\n",
        "648": "me the second column you can also use\n",
        "650": "this to do assignment so I can take the\n",
        "653": "second column of a and assign that to 10\n",
        "658": "11 12 and if I do that I'm now you know\n",
        "659": "taking the second column of a and I'm\n",
        "663": "assigning this column vector 10 11 12 to\n",
        "666": "it so now a is this matrix that's 1 3 5\n",
        "667": "and the second column has been replaced\n",
        "670": "by 10 11 and 12\n",
        "673": "and here's another operation and it said\n",
        "678": "a let's set a to be equal to a comma 100\n",
        "683": "101 102 like so and what this will do is\n",
        "688": "append another column vector to the\n",
        "692": "right so now\n",
        "697": "oops I think I made a mistake should I\n",
        "703": "put semicolons there and now it is equal\n",
        "705": "to this okay hold that make sense so\n",
        "710": "this 100 101 102 this is a column vector\n",
        "713": "and what we did was we set a tink a and\n",
        "715": "set it to the original definition and\n",
        "718": "then we put that column vector to the\n",
        "720": "right and so we ended up taking the\n",
        "724": "matrix a and which was a this six\n",
        "726": "elements on the left so he told the\n",
        "728": "matrix a and we appended another column\n",
        "730": "vector to the right which is not why now\n",
        "734": "a is a three by three matrix that looks\n",
        "738": "like that and finally one neat trick\n",
        "740": "that I sometimes use if you do a and\n",
        "742": "then just a colon like so there's a\n",
        "744": "somewhat special case syntax what this\n",
        "748": "means is that put all elements of a into\n",
        "752": "a single column vector and this gives me\n",
        "755": "a nine by one vector that just has all\n",
        "758": " \n",
        "763": "just a couple more examples I can also\n",
        "766": "let's see let's say I set a to be equal\n",
        "774": "to 1 2 3 4 5 6 again and let's say I set\n",
        "779": "B to be equal to 11 12 13 14 15 16 I can\n",
        "783": "create a new matrix C as a B this just\n",
        "786": "means so here's my matrix a here's my\n",
        "789": "matrix B and if I set C to be equal to a\n",
        "792": "B what I'm doing is I'm taking these two\n",
        "794": "matrices and just concatenating them\n",
        "797": "onto each other so the left on matrix a\n",
        "800": "on the left and I have the matrix B on\n",
        "802": "the right and that's how I form this you\n",
        "805": "know this matrix C by putting them\n",
        "808": "together I can also do C equals a\n",
        "814": "semicolon B the semicolon notation means\n",
        "817": "that means it means that go put the next\n",
        "819": "thing at the bottom side with your C\n",
        "821": "equals a semicolons B it also puts the\n",
        "824": "matrices a and B together except that it\n",
        "825": "now puts them on top\n",
        "827": "of each other so now I have a on top and\n",
        "830": "B at the bottom and C here is now a six\n",
        "833": "by two matrix so so just say again the\n",
        "835": "semicolon thing usually means you know\n",
        "838": "go to the next line so C is comprised by\n",
        "840": "a and then go to the bottom of that and\n",
        "842": "then put B at the bottom and by the way\n",
        "845": "this a B is the same as a comma B and so\n",
        "847": "you know either of these gives you the\n",
        "851": "same result so with that hopefully you\n",
        "855": "now know how to construct matrices and\n",
        "857": "hopefully this starts to show you some\n",
        "859": "of the commands that you can use to\n",
        "862": "quickly put together matrices and take\n",
        "863": "matrices and you know slam them together\n",
        "866": "to form big bigger matrices and with\n",
        "869": "just a few lines of code octave is very\n",
        "871": "convenient in terms of how quickly we\n",
        "874": "can assemble complex matrices and move\n",
        "877": "data around so that's it for moving data\n",
        "879": "around in the next video we'll start to\n",
        "881": "talk about how to actually do complex\n",
        "885": "computations on justice on our data\n",
        "888": "so hopefully that gives you a sense of\n",
        "891": "how with just a few commands you can\n",
        "893": "very quickly move data around in octave\n",
        "895": "you you know though then say vectors and\n",
        "898": "matrices alone the sake data put\n",
        "900": "together matrices create bigger matrices\n",
        "903": "index into or select specific elements\n",
        "905": "all the matrices I know I went through a\n",
        "907": "lot of commands but so I think the best\n",
        "909": "thing for you to do is afterwards to\n",
        "911": "look at the transcript of the things I\n",
        "913": "was typing you look at look at the\n",
        "914": "course website and download the\n",
        "916": "transcript of the session from there and\n",
        "918": "look for the transcript and type some of\n",
        "921": "those commands into octave yourself so\n",
        "922": "that you can start to play these\n",
        "925": "commands and get to work and obviously\n",
        "926": "you know there's no point at all to try\n",
        "928": "to memorize all these commands it's just\n",
        "931": "yeah but what you should do is hopefully\n",
        "932": "from this video you have gotten a sense\n",
        "935": "of the sorts of things you can do so\n",
        "936": "that when later on when you're trying to\n",
        "938": "program a learning algorithm system\n",
        "940": "yourself if you are trying to find a\n",
        "942": "specific command that maybe you think\n",
        "943": "octave can do because you think you\n",
        "945": "might have seen it here you should refer\n",
        "948": "to the transcript of the session and\n",
        "949": "look through that and over to find the\n",
        "952": "commands you want to use so that's it\n",
        "955": "for moving data around and in the next\n",
        "957": "video what I'd like to do is start to\n",
        "959": "tell you how to actually do complex\n",
        "963": "computations on our data and how to\n",
        "965": "compute on our data and actually start\n"
    },
    "FZ1qPqVeMSQ": {
        "0": " \n",
        "1": "in this video once talked about the\n",
        "4": "normal equation and non-invertibility\n",
        "7": "this is a somewhat more advanced concept\n",
        "9": "but it's something that I often been\n",
        "11": "asked about and so once talked about\n",
        "12": "here an address in here but this is\n",
        "15": "somewhat more advanced concept so feel\n",
        "17": "free to consider this optional material\n",
        "21": "and it's a phenomenon that you may run\n",
        "23": "into that may be somewhat useful to\n",
        "24": "understand but even if you don't\n",
        "26": "understand it the normal equation and\n",
        "28": "linear regression you should get back to\n",
        "33": "work okay here's the issue for those of\n",
        "35": "you there are maybe somewhat more\n",
        "37": "familiar with linear algebra on what\n",
        "39": "some students have asked me is when\n",
        "43": "computing this theta equals X transpose\n",
        "45": "X inverse X transpose Y what if the\n",
        "48": "matrix x transpose x is non-invertible\n",
        "51": "so for those of you that know a bit more\n",
        "53": "linear algebra you may know that you\n",
        "54": "know only some matrices are invertible\n",
        "58": "and some matrices do not have an inverse\n",
        "60": "we call those non-invertible matrices\n",
        "64": "singular or degenerate matrices the\n",
        "66": "issue or the problem of x transpose X\n",
        "69": "being non invertible or should happen\n",
        "72": "pretty rarely and in octave if you\n",
        "76": "implement this to compute theta it turns\n",
        "78": "out that this will actually do the right\n",
        "81": "thing I'm getting a little technical now\n",
        "83": "and and I don't want to go into details\n",
        "86": "but octave has two functions for\n",
        "89": "inverting matrices one is called PNF and\n",
        "92": "the other is called in and the\n",
        "93": "differences between these two are\n",
        "95": "somewhat technical ones called the\n",
        "97": "pseudo-inverse one's called the inverse\n",
        "100": "but you can show mathematically that so\n",
        "102": "long as you use the P in function then\n",
        "105": "this will this will actually compute the\n",
        "108": "value of theta that you want even if x\n",
        "111": "transpose x is non-invertible the\n",
        "113": "specific details between here what is\n",
        "114": "the difference between pinioned and was\n",
        "117": "what is in that's somewhat advanced\n",
        "119": "numerical computing concepts that I\n",
        "121": "don't really want to get into but um I\n",
        "123": "taught in this optional video I'll give\n",
        "124": "try to give you a little bit of\n",
        "126": "intuition about what it means for x\n",
        "128": "transpose x to be non-invertible\n",
        "130": "for those of you that know a bit more\n",
        "132": "linear algebra and might be interested\n",
        "134": "I'm not going to prove this\n",
        "136": "mathematically but if x transpose x is\n",
        "139": "non-invertible there are usually two\n",
        "142": "most common causes for this the first\n",
        "144": "cause is if somehow in your learning\n",
        "147": "problem you have redundant features\n",
        "149": "concretely if you're trying to predict\n",
        "152": "housing prices and if x1 is the size of\n",
        "154": "the house and feet that the square feet\n",
        "156": "and x2 is the size of the house and\n",
        "159": "square meters then you know one meter is\n",
        "160": "square meters then you know one meter is\n",
        "163": "equal to three point two eight feet\n",
        "165": "rounded up rounded to two decimals and\n",
        "167": "so your two features will always satisfy\n",
        "170": "the constraint that x1 equals three\n",
        "175": "point two eight squared times x2 and you\n",
        "177": "can show for those who there's somewhat\n",
        "179": "advanced in the algebra now but if\n",
        "180": "you're an expert in linear algebra you\n",
        "182": "can actually show that if you're two\n",
        "184": "features are related via a linear\n",
        "186": "equation like this then the matrix x\n",
        "188": "transpose x will be non-invertible the\n",
        "190": "second thing that can cause x transpose\n",
        "193": "x to be non-invertible is if you are\n",
        "195": "training if you're trying to run the\n",
        "197": "learning algorithm with a lot of\n",
        "201": "features concretely if m is less than or\n",
        "203": "equal to n for example if you imagine\n",
        "206": "that you have M equals 10 training\n",
        "210": "examples that you have N equals 100\n",
        "212": "features then you're trying to fit a\n",
        "216": "parameter vector theta which is you know\n",
        "217": "n plus 1 dimensional so it's one hundred\n",
        "219": "and one dimensional you trying to fit\n",
        "221": "101 parameters from just 10 training\n",
        "226": "examples and this turns out to sometimes\n",
        "228": "work but to not always be a good idea\n",
        "231": "because as we'll see later you might not\n",
        "233": "have enough data if you have only ten\n",
        "236": "examples to fit you know 100 or 101\n",
        "238": "parameters we'll see later in this\n",
        "241": "course why this might be too little data\n",
        "244": "to fit these this many parameters but\n",
        "247": "commonly what we do then if M is less\n",
        "250": "than n is to COC we can either delete\n",
        "252": "some features or to use a technique\n",
        "253": "called regularization which is something\n",
        "255": "that we'll talk about later in this\n",
        "258": "course as well that will kind of let you\n",
        "260": "fit in all the parameters\n",
        "262": "use of all the features even if you have\n",
        "263": "a relatively small training set but is\n",
        "265": "this this regularization will be a later\n",
        "268": "topic in this course but to summarize if\n",
        "271": "ever you find that X transpose x is\n",
        "274": "singular or alternatively finder is\n",
        "275": "non-invertible\n",
        "278": "what I would recommend you do is first\n",
        "280": "look at your features and see if you\n",
        "282": "have redundant features like this x1 x2\n",
        "284": "you are being linearly dependent or\n",
        "286": "being a linear function of each other\n",
        "288": "like so and if you do have redundant\n",
        "290": "features and if you just you know delete\n",
        "291": "one of these features you really don't\n",
        "293": "need both of these features to just\n",
        "295": "delete one of these features that would\n",
        "298": "solve your non-invertibility problem and\n",
        "300": "so we first think through my features\n",
        "302": "and check if any are redundant and if so\n",
        "304": "then you know to keep deleting redundant\n",
        "306": "features until they no longer redundant\n",
        "309": "and if your features not redundant\n",
        "310": "I will check if I might have too many\n",
        "312": "features and if that's the case I would\n",
        "315": "either delete some features if I can\n",
        "317": "bear to use fewer features or else I\n",
        "319": "would consider using regularization\n",
        "321": "which is this topic that we'll talk\n",
        "325": "about later so that's it for the normal\n",
        "328": "equation and what it means for if the\n",
        "330": "matrix x transpose x is non-invertible\n",
        "332": "but this is a problem that you should\n",
        "335": "run hopefully you run into pretty rarely\n",
        "337": "and if you just implement it in octave\n",
        "340": "using p in using the p in function which\n",
        "342": "is called the pseudo-inverse function so\n",
        "344": "we use a different linear algebra\n",
        "346": "library it's called a pseudo-inverse\n",
        "348": "but that implementations should just do\n",
        "351": "the right thing even if x transpose x is\n",
        "353": "non-invertible which should happen\n",
        "355": "pretty rarely anyway so this should not\n",
        "357": "be a problem for most implementations of\n"
    },
    "F_VG4LNjZZw": {
        "0": " \n",
        "2": "in the last video we talked about the\n",
        "5": "hypothesis representation for logistic\n",
        "8": "regression what I like to do now is tell\n",
        "9": "you about something called the decision\n",
        "12": "boundary and this will give us a better\n",
        "14": "sense of what the logistic regressions\n",
        "17": "hypothesis function is computing to\n",
        "20": "recap this is where we wrote our last\n",
        "23": "time where we said that the hypothesis\n",
        "25": "is represented as H of X equals G of\n",
        "28": "theta transpose X where G is this\n",
        "30": "function called the sigmoid function\n",
        "33": "which looks like this so if slowly\n",
        "38": "increases from 0 to 1 asymptoting at 1\n",
        "40": "what I want to do now is try to\n",
        "43": "understand better when this hypothesis\n",
        "45": "will make predictions that Y is equal to\n",
        "47": "1 versus when it might make predictions\n",
        "50": "that Y is equal to 0 and understand\n",
        "52": "better what the hypothesis function\n",
        "54": "looks like particularly when we have\n",
        "57": "more than one feature concretely this\n",
        "60": "hypothesis is outputting estimates of\n",
        "62": "the probability that Y is equal to 1\n",
        "65": "given X and parametrized by theta so if\n",
        "66": "given X and parametrized by theta so if\n",
        "68": "we wanted to predict is y equals to 1 or\n",
        "71": "is y equal to 0 here's something we\n",
        "73": "might do whenever the hypothesis outputs\n",
        "76": "that the probability of Y being 1 is\n",
        "78": "greater than or equal to 0.5 so this\n",
        "81": "means that it is more likely to be y\n",
        "84": "equals 1 then y equals 0 then let's\n",
        "87": "predict y equals 1 and otherwise if the\n",
        "90": "probably of the estimated probability of\n",
        "93": "Y being 1 is less than 0.5 then let's\n",
        "96": "predict y equals 0 and I chose a greater\n",
        "98": "than or equal to here and less than here\n",
        "102": "if H of X is equal to 0.5 exactly then\n",
        "104": "you know we could predict positive or\n",
        "106": "negative but I put it greater than or\n",
        "108": "equal to 2 here so we default maybe to\n",
        "112": "predicting a positive if H of X is 0.5\n",
        "114": "but that's a detail that really doesn't\n",
        "117": "matter that much what I want to do is\n",
        "119": "understand better when is it exactly\n",
        "122": "that H of X will be greater than or\n",
        "125": "equal to 0.5 so that will end up\n",
        "130": "predicting Y is equal to 1 if we look at\n",
        "133": "this plot of the sigmoid function\n",
        "137": "notice that the sigmoid function G of Z\n",
        "141": "is greater than or equal to 0.5 whenever\n",
        "145": "Z is greater than or equal to 0\n",
        "148": "so as in this half of the figure that\n",
        "151": "you know G takes on values that are 0.5\n",
        "153": "and higher because it's not chair that's\n",
        "156": "the 0.5 and so when G when Z is positive\n",
        "159": "G of Z the sigmoid function is greater\n",
        "162": "than or equal to 0.5 since the\n",
        "165": "hypothesis for logistic regression is H\n",
        "169": "of X equals G of theta transpose X this\n",
        "172": "is therefore going to be greater than or\n",
        "178": "equal to 0.5 whenever theta transpose X\n",
        "182": "is greater than or equal to 0 so what\n",
        "184": "we've shown right because here theta\n",
        "188": "transpose X takes the row of Z so what\n",
        "191": "we've shown is that our hypothesis is\n",
        "193": "going to predict y equals 1 whenever\n",
        "196": "theta transpose X is greater than or\n",
        "199": "equal to 0 let's now consider the other\n",
        "202": "case of when our hypothesis will predict\n",
        "206": "Y is equal to 0 well by similar argument\n",
        "209": "H of X is going to be less than 0.5\n",
        "213": "whenever G of Z is less than 0.5 because\n",
        "217": "the range of values of Z that cause G of\n",
        "220": "Z to take on values less than 0.5 well\n",
        "223": "that's when Z is negative so when G of Z\n",
        "226": "is less than 0.5 our hypothesis will\n",
        "229": "predict that Y is equal to 0 and by\n",
        "231": "similar argument to what we had earlier\n",
        "234": "H of X is equal to G of theta transpose\n",
        "238": "X and so we'll predict y equals 0\n",
        "242": "whenever this quantity theta transpose X\n",
        "246": "is less than 0 to summarize what we just\n",
        "249": "worked out we saw that if we decide to\n",
        "251": "predict whether Y is equal to 1 or Y is\n",
        "253": "equal to 0 depending on whether the\n",
        "255": "estimated probability is greater than or\n",
        "258": "equal to 0.5 or whether it's less than\n",
        "260": "0.5 then that's the same as saying that\n",
        "263": "will predict y equals 1 whenever theta\n",
        "265": "transpose X is straight in\n",
        "267": "then or equal to zero and will predict y\n",
        "269": "is equal to zero whenever theta\n",
        "273": "transpose X is less than zero let's use\n",
        "275": "this to better understand how the\n",
        "278": "hypothesis of logistic regression makes\n",
        "281": "its predictions now let's suppose we\n",
        "283": "have a training set like that shown on\n",
        "286": "this slide and suppose our hypothesis is\n",
        "289": "H of x equals G of theta 0 plus theta 1\n",
        "293": "X 1 plus theta 2 X 2 we haven't helped\n",
        "295": "yet about how to fit the parameters of\n",
        "297": "this model we'll talk about that in the\n",
        "300": "next video but suppose that very\n",
        "303": "procedure to be specified we end up\n",
        "305": "choosing the following values for the\n",
        "307": "parameters let's say we choose theta 0\n",
        "312": "equals 3 theta 1 equals 1 theta 2 equals\n",
        "315": "1 so this means that my parameter vector\n",
        "323": "is going to be theta equals minus 3 1 1\n",
        "327": "so we're given this choice of my\n",
        "328": "so we're given this choice of my\n",
        "330": "hypothesis parameters let's try to\n",
        "333": "figure out where our hypothesis will end\n",
        "335": "up predicting y equals 1 and where it\n",
        "339": "would end up predicting y equals 0 using\n",
        "341": "the formulas that we worked out on the\n",
        "344": "previous slide we know that y equals 1\n",
        "345": "is more likely that is the probability\n",
        "349": "that y equals 1 is greater than 0.5 or\n",
        "352": "greater than or equal to 0.5 whenever\n",
        "356": "theta transpose X is greater than 0 and\n",
        "359": "this formula that I just underlined\n",
        "363": "minus 3 plus x1 plus x2 is of course\n",
        "367": "theta transpose X when theta is equal to\n",
        "370": "this value of the parameters that we\n",
        "374": "just chose so for any example for any\n",
        "378": "example with features x1 and x2 that\n",
        "381": "satisfy this equation then minus 3 plus\n",
        "384": "x1 plus x2 is greater than or equal to 0\n",
        "387": "all hypotheses will think that y equals\n",
        "389": "1 is more likely or will predict that Y\n",
        "391": "is equal to 1\n",
        "394": "we can also take minus three and bring\n",
        "397": "this to the right and rewrite this as X\n",
        "401": "1 plus X 2 is greater than or equal to 3\n",
        "404": "and so equivalently we found that this\n",
        "406": "hypothesis will predict y equals 1\n",
        "409": "whenever X 1 plus X 2 is greater than or\n",
        "413": "equal to 3 let's see what that means on\n",
        "416": "the figure if I write down the equation\n",
        "421": "x1 plus x2 equals 3 this defines the\n",
        "423": "equation of a straight line and if I\n",
        "426": "draw what that straight line looks like\n",
        "428": "it gives me the following line which\n",
        "432": "passes through 3 &amp; 3 on the x1 and the\n",
        "437": "x2 axes so the part of the input space\n",
        "440": "the part of the X 1 X 2 plane that\n",
        "443": "corresponds to when X 1 plus X 2 is\n",
        "445": "greater than or equal to 3 that's going\n",
        "447": "to be this right half plane that is\n",
        "450": "everything to the up and everything to\n",
        "452": "the upper right portion of this magenta\n",
        "455": "line that I just drew and so the region\n",
        "457": "where our hypothesis will predict y\n",
        "459": "equals 1 is this region you know it's\n",
        "461": "really this huge region this half space\n",
        "464": "over to the upper right and let me just\n",
        "466": "write that down I'm going to call this\n",
        "470": "the y equals 1 region and in contrast\n",
        "476": "the region where x1 plus x2 is less than\n",
        "480": "3 that's when we will predict that Y is\n",
        "483": "equal to 0 and that corresponds to this\n",
        "486": "region it's really a half plane but that\n",
        "489": "region on the left is the region where\n",
        "492": "our hypothesis will predict y equals 0 I\n",
        "494": "want to give this line this magenta line\n",
        "495": "want to give this line this magenta line\n",
        "498": "that I drew a name this line there is\n",
        "504": "called the decision boundary and\n",
        "507": "concretely this straight line x1 plus x2\n",
        "510": "equals 3 that corresponds to the center\n",
        "512": "point so that corresponds to the region\n",
        "516": "where H of X is equal to 0.5 exactly and\n",
        "518": "the decision boundary that is this\n",
        "521": "straight line that's the line that\n",
        "523": "separates the region where the\n",
        "526": "when XY equals one from the region where\n",
        "528": "the hypothesis predicts that y is equal\n",
        "531": "to zero and just to be clear the\n",
        "534": "decision boundary is a property of the\n",
        "538": "hypothesis including the parameters\n",
        "541": "theta 0 theta 1 theta 2 and in the\n",
        "543": "figure I drew a training set I drew a\n",
        "545": "data set in order to help the\n",
        "547": "visualization but even if we take away\n",
        "548": "the data set\n",
        "551": "you know this decision boundary and the\n",
        "552": "region where we predict y equals one\n",
        "555": "versus y equals zero that's a property\n",
        "557": "of the hypothesis and of the parameters\n",
        "559": "of the hypothesis and not the property\n",
        "563": "of the data set later on of course we'll\n",
        "565": "talk about how to fit the parameters and\n",
        "567": "there we'll end up using the training\n",
        "570": "set using our data to determine the\n",
        "573": "value of the parameters but once we have\n",
        "575": "particular values for the parameters\n",
        "578": "theta 0 theta 1 theta 2 then that\n",
        "580": "completely defines the decision boundary\n",
        "583": "and we don't actually need to plot a\n",
        "585": "training set in order to plot the\n",
        "588": " \n",
        "591": "let's now look at a more complex example\n",
        "594": "where as usual I have crosses to denote\n",
        "596": "my positive examples and O's to denote\n",
        "599": "my negative examples given a training\n",
        "601": "set like this how can I get logistic\n",
        "604": "regression to fit this sort of data\n",
        "606": "earlier when we were talking about\n",
        "608": "polynomial regression or when we're\n",
        "610": "talking about linear regression we\n",
        "612": "talked about how we can add extra higher\n",
        "614": "order polynomial terms to the features\n",
        "617": "and we can do the same for logistic\n",
        "620": "regression concretely let's say my\n",
        "622": "hypothesis looks like this where I've\n",
        "625": "added two extra features x1 squared and\n",
        "628": "x2 squared to my features so that I now\n",
        "630": "have five parameters theta 0 through\n",
        "635": "theta four as before we'll defer to the\n",
        "637": "next video our discussion on how to\n",
        "640": "automatically choose values for the\n",
        "643": "parameters theta 0 through theta 4 but\n",
        "645": "let's say that very procedure to be\n",
        "648": "specified I end up choosing theta 0\n",
        "652": "equals minus 1 theta 1 equals 0 theta 2\n",
        "656": "equals 0 theta 3 equals 1 and theta 4\n",
        "660": "equals 1 what this means is that with\n",
        "663": "this particular choice of parameters my\n",
        "665": "parameter that vector theta looks like\n",
        "671": "minus 1 0 0 1 1 following our earlier\n",
        "673": "discussion this means that my hypothesis\n",
        "675": "will predict that Y is equal to 1\n",
        "678": "whenever minus 1 plus x1 squared plus x2\n",
        "681": "squared is greater than or equal to 0\n",
        "684": "this is whenever theta transpose times\n",
        "687": "my theta transpose my features is\n",
        "691": "greater than or equal to 0 and if I take\n",
        "693": "minus 1 and just bring this to the right\n",
        "695": "I'm saying that my hypothesis will\n",
        "699": "predict that Y is equal to 1 whenever x1\n",
        "702": "squared plus x2 squared is greater than\n",
        "706": "or equal to 1 so what does this decision\n",
        "708": "boundary look like well if you were to\n",
        "712": "plot the curve for x1 squared plus x2\n",
        "715": "squared equals 1 some of you will\n",
        "717": "recognize that that is the equation for\n",
        "719": "a circle\n",
        "723": "a radius one centered around the origin\n",
        "730": "so that is my decision boundary and\n",
        "732": "everything outside the circle I'm going\n",
        "736": "to predict as y equals one so out here\n",
        "740": "is my y equals one region predict y\n",
        "743": "equals one out here and inside the\n",
        "746": "circle is where I predict Y is equal to\n",
        "750": "0 so by adding these more complex or\n",
        "752": "these polynomial terms to my features as\n",
        "754": "well I can get more complex decision\n",
        "756": "boundaries that don't just try to\n",
        "758": "separate the positive and negative\n",
        "759": "examples of a straight line but I can\n",
        "762": "get in this example a decision boundary\n",
        "765": "that's a circle once again the decision\n",
        "768": "boundary is a property not of the\n",
        "770": "training set but of the hypothesis enter\n",
        "773": "the parameters so so long as we given my\n",
        "776": "parameter vector theta that defines the\n",
        "778": "decision boundary which is the circle\n",
        "781": "but the training set is not what we use\n",
        "783": "to define the decision boundary the\n",
        "785": "training set may be used to fit the\n",
        "787": "parameters theta we'll talk about how to\n",
        "789": "do that later but once you have the\n",
        "792": "parameters theta that is what defines\n",
        "793": "the decision boundary\n",
        "796": "let me put back the training set just\n",
        "799": "just for visualization and finally let's\n",
        "802": "look at a more complex example so can we\n",
        "803": "look at a more complex example so can we\n",
        "804": "come up with even more complex decision\n",
        "808": "boundaries Enders if I have even higher\n",
        "813": "order polynomial terms so things like x1\n",
        "816": "squared x1 squared x2 x1 squared x2\n",
        "818": "squared and so on if I have much higher\n",
        "821": "order polynomials then it is possible to\n",
        "823": "show that you can get even more complex\n",
        "825": "decision boundaries and the just\n",
        "828": "regression can be used to find decision\n",
        "830": "boundaries that may for example be an\n",
        "832": "ellipse like that or maybe with a\n",
        "834": "different setting of the parameters\n",
        "836": "maybe you can get and set a different\n",
        "838": "decision boundary which may even look\n",
        "842": " \n",
        "846": "or for even more complex examples may be\n",
        "848": "you can also get decision boundaries\n",
        "850": "that could look like you know more\n",
        "852": "complex shapes like that where\n",
        "855": "here you predict y equals 1 and\n",
        "857": "everything outside you predict y equals\n",
        "859": "0 so these higher order polynomial\n",
        "861": "features you can get very complex as a\n",
        "863": "new boundaries so with these\n",
        "865": "visualizations I hope that gives you a\n",
        "868": "sense of what's the range of hypothesis\n",
        "870": "functions we can represent using the\n",
        "872": "representation that we have for logistic\n",
        "876": "regression now that we know what H of X\n",
        "878": "can represent what I'd like to do next\n",
        "881": "in the following video is talk about how\n",
        "883": "to automatically choose the parameters\n",
        "885": "theta so that I given a training set we\n",
        "888": "can automatically fit the parameters to\n"
    },
    "G97ZtT8mKXk": {
        "0": " \n",
        "1": "you now know about these stochastic\n",
        "4": "gradient descent algorithm but when\n",
        "5": "you're running the algorithm how do you\n",
        "7": "make sure that is sort of completely\n",
        "10": "debug than this conversion okay and\n",
        "13": "equally important how do you tune the\n",
        "15": "learning rate alpha was the costly\n",
        "17": "gradient descent in this video we'll\n",
        "19": "talk about some techniques for doing\n",
        "20": "these things for making sure it's\n",
        "23": "converging and for picking the learning\n",
        "27": "rate alpha back when we were using batch\n",
        "28": "gradient descent\n",
        "30": "I'll stand away from making sure that\n",
        "32": "gradient descent was converging was we\n",
        "34": "would plot the optimization cost\n",
        "35": "function as a function of the number of\n",
        "39": "iterations so that was the cost function\n",
        "41": "and we would make sure that this cost\n",
        "43": "function is decreasing on every\n",
        "46": "iteration when the training set size was\n",
        "47": "small we could do that because we could\n",
        "50": "compute this sum pretty efficiently that\n",
        "52": "when you have a massive training set\n",
        "55": "size then you don't want to have to\n",
        "58": "pause your algorithm periodically you\n",
        "59": "don't want to have the paused stochastic\n",
        "60": "don't want to have the paused stochastic\n",
        "62": "gradient descent periodically in order\n",
        "64": "to compute this cost function since it\n",
        "66": "requires a sum over your entire training\n",
        "68": "set size and the whole point of\n",
        "70": "stochastic gradient descent was that you\n",
        "72": "wanted to start to make progress after\n",
        "75": "looking at just a single example what\n",
        "77": "about needing to occasionally scan\n",
        "79": "through your entire training set right\n",
        "81": "in the middle of the algorithm just to\n",
        "83": "compute things like the cost function\n",
        "86": "over your entire training set so first\n",
        "89": "oh constant gradient descent in order to\n",
        "91": "check that the Abril is converging\n",
        "93": "here's what we can do instead let's take\n",
        "95": "the definition of the cost that we have\n",
        "97": "previously so the cost of the parameters\n",
        "98": "theta with respect to a single training\n",
        "101": "example is just 1/2 of the squared error\n",
        "103": "on that tree\n",
        "105": "then while stochastic gradient descent\n",
        "108": "is learning right before we train on a\n",
        "110": "specific example so in Stoke African\n",
        "112": "descent we're going to look at the\n",
        "114": "examples X I Y I in order and then sort\n",
        "116": "of take a little update with respect to\n",
        "118": "this example and then go on to the next\n",
        "119": "example\n",
        "124": "X I plus 1 y I plus 1 and so on right\n",
        "125": "that's what's the cost appear descent\n",
        "129": "does so while the algorithm is looking\n",
        "133": "at the example X I Y I but before it has\n",
        "136": "updated the parameters data using that\n",
        "139": "example let's compute the cost of that\n",
        "142": "example just to say the same thing again\n",
        "144": "by using slightly different words as\n",
        "146": "stochastic gradient descent is scanning\n",
        "149": "through our training set right before we\n",
        "151": "have updated theta using a particular\n",
        "153": "training example X I comma Y I let's\n",
        "156": "compute how well our hypothesis is doing\n",
        "158": "on that training example yeah and we\n",
        "160": "want to do this before updating theta\n",
        "162": "because if we've just updated theta\n",
        "164": "using that training example you know\n",
        "166": "then it might be doing better on that\n",
        "169": "example then would be representative\n",
        "171": "finally in order to check for the\n",
        "172": "convergence of stochastic gradient\n",
        "176": "descent what we can do is every say\n",
        "178": "every thousand innovations we can plot\n",
        "180": "these costs that we've been computing\n",
        "182": "and previous step we can plot those\n",
        "185": "costs average over say the last thousand\n",
        "187": "examples processed by the algorithm and\n",
        "190": "if you do this it kind of gives you a\n",
        "191": "running estimate of how well the\n",
        "193": "algorithm is doing on you know the last\n",
        "196": "1,000 training examples that your\n",
        "199": "algorithm has seen so in contrast to\n",
        "202": "computing g-train periodically which\n",
        "203": "needed to scan through the entire\n",
        "205": "training set with this order procedure\n",
        "207": "well as part of stochastic gradient\n",
        "209": "descent it doesn't cost much to compute\n",
        "211": "these costs as well right before\n",
        "213": "updating to parameter theta and all\n",
        "215": "we're doing is every thousand iterations\n",
        "218": "or so we just average the last 1,000\n",
        "220": "cost that we computed and we plot that\n",
        "223": "and by looking at those plots this will\n",
        "225": "allow us to\n",
        "226": "if stochastic great into census\n",
        "229": "conversion so here are a few examples of\n",
        "232": "what these plots might look like suppose\n",
        "234": "we were to plot the cost average over\n",
        "236": "the last thousand examples because these\n",
        "238": "are average over just a thousand\n",
        "240": "examples they are going to be a little\n",
        "242": "bit noisy and so it may not decrease on\n",
        "244": "every single iteration but if you get a\n",
        "247": "figure that looks like this so the plot\n",
        "249": "is noisy because this average over\n",
        "250": "you're just a small subset say a\n",
        "253": "thousand training examples you can think\n",
        "254": "of that looks like this you know that\n",
        "256": "would be a pretty decent run with the\n",
        "258": "algorithm maybe where it looks like the\n",
        "261": "cost has gone down and then this plateau\n",
        "262": "that was kind of flattened now you're\n",
        "264": "starting from around that point so it\n",
        "266": "looks like that there's no way your cost\n",
        "268": "looks like then maybe your learning\n",
        "271": "algorithm has converged if you were to\n",
        "272": "try using a smaller learning rate\n",
        "274": "something that you might see it is that\n",
        "276": "the opera may initially learn more\n",
        "279": "slowly so the cost goes down more slowly\n",
        "281": "but then eventually you have a smaller\n",
        "283": "learning rate there's actually possible\n",
        "286": "for the algorithm to end up at a maybe a\n",
        "288": "very slightly better solution so the red\n",
        "290": "line may represent the behavior of\n",
        "291": "stochastic we understand using a slower\n",
        "294": "using a smaller learning rate and the\n",
        "296": "reason this is the case is because you\n",
        "298": "remember stochastic gradient descent\n",
        "300": "doesn't just converge the global minimum\n",
        "302": "is that what it does is the parameters\n",
        "304": "will oscillate a bit around the global\n",
        "306": "minimum and so by using a smaller\n",
        "308": "learning rate you end up with smaller\n",
        "311": "oscillations and sometimes this little\n",
        "313": "difference will be negligible and\n",
        "315": "sometimes with a smaller learning ready\n",
        "319": "can get a slightly better value for the\n",
        "320": "parameters here are some other things\n",
        "322": "that might happen let's say he runs the\n",
        "324": "Kospi in descent and you average over a\n",
        "327": "thousand examples when when plotting\n",
        "329": "these costs so you know here it might be\n",
        "332": "a result of another one of these plots\n",
        "333": "and again the trial looks like this\n",
        "334": "converge\n",
        "338": "if you were to take this number 8,000\n",
        "341": "and increase it to averaging over 5,000\n",
        "343": "examples then it's possible that you\n",
        "347": "might get a smoother curve that looks\n",
        "350": "more like this and by averaging over say\n",
        "354": "5,000 examples instead of 1,000 you\n",
        "355": "might be really get a smoother curve\n",
        "356": "might be really get a smoother curve\n",
        "357": "like this and so that's the effect of\n",
        "358": "like this and so that's the effect of\n",
        "359": "increasing the number of examples your\n",
        "361": "average over the disadvantage of making\n",
        "363": "this too big of course is that now you\n",
        "365": "get one day to point only every 5,000\n",
        "368": "examples and so the feedback you get on\n",
        "370": "how well your learning outcome is doing\n",
        "371": "is sort of maybe it's more delay because\n",
        "374": "you get one data point on your plot or\n",
        "376": "the every 5,000 examples rather than\n",
        "380": "early 1001 was similar vein sometimes\n",
        "382": "you may run stochastic great descent and\n",
        "384": "end up with a plot that looks like this\n",
        "386": "and with a plot that looks like this you\n",
        "391": "know it looks like the cost just is not\n",
        "392": "the appeasing at all it looks like the\n",
        "394": "algorithm is just not learning well it\n",
        "396": "just looks like bithia flat curve and\n",
        "398": "it's just the cost is just not\n",
        "401": "decreasing but again if you were to\n",
        "405": "increase this to averaging over a larger\n",
        "407": "number of examples it's possible that\n",
        "409": "you see something like this red line it\n",
        "411": "looks like the cost actually is\n",
        "413": "decreasing it's just that the blue line\n",
        "415": "averaging over two three examples the\n",
        "417": "blue line was too noisy so you couldn't\n",
        "420": "see the actual trend in the cost\n",
        "423": "actually decreasing and possibly\n",
        "425": "averaging over 5,000 examples and said\n",
        "427": "1,000 may help of course when you\n",
        "429": "average over a larger number of examples\n",
        "431": "which was average here over 5,000\n",
        "433": "examples to just use a different color\n",
        "435": "it's also possible that you see that the\n",
        "437": "learning curve ends up looking like this\n",
        "439": "there's still fact even when you're\n",
        "440": "averaging over a larger number of\n",
        "443": "examples and if you get that then that's\n",
        "446": "maybe just a more firm verification that\n",
        "448": "unfortunately the outer just isn't very\n",
        "449": "much for whatever reason\n",
        "451": "and you need to either change the\n",
        "453": "learning rate or change the features or\n",
        "454": "change something else about the\n",
        "456": "algorithm finally one last thing that\n",
        "458": "you might see would be if you were to\n",
        "460": "plot these curves and you see a curve\n",
        "461": "that looks like this where it actually\n",
        "464": "looks like this increasing and if that's\n",
        "466": "the case then this is a sign that the\n",
        "469": "algorithm is diverging and what you\n",
        "471": "really should do is use a smaller value\n",
        "475": "of the learning rate alpha so hopefully\n",
        "477": "this gives you a sense of the range of\n",
        "478": "phenomena you might see when you plot\n",
        "481": "these cost average over some range of\n",
        "484": "examples as well as suggest the sorts of\n",
        "486": "things you might try to do in response\n",
        "488": "to seeing different plots I serve the\n",
        "491": "plot looks too noisy of it so it goes up\n",
        "493": "and down too much then try increasing\n",
        "495": "the number of examples you're averaging\n",
        "497": "over so you can see the overall trend in\n",
        "501": "the plot better and if you see that the\n",
        "502": "errors are actually increasing the costs\n",
        "504": "are actually increasing try using a\n",
        "507": "smaller value of alpha finally it's\n",
        "509": "worth examining the issue of the\n",
        "512": "learning rate just a little bit more we\n",
        "514": "saw that when you run stochastic green\n",
        "516": "descent the algorithm will start here\n",
        "519": "and sort of meander towards the minimum\n",
        "521": "and then it won't really converge but\n",
        "522": "instead a wonder around the minimum\n",
        "525": "forever and so you end up with a\n",
        "526": "parameter value that is hopefully close\n",
        "528": "to the global minimum that won't be\n",
        "531": "exactly at the global minimum in most\n",
        "533": "typical implementations of stochastic\n",
        "536": "gradient descent the learning rate alpha\n",
        "538": "is typically held constant and so what\n",
        "540": "you typically ends up with is exactly\n",
        "543": "the picture like this if you want\n",
        "545": "stochastic Granderson to actually\n",
        "547": "converge to the global minimum there's\n",
        "549": "one thing which you can do which is you\n",
        "550": "can slowly decrease the learning rate\n",
        "554": "alpha over time so a pretty typical way\n",
        "557": "of doing that would be to set alpha\n",
        "559": "equals some constant 1 divided by\n",
        "560": "equals some constant 1 divided by\n",
        "562": "iteration number plus constant 2 so\n",
        "564": "aeration number is the number of\n",
        "566": "iterations you've run of Circosta\n",
        "568": "quintessence is really the number of\n",
        "570": "training examples you've seen and cons 1\n",
        "573": "and cos 2 are additional parameters of\n",
        "575": "then you might have to play with a bit\n",
        "579": "in order to get good performance one of\n",
        "580": "the reason is people tend not to do this\n",
        "582": "is because you end up needing to spend\n",
        "584": "time playing with these two extra\n",
        "586": "parameters cause when it comes to and so\n",
        "588": "this makes the average more finicky it\n",
        "590": "was just more parameters easy to fiddle\n",
        "591": "with in order to meet the other work\n",
        "594": "well but if you manage to tune the\n",
        "596": "parameters well then the picture you can\n",
        "598": "get is that the algorithm will actually\n",
        "601": "meander around towards the minimum but\n",
        "603": "as it gets closer because you're\n",
        "605": "decreasing the learning rate the\n",
        "606": "meanderings would get smaller and\n",
        "608": "smaller until it pretty much just\n",
        "611": "converges to the global minimum I hope\n",
        "613": "this makes sense right and the reason\n",
        "615": "this formula makes sense is because as\n",
        "617": "the album runs the iteration number\n",
        "620": "becomes large and so alpha will slowly\n",
        "623": "become small and so you take smaller and\n",
        "625": "smaller steps until it some of you know\n",
        "627": "hopefully converges to the global\n",
        "630": "minimum so if you do slowly decrease\n",
        "632": "alpha to zero you can end up with a\n",
        "634": "slightly better hypothesis but because\n",
        "636": "of the extra work needed to fit of the\n",
        "639": "constants and because frankly usually\n",
        "641": "were pretty happy with any parameter\n",
        "643": "value that's you know pretty close to\n",
        "645": "global minimum typically this process of\n",
        "648": "decreasing alpha slowly is usually not\n",
        "650": "done and keeping the learning rate alpha\n",
        "653": "constant is the more common application\n",
        "655": "mr. Casas Grandes and although you will\n",
        "658": "see people using either version\n",
        "661": "to summarize in this video we talked\n",
        "663": "about a way for approximately monitoring\n",
        "666": "how stochastic gradient descent is doing\n",
        "668": "in terms of optimizing the cost function\n",
        "670": "and this is a method that does not\n",
        "672": "require scanning over the entire\n",
        "675": "training set periodically to compute the\n",
        "677": "cost function on the entire training set\n",
        "679": "but instead it looks at I'd say only the\n",
        "682": "last thousand examples or so and you can\n",
        "684": "use this method both to make sure that\n",
        "686": "stochastic gradient descent is really\n",
        "689": "okay and is converging or to use it to\n"
    },
    "GZqgeFBxOKc": {
        "0": " \n",
        "1": "now that you know how to load and save\n",
        "3": "data and alter put your data into\n",
        "5": "matrices and so on in this video I'd\n",
        "8": "like to show you how to do computational\n",
        "11": "operations on data and later on we'll be\n",
        "13": "using these sorts of computational\n",
        "14": "operations to implement our learning\n",
        "15": "operations to implement our learning\n",
        "19": "algorithms let's get started here's my\n",
        "21": "octave window let me just quickly\n",
        "25": "initialize some variables to use for our\n",
        "27": "examples and set a to be a three by two\n",
        "32": "matrix and set B to a three by two\n",
        "36": "matrix and let's set C to a two by two\n",
        "40": "matrix like so now let's say I want to\n",
        "42": "multiply two of my matrices so let's say\n",
        "44": "I want to compute a times C I just type\n",
        "46": "a times C so it's a three by two matrix\n",
        "49": "times the 2 by 2 matrix this gives me\n",
        "52": "this 3 by 2 matrix you can also do\n",
        "55": "element wise operations into a dot times\n",
        "58": "B and what this will do is though taking\n",
        "60": "elements of a and multiply it by the\n",
        "63": "corresponding elements of B so that's a\n",
        "66": "does B that's a dot times B so for\n",
        "69": "example the first out first element is 1\n",
        "72": "times 11 which gives 11\n",
        "74": "the second element gives 2 times 12\n",
        "77": "which gives 24 and so on so this is the\n",
        "79": "element wise multiplication of two\n",
        "82": "matrices and in general the period tends\n",
        "85": "to is usually used to denote element\n",
        "88": "wise operations in octave so here is a\n",
        "91": "matrix a and if I do a not to carat 2\n",
        "94": "this gives me the multiplier element\n",
        "97": "wise squaring of a so you know 1 squared\n",
        "100": "is 1 2 squared is 4 and so on\n",
        "103": "let's set V to a vectors must and B is 1\n",
        "107": "2 3 is a column vector you can also do\n",
        "110": "one dot over B to do the element wise\n",
        "113": "reciprocal of V so this gives me 1 over\n",
        "115": "1 1 over 2 and 1 over 3 this were a\n",
        "118": "super matrices of 1 dot over a gives me\n",
        "122": "that element wise your inverse of a and\n",
        "124": "once again the period here gives us a\n",
        "126": "clue that this is an element wise\n",
        "129": "creation can also do things like log v\n",
        "132": "this is a element-wise logarithm of the\n",
        "136": "element of the the b e to the B is you\n",
        "139": "know base e exponentiation of these\n",
        "141": "elements so this is e this is e squared\n",
        "146": "EQ because this was B to this and I can\n",
        "149": "also do ABS V to take the element wise\n",
        "152": "absolute value of V so here you know V\n",
        "154": "was all positive related apps say minus\n",
        "157": "one two minus three the element wise\n",
        "159": "absolute value just me back these are\n",
        "164": "non negative values and negative V gives\n",
        "166": "me the minus of V this is the same as\n",
        "169": "negative 1 times V but usually just\n",
        "171": "write negative B instead of negative 1\n",
        "176": "times V and what else can you do here's\n",
        "178": "another neat trick so let's see let's\n",
        "180": "say I want to take V and increment each\n",
        "182": "of its elements by one well one way to\n",
        "188": "do it is by constructing a 3 by 1 vector\n",
        "191": "there's all one's and adding that to V\n",
        "193": "so if I do that this increments V by\n",
        "196": "from 1 2 3 2 2 3 4 there I did that was\n",
        "204": "length of V is 3 so once length of V by\n",
        "208": "1 this is let 1 of 3 by 1 so that's 1 3\n",
        "212": "by 1 all right and what I did was B plus\n",
        "216": "1 3 by 1 which is adding this vector of\n",
        "218": "all ones to be until this increments V\n",
        "222": "by one and you another simpler way to do\n",
        "224": "that sheets type B plus 1 right so yes V\n",
        "227": "and V plus 1 also means to add one\n",
        "231": "element wise to each of my elements of V\n",
        "235": "now let's talk about more operations so\n",
        "237": "here's my matrix a if you want to write\n",
        "239": "a transpose the way to do that is to\n",
        "242": "write a prime that's be a prosody symbol\n",
        "244": "is the left quote so and then on your\n",
        "246": "keyboard you probably have a left quote\n",
        "249": "and a right quote so this is a decision\n",
        "252": "of the standard quotation mark ISM I'll\n",
        "255": "just type a transpose this gives me the\n",
        "257": "you know transpose of my matrix\n",
        "260": "and of course a transpose if I transpose\n",
        "262": "that again then I should get back my\n",
        "266": "matrix 8 some more useful functions\n",
        "271": "let's say lowercase a is 1 15 to 0.5 so\n",
        "274": "it's a you know 1 by 4 matrix let's I\n",
        "277": "said thou equals max of a this returns\n",
        "279": "the maximum value of a which in this\n",
        "284": "case is 15 and I can do thou IND max a\n",
        "288": "and this returns now an end which are\n",
        "291": "going to be the maximum value of a which\n",
        "294": "is 15 as was the index o is the element\n",
        "296": "number 2 of a that was 15 so in since my\n",
        "300": "index into this just as a warning if you\n",
        "303": "do max a where a is a matrix what this\n",
        "305": "does is this does actually does the\n",
        "308": "column wise maximum but say a little bit\n",
        "311": "more about this in a second so using\n",
        "314": "this example the variable lowercase a if\n",
        "318": "I do a less than 3 this does the element\n",
        "320": "wise operation element wise comparison\n",
        "323": "so the first element of a is less than\n",
        "326": "three so this one second element of a is\n",
        "328": "not less than three so this values is\n",
        "330": "zero because it's false the third and\n",
        "333": "fourth elements of a are I'm in less\n",
        "335": "than three third and fourth elements are\n",
        "336": "less than three so this is one one so\n",
        "339": "this does the element wise comparison of\n",
        "341": "all four elements of the variable lower\n",
        "343": "case eight to three and it returns true\n",
        "345": "or false depending on whether or not is\n",
        "349": "less than three now if I do find a less\n",
        "352": "than three this will tell me which are\n",
        "354": "the elements of a the Devere for eight\n",
        "356": "or less than three and in this case the\n",
        "358": "first third and fourth elements are less\n",
        "361": "than three for my next example let me\n",
        "365": "set a to be equal to Magic three the\n",
        "367": "magic function returns on type help\n",
        "370": "magic the magic function returns\n",
        "373": "functions called magics the turns these\n",
        "375": "matrices called magic squares they have\n",
        "378": "this um you know mathematical property\n",
        "381": "that all of their rows and columns and\n",
        "383": "diagonals sum up to the same thing so\n",
        "384": "you know\n",
        "387": "it's not actually useful for machine\n",
        "389": "learning as far as I know but I'm just\n",
        "391": "using this as a convenient way you know\n",
        "394": "to generate a three by three matrix and\n",
        "396": "and this magic square screen right have\n",
        "398": "the property that each row each column\n",
        "401": "and the diagonals all add up to the same\n",
        "402": "thing so it's kind of a mathematical\n",
        "405": "construct I use magic I use this magic\n",
        "407": "function only when I'm doing demos or\n",
        "409": "when I'm teaching octave like this and I\n",
        "411": "don't actually use it for any you know\n",
        "414": "useful machine learning application but\n",
        "417": "let's see if I type RC equals find a\n",
        "420": "greater than or equals seven this finds\n",
        "424": "all the elements of a did I greater than\n",
        "426": "equal to seven and so RC sensor row and\n",
        "428": "column so the one one element is greater\n",
        "431": "than 7 the three two elements is greater\n",
        "432": "than 7 and a two three element is\n",
        "434": "greater than 7 so let's see the to three\n",
        "438": "element for example is a two three is\n",
        "441": "seven is this element out here and that\n",
        "444": "is indeed created equal seven by the way\n",
        "447": "I actually don't even memorize myself\n",
        "448": "what these fine functions do and what\n",
        "450": "all of these things to myself and\n",
        "451": "whenever I use the find function\n",
        "454": "sometimes I forget myself exactly what\n",
        "456": "it does and you know I type HopeLine to\n",
        "458": "look up the document okay just two more\n",
        "460": "things will quickly show you one is the\n",
        "463": "sum function so here's my a and then\n",
        "465": "type sum a this adds up all the elements\n",
        "468": "of a and if I multiply them together I\n",
        "471": "type prod a process or product and this\n",
        "473": "returns the product of these four\n",
        "478": "elements of a floor a rounds down these\n",
        "479": "elements of eight so zero point five\n",
        "482": "gets rounded down to zero and Cu or\n",
        "484": "ceiling a gets rounded up so zero point\n",
        "486": "five rounded up to the nearest integer\n",
        "488": "so zero point five gets rounded up to\n",
        "490": "one\n",
        "493": "you can also let's see you let me type\n",
        "495": "ran three the generator three by three\n",
        "499": "matrix if I type max ran three R and V\n",
        "502": "what this does is it takes the element\n",
        "505": "wise maximum of two random three by\n",
        "507": "three matrices so you know that all\n",
        "509": "these numbers tend to be a bit on the\n",
        "511": "large side because each of these is\n",
        "512": "actually the max\n",
        "515": "a randomly of element-wise mass of two\n",
        "518": "randomly generated matrices this is a\n",
        "520": "this is my magic number\n",
        "523": "this was my a magic square 3x3 a let's\n",
        "527": "say I type max a and then this weird\n",
        "530": "open close square bracket comma 1 what\n",
        "532": "this does is this takes the column wise\n",
        "535": "maximum so the max of the first column\n",
        "538": "is 8 max a 7 second column is 9 the max\n",
        "541": "in the third column is 7 this one means\n",
        "542": "to take the max along the first\n",
        "546": "dimension of a in contrast if I were to\n",
        "549": "type max a this funny notation to then\n",
        "552": "this takes the per row maximum so the\n",
        "554": "max to the first row is 8 men so second\n",
        "558": "row is 7 max of the third row is 9 and\n",
        "561": "so this allows you to take max is either\n",
        "565": "you know per row or per column and if\n",
        "568": "you want to and remember the default to\n",
        "570": "a column art wise elements you won't\n",
        "572": "find if you want to find the maximum\n",
        "575": "element in the entire matrix a you can\n",
        "579": "type max max of a like so which is 9 or\n",
        "582": "you can turn a into vector and type max\n",
        "586": "of a colon like so and this treats this\n",
        "589": "as a vector and takes the max of max\n",
        "593": "element of that vector finally let's set\n",
        "596": "a to be a 9 by 9 Magic Square so\n",
        "598": "remember the Magic Square has this\n",
        "601": "property that every column in every row\n",
        "602": "sums the same thing and also the\n",
        "604": "diagonals so here's a 9 by 9 matrix\n",
        "608": "square so let me do some a1 so this does\n",
        "610": "a per column sum so I'm going to take a\n",
        "613": "column of a and add them up and this you\n",
        "615": "know lessons verify that indeed for a 9\n",
        "617": "by 9 matrix square every column adds up\n",
        "618": "by 9 matrix square every column adds up\n",
        "620": "to 3 6 9 s of the same thing now let's\n",
        "622": "do the row wise some so there's some a\n",
        "628": "calmer of 2 and this sums up each row of\n",
        "631": "a and indeed each row of a also sums up\n",
        "632": "to 3 6 9\n",
        "635": "now let's sum the diagonal elements of a\n",
        "637": "and make sure the day that that also\n",
        "640": "sums up to the same thing so what I'm\n",
        "642": "going to do is construct a 9 by 9\n",
        "645": "identity matrix that's a 9\n",
        "647": "and we're going to take a and construct\n",
        "651": "your multiply a element-wise so here's\n",
        "656": "my matrix a going to 8 times i9 and what\n",
        "658": "this will do is take the element wise\n",
        "660": "product of these two matrices and so\n",
        "662": "this should wipe out everything in a\n",
        "666": "except for the diagonal entries and now\n",
        "669": "I'm going to do some sum of a of that\n",
        "674": "and this gives me the sum of this these\n",
        "677": "diagonal elements and indeed it is 3 6 9\n",
        "679": "you can sum up the other diagonal as\n",
        "680": "you can sum up the other diagonal as\n",
        "681": "well so instead of top left to bottom\n",
        "683": "right you can sum of the opposite\n",
        "685": "diagonal from bottom left to top right\n",
        "688": "the sum of the commands for this is a\n",
        "690": "somewhat more cryptic you don't really\n",
        "692": "need to know this I'm just showing you\n",
        "694": "this in case any of you are curious but\n",
        "698": "um let's see flip you flip you these\n",
        "701": "tends to flip up down but if you do that\n",
        "704": "that turns out to sum up the elements\n",
        "707": "and the opposites at the other diagram\n",
        "710": "of a and then also sums up to 3 6 9 here\n",
        "713": "let me show you where as I 9 is this\n",
        "718": "matrix flip up/down of I 9 you know\n",
        "720": "takes the identity matrix and flips it\n",
        "722": "vertically so you end up with excuse me\n",
        "725": "WD end up with ones on this opposite\n",
        "729": "diagonal as well just one last command\n",
        "731": "and they're not that's it and then I'll\n",
        "733": "that'll be it for this video let's say a\n",
        "736": "to be the magic 3x3 magic square game if\n",
        "738": "you want to invert a matrix you type P\n",
        "741": "in a just technically called the\n",
        "743": "pseudo-inverse but it does matter just\n",
        "745": "think it is basically the inverse of a\n",
        "748": "and that's the inverse of a and so I can\n",
        "752": "set you know 10 equals P in of a and if\n",
        "755": "I take 10 times a this is indeed the\n",
        "757": "identity matrix with essentially ones on\n",
        "759": "the diagonals and zeros on the off\n",
        "763": "diagonals up to a numerical roundoff\n",
        "766": "so that's it for how to do different\n",
        "769": "computational operations on the data and\n",
        "773": "matrices and after running a learning\n",
        "775": "algorithm often one of the most useful\n",
        "777": "things is to be able to look at your\n",
        "779": "results or to plot or visualize your\n",
        "781": "result and in the next video I'm going\n",
        "784": "to very quickly show you how again with\n",
        "786": "one or two lines of code using octave\n",
        "788": "you can quickly visualize your data or\n",
        "791": "plot your data and use that to better\n",
        "793": "understand what your learning algorithms\n"
    },
    "GtSf2T6Co80": {
        "0": " \n",
        "2": "in previous videos we talked about the\n",
        "4": "gradient descent algorithm and we talked\n",
        "7": "about the linear regression model and\n",
        "9": "the squared error cost function in this\n",
        "11": "video we're going to put together a\n",
        "13": "gradient descent with our cost function\n",
        "16": "and that will give us an algorithm for\n",
        "17": "linear regression for fitting a straight\n",
        "22": "line to our data so this is what we\n",
        "24": "worked out in their previous videos\n",
        "26": "there's our gradient descent algorithm\n",
        "29": "which should be familiar and here's the\n",
        "31": "linear regression model with our linear\n",
        "34": "linear hypothesis and our squared error\n",
        "37": "cost function what we're going to do is\n",
        "44": " \n",
        "48": "squared error cost function now in order\n",
        "50": "to apply gradient descent in order to\n",
        "53": "you know write this piece of code the\n",
        "56": "key term we need is this derivative term\n",
        "60": "over here so you need to figure out what\n",
        "63": "is this partial derivative term and\n",
        "66": "plugging in the definition of the cost\n",
        "73": "function J this turns out to be this sum\n",
        "76": "from I equals 1 through n of this\n",
        "81": "squared error cost function term and all\n",
        "83": "I did here was I just you know plugged\n",
        "85": "in the definition of the cost function\n",
        "88": "there and simplifying a little bit more\n",
        "94": "this turns out to be equal to this sum\n",
        "97": "from I equals 1 through m of theta 0\n",
        "103": "plus theta 1 X I minus y I squared and\n",
        "104": "all I did there was I took the\n",
        "107": "definition for my hypothesis and plugged\n",
        "110": "it in there and turns out we need to\n",
        "112": "figure out what is this partial\n",
        "115": "derivative for two cases for J equals 0\n",
        "117": "and for J equals 1 so we want to figure\n",
        "119": "out what is this partial derivative for\n",
        "122": "you know both the theta 0 case and the\n",
        "125": "theta 1 case and I'm just going to write\n",
        "127": "out the answers it turns out this first\n",
        "129": "term is\n",
        "134": "simplifies to 1 over m sum from open my\n",
        "139": "training set of just that X I minus y i\n",
        "143": "and for this term partial derivative\n",
        "146": "respect to theta 1 it turns out I get\n",
        "155": "this term minus y I times X I okay\n",
        "159": "and computing these partial derivatives\n",
        "161": "to let you know going from this equation\n",
        "164": "right going from this equation to either\n",
        "166": "of the equations down there computing\n",
        "168": "those partial derivative terms requires\n",
        "171": "some multivariate calculus if you know\n",
        "173": "calculus feel free to work through the\n",
        "175": "derivations yourself and check that if\n",
        "177": "you take the derivatives you actually\n",
        "180": "get the answers that I got but if you\n",
        "182": "are less familiar with calculus you know\n",
        "184": "don't worry about it and it's fine to\n",
        "186": "just take these equations that worked\n",
        "188": "out and you won't need to know calculus\n",
        "190": "or anything like that in order to do the\n",
        "192": "homeworks or to implement gradient\n",
        "194": "descent and get that to work but so\n",
        "196": "armed with these definitions or armed\n",
        "198": "with what we worked out to be the\n",
        "200": "derivatives which is really just the\n",
        "203": "slope of the cost function J we can now\n",
        "205": "plug them back into our gradient descent\n",
        "206": "algorithm\n",
        "208": "so here's gradient descent for linear\n",
        "209": "regression\n",
        "211": "we're going to repeat until convergence\n",
        "215": "theta 0 and theta 1 get updated as you\n",
        "217": "know just a minus alpha times the\n",
        "222": "derivative term so this term here so\n",
        "226": "here's our linear regression algorithm\n",
        "231": " \n",
        "232": " \n",
        "234": "that term is of course just the partial\n",
        "237": "derivative respect to theta 0 it will\n",
        "240": "worked out on a previous slide and this\n",
        "244": "second term here that term is just the\n",
        "248": "partial derivative respect to theta 1\n",
        "250": "then we worked out on the previous line\n",
        "253": "and just as a quick reminder you must\n",
        "255": "when implementing great incentives\n",
        "256": "actually this detail that you know you\n",
        "259": "should be implementing it so the update\n",
        "264": "theta 0 and theta 1 simultaneously so\n",
        "268": "let's see how gradient descent works one\n",
        "270": "of the issues we solve gradient descent\n",
        "271": "is that it can be susceptible to local\n",
        "274": "optima so when I first explained radians\n",
        "276": "then I showed you this picture of it you\n",
        "278": "know going down along the surface and we\n",
        "280": "saw how depending on where you\n",
        "281": "initialize it you can end up at\n",
        "284": "different local optima and 1wo here or\n",
        "287": "here but it turns out that the cost\n",
        "290": "function for gradient of cost function\n",
        "292": "for linear regression is always going to\n",
        "295": "be a bow shaped function like this um\n",
        "298": "the technical term for this is that this\n",
        "304": "is called a convex function and not\n",
        "306": "going to give the formal definition so\n",
        "309": "what is a convex function Co and V X but\n",
        "311": "then formally a convex function means a\n",
        "314": "bowl-shaped function you know kinda like\n",
        "317": "a bow shaped and so this function\n",
        "320": "doesn't have any local optima except for\n",
        "323": "the one global optimum and does gradient\n",
        "325": "descent on this type of cost function\n",
        "326": "which you get whenever you're using\n",
        "328": "linear regression it will always\n",
        "330": "converge to the global optimum because\n",
        "332": "there are no other local optima auditing\n",
        "335": "or global optimum so now let's see this\n",
        "339": "algorithm in action as usual here are\n",
        "342": "plots of the hypothesis function and of\n",
        "346": "my cost function J and so let's say I've\n",
        "349": "initialized my parameters at this value\n",
        "351": "you know let's say instead usually you\n",
        "354": "initialize your parameters at 0 0 theta\n",
        "358": "there we go zero but for illustration in\n",
        "359": "this in this particular implementation\n",
        "361": "of unison I've initialized you know\n",
        "363": "theta zero at about nine hundred and\n",
        "366": "theta one at about minus 0.1 okay\n",
        "370": "and so this corresponds to H of x equals\n",
        "375": "you know minus 900 minus 0.1 X so this\n",
        "377": "is this line so out here on on the cost\n",
        "380": "function now if we take one step with\n",
        "383": "great descent we end up going from this\n",
        "388": "point out here a little bit to the down\n",
        "390": "and left to that second point over there\n",
        "394": "and you notice that my line changed a\n",
        "396": "little bit and as I take another step of\n",
        "399": "grading the Zen my line on the Left will\n",
        "404": "change right and I've also moved to a\n",
        "408": "new point on my cost function and as I\n",
        "409": "take further steps of grading descent\n",
        "413": "I'm going down in cost right so my\n",
        "415": "parameter is instead of following this\n",
        "418": "trajectory and if you look on the left\n",
        "423": "this corresponds to hypotheses that you\n",
        "425": "know seem to be getting to be better and\n",
        "428": "better fits in the data until eventually\n",
        "431": "I have now wound up at the global\n",
        "434": "minimum and this global minimum\n",
        "437": "corresponds to this hypothesis which\n",
        "440": "gives me a good fit to the data\n",
        "444": "and so that's gradient descent and we've\n",
        "448": "just run it and gotten a good fit to my\n",
        "451": "data set of housing prices and you can\n",
        "454": "now use it to predict you know if your\n",
        "457": "friend has a house that costs website\n",
        "459": "1250 square feet you can now read off\n",
        "462": "the value and tell them that I know\n",
        "463": "maybe they can get two hundred and fifty\n",
        "468": "thousand dollars for their house finally\n",
        "471": "just to give this another name it turns\n",
        "473": "out that the algorithm that we just went\n",
        "476": "over is sometimes called batch gradient\n",
        "478": "descent and it turns out in machine\n",
        "480": "learning on there I feel like us machine\n",
        "482": "learning people were not always great at\n",
        "484": "giving me into algorithms but the term\n",
        "486": "batch gradient descent\n",
        "488": "means that refers to the fact that in\n",
        "490": "every step of gradient descent were\n",
        "493": "looking at all of the training examples\n",
        "495": "so in gradient descent you know when\n",
        "497": "computing different derivatives we're\n",
        "500": "computing these sums right this summer\n",
        "502": "so in every step of gradient descent we\n",
        "504": "end up computing something like this\n",
        "507": "that sums over our M training examples\n",
        "509": "and so the term batch gradient descent\n",
        "511": "refers the fact that we're looking at\n",
        "514": "the entire batch of training examples\n",
        "515": "again this is really maybe not a great\n",
        "517": "name but this is what machine learning\n",
        "518": "name but this is what machine learning\n",
        "520": "people call it and it turns out that\n",
        "522": "there are sometimes other versions of\n",
        "524": "gradient descent that are not batch\n",
        "526": "versions but that instead do not look at\n",
        "529": "the entire training set but look at\n",
        "531": "small subsets on the training set at a\n",
        "533": "time and we'll talk about those versions\n",
        "535": "later in the schools as well but for now\n",
        "536": "using the album you just learned about\n",
        "539": "or using batch gradient descent you now\n",
        "542": "know how to implement gradient descent\n",
        "546": "for linear regression so that's linear\n",
        "548": "regression with gradient descent if\n",
        "551": "you've seen advanced linear algebra\n",
        "553": "before so some of you may have taken a\n",
        "554": "calls in you know advanced linear\n",
        "557": "algebra you might know that there exists\n",
        "559": "a solution for numerically solving for\n",
        "562": "the minimum of the cost function J we're\n",
        "563": "about needing to use an iterative\n",
        "566": "algorithm like gradient descent later in\n",
        "567": "this course we'll talk about that method\n",
        "570": "as well that just solves for the minimum\n",
        "571": "the cost function J without needing you\n",
        "573": "know this multiple steps of gradient\n",
        "575": "descent that other method is called the\n",
        "578": "normal equations method and but in case\n",
        "580": "you've heard of that method it turns out\n",
        "582": "that gradient descent will scale better\n",
        "584": "to larger data sets than that normal\n",
        "587": "equation method and now that we know\n",
        "589": "about grading descent we'll be able to\n",
        "591": "use it in lots of different contexts and\n",
        "592": "we'll use it in lots of different\n",
        "595": "machine learning problems as well so\n",
        "598": "congrats on learning about your first\n",
        "600": "machine learning algorithm well later\n",
        "603": "have exercises in which we'll ask you to\n",
        "605": "implement gradient descent and hopefully\n",
        "606": "see these albums were open yourselves\n",
        "610": "but before that I first want to tell you\n",
        "611": "in the NICS of the video so the first\n",
        "614": "one to tell you about a generalization\n",
        "615": "of the gradient descent algorithm\n",
        "618": "they'll make it much more powerful and I\n",
        "619": "guess I'll tell you about that in the\n"
    },
    "HFtJbRKuwtI": {
        "0": " \n",
        "1": "in this video I'd like to tell you how\n",
        "3": "to write control statements for your\n",
        "6": "octave programs so things like for while\n",
        "9": "and if statements and also how to define\n",
        "12": "and use functions here's my octave\n",
        "15": "window let me first show you how to use\n",
        "17": "a for loop I'm going to start by setting\n",
        "19": "V to be a ten by one vector of Syrian\n",
        "22": "zeros now here's how I write a for loop\n",
        "24": "for I equals one to ten\n",
        "28": "that's a for I equals one colon ten and\n",
        "31": "let's see I'm going to set V of I equals\n",
        "36": "two to the power of I and finally and\n",
        "39": "the white space doesn't matter so I'm\n",
        "41": "putting the spaces just to make it look\n",
        "43": "nicely indented but you know the spacing\n",
        "46": "doesn't matter but if I do this then the\n",
        "49": "result is that we get set to you know to\n",
        "51": "about 1 to the power 2 and so on so this\n",
        "54": "is syntax well I equals 1 colon 10 that\n",
        "58": "makes I loop through the the values 1\n",
        "60": "through 10 and by the way you can also\n",
        "64": "do this by setting your indices equals 1\n",
        "68": "to 10 and so the indices this is an\n",
        "69": "array from 1 to 10 you can also write\n",
        "75": "for I equals indices and this is\n",
        "77": "actually the same as a for I equals 1\n",
        "80": "pretending do you know display I and and\n",
        "83": "this will do the same thing so that was\n",
        "85": "the for loop if you're familiar with\n",
        "89": "break and continue that's break and\n",
        "91": "continue statements you can also use\n",
        "93": "those inside loops in octave but first\n",
        "95": "let me show you how a while loop works\n",
        "100": "so here's a my vector V let's write a\n",
        "104": "while loop I equals 1 while I is less\n",
        "109": "than or equal to 5 let's set V I equals\n",
        "116": "one hundred and increment I by 1 and so\n",
        "120": "this is what I start off equal to 1 and\n",
        "122": "then I'm going to set VI equals one\n",
        "124": "hundred and increment by one\n",
        "127": "two eyes you know greater than five and\n",
        "130": "as a result of that whereas previously V\n",
        "133": "was this powers of two vector I've now\n",
        "136": "taken the first five elements of my\n",
        "138": "vector and overwritten them with this\n",
        "140": "value 100 so that's the syntax for a\n",
        "144": "wire loop let's do another example I\n",
        "149": "equals 1 yr true and here I want to show\n",
        "152": "you how to use a break statement the set\n",
        "158": "VI equals 999 and I equals I plus 1 if I\n",
        "168": "equals 6 break and and this is also our\n",
        "172": "first use of an if statement so I hope\n",
        "173": "the logic of this makes any sense for I\n",
        "176": "equals I equals 1 and you know infinite\n",
        "180": "loop while repeatedly said VI equals 1\n",
        "183": "in increment I by 1 and then once I gets\n",
        "185": "up to 6 do a break which breaks you out\n",
        "187": "of the while loop and so the effect of\n",
        "189": "this should be to take the first five\n",
        "191": "elements of this vector V and set them\n",
        "195": "to 999 and yes V we've taken B and\n",
        "197": "overwritten the first five elements with\n",
        "201": "999 so this is the syntax for if\n",
        "203": "statement and for a while statement and\n",
        "206": "notice via and the we have two ends here\n",
        "209": "this end here ends the F statement and\n",
        "211": "the second end here ends the while\n",
        "214": "statement now let me show you the more\n",
        "216": "general syntax for how the user if-else\n",
        "220": "statement so let's see v1 is equal to\n",
        "225": "999 let's say v1 equals to 2 for this\n",
        "232": "example so let me type if v1 equals 1\n",
        "237": "display the value is 1 here's how you\n",
        "239": "write in else statements or rather\n",
        "245": "here's an LC if v1 equals 2 this is\n",
        "246": "going to be the case that's true in our\n",
        "253": "example display the value is 2 else\n",
        "258": "display the value is not one or two okay\n",
        "262": "so that's a if-else if-else statement\n",
        "265": "that end and of course here we've just\n",
        "267": "said you want equal to so hopefully yep\n",
        "270": "displays that the value is two and\n",
        "273": "finally I don't think I talked about\n",
        "274": "this earlier but if you ever need to\n",
        "276": "exit octave can type the exit command if\n",
        "278": "you hit enter that will cause octave to\n",
        "282": "quit or the to quit command also works\n",
        "284": "finally let's talk about functions and\n",
        "287": "how to define them and how to use them\n",
        "290": "here's my desktop and i have predefined\n",
        "294": "a file or pre saved on my desktop a file\n",
        "297": "called square this number m this is how\n",
        "299": "you define functions in octave you\n",
        "302": "create a file called you know with your\n",
        "304": "function name and then ending in dot m\n",
        "306": "and when octave finds inspire it knows\n",
        "308": "that this is where it should look for\n",
        "310": "the definition of the function square\n",
        "312": "this number dot m let's open it this\n",
        "313": "this number dot m let's open it this\n",
        "315": "file notice that dumb I'm using the\n",
        "318": "Microsoft program word pad to open up\n",
        "320": "this file but you I encourage you if\n",
        "322": "you're using Microsoft to Microsoft's\n",
        "325": "Windows to use WordPad rather than\n",
        "328": "notepad to open up these files if you\n",
        "329": "have a different text editor that's fine\n",
        "332": "too but notepad sometimes messes up the\n",
        "334": "spacing if you only have notepad that\n",
        "337": "should work too that could work too but\n",
        "339": "if you have WordPad as well I will you I\n",
        "340": "would rather use that or some other text\n",
        "342": "set so if you have a different text\n",
        "345": "editor for editing your functions so\n",
        "346": "here's how you define the function in\n",
        "348": "octave then just zoom in a little bit\n",
        "352": "and this fire has just three lines in it\n",
        "354": "the first line says function y equals\n",
        "356": "square this number X this tells octave\n",
        "359": "that I'm going to return the value Y and\n",
        "361": "I'm going to return one value and if\n",
        "362": "that values must be safe than the\n",
        "365": "variable Y and moreover it tells octave\n",
        "367": "that this function has one argument\n",
        "371": "which is the argument X and the way the\n",
        "373": "function body is defined is y equals x\n",
        "376": "squared so let's try to call this\n",
        "380": "function square this number five and\n",
        "382": "this actually isn't going to work an\n",
        "385": "octave says square those numbers on the\n",
        "387": "fine that's because octave doesn't\n",
        "389": "where to find a find this file so as\n",
        "391": "usual let's use PWD or not in the right\n",
        "395": "directory so let's CD users ang slash\n",
        "400": "desktop that's where my desktop is oops\n",
        "402": "a little typo there uses ang desktop and\n",
        "404": "if I now type square this number 5a\n",
        "405": "if I now type square this number 5a\n",
        "408": "returns the answer 25 it's kind of an\n",
        "411": "advanced speaker this is only for those\n",
        "413": "of you that know what the terms search\n",
        "416": "path means but um so if you want to\n",
        "418": "modify the octave search path and you\n",
        "420": "could just think of this next part as a\n",
        "423": "advanced or optional material only for\n",
        "425": "those either familiar with the concept\n",
        "426": "of search paths in programming languages\n",
        "430": "but you can use the term add path safety\n",
        "435": "colon slash users slash ng slash desktop\n",
        "438": "to add that directory to the octave\n",
        "441": "search path so that even if I go to some\n",
        "444": "other directory I can store octave still\n",
        "446": "knows to look in the users and Ji\n",
        "449": "desktop directory for functions so that\n",
        "450": "even though i'm in a different directory\n",
        "452": "now it still knows where to find the\n",
        "455": "square this number function okay but if\n",
        "458": "you're not familiar with the concept of\n",
        "460": "search path don't worry about it just\n",
        "461": "make sure to use the CD command to go to\n",
        "462": "make sure to use the CD command to go to\n",
        "464": "the directory of your function before\n",
        "465": "you run it and that should work just\n",
        "466": "fine\n",
        "467": "fine\n",
        "470": "one concept that octave has that many\n",
        "472": "other program languages don't is that it\n",
        "475": "can also define let you define functions\n",
        "478": "that return multiple values or multiple\n",
        "479": "arguments so here's an example of that\n",
        "482": "I've defined a function called square\n",
        "485": "and cube this number X and what this\n",
        "487": "says is this dysfunction returns two\n",
        "489": "values y1 and y2 when I set them as\n",
        "492": "follows why would they sweat y2 is X\n",
        "494": "cubed and what this does is this really\n",
        "497": "returns you know two numbers though so\n",
        "498": "some of you depending on what\n",
        "500": "programming languages you've used if\n",
        "501": "you're familiar with if you know C C++\n",
        "504": "Java often we think of a function as\n",
        "506": "returning just one value but this all\n",
        "508": "the syntax in octave that's a return\n",
        "511": " \n",
        "514": "now back in the octave window if I type\n",
        "520": "you know a B equals square and cube this\n",
        "526": "number five then a is not equal to 25\n",
        "528": "and B is equal to the cube of five is\n",
        "531": "equal to 125 so this is some often\n",
        "532": "convenient if you need to define a\n",
        "535": "function that returns multiple values\n",
        "538": "finally I'm going to show you just one\n",
        "540": "more sophisticated example of a function\n",
        "542": "let's say I have a data set that looks\n",
        "545": "like this with data points at 1 1 2 2 3\n",
        "548": "3 and what I'd like to do is to define\n",
        "550": "an octave function to compute the cost\n",
        "552": "function J of theta for different values\n",
        "555": "of theta first let's put the data into\n",
        "557": "octave so it does not set my design\n",
        "563": "matrix to be 1 1 1 2 1 3 so this is a my\n",
        "566": "design matrix X with X 0 the first\n",
        "567": "column being in the set term and the\n",
        "569": "second term being my you know the X\n",
        "571": "values of my three training examples and\n",
        "575": "let me say Y to be 1 2 3 as follows\n",
        "579": "which were the y-axis values so let's\n",
        "584": "say theta is equal to 0 semicolon 1 here\n",
        "586": "on my desktop are predefined this cost\n",
        "589": "function J and if I bring up the\n",
        "591": "definition of that function it looks as\n",
        "593": "follows so function J equals cost\n",
        "596": "function J inputs X Y theta some\n",
        "599": "comments specifying the inputs and then\n",
        "601": "fire a few steps set m to be the number\n",
        "603": "training examples there's a number of\n",
        "606": "rows and X computer predictions\n",
        "609": "predictions equals x times theta and\n",
        "611": "this is a comment that's wrapped around\n",
        "613": "so this is for the preceding comment\n",
        "616": "line computer squared errors by taking\n",
        "618": "the difference between their predictions\n",
        "620": "and the y-values and taking element wise\n",
        "623": "squaring and then finally computing the\n",
        "626": "cost function J and octave knows that J\n",
        "628": "is the value I want to return because J\n",
        "630": "over here here in the function\n",
        "632": "definition feel free by the way to\n",
        "636": "oppose this video if you want to look at\n",
        "638": "this function definition for longer and\n",
        "640": "kind of make sure that you understand is\n",
        "643": "understand there's the different steps\n",
        "646": "but when I run it in octave I find J\n",
        "651": "equals cost function J X Y theta it\n",
        "655": "computes oops I mean typo there should\n",
        "658": "have been capital X it computes J equals\n",
        "662": "zero because if my data set was you know\n",
        "665": "one two three one two three then setting\n",
        "667": "right David zero equals zero theta one\n",
        "670": "equals one this gives me exactly the\n",
        "672": "45-degree line that fits my dataset\n",
        "674": "perfectly\n",
        "677": "whereas in contrast if I set theta\n",
        "681": "equals say zero zero then this\n",
        "683": "hypothesis is predicting zeros and\n",
        "685": "everything to the same theta zero equals\n",
        "688": "zero theta one equals zero and a compute\n",
        "690": "the cost function then there's a two\n",
        "691": "point three three three and that's\n",
        "695": "actually equal to 1 squared which is my\n",
        "698": "squared error on the first example plus\n",
        "702": "two squared plus three squared and then\n",
        "705": "divided by 2m which is two times number\n",
        "708": "training examples which is oops which is\n",
        "711": "indeed two point three three and so that\n",
        "714": "sanity checks that this function here is\n",
        "716": "you know computing the correct cost\n",
        "718": "function and using the couple examples\n",
        "720": "we tried out on on our simple training\n",
        "725": "example and so that sanity checks that\n",
        "729": "the cost function J as as defined here\n",
        "731": "that it is indeed you know seeming to\n",
        "734": "compute the correct cost function at\n",
        "737": "least on our simple training set that we\n",
        "741": "had here with X&amp;Y being the simple\n",
        "744": "training example that we saw\n",
        "746": "so now you know how to write control\n",
        "748": "statements like for loops while loops\n",
        "750": "and if statements in octave as well as\n",
        "753": "how to define and use functions in the\n",
        "755": "next video I'm going to just very\n",
        "757": "quickly step you through the logistics\n",
        "760": "of working on and submitting problem\n",
        "763": "sets for this class and how to use our\n",
        "766": "submission system and finally after that\n",
        "769": "in the final opto tutorial video I want\n",
        "771": "to tell you about vectorization which is\n",
        "773": "an idea for how to make your octave\n"
    },
    "HIQlmHxI6-0": {
        "0": " \n",
        "1": "in this video we'll talk about how to\n",
        "4": "fit the parameters theta for logistic\n",
        "7": "regression in particular I'd like to\n",
        "10": "define the optimization objective of the\n",
        "12": "cost function that we'll use to fit the\n",
        "16": "parameters use the supervised learning\n",
        "19": "problem of fitting a logistic regression\n",
        "22": "model we have a training set of M\n",
        "26": "training examples and as usual each of\n",
        "28": "our examples is represented via a\n",
        "30": "feature vector there's a n plus 1\n",
        "35": "dimensional and as usual we have X 0\n",
        "38": "equals 1 the first feature or a 0\n",
        "40": "feature is always equal to 1 and because\n",
        "42": "this is a classification problem our\n",
        "44": "training set has the property that every\n",
        "45": "training set has the property that every\n",
        "49": "label Y is either 0 1 this is a\n",
        "52": "hypothesis and the parameters of the\n",
        "54": "hypothesis is this theta over here and\n",
        "56": "the question I want to talk about is\n",
        "59": "given this training set how do we choose\n",
        "61": "or how do we fit the parameters theta\n",
        "64": "back when we were developing the linear\n",
        "66": "regression model we use the following\n",
        "69": "cost function I've written just slightly\n",
        "72": "differently where instead of 1 over 2m\n",
        "74": "I've taken the 1/2 and put it inside the\n",
        "77": "summation instead now I want to use an\n",
        "79": "alternative way of writing out this cost\n",
        "81": "function which is that instead of\n",
        "83": "writing out the squared error term here\n",
        "85": "let's write in here\n",
        "93": "cost of H of X comma Y and I'm going to\n",
        "98": "define that term cost a vege of X comma\n",
        "100": "Y to be equal to this it's just equal to\n",
        "103": "this 1/2 of the squared error so now we\n",
        "105": "can see more clearly that the cost\n",
        "109": "function is a sum over my training set\n",
        "111": "or as 1 over m times the sum over my\n",
        "116": "training set of this cost term here and\n",
        "118": "to simplify this equation a little bit\n",
        "120": "more it's going to be convenient to get\n",
        "123": "rid of those super scripts so just\n",
        "125": "define cost of H of X comma Y to be\n",
        "127": "equal to 1/2 of the squared error and\n",
        "130": "the interpretation of this cost function\n",
        "132": "is that this is the cost\n",
        "133": "I want my learning over\n",
        "136": "them too you know how to pay if it\n",
        "139": "outputs that value it is prediction is H\n",
        "143": "of X and the actual label was y so just\n",
        "147": "cross off those superscripts right and\n",
        "150": "no suprise for linear regression the\n",
        "152": "cost we've defined is that what the cost\n",
        "155": "for this is that is one-half times the\n",
        "156": "squared difference between what are\n",
        "158": "predicted and the actual value that we\n",
        "161": "observe for Y now this cost function\n",
        "163": "worked fine for linear regression but\n",
        "165": "here we're interested in logistic\n",
        "168": "regression if we could minimize this\n",
        "171": "cost function that is plugged into J\n",
        "174": "here that will work okay but it turns\n",
        "176": "out that if we use this particular cost\n",
        "178": "function this would be a non convex\n",
        "182": "function of the parameters theta here's\n",
        "184": "what I mean by non convex we have some\n",
        "187": "cost function J of theta and for\n",
        "190": "logistic regression this function egg\n",
        "193": "here has a non linearity right it's in\n",
        "195": "your 1 over 1 plus e to the negative\n",
        "197": "theta transpose X so this is a pretty\n",
        "199": "complicated nonlinear function and if\n",
        "201": "you take the sigmoid function and plug\n",
        "202": "it in here\n",
        "204": "and then take this cost function and\n",
        "207": "plug it in there and then plot what J of\n",
        "209": "theta looks like you find that J of\n",
        "211": "theta can look like the function that's\n",
        "214": "like this you know with many local\n",
        "217": "optima and the formal term for this is\n",
        "219": "that this is a non convex function and\n",
        "221": "you can kind of tell if you were to run\n",
        "223": "gradient descent on disorder function it\n",
        "226": "is not guaranteed to converge to the\n",
        "228": "global minimum whereas in contrast what\n",
        "230": "we would like is to have a cost function\n",
        "233": "J of theta that is convex that is a\n",
        "235": "single bone shape function that looks\n",
        "237": "like this so that if you run gradient\n",
        "239": "descent we would be guaranteed that\n",
        "242": "gradient descent you know would converge\n",
        "245": "to the global minimum and the problem\n",
        "248": "we're using the square cost function is\n",
        "251": "that because of this very nonlinear\n",
        "252": "sigmoid function that appears in the\n",
        "256": "middle here J of theta ends up being a\n",
        "258": "non convex function if you were to\n",
        "261": "define it as the square cost function so\n",
        "263": "what we'd like to do is to instead come\n",
        "265": "up with a different cost function that\n",
        "267": "is convex\n",
        "269": "and so that we can apply a great\n",
        "271": "algorithm like gradient descent and be\n",
        "272": "guaranteed they'll find the global\n",
        "274": "minimum here's the cost function that\n",
        "276": "we're going to use for logistic\n",
        "278": "regression we're going to say that the\n",
        "280": "cost or the penalty that the algorithm\n",
        "284": "pays if it outputs the value H of X so\n",
        "286": "this is some number like zero point\n",
        "288": "seven right but if it predicts the value\n",
        "291": "H of X and the actual cost label turns\n",
        "294": "out to be Y the cost is going to be\n",
        "297": "minus log H of X if Y is equal to 1 and\n",
        "301": "minus log 1 minus H of X if Y is equal\n",
        "303": "to zero this looks like a pretty\n",
        "305": "complicated function but let's plot this\n",
        "306": "function to gain some intuition about\n",
        "309": "what is doing let's start off with the\n",
        "312": "case of y equals 1 if y is equal to 1\n",
        "316": "then the cost function is minus log H of\n",
        "319": "X and then we plot that so let's say\n",
        "323": "that the horizontal axis is H of X so we\n",
        "325": "know that our hypothesis is going to\n",
        "328": "output a value between 0 and 1 right so\n",
        "331": "H of X that varies between 0 and 1\n",
        "333": "if you plot what this cost function\n",
        "336": "looks like you find that it looks like\n",
        "339": "this one way to see why the plot looks\n",
        "342": "like this is because um if you were to\n",
        "346": "plot log Z with Z on the horizontal axis\n",
        "349": "then that looks like that and it sort of\n",
        "351": "approaches minus infinity right so this\n",
        "353": "is what the log function looks like and\n",
        "357": "this is 0 this is 1 here Z is of course\n",
        "362": "playing the row of H of X and so minus\n",
        "365": "log Z will look like this\n",
        "368": "by just flipping the sign minus log Z\n",
        "371": "and we're interested only in the range\n",
        "373": "of when this function goes between 0 and\n",
        "376": "1 so get rid of that and so we're just\n",
        "378": "left with you know this part of the\n",
        "379": "left with you know this part of the\n",
        "381": "curve and that's what this curve oh no\n",
        "385": "that looks like now this cost function\n",
        "388": "has a few interesting and desirable\n",
        "392": "properties first you notice that if Y is\n",
        "394": "equal to 1 and H of X is equal to 1 in\n",
        "398": "other words if the hypothesis exactly\n",
        "401": "x equals one and y is exactly equal to\n",
        "403": "what I predicted then the cost is equal\n",
        "406": "to 0 right that corresponds to the curve\n",
        "407": "doesn't actually flatten out the curve\n",
        "410": "is still going first notice that if H of\n",
        "413": "x equals 1 if the hypothesis predicts\n",
        "416": "that y is equal to 1 and if in dy is\n",
        "418": "equal to 1 then the cost is equal to 0\n",
        "420": "that corresponds to this point down here\n",
        "423": "right if H of X is equal to 1 and we're\n",
        "425": "only considering the case at y equals 1\n",
        "428": "here but if H of X is equal to 1 then\n",
        "430": "the cost is down here is equal to 0 and\n",
        "432": "that's that that's what we'd like it to\n",
        "434": "be because you know if we correctly\n",
        "438": "predict the output Y then the cost 0 but\n",
        "442": "now notice also that as H of X\n",
        "445": "approaches 0 so as H is the output of a\n",
        "448": "hypothesis approaches 0 the cost blows\n",
        "450": "up and it goes to infinity and what this\n",
        "453": "does is this captures the intuition that\n",
        "457": "if our hypothesis you know outputs 0 the\n",
        "459": "side saying our hypothesis is saying the\n",
        "461": "chance of y equals 1 is equal to 0 it's\n",
        "463": "kind of like our going to our medical\n",
        "465": "patients and saying the probability that\n",
        "467": "you have a malignant tumor the property\n",
        "469": "that y equals 1 is 0 so it's like\n",
        "472": "absolutely impossible that your\n",
        "473": "malignant that you're a tumor is\n",
        "476": "malignant but if it turns out that the\n",
        "479": "tumor the patient's tumor actually is\n",
        "481": "malignant so if Y is equal to 1\n",
        "483": "even after we told them you know the\n",
        "485": "probability of it happening is zeros\n",
        "486": "it's absolutely impossible for it to\n",
        "489": "them to be malignant but we told them\n",
        "491": "this with that level of certainty and we\n",
        "493": "turn out to be wrong then we penalize\n",
        "495": "the learning algorithm by very very\n",
        "497": "large cost and that's captured by having\n",
        "500": "this cost go to infinity if y equals 1\n",
        "501": "this cost go to infinity if y equals 1\n",
        "505": "and H of X approaches 0 this slide\n",
        "508": "considered the case of y equals 1 let's\n",
        "509": "look at what the cost function looks\n",
        "513": "like for y equals 0 if Y is equal to 0\n",
        "516": "then the cost looks like this it looks\n",
        "519": "like this expression over here and if\n",
        "522": "you plot the function minus log 1 minus\n",
        "527": "Z it what you get is the cost function\n",
        "528": "actually looks like\n",
        "531": "so it goes from zero to one unlike that\n",
        "535": "and so if you plot the cost function for\n",
        "536": "the case of y equals zero\n",
        "540": "you find it it looks like this and what\n",
        "545": "this curve does is it now blows up it\n",
        "547": "goes to plus infinity as H of X goes to\n",
        "550": "one because on the saying that if Y\n",
        "552": "turns out to be equal to zero but we\n",
        "554": "predicted that you know Y is equal to\n",
        "556": "one with with almost certainty we're\n",
        "558": "probably 1 then we end up paying a very\n",
        "562": "large cost let's plot the cost function\n",
        "565": "for the case of y equals 0 so if y\n",
        "567": "equals 0 that's going to be our cost\n",
        "568": "equals 0 that's going to be our cost\n",
        "571": "function if you are look at this\n",
        "573": "expression and if you plot your minus\n",
        "576": "log 1 minus Z if you figure out what\n",
        "578": "that looks like you get a figure that\n",
        "582": "looks like this where which goes from 0\n",
        "584": "to 1 with the z axis on the horizontal\n",
        "587": "axis so if you take this cost function\n",
        "589": "and plot it for the case of y equals 0\n",
        "592": "what you get is that the cost function\n",
        "596": "looks like this and what this cost\n",
        "599": "function does is that it blows up or it\n",
        "601": "goes to a positive infinity as H of X\n",
        "605": "goes to 1 and this captures the\n",
        "607": "intuition that if a hypothesis predicted\n",
        "609": "that you know H of X is equal to 1 with\n",
        "610": "that you know H of X is equal to 1 with\n",
        "612": "certainty with like probably one is\n",
        "614": "absolutely going to be y equals 1 but if\n",
        "617": "Y turns out to be equal to 0 then it\n",
        "619": "makes sense to make the hypothesis and\n",
        "620": "make the learning algorithm pay a very\n",
        "625": "large cost and conversely if H of X is\n",
        "627": "equal to 0 and y equals 0 then the\n",
        "629": "hypothesis nailed it predicted Y is\n",
        "633": "equal to 0 and it turns out Y is equal\n",
        "637": "to 0 so at this point the cost function\n",
        "640": "is going to be 0\n",
        "643": "in this video we've defined the cost\n",
        "645": "function for a single training example\n",
        "648": "the topic of convexity analysis is\n",
        "650": "beyond the scope of this course but it\n",
        "652": "is possible to show that with our\n",
        "654": "particular choice of cost function this\n",
        "656": "will give us a convex optimization\n",
        "658": "problem since our cost function our\n",
        "661": "overall cost function J of theta will be\n",
        "665": "convex and local Optima free in the next\n",
        "666": "video we're going to take these ideas of\n",
        "669": "the cost function for a single training\n",
        "671": "example and develop that further and\n",
        "673": "define the cost function for the entire\n",
        "676": "training set and we'll also figure out a\n",
        "678": "simpler way to write it than we have\n",
        "680": "been using so far and based on that\n",
        "682": "we'll work out gradient descent and that\n",
        "684": "would give us our logistic regression\n"
    },
    "HREeLryOh4Q": {
        "0": " \n",
        "2": "in the next few videos I'd like to talk\n",
        "3": "in the next few videos I'd like to talk\n",
        "5": "about machine learning system design\n",
        "7": "these videos will touch on the main\n",
        "9": "issues that you may face when designing\n",
        "12": "a complex machine learning system and\n",
        "14": "I'd like to try to give advice on how to\n",
        "16": "strategize putting together a complex\n",
        "19": "machine learning system in case this\n",
        "21": "next set of videos seems a little\n",
        "23": "disjointed that's because these videos\n",
        "25": "will touch on a range of the different\n",
        "27": "issues that you may come across when\n",
        "29": "designing complex learning systems and\n",
        "32": "even though the next set of videos may\n",
        "35": "seem someone less mathematical I think\n",
        "36": "that this material may turn out to be\n",
        "39": "very useful and potentially huge time\n",
        "41": "savers when you're building make machine\n",
        "44": "learning systems concretely I'd like to\n",
        "47": "begin with the issue of prioritizing how\n",
        "49": "to spend your time on what to work on\n",
        "52": "and I'll begin with an example on spam\n",
        "56": "classification let's say you want to\n",
        "58": "build a spam classifier here are a\n",
        "61": "couple examples of obvious spam and\n",
        "64": "non-spam email with the one on the Left\n",
        "67": "tried to sell things and notice how\n",
        "69": "stamens was sometimes deliberately\n",
        "72": "misspell words like medicine with one\n",
        "75": "there and mortgages and on the right as\n",
        "78": "a maybe obvious example of non spam you\n",
        "80": "know actually email from my younger\n",
        "83": "brother let's say we have a label\n",
        "85": "training set of some number of spam\n",
        "87": "emails and some non-spam emails denoted\n",
        "91": "with labels my equals one or zero how do\n",
        "94": "we build a classifier using supervised\n",
        "96": "learning to distinguish between spam and\n",
        "99": "non-spam in order to apply supervised\n",
        "101": "learning the first decision we must make\n",
        "105": "is how do we want to represent X that is\n",
        "107": "the features of the email given the\n",
        "110": "features X and the labels Y in our\n",
        "112": "training set we can then train a\n",
        "114": "classifier for example or using logistic\n",
        "117": "regression here's one way to choose a\n",
        "121": "set of features for our emails we could\n",
        "123": "come up with say a list of maybe a\n",
        "125": "hundred words that we think are\n",
        "127": "indicative of whether email is\n",
        "130": "or non-spam for example that a piece of\n",
        "132": "email contains the word deal maybe a\n",
        "134": "small likely to be spam that contains\n",
        "137": "the word buy may be more likely to be\n",
        "139": "we're like discounts more likely to be\n",
        "141": "spam whereas if a piece of email\n",
        "145": "contains my name Andrew maybe it that\n",
        "146": "means the person actually knows who I am\n",
        "148": "and that I mean it's less likely to be\n",
        "152": "spam and maybe for some reason i think\n",
        "155": "the word now may be indicative of non\n",
        "157": "Stan because I get a lot of urgent\n",
        "160": "emails and so on and maybe we choose a\n",
        "163": "hundred words also given a piece of\n",
        "165": "email we can then take this piece of\n",
        "168": "email and encode it into a feature\n",
        "171": "vector as follows I'm going to take my\n",
        "175": "list of 100 words and sort them in\n",
        "177": "alphabetical order say doesn't have to\n",
        "180": "be sorted but you know here's a here's\n",
        "183": "my list of words there's a count and so\n",
        "185": "on and eventually I get down to now and\n",
        "187": "so on and given a piece of email like\n",
        "189": "that shown on the right I'm going to\n",
        "192": "check and see whether or not each of\n",
        "195": "these words appears in an email and then\n",
        "197": "I'm going to define a feature vector X\n",
        "200": "where in this piece of an email on the\n",
        "202": "right my name doesn't appear so I'm\n",
        "205": "gonna put a zero there the word bye does\n",
        "208": "appear so I went for the one there then\n",
        "210": "I'm just gonna put once or zeros I'm\n",
        "212": "gonna put the one even though that worth\n",
        "215": "by or cursed twice I'm not gonna really\n",
        "216": "come home any time to where the tourists\n",
        "221": "the word deal appears that one there the\n",
        "222": "word discount doesn't appear at least\n",
        "225": "known in this this little short email\n",
        "228": "and so on the word now does appear and\n",
        "230": "so on so I put once and zeros in this\n",
        "232": "feature vector depending on whether or\n",
        "235": "not a particular word appears and in\n",
        "238": "this example my feature vector would\n",
        "244": "have dimension 100 if I have a hundred\n",
        "247": "if I chose hundred words to use for this\n",
        "251": "representation and each of my features\n",
        "255": "XJ will basically be one\n",
        "257": "you know a particular word that we call\n",
        "261": "this where J appears in the email and\n",
        "266": "exchange would be zero otherwise so that\n",
        "268": "gives me a representation a feature\n",
        "271": "representation or a piece of email by\n",
        "272": "the way even though I described this\n",
        "275": "process as manually picking a hundred\n",
        "278": "words in practice what's most commonly\n",
        "281": "done is to look through a training set\n",
        "284": "and in the training set to pick the most\n",
        "286": "frequently occurring in words where n is\n",
        "288": "usually between 10,000 and 50,000 and\n",
        "292": "use those as your features so rather\n",
        "294": "than manually picking 100 words near you\n",
        "296": "look through the training examples and\n",
        "297": "pick the most frequently occurring words\n",
        "300": "like 10,000 to 50,000 words and those\n",
        "301": "form the features that you're going to\n",
        "304": "use to represent your email for spam\n",
        "306": "classification now if you're building a\n",
        "309": "spam classifier one question that you\n",
        "312": "may face is what's the best use of your\n",
        "315": "time in order to make your spam\n",
        "317": "classifier have high accuracy you have\n",
        "320": "low error what natural inclination is to\n",
        "322": "go and collect lots of data right and\n",
        "324": "the fact of this is tendency to think\n",
        "325": "that well the more data we have the\n",
        "328": "better the album will do and in fact in\n",
        "330": "the email spam domain there are actually\n",
        "333": "pretty serious projects called honeypot\n",
        "335": "projects which create fake email\n",
        "337": "addresses and try to get these fake\n",
        "339": "email addresses into the hands of\n",
        "341": "spammers and use that to try to collect\n",
        "344": "tons of spam email and therefore get a\n",
        "346": "lot of spam data to train learning\n",
        "348": "algorithm but we've already seen in the\n",
        "351": "previous sets of videos that getting\n",
        "353": "lots of data will often help but not all\n",
        "355": "the time but for most machine learning\n",
        "357": "problems there are a lot of other things\n",
        "359": "you could usually imagine doing to\n",
        "362": "improve performance for spam one thing\n",
        "364": "you might think of is to develop more\n",
        "365": "sophisticated features on the email\n",
        "367": "maybe based on the email routing\n",
        "370": "information this is the this would be\n",
        "371": "information contained in the email\n",
        "375": "header so when spammers send email very\n",
        "377": "often they will try to obscure the\n",
        "380": "origins of the email and maybe use fake\n",
        "383": "email headers or send an email through\n",
        "386": "very unusual sets of computer servers\n",
        "387": "through very\n",
        "390": "raus in order to get the spam to you and\n",
        "392": "some of this information will be\n",
        "396": "reflected in the email header and so one\n",
        "398": "can imagine looking at the email headers\n",
        "400": "and trying to develop more sophisticated\n",
        "403": "features to capture this sort of email\n",
        "405": "routing information to identify if\n",
        "407": "something misspell something else you\n",
        "409": "might consider doing is to look at the\n",
        "411": "email message body that is the email\n",
        "413": "text and try to develop more\n",
        "415": "sophisticated features for example\n",
        "417": "should the word discount and the word\n",
        "420": "discounts be treated as the same words\n",
        "422": "or should we have treated the worst\n",
        "424": "DeLand dealer as the same word maybe\n",
        "427": "even though one is lowercase and one is\n",
        "429": "capitalized in this example or do we\n",
        "430": "want more complex features about\n",
        "433": "punctuation because maybe spammers use\n",
        "435": "exclamation a lot more I don't know and\n",
        "437": "along the same lines maybe we also want\n",
        "439": "to develop more sophisticated algorithms\n",
        "442": "to detect and maybe to correct to\n",
        "443": "deliberate misspellings like the\n",
        "445": "mortgage medicine watches because\n",
        "447": "spammers actually do this because if you\n",
        "450": "have your watches with a four in there\n",
        "452": "then wrong with the simple technique\n",
        "455": "that we talked about just now the spam\n",
        "457": "classifier might not equate this as the\n",
        "459": "same thing as the words watches and so\n",
        "461": "it may have a harder time realizing\n",
        "463": "there's something a spam with these\n",
        "465": "deliberate misspellings and this is why\n",
        "467": "spammers do it\n",
        "469": "we're working on the machine learning\n",
        "472": "problem very often you can brainstorm\n",
        "474": "lists of different things to try like\n",
        "476": "these and by the way I've actually\n",
        "479": "worked on spam the spam problem myself\n",
        "480": "for a while and then she spent quite\n",
        "483": "some time on it and even though I kind\n",
        "485": "of understand the spam problem actually\n",
        "486": "know a bit about it I would actually\n",
        "489": "have a very hard time telling you of\n",
        "491": "these four options which is the best use\n",
        "494": "of your time so what happens frankly\n",
        "496": "what happens far too often is that a\n",
        "498": "research group or private group will\n",
        "500": "randomly fixate on one of these options\n",
        "503": "and sometimes that turns out not to be\n",
        "506": "the most fruitful way to spend your time\n",
        "508": "depending on you know which of these\n",
        "509": "options someone ends up randomly\n",
        "512": "fixating on by the way um in fact if you\n",
        "514": "even get to the stage where you\n",
        "516": "brainstorm a list of different options\n",
        "518": "to try you're probably already ahead of\n",
        "520": "the curve sadly what most people do is\n",
        "522": "instead of trying to list out the\n",
        "524": "options of things you might try what far\n",
        "526": "too many people do is wake up one\n",
        "528": "morning and for some reason just you\n",
        "530": "know have a weird gut feeling that oh\n",
        "533": "let's just have a huge honeypot project\n",
        "535": "to go and collect tons more data and for\n",
        "537": "whatever strange reason just to let wake\n",
        "539": "up one morning and randomly fixate on\n",
        "541": "one thing and just work on that for six\n",
        "544": "months but I think we can do better and\n",
        "547": "in particular what I'd like to do in the\n",
        "549": "next video is tell you about the concept\n",
        "552": "of error analysis and talk about the way\n",
        "555": "where you can try to have a more\n",
        "558": "systematic way to choose amongst the\n",
        "560": "options of the many different things you\n",
        "562": "might work on and therefore be more\n",
        "564": "likely to select what is actually a good\n",
        "566": "way to spend your time you know for the\n",
        "568": "next few weeks or the next few days or\n"
    },
    "Hwj_9wMXDVo": {
        "0": " \n",
        "1": "you now know about linear regression\n",
        "4": "with multiple variables in this video I\n",
        "6": "want to tell you a bit about the choice\n",
        "8": "of features that you have and how you\n",
        "10": "can get different learning algorithms\n",
        "12": "sometimes very powerful ones by choosing\n",
        "14": "appropriate features and in particular\n",
        "15": "I'll also want to tell you about\n",
        "17": "polynomial regression which allows you\n",
        "19": "to use the machinery of linear\n",
        "22": "regression to fit very complicated even\n",
        "26": "very nonlinear functions let's take the\n",
        "28": "example of predicting the price of a\n",
        "31": "house suppose you have two features the\n",
        "33": "frontage of the house and the depth of\n",
        "34": "the whole so here's a picture of the\n",
        "36": "house we're trying to sell the frontage\n",
        "41": "is defined as this distance it's\n",
        "45": "basically the width or the length of how\n",
        "47": "wide your lot is of this plot of land at\n",
        "51": "your own and the depth of the house is\n",
        "54": "how far how deep your property is so\n",
        "57": "there's a frontage that's the depth so\n",
        "59": "you have two features called frontage\n",
        "60": "and def you might build a linear\n",
        "63": "regression model like this where\n",
        "65": "frontage is your first feature x1 and\n",
        "70": "depth is your second feature x2 but when\n",
        "71": "you're applying linear regression you\n",
        "73": "don't necessarily have to use just the\n",
        "76": "features x1 and x2 then you're given\n",
        "78": "what you can do is actually create new\n",
        "81": "features by yourself so if I want to\n",
        "82": "predict the price of a house what I\n",
        "85": "might do instead is decide that what\n",
        "88": "really determines the size of house is\n",
        "91": "the area really the land area that I own\n",
        "93": "so I might create a new feature just\n",
        "99": "causes feature X which is frontage times\n",
        "102": "depth this is a multiplication symbol\n",
        "104": "right so frontage times death because\n",
        "107": "this is the land area that I own and I\n",
        "112": "might then select my hypothesis as that\n",
        "115": "using just one feature which is my land\n",
        "119": "area right because the area of a\n",
        "121": "rectangle is you know the product of the\n",
        "124": "length of a size so depending on what\n",
        "125": "insight you might have into a particular\n",
        "128": "problem rather than just taking the\n",
        "129": "features frontage and depth that we\n",
        "132": "happen to have started off with\n",
        "134": "sometimes by defining new features you\n",
        "136": "might actually get a better model\n",
        "139": "closely related to the idea of choosing\n",
        "141": "your features is this idea called\n",
        "143": "polynomial regression let's say you have\n",
        "145": "a housing price data set that looks like\n",
        "147": "this then there are a few different\n",
        "150": "models you might fit to this one thing\n",
        "151": "you could do is fit a quadratic model\n",
        "153": "like this it doesn't look like a\n",
        "154": "straight line fits this data very well\n",
        "157": "so maybe you want to fit a quadratic\n",
        "159": "model like this where you think the size\n",
        "161": "where you think the price is a quadratic\n",
        "164": "function and maybe that would give you\n",
        "165": "you know a fit to the data that looks\n",
        "168": "like that but then you might decide that\n",
        "169": "your quadratic model doesn't make sense\n",
        "171": "because of a quadratic function\n",
        "173": "eventually this function comes back down\n",
        "175": "and well we don't think housing prices\n",
        "177": "should go down when the size goes up too\n",
        "181": "high so then maybe we might choose a\n",
        "183": "different polynomial model and choose to\n",
        "187": "use instead a cubic function and where\n",
        "189": "we have now a third order term and we\n",
        "192": "fit that maybe we will get this sort of\n",
        "194": "model then maybe the green line is a\n",
        "196": "somewhat better fit to the data because\n",
        "198": "it doesn't eventually come back down so\n",
        "200": "how do we actually fit a model like this\n",
        "203": "to our data using the machinery of\n",
        "207": "multivariate linear regression we can do\n",
        "209": "this with a pretty simple modification\n",
        "211": "to our algorithm the form of the\n",
        "214": "hypothesis we know how the fit looks\n",
        "216": "like this where we say that H of X is\n",
        "219": "theta 0 plus theta 1 X 1 plus a 2 X 2\n",
        "223": "plus a 3 X 3 and if we want to fit this\n",
        "226": "cubic model that I have a box in green\n",
        "229": "what we're saying is that the predictor\n",
        "231": "price of a house is theta 0 plus theta 1\n",
        "233": "times the size of the house plus theta 2\n",
        "236": "times the square size of the house so\n",
        "239": "this term singular that term and then\n",
        "242": "plus theta 3 times the cube of the size\n",
        "244": "of the house right since that third term\n",
        "247": "in order to map these two definitions to\n",
        "250": "each other well the natural way to do\n",
        "253": "that is to set the first feature x1 to\n",
        "255": "be the size of the house and set the\n",
        "257": "second feature x2 to be the square of\n",
        "258": "the size of house and set the third\n",
        "261": "feature x three to be the cube of the\n",
        "262": "size of the halls\n",
        "264": "and just by choosing my three features\n",
        "267": "this way and deploying the machinery of\n",
        "270": "linear regression I can fit this model\n",
        "274": "and end up with a cubic fit to my data I\n",
        "275": "just want to point out one more thing\n",
        "278": "which is that if you choose your\n",
        "280": "features like this then feature scaling\n",
        "284": "becomes increasingly important so if the\n",
        "286": "size of a house ranges from one to a\n",
        "288": "thousand so you know from 1 to 1,000\n",
        "291": "square feet say then the size squared of\n",
        "294": "the house will range from 1 to 1 million\n",
        "298": "the square of a thousand and your third\n",
        "302": "feature X cubed excuse me your xxx\n",
        "305": "feature X 3 which is the size cubed of\n",
        "307": "the house will range from 1 to 10 to the\n",
        "310": "9 and so these 3 features take on very\n",
        "313": "different ranges of values and it's\n",
        "315": "important to apply feature scaling if\n",
        "317": "you're using gradient descent to get\n",
        "321": "them into comparable ranges of values\n",
        "323": "finally here's one last example of how\n",
        "327": "you really have broad choices in the\n",
        "329": "features you use earlier we talked about\n",
        "331": "hard quadratic model like this might not\n",
        "333": "be ideal because you know maybe a\n",
        "335": "quadratic model fits the data ok but the\n",
        "337": "quadratic function goes back down and we\n",
        "339": "really don't want right housing prices\n",
        "342": "that go down to predict that as the size\n",
        "345": "of housing freezes but rather than going\n",
        "347": "to a cubic model there you have maybe\n",
        "349": "other choices of features and there are\n",
        "351": "many possible choices but just to give\n",
        "353": "you an another example of a reasonable\n",
        "355": "choice another reasonable choice might\n",
        "358": "be to say that the price of a house is\n",
        "360": "theta 0 plus theta 1 times the size and\n",
        "362": "then plus theta 2 times the square root\n",
        "364": "of the size right so the square root\n",
        "366": "function is this sort of function and\n",
        "369": "maybe there will be some value of theta\n",
        "372": "1 theta 2 theta 3 that will let you take\n",
        "375": "this model and to the curve that looks\n",
        "377": "like that and you know goes up but\n",
        "380": "doesn't but sort of flattens out a bit\n",
        "384": "and doesn't ever come back down and so\n",
        "387": "by having insight into in this case the\n",
        "390": "shape of the square root function and\n",
        "392": "into the shape of the data by choosing\n",
        "393": "different feature\n",
        "395": "so you can sometimes get better models\n",
        "398": "in this video we talked about polynomial\n",
        "399": "regression that is how to fit a\n",
        "401": "polynomial like a quadratic function or\n",
        "404": "a cubic function to your data with all\n",
        "405": "throw this idea that you have a choice\n",
        "407": "in what features to use such as that\n",
        "409": "instead of using the frontage and the\n",
        "411": "depth of a house maybe you can multiply\n",
        "413": "them together to get a feature that\n",
        "416": "captures the land area of a house in\n",
        "418": "case this seems a little bit bewildering\n",
        "419": "then you know while with all these\n",
        "421": "different feature choices so how do I\n",
        "423": "decide what features to use later in\n",
        "424": "this class we'll talk about some\n",
        "426": "algorithms for automatically choosing\n",
        "429": "what features to use so you can have an\n",
        "430": "algorithm look at the data and\n",
        "432": "automatically choose for you whether you\n",
        "434": "want to fit a quadratic function or a\n",
        "436": "cubic function or something else but\n",
        "438": "until we get to those algorithms for now\n",
        "440": "I just want you to be aware that you you\n",
        "442": "have a choice in what features to use\n",
        "445": "and by designing different features you\n",
        "447": "can fit more complex functions your data\n",
        "448": "than just fitting a straight line to the\n",
        "451": "data and in particular you can say\n",
        "453": "polynomial functions as well and\n",
        "455": "sometimes by appropriate insights into\n",
        "457": "the features you can get a much better\n"
    },
    "ISBGFY-gBug": {
        "0": " \n",
        "1": "in this video I'd like to tell you about\n",
        "4": "learning curves learning curves is often\n",
        "6": "a very useful thing to plot if either\n",
        "8": "you want to sanity check that your\n",
        "10": "algorithm is working correctly or if you\n",
        "12": "want to improve the performance of the\n",
        "15": "algorithm and learning curves is a tool\n",
        "17": "that I actually use very often to try to\n",
        "20": "diagnose if a particular learning\n",
        "22": "algorithm may be suffering from a bias\n",
        "26": "or variance problem or a bit of both\n",
        "29": "case what a learning curve is to plot a\n",
        "31": "learning curve what I usually do is plot\n",
        "32": "learning curve what I usually do is plot\n",
        "35": "J train which is the say average squared\n",
        "39": "error on my training set or J CV which\n",
        "41": "is the average squared error on my cross\n",
        "43": "validation set and I'm going to plot\n",
        "46": "that as a function of M that is of as a\n",
        "47": "function of the number of training\n",
        "51": "examples I have and so M is usually a\n",
        "53": "constant like maybe I just have you know\n",
        "55": "100 training examples but what I'm going\n",
        "58": "to do is artificially reduce my training\n",
        "60": "set size so that deliberately limit\n",
        "63": "myself to using only say 10 or 20 or 30\n",
        "66": "or 40 training examples and plots what\n",
        "67": "the training error is and what the\n",
        "69": "cross-validation error is for these\n",
        "73": "smaller training set sizes so let's see\n",
        "74": "what these parts might look like\n",
        "76": "suppose I have only one training example\n",
        "78": "like that shown in this first example\n",
        "80": "here and let's say I'm fitting a\n",
        "82": "quadratic function well if I have only\n",
        "85": "one training example I'm going to be the\n",
        "87": "fitted perfectly right this you know for\n",
        "89": "the quadratic function going to have\n",
        "91": "zero error on the one training example\n",
        "94": "if I have two training examples well\n",
        "95": "with a quadratic function that can also\n",
        "97": "fit that very well so even if I were\n",
        "99": "using regularization I can probably fit\n",
        "101": "this quite well and if I'm using the\n",
        "103": "regularization i pre lifted this\n",
        "106": "perfectly and if I have three training\n",
        "108": "examples again you can fit the quadratic\n",
        "113": "function perfectly so if M equals 1 or M\n",
        "116": "equals 2 or M equals 3 my training error\n",
        "120": "on my training set is going to be 0\n",
        "122": "assuming I'm not using\n",
        "124": "or it may be slightly larger than 0 if I\n",
        "126": "am using regularization and by the way\n",
        "129": "if I have a large training set and I'm\n",
        "131": "artificially restricting the size of my\n",
        "133": "training set in order to apply j-train\n",
        "137": "here if I set M equals 3 say and I train\n",
        "140": "on only 3 examples then for this figure\n",
        "142": "I'm going to measure my training error\n",
        "144": "only on the three examples that I\n",
        "147": "actually fit my data to and so even if I\n",
        "150": "have say 100 training examples but if I\n",
        "152": "want to plot what my training error is\n",
        "154": "for M equals 3 what I'm going to do is\n",
        "156": "measure my training error only on the\n",
        "159": "three examples that I've actually fit my\n",
        "161": "hypothesis to and not on all the other\n",
        "163": "examples that I had deliberately omitted\n",
        "166": "from the training process so just to\n",
        "168": "summarize what we've seen is that if the\n",
        "170": "training set size is small then the\n",
        "172": "training error is going to be small as\n",
        "174": "well because you know you have a small\n",
        "176": "training set it's going to be very easy\n",
        "179": "to fit your training set very well maybe\n",
        "184": "even perfectly now say I have M equals 4\n",
        "185": "examples well then the quadratic\n",
        "188": "function can no longer fit this data set\n",
        "191": "perfectly and if I have M equals 5 then\n",
        "192": "you know maybe a quadratic function can\n",
        "195": "fit this data or so so but as my\n",
        "197": "training set gets larger it becomes\n",
        "199": "harder and harder to ensure that I can\n",
        "201": "find the quadratic function that passes\n",
        "205": "through all my examples perfectly so in\n",
        "208": "fact as the training set size grows what\n",
        "210": "you find is that my average training\n",
        "213": "error actually increases and so if you\n",
        "215": "plot this figure what you find is that\n",
        "217": "the training set error that is the\n",
        "220": "average error on your hypothesis grows\n",
        "223": "as M grows and just to repeat the\n",
        "226": "intuition is that when M is small when\n",
        "228": "you have very few training examples it's\n",
        "229": "pretty easy to fit every single one of\n",
        "231": "your training examples perfectly and so\n",
        "234": "your error is going to be small whereas\n",
        "236": "when M is larger then it gets somewhat\n",
        "238": "harder to fit all the new training\n",
        "240": "examples perfectly\n",
        "242": "your training set error becomes a little\n",
        "245": "bit larger now how about the cross\n",
        "246": "validation error\n",
        "248": "well the cross validation error is my\n",
        "251": "error on this cross validation set that\n",
        "254": "I haven't seen and so you know when I\n",
        "255": "have a very small training set\n",
        "257": "I'm not going to generalize well what\n",
        "259": "just not going to do well in that so\n",
        "261": "right this this hypothesis here doesn't\n",
        "263": "look like a good one and is only when I\n",
        "265": "get to a larger training set that you\n",
        "267": "know I'm starting to get hypotheses that\n",
        "270": "maybe fit the data somewhat better so\n",
        "272": "your cross-validation error and your\n",
        "276": "test set error will tend to decrease as\n",
        "278": "your training set size increases because\n",
        "281": "the more data you have the better you do\n",
        "283": "at generalizing to new examples or just\n",
        "285": "the more data you have the better the\n",
        "288": "hypothesis you fit so if you plot J\n",
        "291": "train and JCV this is the sort of figure\n",
        "293": "you get now let's look at what the\n",
        "296": "learning curves may look like if we have\n",
        "298": "either high bias or high variance\n",
        "300": "problems suppose your hypothesis has\n",
        "302": "high bias and to explain this I'm going\n",
        "305": "to use our stand an example of fitting a\n",
        "306": "straight line to data that you know\n",
        "308": "can't really be fit well by a straight\n",
        "311": "line so we end up with a hypothesis that\n",
        "314": "maybe looks like that now let's think\n",
        "317": "what would happen if we were to increase\n",
        "319": "the training set size so if instead of 5\n",
        "322": "examples like what's drawn there imagine\n",
        "324": "that we have a lot more training\n",
        "326": "examples well what happens if you fit a\n",
        "328": "straight line to this what you find is\n",
        "330": "that you end up with you know pretty\n",
        "331": "much the same straight line\n",
        "335": "I mean you destroy a line just conflict\n",
        "337": "this data and getting a ton more data\n",
        "338": "well the straight line isn't going to\n",
        "340": "change that much this is the best\n",
        "342": "possible straight line fit to this data\n",
        "344": "but the straight line just can't fit\n",
        "346": "this data set that well so if you plot\n",
        "349": "the cross-validation error this is what\n",
        "352": "it will look like over here on the left\n",
        "353": "if you have a very miniscule training\n",
        "355": "set size that you have you know maybe\n",
        "357": "just one training example it's not going\n",
        "359": "to do well but by the time you've\n",
        "360": "reached a certain number of training\n",
        "363": "examples you've almost fit the best\n",
        "365": "possible straight line and even if you\n",
        "367": "end up with a much larger training set\n",
        "369": "with much larger value of em you know\n",
        "371": "you're basically getting the same\n",
        "372": "straight line and so the\n",
        "375": "cross-validation error let me label that\n",
        "378": "what test set error will plateau out or\n",
        "380": "flatten out pretty soon once you've\n",
        "383": "reached beyond a certain number of\n",
        "385": "training examples so that you pretty\n",
        "387": "much fit the best possible straight line\n",
        "390": "and how about training error\n",
        "393": "well the training error will again be\n",
        "397": "small and what you find in the high bias\n",
        "400": "case is that the training error will end\n",
        "403": "up close to the cross validation error\n",
        "406": "because you have so few parameters in so\n",
        "408": "much data at least when M is large the\n",
        "410": "performance on the training set and when\n",
        "412": "the cross-validation set would be very\n",
        "414": "similar and so this is what your\n",
        "416": "learning curves will look like before if\n",
        "419": "you have an algorithm that has high bias\n",
        "422": "and finally the problem of high bias is\n",
        "425": "reflected in the fact that both the\n",
        "427": "cross validation and error and the\n",
        "430": "training error are high and so you end\n",
        "432": "up with a relatively high value of both\n",
        "436": "JCV and the j-train this also implies\n",
        "438": "something very interesting which is that\n",
        "440": "if a learning algorithm that has high\n",
        "443": "bias as we get more and more training\n",
        "445": "examples that is as we move to the right\n",
        "447": "to this figure will notice that the\n",
        "449": "cross-validation error isn't going down\n",
        "452": "much it's basically flattened out and so\n",
        "454": "if a learning algorithm is already\n",
        "457": "suffering from high bias getting more\n",
        "459": "training data by itself will actually\n",
        "460": "not help that much and there's a\n",
        "463": "complete example in the figure on the\n",
        "466": "right here we had only five training\n",
        "467": "examples and we fit a certain straight\n",
        "470": "line and when we had a ton more training\n",
        "472": "data we still ended up with roughly the\n",
        "474": "same straight line and so the learning\n",
        "476": "algorithm has high bias just give me a\n",
        "478": "lot more training data that doesn't\n",
        "481": "actually help you get a much lower cross\n",
        "483": "validation error or test set error so\n",
        "485": "knowing if your learning algorithm is\n",
        "487": "suffering from high bias seems like a\n",
        "488": "useful thing to know\n",
        "491": "this can prevent you from wasting a lot\n",
        "492": "of time collecting more training data\n",
        "495": "where it might just not end up being\n",
        "497": "helpful next let's look at the setting\n",
        "500": "of a learning algorithm that may have\n",
        "503": "high variance let's first look at the\n",
        "506": "training error if you have a very small\n",
        "508": "training set like five training examples\n",
        "511": "shown on the figure on the right and if\n",
        "512": "we're fitting say a very high order\n",
        "515": "polynomial I've written 100 degree 100\n",
        "517": "degree polynomial which really no one\n",
        "520": "uses but just for illustration and if\n",
        "522": "we're using a fairly small value of\n",
        "524": "lambda maybe not maybe not zero but\n",
        "527": "fairly small value of lambda then we'll\n",
        "529": "end up you know fitting this data very\n",
        "533": "well that with a function that over fit\n",
        "536": "says so if the training set size is\n",
        "539": "small our training error does J train of\n",
        "543": "theta will be small and as this training\n",
        "546": "set size increases a bit you know we may\n",
        "547": "still be overfitting this data a little\n",
        "551": "bit but it also becomes slightly harder\n",
        "554": "to fit this data set perfectly and so as\n",
        "556": "a training set size increases we'll find\n",
        "560": "that J train increases because it's just\n",
        "561": "a little harder to fit the training set\n",
        "563": "perfectly when we have more examples but\n",
        "565": "the training set error will still be\n",
        "567": "pretty low now how about the cross\n",
        "570": "validation error well in the high\n",
        "572": "variance setting our hypothesis is\n",
        "574": "overfitting and so the cross-validation\n",
        "577": "error will remain high even as we get\n",
        "578": "error will remain high even as we get\n",
        "579": "you know a moderate number of training\n",
        "582": "examples and so maybe the cross\n",
        "584": "validation error may look like that and\n",
        "588": "the indicative diagnostic that we have a\n",
        "591": "high variance problem is the fact that\n",
        "594": "there's this large gap between the\n",
        "595": "training error and the cross-validation\n",
        "598": "error and looking at this figure if we\n",
        "599": "error and looking at this figure if we\n",
        "601": "think about adding more training data\n",
        "602": "that is taking this figure and\n",
        "605": "extrapolating to the right we can kind\n",
        "607": "of tell that you know the two curves the\n",
        "609": "blue curve and the magenta curve are\n",
        "612": "converging to each other and so if we\n",
        "614": "were to extrapolate this figure to the\n",
        "620": " \n",
        "623": "then it seems likely that the training\n",
        "625": "error would keep on going up and the\n",
        "628": "cross-validation error would keep on\n",
        "630": "going down and the thing we really care\n",
        "633": "about is the cross-validation error or\n",
        "636": "the test set error right and so in this\n",
        "637": "sort of figure I can kind of tell that\n",
        "639": "you know if we keep on adding training\n",
        "641": "examples to extrapolate to the right\n",
        "643": "well our cross validation error will\n",
        "646": "keep on coming down and so any high\n",
        "648": "variance setting getting more training\n",
        "651": "data is indeed likely to help and so\n",
        "653": "this again seems like a useful thing to\n",
        "655": "know if your learning algorithm is\n",
        "657": "suffering from a high variance problem\n",
        "659": "because that tells you for example then\n",
        "661": "it may be worth your while to see if you\n",
        "662": "can go and get some more training data\n",
        "665": "now on the previous slide and this slide\n",
        "668": "I've drawn fairly clean fairly idealized\n",
        "670": "curves if you plot these curves for an\n",
        "672": "actual learning algorithm sometimes you\n",
        "674": "will actually see you know pretty much\n",
        "676": "curves like what I've drawn here\n",
        "678": "although sometimes you see curves a\n",
        "679": "little bit noisier a little bit Messier\n",
        "682": "than this but plotting learning curves\n",
        "684": "like these can often tell you that can\n",
        "686": "often help you figure out if your\n",
        "687": "learning algorithm is suffering from\n",
        "689": "bias or variance or even a little bit\n",
        "692": "ago so when I'm trying to improve the\n",
        "694": "performance of a learning algorithm one\n",
        "696": "thing that I'll almost always do is plot\n",
        "699": "these learning curves and usually this\n",
        "700": "will give you a better sense of where\n",
        "703": "this adviser variance problem\n",
        "706": "and in the next video we'll see how this\n",
        "708": "can help suggest specific actions to\n",
        "711": "take or to not take in order to try to\n",
        "712": "improve the performance of your learning\n"
    },
    "IXPgm1e0IOo": {
        "0": " \n",
        "1": "for logistic regression we previously\n",
        "4": "talked about two types of optimization\n",
        "6": "algorithms we talked about how to use\n",
        "8": "gradient descent to optimize this cost\n",
        "10": "function J of theta and we also talked\n",
        "13": "about advanced optimization methods once\n",
        "15": "they are required that you provide a way\n",
        "18": "to compute the cost function J of theta\n",
        "20": "and that you provide a way to compute\n",
        "23": "the derivatives in this video we'll show\n",
        "25": "how you can adapt both of those\n",
        "27": "techniques of gradient descent and the\n",
        "30": "more advanced optimization techniques in\n",
        "32": "order to have them work for regularized\n",
        "37": "logistic regression so here's the idea\n",
        "39": "we saw earlier that logistic regression\n",
        "42": "can also be prone to overfitting if you\n",
        "44": "fit it with a very sort of high order\n",
        "47": "polynomial features like this where G is\n",
        "50": "the sigmoid function and in particular\n",
        "52": "you may end up with the hypothesis you\n",
        "54": "know whose decision boundary is this\n",
        "57": "sort of a overly complex extremely\n",
        "59": "contorted function that really isn't\n",
        "60": "such a great hypothesis for this\n",
        "63": "training set and more generally if you\n",
        "64": "have logistic regression with a lot of\n",
        "66": "features not necessarily polynomial ones\n",
        "68": "but just with a lot of features you can\n",
        "72": "end up with overfitting this one's our\n",
        "74": "cost function for logistic regression\n",
        "77": "and if we want to modify it to use\n",
        "80": "regularization all we need to do is add\n",
        "83": "to it the following term plus lambda\n",
        "86": "over 2m sum from J equals 1 and as\n",
        "89": "usuals from 1 and sum from J equals 1\n",
        "93": "rather than sum from J equals 0 of theta\n",
        "95": "J squared and this has the effect\n",
        "97": "therefore of penalizing the parameters\n",
        "101": "theta 1 theta 2 and so on up to theta n\n",
        "104": "from being too large and if you do this\n",
        "107": "then it will have the effect that even\n",
        "109": "though you you're fitting a very high\n",
        "111": "order polynomial with a lot of\n",
        "113": "parameters so long as you apply\n",
        "115": "regularization and keep the parameters\n",
        "117": "small you're more likely to get a\n",
        "119": "decision boundary you know there may be\n",
        "120": "looks more like this and looks more\n",
        "122": "reasonable for separating out the\n",
        "124": "positive and negative examples\n",
        "128": "so when using regularization even when\n",
        "130": "you have a lot of features the\n",
        "131": "regularization can\n",
        "133": "helped take care of the overfitting\n",
        "135": "problem how do we actually implement\n",
        "138": "this well for the original gradient\n",
        "140": "descent algorithm this was the update we\n",
        "142": "had we were repeatedly perform the\n",
        "145": "following update to theta J this one\n",
        "146": "looks a lot like the previous one for\n",
        "148": "linear regression but what I'm going to\n",
        "150": "do is write the update for theta 0\n",
        "153": "separately so the first line is my\n",
        "155": "update for theta 0 and the second line\n",
        "158": "is now my update for theta 1 up to theta\n",
        "160": "n because I'm going to treat theta 0\n",
        "164": "separately and in order to modify this\n",
        "168": "algorithm to use a regular rise cost\n",
        "171": "function all I need to do is a pretty\n",
        "173": "similar to what we did for linear\n",
        "175": "regression is actually to just multiply\n",
        "178": "this second update rule as follows and\n",
        "180": "once again this you know cosmetically\n",
        "183": "looks identical to what we had for\n",
        "184": "linear regression\n",
        "186": "but of course is not the same algorithm\n",
        "189": "as we had because now the hypothesis is\n",
        "192": "defined using this services are not the\n",
        "194": "same algorithm as regular rise linear\n",
        "196": "regression because the hypothesis is\n",
        "198": "different even though this update that I\n",
        "199": "wrote down then it actually looks\n",
        "201": "cosmetically the same as when we had\n",
        "202": "earlier when working out\n",
        "205": "gradient descent for regularized linear\n",
        "207": "regression and of course just a wrap of\n",
        "210": "this discussion this term here in the\n",
        "214": "square brackets so this term here this\n",
        "217": "term is of course the new partial\n",
        "219": "derivative with respect to theta J of\n",
        "222": "the new cost function J of theta where J\n",
        "224": "of theta here is the cost function we\n",
        "227": "defined on the previous slide that does\n",
        "229": "use regularization\n",
        "231": "so that's gradient descent for\n",
        "234": "regularized linear regression\n",
        "237": "let's talk about how to get regularize\n",
        "240": "linear regression to work using the more\n",
        "243": "advanced optimization methods and just\n",
        "246": "remind you for those methods what we\n",
        "248": "needed to do was to define the function\n",
        "250": "that's called the cost function that\n",
        "252": "takes us input the parameter vector\n",
        "256": "theta and once again in the equations\n",
        "258": "we've been writing here we use zero\n",
        "261": "index vectors so we had you know theta 0\n",
        "264": "up to theta n but because octave indexes\n",
        "267": "the vectors starting from one theta zero\n",
        "271": "is written in octave as theta 1 theta 1\n",
        "273": "is written in octave as theta 2 and so\n",
        "278": "on down to theta n plus 1 and what we\n",
        "281": "needed to do was a provider function was\n",
        "283": "provided a function called cost function\n",
        "286": "that we would then pass in to what we\n",
        "288": "have we saw earlier we will use the F\n",
        "292": "min UNC and then you know at cost\n",
        "296": "function and so on right but the F min\n",
        "299": "UNC it was the F min unconstrained and\n",
        "301": "this will what would an F min UNC was\n",
        "303": "what would take the cost function and\n",
        "306": "minimize it for us so the two main\n",
        "308": "things that the cost function needed to\n",
        "312": "return were first j-val and for that we\n",
        "314": "need to write code to compute the cost\n",
        "317": "function J of theta now when we're using\n",
        "320": "regularize logistic regression of course\n",
        "323": "the cost function J of theta changes and\n",
        "325": "in particular now our cost function\n",
        "327": "needs to include this additional\n",
        "329": "regularization term at the end as well\n",
        "332": "so when you compute J of theta be sure\n",
        "334": "to include that term at the end and then\n",
        "337": "the other thing that this this cost\n",
        "338": "function things need to provide were the\n",
        "339": "function things need to provide were the\n",
        "342": "gradient so gradient one needs to be set\n",
        "344": "to the partial derivative of J of theta\n",
        "347": "with respect to theta 0 gradient 2 needs\n",
        "349": "to be set to that and so on once again\n",
        "351": "the index is up by 1 right because of\n",
        "354": "the indexing from one data octave users\n",
        "358": "and looking at these terms this term\n",
        "360": "over here we actually work dissolved on\n",
        "362": "a previous slide on is actually equal to\n",
        "363": "this\n",
        "364": "it doesn't change because the derivative\n",
        "367": "for theta 0 doesn't change\n",
        "369": "compared to the version without\n",
        "373": "regularization and the other terms do\n",
        "375": "change and in particular the derivative\n",
        "377": "of respect to theta one we work this out\n",
        "379": "on the previous slide as well is equal\n",
        "382": "to you know the original term and then\n",
        "385": "minus lambda over m times theta one just\n",
        "387": "so make sure we pause this correctly if\n",
        "390": "we can add parentheses here right so the\n",
        "392": "summation doesn't extend and similarly\n",
        "395": "you know this other term here looks like\n",
        "397": "this with this additional term that we\n",
        "398": "had on the previous slide that\n",
        "400": "corresponds to the gradient from their\n",
        "402": "regularization objective so if you\n",
        "405": "implement this cost function and pass\n",
        "408": "this into F min UNC or to one of those\n",
        "411": "advanced optimization techniques that\n",
        "415": "will minimize the new regular rise cost\n",
        "417": "function J of theta and the parameters\n",
        "420": "you get out will be the ones that\n",
        "422": "correspond to logistic regression with\n",
        "425": "regularization so now you know how to\n",
        "428": "implement regularized which is a\n",
        "430": "regression when I walk around Silicon\n",
        "432": "Valley I live here in Silicon Valley\n",
        "434": "there are a lot of engineers that are\n",
        "436": "frankly making a ton of money for their\n",
        "438": "companies using machine learning\n",
        "441": "algorithms and I know we've only been\n",
        "442": "you know studying this stuff for a\n",
        "444": "little while but if you understand\n",
        "447": "linear regression the dis aggression the\n",
        "449": "advanced optimization algorithms and\n",
        "451": "regularization by now frankly you\n",
        "454": "probably know quite a lot more machine\n",
        "456": "learning than many certainly not all but\n",
        "458": "you probably know quite a long known\n",
        "460": "machine learning right now then frankly\n",
        "462": "many of the Silicon Valley engineers\n",
        "464": "well out there having very successful\n",
        "466": "careers you know making tons of money\n",
        "468": "for the companies or or building great\n",
        "469": "products using machine learning\n",
        "472": "algorithms so in congratulations you've\n",
        "474": "actually come a long ways and you can\n",
        "475": "actually you actually know enough to\n",
        "477": "apply this stuff and get to work for\n",
        "480": "many problems so congratulations for\n",
        "482": "that but of course it's still a lot more\n",
        "485": "that we want to teach you and in the\n",
        "487": "next set of videos after this we'll\n",
        "489": "start to talk about a very powerful\n",
        "491": "class of nonlinear classifiers so\n",
        "493": "whereas linear regression with just\n",
        "495": "regression you know you can throw in\n",
        "498": "polynomial terms but it turns out that\n",
        "499": "they're much more powerful normally\n",
        "502": "classifiers that can then dental a\n",
        "505": "polynomial regression and in the next\n",
        "507": "set of videos after this one I'll start\n",
        "509": "telling you about them so that you have\n",
        "510": "even more powerful learning algorithms\n",
        "512": "than you have now to apply to different\n"
    },
    "JjB58InuTqM": {
        "0": " \n",
        "2": "in this and the next video I'd like to\n",
        "5": "tell you about one possible extension to\n",
        "7": "the anomaly detection algorithm that\n",
        "9": "we've developed so far this extension\n",
        "11": "uses something called the multivariate\n",
        "14": "Gaussian distribution it has some\n",
        "16": "advantages and some disadvantages and it\n",
        "19": "can sometimes catch some anomalies that\n",
        "22": "the earlier album didn't to motivate\n",
        "25": "this list starts with an example let's\n",
        "27": "say that our unlabeled data those that\n",
        "30": "were not plotted here and I'm going to\n",
        "32": "use the example of monitoring machines\n",
        "34": "in the data center Monica computers in\n",
        "36": "the data center so my two features are\n",
        "39": "x1 which is the CPU load and x2 which is\n",
        "42": "maybe the memory use so if I take my two\n",
        "44": "features if C 1 X 2 and if I model them\n",
        "47": "as Gaussian you know here's a plot of my\n",
        "50": "x1 features here's a plot of my x2\n",
        "52": "features and so if I fed a Gaussian to\n",
        "55": "that maybe i'll get a gaussian like this\n",
        "60": "so here's P of x1 which is her which\n",
        "62": "depends on my parameters nu 1 and Sigma\n",
        "65": "square is 1 and here's my memory used\n",
        "67": "you know maybe i'll get a gaussian that\n",
        "68": "looks like this\n",
        "71": "and this is my P of x2 which depends on\n",
        "74": "mu 2 and Sigma square 2 and so this is\n",
        "76": "how the anomaly detection algorithm\n",
        "80": "Martha links 1 and X 2 now let's say\n",
        "82": "that in the test set I have an example\n",
        "86": "that looks like this right the location\n",
        "89": "of that Green Cross so the value of x1\n",
        "91": "is about 0.4 and the value of x2 is\n",
        "95": "about 1.5 now if you look the data looks\n",
        "97": "like most of the data lies in this\n",
        "101": "region and so that Green Cross is pretty\n",
        "103": "far away from any of the days where I've\n",
        "105": "seen it looks like that should be raised\n",
        "107": "in an anomaly right stuff in my data in\n",
        "110": "my in the data of my good examples it\n",
        "112": "looks like you know the CPU load and the\n",
        "115": "memory use they sort of grow linearly\n",
        "117": "with each other so if I\n",
        "120": "machine using lots of CPU you know\n",
        "121": "memory use will also be very high\n",
        "124": "whereas this example this being example\n",
        "127": "it looks like here the CPU load is very\n",
        "129": "low but the memory used is very high and\n",
        "130": "just have not seen that before my\n",
        "131": "training set it looks like that should\n",
        "133": "be an anomaly but let's see what the\n",
        "135": "anomaly out to touch an apple will do\n",
        "137": "well for the CPU load that puts it at\n",
        "140": "around their 0.5 and reasonably high\n",
        "142": "probability is not that far from other\n",
        "146": "examples we'll see maybe where as for\n",
        "149": "the memory use the supply of 0.5 whereas\n",
        "152": "for the memory uses about 1.5 which is\n",
        "154": "there again you know this this all to us\n",
        "156": "to tell the Gaussian but the value here\n",
        "159": "and the value here is not that different\n",
        "161": "from many other of the examples of seen\n",
        "165": "and so P of x1 will be pretty high\n",
        "167": "reasonably high P of x2 or reasonably\n",
        "170": "high I mean if you look at this plot\n",
        "171": "right this point here it doesn't look\n",
        "174": "that bad and you look at this plot you\n",
        "176": "know across here doesn't look that bad I\n",
        "179": "mean a half an examples would be even\n",
        "182": "greater memory use or with even less CPU\n",
        "184": "use and so this example doesn't look\n",
        "187": "that anomalous and so anomaly detection\n",
        "189": "algorithm will fail to fly at this point\n",
        "192": "as an anomaly and it turns out that what\n",
        "194": "our anomaly detection algorithm is doing\n",
        "196": "is just not realizing that this blue\n",
        "198": "ellipse shows the high probability\n",
        "200": "region instead of other things is that\n",
        "203": "examples here the high probability and\n",
        "206": "examples the NICS circles are somewhat\n",
        "209": "lower probably in examples here even\n",
        "211": "though our probability and somehow you\n",
        "212": "know things that are Green Cross there\n",
        "214": "is pretty hard probability and in\n",
        "216": "particular it tends to think that you\n",
        "218": "know everything in this region\n",
        "221": "everything on the line did I'm circling\n",
        "223": "over has you know about equal\n",
        "225": "probability and it doesn't realize that\n",
        "229": " \n",
        "232": "actually has much lower probability than\n",
        "234": "something over there\n",
        "237": "so in order to fix this we can we're\n",
        "239": "going to develop a modified version of\n",
        "241": "the anomaly detection algorithm using\n",
        "242": "something called the multivariate\n",
        "244": "Gaussian distribution also called the\n",
        "247": "multivariate normal distribution so\n",
        "249": "here's what we're going to do we have\n",
        "252": "features X which are in RN and instead\n",
        "254": "of modeling P of X 1 P of X 2 separately\n",
        "256": "we're going to model P of X all in one\n",
        "258": "go so model of P of X you're all at the\n",
        "261": "same time so the parameters of the\n",
        "264": "multivariate Gaussian distribution of MU\n",
        "266": "which is a vector and Sigma which is an\n",
        "268": "N by n matrix called the covariance\n",
        "271": "matrix and the similar to the covariance\n",
        "273": "matrix that we saw when we were working\n",
        "275": "with the PCA with the principal\n",
        "278": "component analysis algorithm for the\n",
        "279": "sake of completeness let me just write\n",
        "281": "out the formula for the multivariate\n",
        "283": "Gaussian distribution so to say that\n",
        "285": "probably of X and this is parameterize\n",
        "289": "by my parameters mu and Sigma but the\n",
        "292": "property of X is equal to once again\n",
        "294": "this absolutely no need to memorize this\n",
        "296": "formula you know you can look it up for\n",
        "298": "whatever mean you need to use it but\n",
        "301": "this is what they're probably of X looks\n",
        "302": "like\n",
        "308": "transpose inverse X minus mu and this\n",
        "311": "thing here the absolute value of Sigma\n",
        "313": "this thing here when we write this\n",
        "316": "symbol this is called the determinant of\n",
        "319": "Sigma and this is a mathematical\n",
        "321": "function of a matrix and you really\n",
        "323": "don't need to know what the determinant\n",
        "325": "of a matrix is but I really all you need\n",
        "328": "to know is that you can compute it in\n",
        "331": "octave by using the octave command Det\n",
        "335": "of you know Sigma okay and then again\n",
        "337": "just be clear writing this expression\n",
        "341": "the Sigma is here these are this n by n\n",
        "343": "matrix this is not a summation you know\n",
        "346": "the Sigma there is an N by n matrix so\n",
        "351": "that's the formula for P of X but more\n",
        "353": "interestingly or more importantly is\n",
        "355": "what does P of X actually look like\n",
        "357": "let's look at some examples of\n",
        "360": "multivariate among download Gaussian\n",
        "363": "distributions so let's take a\n",
        "365": "two-dimensional example so if I have N\n",
        "368": "equals 2 I have two features x1 and x2\n",
        "371": "let's say I said Mew to be equal to 0\n",
        "373": "and Sigma to be equal to this matrix\n",
        "375": "here with ones on the diagonals and\n",
        "378": "zeros on the off diagonals this matrix\n",
        "379": "is sometimes also called the identity\n",
        "384": "matrix in that case P of X is will look\n",
        "388": "like this and what I'm showing this\n",
        "390": "figure is you know for a specific value\n",
        "394": "of x1 and for a specific value of x2 the\n",
        "396": "height of this surface gives the value\n",
        "399": "of P of X and so with this setting of\n",
        "402": "the parameters P of X is highest when x1\n",
        "405": "x2 equals 0 so that's the peak of this\n",
        "407": "Gaussian distribution and the\n",
        "409": "probability falls off with this sort of\n",
        "411": "two-dimensional Gaussian or this\n",
        "413": "bell-shaped 2-dimensional bell-shaped\n",
        "417": "surface down below is the same thing but\n",
        "419": "FATA using a contour plot\n",
        "421": "partly using the different colors and so\n",
        "424": "this heavy intense red in the middle\n",
        "427": "corresponds to the highest values and\n",
        "428": "then the other values decrease with the\n",
        "431": "yellow being slightly lower values for\n",
        "433": "cyan being lower values in this deep\n",
        "435": "blue being the lowest value so this is\n",
        "437": "really the same figure but plotted you\n",
        "438": "know we're from viewed from the top\n",
        "442": "instead using colors instead and so with\n",
        "444": "this distribution you see that it places\n",
        "447": "most of the probability near zero zero\n",
        "449": "and then as you go away as you go out go\n",
        "452": "away from zero zero the probability of\n",
        "456": "x1 and x2 goes down now let's try\n",
        "458": "varying some of the parameters and see\n",
        "461": "what happens so let's take Sigma and\n",
        "462": "what happens so let's take Sigma and\n",
        "464": "change it so let's say Sigma the\n",
        "466": "strengths evolving Sigma is a covariance\n",
        "468": "matrix and so it measures the variance\n",
        "470": "of the variability of the features x1 x2\n",
        "471": "of the variability of the features x1 x2\n",
        "473": "so if you strength Sigma then what you\n",
        "475": "get is that the width of this bump\n",
        "476": "get is that the width of this bump\n",
        "479": "diminishes and the height also increment\n",
        "481": "also increases a bit because of the area\n",
        "483": "under the surface is equal to one so the\n",
        "485": "integral of the volume under the surface\n",
        "488": "is equal of one because probably\n",
        "490": "distribution must integrate to one but\n",
        "493": "so if you strength of Aryans yes it's\n",
        "496": "kind of like shrinking Sigma squared you\n",
        "499": "end up with a narrower distribution and\n",
        "501": "one does love its hollow and so you see\n",
        "504": "here also the concentric ellipses has\n",
        "505": "shrunk\n",
        "507": "whereas in contrast if you were to\n",
        "508": "whereas in contrast if you were to\n",
        "510": "increase sigma22\n",
        "512": "to on the diagonals now two times the\n",
        "514": "identity then you end up with it much\n",
        "516": "why there are much flatter Gaussian and\n",
        "519": "so the width of this is much wider then\n",
        "520": "it's hard to see but this is still a\n",
        "522": "bell-shaped bump let's just flatten down\n",
        "524": "the lot that's become much lighter and\n",
        "526": "so the variance here are the variability\n",
        "530": "of x1 x2 just becomes wider here a few\n",
        "532": "more examples now let's try the air\n",
        "534": "let's try varying one of the elements of\n",
        "537": "Sigma at a time let's say I set Sigma to\n",
        "541": "0.6 there and one over there what this\n",
        "545": "does is this reduces the variance of the\n",
        "548": "first feature x1 while keeping the\n",
        "551": "variance of the second feature x2 the\n",
        "553": "same and so with this setting of Francis\n",
        "556": "you can model things like that x1 has\n",
        "559": "smaller the aryans and x2 as logic areas\n",
        "563": "whereas if I do this if I set this\n",
        "566": "matrix 2 to 1 then you can also model\n",
        "568": "examples well you know here looks like\n",
        "571": "x1 can have take on a large range of\n",
        "573": "values whereas x2 takes on the\n",
        "575": "relatively narrow range of values and\n",
        "577": "that's reflected in this figure as well\n",
        "579": "aware you know that the distribution\n",
        "583": "falls off more slowly as x1 moves away\n",
        "587": "from 0 the Falls are very rapidly as x2\n",
        "591": "moves away from 0 and similarly if we\n",
        "593": "were to modify this element of the\n",
        "595": "matrix instead and similar to the\n",
        "599": "previous slide except that here we're\n",
        "600": "you know playing around here we're\n",
        "603": "saying that x2 can take on a very small\n",
        "606": "range of values and so here if this is\n",
        "610": "0.6 and notice now x2 tends to take on\n",
        "612": "that much smaller range of values that\n",
        "613": "the origin\n",
        "616": "example whereas if we were to set Sigma\n",
        "618": "to the tune and that's a saying that x2\n",
        "620": "you know has a much larger range of\n",
        "624": "values now one of the cool things about\n",
        "625": "the multivariate Gaussian distribution\n",
        "628": "is then you can also use it to model\n",
        "631": "correlations between the data that is\n",
        "633": "you can model just the model of the fact\n",
        "635": "that x1 and x2 tend to be highly\n",
        "636": "correlated with each other for example\n",
        "639": "so specifically if you start to change\n",
        "642": "the off-diagonal entries of this\n",
        "644": "covariance matrix so you can get a\n",
        "645": "different type of Gaussian distribution\n",
        "649": "and so as I increase the off-diagonal\n",
        "652": "entries from from 52.8 what I get is a\n",
        "654": "distribution that's more and more thinly\n",
        "657": "peaks along this sort of x equals y\n",
        "658": "alignment\n",
        "660": "so here the contour says that you know\n",
        "663": "it's a Y tend to grow together and the\n",
        "664": "large the things with large probability\n",
        "668": "are if you know either x1 as large and\n",
        "671": "y2 as large or a from small my to a\n",
        "672": "small or somewhere in between\n",
        "676": "and as this entry open it gets large you\n",
        "678": "get a Gaussian distribution this so that\n",
        "680": "where all the probability lies on this\n",
        "681": "where all the probability lies on this\n",
        "684": "sort of own narrow region where X is\n",
        "687": "approximately equal to Y and it is a\n",
        "690": "very tall thin distribution you know\n",
        "693": "lying mostly along this line mostly\n",
        "695": "along this central region where X is\n",
        "698": "close to Y so this is if we set these\n",
        "701": "entries to be positive entries in\n",
        "703": "contrast if we set these to negative\n",
        "706": "values as I decreases the minus 25 down\n",
        "708": "to minus point 8 then what we get is a\n",
        "710": "model where we put most of the\n",
        "714": "probability in this sort of negative x 1\n",
        "715": "and x 2 correlation\n",
        "718": "and so most of the probability now lies\n",
        "720": "in this region where x1 is about equal\n",
        "723": "to minus x2 rather than the x1 equals x2\n",
        "726": "and so this captures a sort of negative\n",
        "730": "correlation between x1 and x2 and so\n",
        "732": "this is a hopefully this gives you a\n",
        "734": "sense of the different distributions\n",
        "736": "that a multivariate Gaussian\n",
        "739": "distribution can capture so far we've\n",
        "742": "been varying the covariance matrix Sigma\n",
        "744": "the other thing you can do is also vary\n",
        "748": "the mean parameter mu and so originally\n",
        "751": "we have medical 0 0 and so the\n",
        "753": "distribution was centered around X 1\n",
        "756": "equals 0 X 2 equals 0 so the peak of the\n",
        "759": "distribution is here whereas if we vary\n",
        "762": "the values of MU then that varies the\n",
        "764": "peak of the distribution so immunity\n",
        "767": "equals 0 0.5 the peak is at you know x1\n",
        "772": "equals 0 and x2 equals 0.5 and so the\n",
        "773": "peak or the center of this distribution\n",
        "779": "has shifted and if nu was you know 1.5\n",
        "783": "minus 0.5 then again similarly the peak\n",
        "786": "of the distribution has now shifted to a\n",
        "788": "different location corresponding to\n",
        "791": "where you know x1 is 1 point 5 and x2 is\n",
        "794": "minus 0.5 and so varying the new\n",
        "796": "parameter just shifts around the center\n",
        "799": "of this whole distribution so hopefully\n",
        "800": "looking at all these different pictures\n",
        "802": "gives you a sense of the sort of\n",
        "805": "probability distributions that the\n",
        "807": "multivariate Gaussian distribution\n",
        "809": "the capture and the key advantage of it\n",
        "812": "it allows you to capture when you expect\n",
        "814": "two different features to be positively\n",
        "816": "correlated or maybe negatively\n",
        "819": "correlated in the next video we'll take\n",
        "820": "this multivariate Gaussian distribution\n"
    },
    "KvtGD37Rm5I": {
        "0": " \n",
        "2": "in this video I'd like to convey to you\n",
        "4": "the main intuitions behind how\n",
        "6": "regularization works and we'll also\n",
        "8": "write down the cost function that we'll\n",
        "11": "use when we're using regularization with\n",
        "13": "the hand-drawn examples that we'll have\n",
        "15": "on these slides I think I'll be able to\n",
        "19": "convey part of the intuition but an even\n",
        "21": "better way to see for yourself how\n",
        "23": "regularization works is if you implement\n",
        "25": "it and sort of see it work for yourself\n",
        "28": "and if you do there promo exercises\n",
        "30": "after this you get a chance to sort of\n",
        "32": "see regularization in action for\n",
        "33": "yourself\n",
        "37": "so here's the intuition in the previous\n",
        "39": "video we saw that if we were to fit a\n",
        "42": "quadratic function to this data it'd\n",
        "43": "give us a pretty good fit to the data\n",
        "46": "whereas if we were to fit an overly high\n",
        "49": "order polynomial we end up with a curve\n",
        "51": "they may fit the training set very well\n",
        "54": "that really not be a not.but overfit the\n",
        "57": "data poorly and not generalize well\n",
        "60": "consider the following suppose we were\n",
        "62": "to penalize and make the parameters\n",
        "66": "theta 3 and theta 4 really small here's\n",
        "69": "what I mean here's our optimization\n",
        "71": "objective but here's the optimization\n",
        "72": "problem where we minimize our usual\n",
        "75": "squared error cost function let's say I\n",
        "78": "take this objective and I modify it and\n",
        "85": "add to it plus 1000 theta 3 squared plus\n",
        "89": "1000 theta 4 squared 1000 I'm just\n",
        "91": "writing down the sum as some huge number\n",
        "94": "now if we were to minimize this function\n",
        "97": "well the only way to make this new cost\n",
        "100": "function small is if theta 3 and theta 4\n",
        "103": "are small right because otherwise you\n",
        "104": "know if you have a thousand times theta\n",
        "106": "3 this disc over this new cost function\n",
        "108": "is going to be paid so when you minimize\n",
        "110": "this new function we're going to end up\n",
        "114": "with theta 3 close to 0 and theta 4\n",
        "118": "close to 0 and that's as if we're\n",
        "121": "getting rid of these two terms over\n",
        "125": "there and if we do that well then if\n",
        "127": "theta 3 and theta 4 equals 0 then we're\n",
        "129": "basically left with a quadratic function\n",
        "131": "and so we'll end up with a fit to the\n",
        "133": "data there's you know a quadratic\n",
        "134": "function\n",
        "137": "Plus maybe tiny contributions from small\n",
        "139": "terms theta 3 theta folder that may be\n",
        "140": "very close to zero\n",
        "147": "mommy and so we end up with essentially\n",
        "149": "a quadratic function which is good\n",
        "152": "because this is a much better hypothesis\n",
        "156": "in this particular example we looked at\n",
        "159": "the effect of penalizing two of the\n",
        "161": "parameter values being large more\n",
        "163": "generally here's the idea behind\n",
        "168": "regularization the idea is that if we\n",
        "171": "have small values for the parameters\n",
        "174": "then having small values for the\n",
        "176": "parameters will somehow will usually\n",
        "179": "correspond to having a simpler\n",
        "181": "hypothesis so for our last example we\n",
        "184": "penalize just data 3 and theta 4 and\n",
        "187": "when both of these were close to zero we\n",
        "189": "wound up with a much simpler hypothesis\n",
        "190": "that was essentially a quadratic\n",
        "193": "function but more broadly if we penalize\n",
        "196": "all the parameters usually that we can\n",
        "198": "think of that as trying to give us a\n",
        "200": "simple hypothesis as well because when\n",
        "202": "you know these parameters are close to\n",
        "204": "zero in this example that gave us a\n",
        "208": "quadratic function but more generally is\n",
        "210": "possible to show that having smaller\n",
        "213": "values of the parameters corresponds to\n",
        "215": "usually smoother functions as well\n",
        "217": "that's simpler and which are therefore\n",
        "222": "also less prone to overfitting I realize\n",
        "224": "that the reasoning for why having all\n",
        "225": "the parameters be small Y that\n",
        "227": "corresponds to simple hypotheses I\n",
        "229": "realized that reasoning may not be\n",
        "232": "entirely clear to you right now and it\n",
        "233": "is kind of hard to explain unless you\n",
        "235": "implement it yourself and see it for\n",
        "237": "yourself but I hope that the example of\n",
        "240": "having theta 3 and theta 4 be small and\n",
        "243": "how that gave us a simpler hypothesis I\n",
        "245": "hope that helps explain why and these\n",
        "247": "give some intuition as to why this might\n",
        "251": "be true let's look at a specific example\n",
        "254": "for housing price prediction we may have\n",
        "255": "our hundred features that we talked\n",
        "258": "about where maybe x1 is the size X is\n",
        "260": "the number of bedrooms x3 is the number\n",
        "262": "of floors and so on and we may have a\n",
        "263": "hundred features\n",
        "267": "and unlike the polynomial example we\n",
        "269": "don't know right we don't know that\n",
        "271": "theta 3 theta 4 are the high order\n",
        "274": "polynomial terms so we have just a bag\n",
        "276": "or if you have just a set of 100\n",
        "279": "features is hard to pick an advance\n",
        "281": "which are the ones that are less likely\n",
        "284": "to be relevant so we have you know 100\n",
        "286": "100 101 parameters and we don't know\n",
        "290": "which ones to pick tunes we don't know\n",
        "292": "which parameters to pick to try the\n",
        "296": "string so in regularization what we're\n",
        "298": "going to do is take our cost function\n",
        "300": "here's my cost function for linear\n",
        "302": "regression and what I'm going to do is\n",
        "305": "modify this cost function to shrink all\n",
        "307": "of my parameters because you know I I\n",
        "309": "don't know which one or two to try that\n",
        "310": "don't know which one or two to try that\n",
        "311": "string so I'm going to modify my cost\n",
        "317": "function to add a term at the end like\n",
        "319": "so let me add square brackets here as\n",
        "321": "well I'm going to add an extra\n",
        "324": "regularization term at the end to\n",
        "326": "strength every single parameter and so\n",
        "329": "this term will tend to shrink all of my\n",
        "332": "parameters theta 1 theta 2 theta 3 up to\n",
        "338": "theta 100 by the way by convention the\n",
        "341": "summation here in starts from 1 so I'm\n",
        "343": "not actually going to penalize theta 0\n",
        "346": "being large that's sort of a convention\n",
        "348": "that the sum is from I equals 1 through\n",
        "351": "n rather than I equals 0 through n but\n",
        "352": "in practice it makes very little\n",
        "354": "difference and whether you include you\n",
        "357": "know theta 0 or not in practice make\n",
        "359": "very little difference in results but by\n",
        "362": "convention usually we regularize only\n",
        "364": "theta 1 through theta one hundred\n",
        "367": "writing down our regular rise\n",
        "369": "optimization objective our regularized\n",
        "371": "cost function again here this US J of\n",
        "374": "theta where this term on the right is\n",
        "377": "the regularization term and lambda here\n",
        "383": " \n",
        "387": "and what lambda does is it controls a\n",
        "389": "trade-off between two different goals\n",
        "393": "the first go captured by the first term\n",
        "395": "in the objective is that we would like\n",
        "396": "to train you is that we would like to\n",
        "398": "fit the training data well we would like\n",
        "400": "to train fit the training set well and\n",
        "403": "the second goal is we want to keep the\n",
        "405": "parameters small and that's captured by\n",
        "407": "the second term by the regularization\n",
        "411": "objective and by the regularization term\n",
        "413": "and what lambda the regularization\n",
        "415": "parameter does is it controls the\n",
        "417": "trade-off between these two goals\n",
        "419": "between the goal of fitting the training\n",
        "421": "set well and the goal of keeping the\n",
        "424": "parameter small and therefore keeping\n",
        "426": "the hypothesis relatively simple to\n",
        "430": "avoid overfitting for our housing price\n",
        "432": "prediction example whereas previously if\n",
        "435": "we had fit a very high order polynomial\n",
        "437": "we may have wound up with a very Wiggly\n",
        "440": "or curvy function like this if you still\n",
        "443": "fit a high order polynomial with all the\n",
        "445": "polynomial features in there but instead\n",
        "448": "you just make sure to use this sort of\n",
        "451": "regular rise objective then what you can\n",
        "454": "get out is in fact a curve that isn't\n",
        "456": "quite a quadratic function but is much\n",
        "458": "smoother and much simpler and maybe a\n",
        "460": "curve like the magenta line that you\n",
        "462": "know fits that gives a much better\n",
        "464": "hypothesis for this data\n",
        "467": "once again I realize it can be a bit\n",
        "469": "difficult to see why shrinking the\n",
        "471": "parameters can have this effect but if\n",
        "473": "you implement this album yourself with\n",
        "475": "regularization you will be able to see\n",
        "476": "regularization you will be able to see\n",
        "479": " \n",
        "482": "in regularized linear regression if the\n",
        "484": "parameter if the regularization\n",
        "486": "parameter lambda is set to be very large\n",
        "490": "then what will happen is we will end up\n",
        "492": "penalizing the parameters theta 1 theta\n",
        "498": "2 theta 3 theta 4 very high D that is if\n",
        "500": "our hypothesis is this one down at the\n",
        "503": "bottom and if we end up penalizing theta\n",
        "505": "1 theta 2 theta 3 theta 4 very heavily\n",
        "507": "then we'll end up with all of these\n",
        "509": "parameters close to 0 right\n",
        "511": "theta 1 we close to 0 theta 2 equals 0\n",
        "515": "theta 3 and theta 4 will end up being\n",
        "517": "close to 0 and that we do that is as if\n",
        "519": "we're getting rid of these terms and the\n",
        "521": "hypothesis so that we're just left with\n",
        "524": "a hypothesis that looks like that it\n",
        "526": "says that well housing prices are equal\n",
        "530": "to theta 0 and that is akin to fitting a\n",
        "533": "flat horizontal straight line to the\n",
        "537": "data and this is an example of\n",
        "539": "underfitting and in particular this\n",
        "541": "hypothesis this straight line it just\n",
        "544": "fails to fit the training set well it's\n",
        "546": "just a flat straight line it doesn't go\n",
        "548": "you know go near it doesn't go anywhere\n",
        "549": "near most of our training examples and\n",
        "552": "another way of saying this is that this\n",
        "554": "hypothesis has too strong a\n",
        "556": "preconception or too high a bias that\n",
        "558": "housing prices are just equal to theta 0\n",
        "561": "and despite the clear data to the\n",
        "564": "you know chooses to fint a sort of flat\n",
        "566": "line just a horace flat horizontal line\n",
        "568": "i didn't draw that very well if it's\n",
        "571": "just a horizontal flat line to the data\n",
        "575": "so for regularization to work well some\n",
        "578": "care should be taken to choose a good\n",
        "580": "choice for the regularization parameter\n",
        "583": "lambda as well and when we talk about\n",
        "585": "model selection later in this course\n",
        "588": "we'll talk about a way of a variety of\n",
        "590": "ways for automatically choosing the\n",
        "593": "regularization parameter lambda as well\n",
        "595": "so that's the idea behind regularization\n",
        "596": "so that's the idea behind regularization\n",
        "598": "and the cost function we'll use in order\n",
        "600": "to use regularization in the next two\n",
        "603": "videos let's take these ideas and apply\n",
        "605": "them to linear regression and to\n",
        "607": "logistic regression so that we can then\n"
    },
    "LvgcfMOyREE": {
        "0": " \n",
        "1": "most of the supervised learning\n",
        "3": "algorithms have seen things like linear\n",
        "5": "regression logistic regression and so on\n",
        "7": "all of those algorithms had an\n",
        "9": "optimization objective or some cost\n",
        "10": "function that the album was trying to\n",
        "13": "minimize it turns out that k-means also\n",
        "16": "has an optimization objective or a cost\n",
        "17": "function that is trying to minimize and\n",
        "20": "in this video I'd like to tell you what\n",
        "23": "that optimization objective is and the\n",
        "25": "reason I want to do so is because this\n",
        "27": "will be useful to us for two purposes\n",
        "29": "first knowing what is the optimization\n",
        "32": "objective of k-means will help us to\n",
        "34": "debug the learning algorithm and just\n",
        "35": "make sure that k-means is running\n",
        "38": "correctly and second and perhaps even\n",
        "41": "more importantly in a later video we'll\n",
        "42": "talk about how we can use this to help\n",
        "45": "k-means find better clusters and avoid\n",
        "47": "local optima but we'll do that in a\n",
        "49": "later video that follows this one just\n",
        "51": "as a quick reminder while k-means is\n",
        "54": "running we're going to be keeping track\n",
        "56": "of two sets of variables first is these\n",
        "57": "of two sets of variables first is these\n",
        "60": "CIS and that keeps track of the index or\n",
        "62": "the number of the cluster to which an\n",
        "65": "example X is currently assigned and then\n",
        "67": "the other set of variables we use this\n",
        "70": "mu subscript K which is the location of\n",
        "73": "cluster centroid K and again it for\n",
        "76": "k-means we use capital k to denote the\n",
        "78": "total number of clusters and here\n",
        "80": "lowercase K you know is going to be an\n",
        "81": "lowercase K you know is going to be an\n",
        "82": "index into them into the cluster\n",
        "84": "centroids and so a lowercase K is going\n",
        "88": "to be a number between 1 and capital K\n",
        "90": "now here's one more bit of notation\n",
        "92": "which is I'm going to use mu subscript\n",
        "96": "CI to denote the cluster centroid of the\n",
        "99": "cluster to which example X I has been\n",
        "101": "assigned right and and you know just to\n",
        "102": "explain that notation a little bit more\n",
        "105": "let's say that X I\n",
        "108": "has been assigned to cluster number five\n",
        "111": "what that means is that CI that is the\n",
        "114": "index of X I that that is equal to five\n",
        "116": "right because you know having CI equals\n",
        "120": "five that's that's what it means for the\n",
        "123": "example X I to be assigned to cluster\n",
        "124": "number five\n",
        "128": "and so new subscript CI is going to be\n",
        "131": "equal to MU subsequent five because CI\n",
        "135": "is equal to five and so this mu\n",
        "137": "subscript CI is the cluster centroid of\n",
        "139": "cluster number five which is the cluster\n",
        "142": "to which my example X I as been assigned\n",
        "145": "armed with this notation we're now ready\n",
        "147": "to write out what is the optimization\n",
        "149": "objective of the k-means clustering\n",
        "151": "algorithm and here it is the cost\n",
        "154": "function that k-means is minimizing is a\n",
        "157": "function J of all of these parameters C\n",
        "159": "1 through CMM u 1 through UK that\n",
        "161": "k-means is varying as the average runs\n",
        "164": "and the optimization objective is shown\n",
        "166": "on the right is the average a 1 over m\n",
        "168": "of sum from I equals 1 through m of this\n",
        "171": "term here that sum I've just drawn the\n",
        "174": "red box around right over the squared\n",
        "176": "distance between each example X I and\n",
        "179": "the location of the cluster centroid to\n",
        "180": "the location of the cluster centroid to\n",
        "183": "which X I has been assigned so let me\n",
        "185": "just draw this in a limb just explained\n",
        "187": "is right so here's the location of\n",
        "189": "training example X I and here's the\n",
        "191": "location of the cluster centroid to\n",
        "194": "which example X I has been assigned so\n",
        "197": "to explain this in pictures if here's x1\n",
        "201": "x2 and if on a point here is my example\n",
        "204": "X I so if that is equal to my exam\n",
        "207": "X I and if X I has been assigned to some\n",
        "209": "cluster centroid will denote my cluster\n",
        "211": "centroid of a cross so if that's the\n",
        "215": "location of you know mu5 let's say if X\n",
        "216": "I has been assigned to cluster centroid\n",
        "219": "5000 bodies I have out there then this\n",
        "221": "squared distance that's the square of\n",
        "224": "the distance between the point X I and\n",
        "228": "this cluster centroid to which XR has\n",
        "230": "been assigned and what k-means can be\n",
        "233": "shown to be doing is that it is trying\n",
        "237": "to find parameter CI and you I trying to\n",
        "239": "find CMU to try to minimize this cost\n",
        "243": "function J this cost function is\n",
        "246": "sometimes also called the distortion\n",
        "248": "cost function of the distortion of a\n",
        "251": "k-means algorithm and just to provide a\n",
        "252": "little bit more detail here's the\n",
        "255": "k-means algorithm is a exactly the\n",
        "256": "algorithm as we have written it up on\n",
        "260": "the earlier slide and what this first\n",
        "263": "step of this algorithm is what this this\n",
        "264": "step of this algorithm is what this this\n",
        "268": "was the cluster assignment step where we\n",
        "271": "assign each point to the closest sync\n",
        "273": "cluster centroid and it's possible to\n",
        "275": "show mathematically that what the\n",
        "277": "cluster assignment step is doing is\n",
        "284": "exactly minimizing J with respect to the\n",
        "288": "variables C 1 C 2 and so on up to CM\n",
        "293": "while holding the cluster centroids new\n",
        "296": "1 up to MU K\n",
        "299": "fixed so what the cluster assignment\n",
        "301": "step does is you know it doesn't change\n",
        "302": "the cluster centroids but it's what it's\n",
        "305": "doing this is exactly picking the values\n",
        "309": "of c 1 c 2 up to CM that minimizes the\n",
        "311": "cost function or the distortion function\n",
        "314": "J and it's possible to prove that\n",
        "316": "mathematically but I won't i won't do so\n",
        "318": "here but it has a pretty intuitive\n",
        "319": "meaning of just you know well let's\n",
        "321": "assign each points to the cluster\n",
        "323": "centroid that is closest to it because\n",
        "324": "that's what minimizes the squared\n",
        "326": "distance between the points and the\n",
        "328": "corresponding cluster centroid and then\n",
        "330": "the other part of the second step of\n",
        "334": "k-means this second step over here the\n",
        "337": "second step was the move centroid step\n",
        "341": "and once again I won't prove it but it\n",
        "342": "can be shown mathematically that what\n",
        "345": "the roof centroid step does is it\n",
        "349": "chooses the values of MU that minimizes\n",
        "351": "J so it minimizes the cost function J\n",
        "355": "with respect to o wrt is my abbreviation\n",
        "357": "for worth respect to but minimizes J\n",
        "360": "with respect to the locations of the\n",
        "363": "cluster centroids mu 1 through mu naught\n",
        "366": "okay so what he means really is doing is\n",
        "367": "this taking the two sets of variables\n",
        "370": "and partitioning them into two halves\n",
        "373": "right here plus the see sets of\n",
        "374": "variables and you have the Meuse as a\n",
        "376": "variables and what it does is it first\n",
        "378": "minimizes J with respect to the variable\n",
        "380": "C and then minimize this gate with\n",
        "382": "respect to the variables view and then\n",
        "384": "it keeps on iterating\n",
        "387": "and so that's all that k-means does and\n",
        "390": "now that we understand k-means is trying\n",
        "392": "to minimize this cost function J we can\n",
        "395": "also use this to try to debug a learning\n",
        "397": "algorithm and just kind of make sure\n",
        "399": "that our implementation of k-means is\n",
        "402": "running correctly so we now understand\n",
        "404": "the k-means algorithm as trying to\n",
        "406": "optimize this cost function J which is\n",
        "407": "optimize this cost function J which is\n",
        "410": "also called the dispersion function we\n",
        "412": "can use that to debug k-means and help\n",
        "414": "me show that k-means is converging and\n",
        "416": "is running properly and in the next\n",
        "418": "video we'll also see how we can use this\n",
        "421": "to help k-means find better clusters and\n"
    },
    "OF8ocg5mgx0": {
        "0": " \n",
        "2": "in the previous videos we put together\n",
        "4": "almost all the pieces you need in order\n",
        "7": "to implement and trade in your network\n",
        "9": "there's just one loss idea I need to\n",
        "11": "share of you which is the idea of random\n",
        "13": "initialization when you're running an\n",
        "14": "initialization when you're running an\n",
        "15": "algorithm like gradient descent or also\n",
        "17": "the advanced optimization algorithms we\n",
        "19": "need to pick some initial value for the\n",
        "22": "parameters theta so for the advanced\n",
        "23": "optimization algorithm you know it\n",
        "25": "assumes that you will pass it some\n",
        "28": "initial value for the parameters theta\n",
        "31": "now let's consider gradient descent for\n",
        "33": "that you know we also need to initialize\n",
        "35": "theta plus to something and then we can\n",
        "37": "slowly take steps that go down the hill\n",
        "39": "using gradient descent to go down hill\n",
        "42": "to minimize the function J of theta so\n",
        "43": "what can we set the initial value of\n",
        "47": "theta to is it possible to set the\n",
        "50": "initial value of theta to the vector of\n",
        "53": "all zeros where is this worked okay when\n",
        "55": "we were using logistic regression\n",
        "57": "initializing all of your parameters to\n",
        "59": "zero actually does not work when you're\n",
        "61": "trading on your own network consider\n",
        "63": "training the following neural network\n",
        "65": "and let's say we initialize all the\n",
        "68": "parameters in the network to zero and if\n",
        "70": "you do that then well you what that\n",
        "73": "means is that at the initialization this\n",
        "75": "blue weight the clogging of blue is\n",
        "77": "going to equal to that way to serve that\n",
        "80": "both 0 and this weight the coloring in\n",
        "83": "in red is equal to that weight in red\n",
        "85": "and also this way to lump which I'm\n",
        "87": "coloring in a green is going to be equal\n",
        "90": "to the value of that weight and what\n",
        "91": "that means is that both of your hidden\n",
        "94": "units a 1 and a 2 are going to compute\n",
        "96": "be computing the same function of your\n",
        "99": "inputs and thus you end up with for\n",
        "101": "every one of your training examples what\n",
        "106": "you end up with a 2 1 equals a 2 2 and\n",
        "109": "moreover because a not Michelle this in\n",
        "111": "too much detail but because these\n",
        "113": "outgoing weights are the same you can\n",
        "115": "also show that the Delta values are also\n",
        "118": "going to be the same so concretely\n",
        "123": "with Delta 1 1 Delta 2 1 equals Delta 2\n",
        "127": "2 and if you work through the math\n",
        "129": "further what you can show is that the\n",
        "131": "partial derivatives with respect to your\n",
        "134": "parameters will satisfy the following\n",
        "139": "that the partial derivative of the cost\n",
        "142": "function with respect to I'm writing out\n",
        "144": "the derivatives respect to these two\n",
        "146": "blue weights in your network you find\n",
        "148": "that these two partial derivatives are\n",
        "152": "going to be equal to each other and so\n",
        "155": "what this means is that even after say 1\n",
        "157": "gradient descent update you're going to\n",
        "159": "update say this first blue weights with\n",
        "161": "your learning rate times this and you're\n",
        "163": "going to update the second blue rates\n",
        "165": "with some learning rate times this but\n",
        "167": "what this means is that even after one\n",
        "169": "gradient descent update those two blue\n",
        "172": "weights those two blue color parameters\n",
        "175": "will end up the same as each other so\n",
        "177": "there'll be some nonzero value now but\n",
        "180": "this value will equal to that value and\n",
        "182": "similarly even after one gradient\n",
        "185": "descent update this value would equal to\n",
        "187": "that value I'll be some nonzero values\n",
        "189": "just at the two red values of equal to\n",
        "191": "each other and similarly the two green\n",
        "192": "each other and similarly the two green\n",
        "194": "ways well they'll both change values but\n",
        "196": "they both end up the same value as each\n",
        "198": "other so after you update the parameters\n",
        "200": "corresponding to the inputs going to\n",
        "202": "each of the hidden to hidden units\n",
        "204": "identical that's just saying that the\n",
        "205": "two green weights is no stranger to\n",
        "208": "racism saying the two blue weights are\n",
        "210": "still the same and what that means is\n",
        "213": "that even after one iteration of safe\n",
        "215": "gradient descent you find that your two\n",
        "218": "hidden units are still computing exactly\n",
        "220": "the same function at the input so you\n",
        "223": "still have this a 1 2 equals a 2 2 and\n",
        "226": "so you're back to this case and as you\n",
        "228": "keep running gradient descent the three\n",
        "230": "ways the two blue weights will stay the\n",
        "231": "same as each other\n",
        "232": "the two red ways to stay in stand each\n",
        "235": "other in two green ways to stay the same\n",
        "237": "and what this means is that your neural\n",
        "239": "network really can't compute very\n",
        "241": "interesting functions right imagine that\n",
        "244": "you had um not only two hidden units but\n",
        "246": "imagine that you had many many hidden\n",
        "247": "imagine that you had many many hidden\n",
        "249": "units then what does the same is that\n",
        "251": "all of your hidden units are computing\n",
        "253": "the exact same feature all of the hidden\n",
        "255": "units are computing the exact same\n",
        "258": "function of the input and this is a\n",
        "260": "highly redundant representation because\n",
        "261": "that means that you find the logistic\n",
        "263": "regression you they don't really get to\n",
        "265": "see only one feature because all of\n",
        "267": "these are the same and this prevents\n",
        "268": "your neural network from they learning\n",
        "272": "something interesting in order to get\n",
        "274": "around this problem the way we\n",
        "275": "initialize the parameters of a neural\n",
        "277": "network therefore is with random\n",
        "283": "initialization concretely the problem we\n",
        "285": "saw on the previous slide is sometimes\n",
        "287": "called the problem of symmetric way\n",
        "288": "instead of the way it's all being the\n",
        "292": "same and so this random initialization\n",
        "295": "is how we perform symmetry breaking so\n",
        "298": "what we do is we initialize each value\n",
        "300": "of theta to a random number between\n",
        "302": "minus epsilon and Epsilon so this is a\n",
        "304": "notation to me numbers between minus\n",
        "305": "epsilon and plus Epsilon\n",
        "308": "so my weights or my parameters are all\n",
        "310": "gonna be randomly initialized between\n",
        "312": "minus epsilon and plus Epsilon the way I\n",
        "314": "write code to do this in octave is I'm I\n",
        "316": "said you know theta want to be equal to\n",
        "321": "this so this R + 10 by 11 that's how you\n",
        "325": "compute a random 10 by 11 dimensional\n",
        "329": "matrix and all the values are between 0\n",
        "330": "&amp; 1\n",
        "332": "so these are going to be roll numbers\n",
        "333": "that take on any continuous values\n",
        "336": "between 0 and 1 and so if you take a\n",
        "339": "number between 0 &amp; 1 x 2 x in the\n",
        "341": "epsilon then minus in the epsilon that\n",
        "342": "you end up with a number that's between\n",
        "344": "minus epsilon and plus Epsilon\n",
        "347": "and incidentally this epsilon here has\n",
        "349": "nothing to do with the epsilon that we\n",
        "351": "were using when we were doing gradient\n",
        "353": "checking so when we're doing the America\n",
        "355": "gradient checking there we were adding\n",
        "357": "some values of epsilon to theta this is\n",
        "359": "you know unrelated value of epsilon\n",
        "361": "which is 1/2 noting it knit 2 epsilon\n",
        "363": "just to distinguish it from the value of\n",
        "365": "epsilon we were using in gradient\n",
        "367": "checking and similarly if you want to\n",
        "370": "initialize theta 2 to a random one by\n",
        "372": "the 11 matrix you can do so using this\n",
        "375": "piece of code here\n",
        "378": "so to summarize to trade in your network\n",
        "380": "where you should do is randomly\n",
        "382": "initialize the ways to you know small\n",
        "384": "values close to zero between minus\n",
        "386": "epsilon and plus Epsilon say and then\n",
        "389": "implement back-propagation do gradient\n",
        "391": "checking and use you think gradient\n",
        "393": "descent or one of the advanced\n",
        "395": "optimization algorithms to try to\n",
        "397": "minimize J of theta as a function of the\n",
        "399": "parameters data starting from this\n",
        "401": "randomly chosen initial value for the\n",
        "404": "parameters and by doing symmetry\n",
        "406": "breaking which is this process hopefully\n",
        "407": "gradient sent or the advanced\n",
        "409": "optimization algorithms we'll be able to\n"
    },
    "OS7KXu0447I": {
        "0": " \n",
        "2": "you now know about linear regression and\n",
        "4": "gradient descent the plan from here on\n",
        "6": "out is to tell you about a couple\n",
        "8": "important extensions of these ideas\n",
        "11": "concretely here they are first it turns\n",
        "14": "out that in order to solve this\n",
        "16": "minimization problem turns out there's\n",
        "18": "an algorithm for solving for theta 0 and\n",
        "21": "theta 1 exactly without needing an\n",
        "23": "iterative algorithm without needing this\n",
        "25": "algorithm like gradient descent that we\n",
        "27": "had to iterate you know multiple times\n",
        "30": "over so it turns out there are\n",
        "31": "advantages and disadvantages of this\n",
        "33": "algorithm that lets you just solve for\n",
        "35": "theta 0 and theta 1 so basically just in\n",
        "36": "theta 0 and theta 1 so basically just in\n",
        "38": "one shot one advantage is that there's\n",
        "40": "no longer a learning rate alpha that you\n",
        "42": "need to worry about and set and so it\n",
        "44": "can be much faster for some problems\n",
        "47": "we'll talk about its advantages and\n",
        "50": "disadvantages later second walls talk\n",
        "51": "about algorithms for learning with a\n",
        "54": "larger number of features so so far\n",
        "56": "we've been learning with just one\n",
        "59": "feature the size of the hulls and using\n",
        "60": "that to predict the price so we're\n",
        "62": "trying to take X and use that to predict\n",
        "65": "Y but for other learning problems we may\n",
        "67": "have a larger number of features so for\n",
        "69": "example let's say that you know not only\n",
        "71": "the size but also the number of bedrooms\n",
        "74": "number of floors and the age of home of\n",
        "76": "these houses and when you want to use\n",
        "78": "that to predict the price of the houses\n",
        "81": "in that case maybe we'll call these\n",
        "86": "features X 1 X 2 X 3 and X 4 so now we\n",
        "88": "have you know four features I want to\n",
        "90": "use these four features to predict Y the\n",
        "93": "price of the house it turns out with all\n",
        "95": "of these features and inform them in\n",
        "97": "this case it turns out that with\n",
        "99": "multiple features becomes it becomes\n",
        "101": "harder to plot or to visualize the data\n",
        "104": "so for example you'll be try to plot\n",
        "107": "this type of data set maybe we'll have\n",
        "110": "the vertical axis be the price and maybe\n",
        "113": "we can have one axis here and another\n",
        "116": "one here with this axis is the size of\n",
        "118": "the house and that axis is the number of\n",
        "121": "bedrooms you know but this is just\n",
        "123": "plotting right my first two features\n",
        "126": "size the number of bedrooms and when we\n",
        "127": "have these additional features and there\n",
        "129": "I just don't know how to plot all of\n",
        "130": "these data right because\n",
        "132": "need like a 4-dimensional or a five\n",
        "133": "dimensional figure they don't really\n",
        "135": "know how to plot you know something more\n",
        "136": "than like a three-dimensional figure\n",
        "139": "like like what I have over here also as\n",
        "141": "you can tell the notation starts to get\n",
        "143": "a little more complicated right so\n",
        "145": "rather than just having Excel features\n",
        "149": "we now have x1 through x4 and we're\n",
        "152": "using these subscripts to denote my four\n",
        "155": "different features it turns out the best\n",
        "157": "notation to keep all of this straight\n",
        "159": "and to understand what's going on with\n",
        "161": "the data even when we don't quite know\n",
        "162": "how to plot it it turns out that the\n",
        "165": "best notation is the notation of linear\n",
        "168": "algebra linear algebra gives us a\n",
        "170": "notation and a set of things or a set of\n",
        "172": "operations that we can do with matrices\n",
        "174": "and vectors for example here's a matrix\n",
        "175": "and vectors for example here's a matrix\n",
        "178": "where the columns of this matrix are the\n",
        "181": "first column is the sizes of the four\n",
        "183": "houses the second column was the number\n",
        "185": "of bedrooms that's the number of floors\n",
        "187": "and that was the age of the home and so\n",
        "190": "a matrix is this block of numbers that\n",
        "192": "lets me take all of my data all of my\n",
        "195": "X's all of my features and organize them\n",
        "197": "efficiently into sort of one big block\n",
        "201": "of numbers like that and here is what we\n",
        "203": "call a vector in any algebra where the\n",
        "206": "phone numbers here are the prices of the\n",
        "208": "four houses that we saw on their\n",
        "211": "previous slide so in the next set of\n",
        "214": "videos what I'm going to do is do a\n",
        "216": "quick review of linear algebra if you\n",
        "218": "haven't seen matrices and vectors before\n",
        "220": "so if all of this everything on the\n",
        "222": "slide is brand new to you or if you've\n",
        "224": "seen linear algebra before but it's been\n",
        "226": "a while so you aren't completely\n",
        "228": "familiar with it anymore then please\n",
        "229": "watch the next set of videos and I'll\n",
        "231": "quickly review the linear algebra you\n",
        "234": "need in order to implement and use the\n",
        "235": "more powerful versions of linear\n",
        "238": "regression it turns out linear algebra\n",
        "240": "isn't just useful for linear regression\n",
        "243": "models but these ideas are matrices and\n",
        "245": "vectors will be useful for helping us to\n",
        "247": "implement and actually get\n",
        "249": "computationally efficient\n",
        "252": "implementations for many later machine\n",
        "254": "learning models as well and as you can\n",
        "256": "tell these sorts of matrices and vectors\n",
        "258": "will give us an efficient way to start\n",
        "261": "to organize large amounts of data when\n",
        "264": "of logic training sites so in case in\n",
        "266": "case you're not familiar with linear\n",
        "269": "algebra or in case linear algebra seems\n",
        "271": "like a complicated or scary concept with\n",
        "273": "moves never seen it before don't worry\n",
        "275": "about it it turns out in order to\n",
        "277": "implement machine learning algorithms we\n",
        "279": "need only the very very basics of linear\n",
        "281": "algebra and you'll be able to very\n",
        "283": "quickly pick up everything you need to\n",
        "287": "know in the next few videos concretely\n",
        "291": "to decide if you should watch the next\n",
        "293": "set of videos here are the topics I'm\n",
        "295": "going to cover talk about what are\n",
        "297": "matrices and vectors and talk about how\n",
        "299": "to add subtract multiply matrices and\n",
        "301": "vectors and talk about the ideas of\n",
        "304": "matrix inverses and transposes and so if\n",
        "306": "you're not sure you should watch the\n",
        "308": "next set of videos take a look at these\n",
        "310": "two things so if you think you know how\n",
        "312": "to compute this quantity that's a matrix\n",
        "315": "transpose x in other matrix you think\n",
        "316": "you know if you've seen this stuff\n",
        "318": "before if you know how to compute the\n",
        "320": "inverse of matrix times the vector minus\n",
        "322": "the number of times another vector if\n",
        "324": "these two things look completely\n",
        "327": "familiar to you then you can safely skip\n",
        "329": "the optional set of videos on linear\n",
        "332": "algebra but if these concepts if you\n",
        "335": "have just slightly uncertain what you\n",
        "336": "know these blocks of numbers what these\n",
        "338": "matrices and numbers mean\n",
        "340": "then please take a look at the next set\n",
        "343": "of videos and it wickley teach you what\n",
        "345": "you need to know about linear algebra in\n",
        "347": "order to program machine learning\n",
        "349": "algorithms and deal with large amounts\n"
    },
    "P6EtCVrvYPU": {
        "0": " \n",
        "1": "in the last few videos we talked about\n",
        "3": "how to do forward propagation and back\n",
        "6": "propagation in a neural network in order\n",
        "9": "to compute derivatives but back prop as\n",
        "12": "an algorithm has a lot of details and\n",
        "13": "you know can be a little bit tricky to\n",
        "17": "implement and one unfortunate property\n",
        "19": "is that there are many ways that have\n",
        "22": "subtle bugs in backdrop so that if you\n",
        "23": "run it with gradient descent or some\n",
        "25": "other optimization algorithm it could\n",
        "27": "actually look like this working and you\n",
        "29": "know your cost function J of theta may\n",
        "31": "end up decreasing on every iteration of\n",
        "34": "gradient descent but this could protrude\n",
        "36": "even though there might be some bug in\n",
        "38": "your implementation of back prop so that\n",
        "40": "looks like J of theta is decreasing but\n",
        "43": "you might just wind up with a neural\n",
        "45": "network that has a higher level of error\n",
        "46": "than you would with a bug-free\n",
        "48": "implementation and you might just not\n",
        "50": "know that there was this subtle bug\n",
        "52": "that's giving you worse performance so\n",
        "54": "what we do about this there's an idea\n",
        "57": "called gradient checking that eliminates\n",
        "59": "almost all of these problems so today\n",
        "61": "every time I implement a back\n",
        "63": "propagation or a similar gradient just\n",
        "64": "an algorithm on the neural network or\n",
        "67": "any other reasonably complex model I\n",
        "69": "always implement gradient checking and\n",
        "71": "if you do this and will help you make\n",
        "73": "sure and so if gain high confidence that\n",
        "75": "your implementation afford prop and back\n",
        "77": "prop or whatever is a hundred percent\n",
        "79": "correct and in what I've seen this\n",
        "81": "pretty much eliminates all the problems\n",
        "83": "associated with sort of a buggy\n",
        "87": "implementation as a background and in\n",
        "88": "the previous videos I have asked you to\n",
        "91": "take on faith that the formulas I gave\n",
        "93": "for computing the deltas and the DS and\n",
        "95": "so on I asked you to take on faith that\n",
        "98": "those actually do compute the gradients\n",
        "100": "of the cost function but once you\n",
        "101": "implement the American gradient checking\n",
        "103": "which is the topic of this video you'll\n",
        "105": "be able to sort of verify for yourself\n",
        "107": "that the code you're writing does indeed\n",
        "110": "it is indeed computing the derivative of\n",
        "113": "the cost function J so here's the idea\n",
        "115": "consider the following example suppose I\n",
        "118": "have a function J of theta and I have\n",
        "122": "some value theta and for this example\n",
        "124": "I'm going to assume that theta is just a\n",
        "126": "and that's me I want to estimate the\n",
        "128": "derivative of this function at this\n",
        "130": "point and so the derivative is equal to\n",
        "133": "the slope of that sort of tangent line\n",
        "136": "here's how I'm going to numerically\n",
        "138": "approximate the derivative or rather\n",
        "139": "here's a procedure about numerically\n",
        "141": "approximating the derivative I'm going\n",
        "142": "approximating the derivative I'm going\n",
        "145": "to compute theta plus Epsilon so value a\n",
        "146": "little bit to the right and I'm going to\n",
        "149": "compute theta minus epsilon and I'm\n",
        "154": " \n",
        "162": " \n",
        "164": "and I'm going to connect these two\n",
        "166": "points by a straight line and I'm going\n",
        "168": "to use the slope of that that little red\n",
        "170": "line as my approximation to the\n",
        "173": "derivative which is the true derivative\n",
        "174": "is the slope of that blue line over\n",
        "176": "there and so you know seems like it'd be\n",
        "177": "a pretty good approximation\n",
        "179": "mathematically the slope of this red\n",
        "183": "line is this vertical height divided by\n",
        "186": "this horizontal width so this point on\n",
        "190": "top is J of theta plus Epsilon\n",
        "192": "this point here is J of theta minus\n",
        "195": "Epsilon so there's vertical difference\n",
        "197": "is J of theta plus epsilon minus J of\n",
        "200": "theta minus epsilon and this horizontal\n",
        "202": "distance is just 2 Epsilon\n",
        "206": "so my approximation is going to be that\n",
        "209": "the derivative with respect to theta of\n",
        "213": "J of theta at this value of theta that\n",
        "216": "that's approximately J of theta plus\n",
        "218": "epsilon minus J of theta minus epsilon\n",
        "223": "over 2 Epsilon usually I'll use a pretty\n",
        "225": "small value for epsilon instead epsilon\n",
        "227": "to be maybe on the order of 10 to the\n",
        "230": "minus 4 there's usually a large range of\n",
        "232": "different values for epsilon that work\n",
        "235": "just fine and in fact if you let epsilon\n",
        "237": "become really small then mathematically\n",
        "240": "this term here actually mathematically\n",
        "242": "you know becomes the derivative becomes\n",
        "244": "exactly the slope of the function at\n",
        "245": "this point it's just that we don't want\n",
        "247": "to use epsilon that's too too small\n",
        "248": "because then you might run into\n",
        "250": "numerical problems so you know how do\n",
        "252": "you use epsilon around 10 to the minus\n",
        "255": "fourth day and by the way some of you\n",
        "257": "may have seen an alternative formula for\n",
        "259": "estimating the derivative which is this\n",
        "260": "formula\n",
        "262": "this one on the right is called a\n",
        "264": "one-sided difference whereas the phone\n",
        "266": "on the left that's called the two-sided\n",
        "268": "difference the two-sided difference\n",
        "269": "gives us a slightly more accurate\n",
        "271": "estimate so I usually use that rather\n",
        "275": "than this one-sided difference estimate\n",
        "277": "so concretely when you implement in\n",
        "279": "octave as you implement the following\n",
        "281": "the implement code to compute gradapprox\n",
        "284": "which is going to be our approximation\n",
        "286": "to the derivative that's just near this\n",
        "288": "formula J of theta plus epsilon minus GF\n",
        "290": "theta minus epsilon divided by two times\n",
        "292": "epsilon and this will give you a\n",
        "295": "numerical estimate of the gradient at\n",
        "297": "that point and in this example it seems\n",
        "301": " \n",
        "304": "now on the previous slide we considered\n",
        "307": "the case of when theta was a real number\n",
        "309": "now let's look at a more general case of\n",
        "312": "when theta is a vector parameter so\n",
        "314": "let's say theta is an RN and it might be\n",
        "316": "an unreal Varon of the parameters about\n",
        "319": "your network so theta is a vector that\n",
        "321": "you know has n elements beta 1 up to\n",
        "326": "theta n we can then use a similar idea\n",
        "328": "to approximate all the partial\n",
        "331": "derivative terms concretely the partial\n",
        "333": "derivative of a cost function with\n",
        "335": "respect to the first parameter of theta\n",
        "339": "1 that can be obtained by taking J and\n",
        "341": "increasing theta 1 so you have J of\n",
        "344": "theta 1 plus epsilon and so on minus J\n",
        "346": "of the stay the 1 minus epsilon divided\n",
        "349": "by 2 epsilon the partial derivative\n",
        "351": "respect to the second parameter theta 2\n",
        "353": "this again this thing except that you\n",
        "356": "take j of kenya increasing theta 2 by\n",
        "358": "epsilon and here decreasing theta 2 by\n",
        "360": "epsilon and so on down to the derivative\n",
        "362": "respect to theta n would be if you\n",
        "365": "increase and decrease theta n by epsilon\n",
        "371": "over there so these equations give you a\n",
        "374": "way to numerically approximate the\n",
        "377": "partial derivative of J with respect to\n",
        "383": " \n",
        "385": "concretely what you implement is\n",
        "388": "therefore the following we implement the\n",
        "390": "following an octave to numerically\n",
        "393": "compute the derivatives we say for I\n",
        "395": "equals 1 through n where n is the\n",
        "397": "dimension of our parameter vector theta\n",
        "399": "and I usually do this with the unrolled\n",
        "401": "version of the parameters so you know\n",
        "403": "theta is just a long list of all of my\n",
        "406": "parameters in my neural network say I'm\n",
        "408": "gonna set theta plus equals theta then\n",
        "411": "increase theta plus the I've element by\n",
        "412": "epsilon and so you know this is\n",
        "413": "epsilon and so you know this is\n",
        "415": "basically theta plus is equal to theta\n",
        "417": "except for theta plus I which is now\n",
        "419": "being fermented by epsilon so so if\n",
        "421": "theta plus is equal to write theta 1\n",
        "424": "theta 2 and so on and then theta I as\n",
        "426": "epsilon added to it and then I go down\n",
        "428": "to theta n so this is one theta plus is\n",
        "431": "and similarly these two these two lines\n",
        "434": "set theta minus 2 something similar\n",
        "436": "except that this instead of theta I plus\n",
        "438": "epsilon this now becomes theta I minus\n",
        "442": "epsilon and then finally you implement\n",
        "446": "this guy the prompts I and this would\n",
        "448": "give you your approximation to the\n",
        "451": "partial derivative respect to theta I of\n",
        "457": "J of theta and the way we use this in a\n",
        "458": "neural network implementation is we\n",
        "461": "would implement this implement this\n",
        "464": "volume to compute the partial derivative\n",
        "466": "of the cost function with respective\n",
        "469": "every parameter in the network and we\n",
        "472": "can then take the gradient that we got\n",
        "475": "from that problem so D Veck was the\n",
        "477": "derivatives we got from back problem\n",
        "480": "right so back prop back propagation was\n",
        "482": "a relatively efficient way to compute\n",
        "483": "the derivatives or the partial\n",
        "485": "derivatives of our cost function with\n",
        "487": "respect to all of our parameters and\n",
        "490": "what I usually do is then take my\n",
        "492": "numerically computed derivative that is\n",
        "494": "this rather problems that we've just had\n",
        "495": "from up here\n",
        "498": "and make sure that that is equal also\n",
        "501": "approximately equal up to you know small\n",
        "502": "values in numerical roundel that is\n",
        "505": "pretty close to the defect that I got\n",
        "507": "the back problem and if these two ways\n",
        "509": "of computing the derivative give me the\n",
        "511": "same answer or at least give me very\n",
        "513": "soon as you know it's up to a few\n",
        "515": "decimal places then I'm much more\n",
        "518": "confident that my implementation of back\n",
        "521": "prop is correct and when I plug these\n",
        "523": "defect vectors into gradient descents\n",
        "524": "for all some advanced optimization\n",
        "527": "algorithm again even then be much more\n",
        "529": "confident that I'm computing the\n",
        "531": "derivatives correctly and therefore that\n",
        "532": "hopefully you might call to run\n",
        "535": "correctly and do a good job optimizing J\n",
        "538": "of theta finally you want to put\n",
        "540": "everything together and tell you how to\n",
        "542": "implement this numerical gradient\n",
        "544": "checking here's what I usually do\n",
        "546": "first thing I do is implement back\n",
        "548": "propagation to compute the effects of\n",
        "550": "this a procedure we talked about in the\n",
        "552": "earlier video to compute D vac which may\n",
        "553": "be our unrolled version of these\n",
        "557": "matrices then what I do is implement a\n",
        "559": "numerical gradient checking to compute\n",
        "561": "gradapprox so this was what I described\n",
        "563": "earlier in this video and in the\n",
        "565": "previous slide then should make sure\n",
        "568": "that defect and granite products give\n",
        "570": "similar values you know let's say up to\n",
        "573": "a few decimal places and finally and\n",
        "576": "just important step before you start to\n",
        "578": "use your code for learning for seriously\n",
        "579": "training your network is important to\n",
        "582": "turn off gradient checking and to no\n",
        "584": "longer compute this dratted frosting\n",
        "587": "using the numerical derivative formulas\n",
        "589": "that we talked about earlier this video\n",
        "593": "and the reason for that is the numerical\n",
        "594": "gradient checking code the stuff we\n",
        "596": "talked about in this video that's a very\n",
        "598": "computationally expensive that's a very\n",
        "600": "slow way to try to approximate the\n",
        "603": "derivative whereas in contrast the back\n",
        "604": "propagation algorithm that we talked\n",
        "606": "about earlier that is the the thing we\n",
        "608": "talked about earlier for computing your\n",
        "609": "d1 d2 d3\n",
        "612": "for defect backprop is a much more\n",
        "614": "computationally efficient way of\n",
        "617": "computing the derivatives so once you've\n",
        "619": "verified that your implementation of\n",
        "621": "backpropagation is correct you should\n",
        "623": "turn off gradient checking and just stop\n",
        "626": "using that so just to reiterate should\n",
        "627": "be sure to disable your gradient\n",
        "629": "checking code before running your\n",
        "631": "algorithm for many iterations of\n",
        "633": "gradient descent or for many iterations\n",
        "635": "of the advanced optimization algorithms\n",
        "637": "in order to train your classifier\n",
        "640": "concretely if you were to run the\n",
        "641": "American gradient checking on every\n",
        "643": "single iteration of gradient descent or\n",
        "645": "if it were in the inner loop of your\n",
        "647": "cost function then your code would be\n",
        "649": "very slow because of the numerical\n",
        "651": "gradient checking code is much slower\n",
        "654": "than the back propagation algorithm then\n",
        "656": "the back propagation method where you\n",
        "658": "remember we were computing Delta 4 Delta\n",
        "661": "3 Delta 2 and so on that was the back\n",
        "663": "propagation algorithm that is a much\n",
        "665": "faster way to compute derivatives than\n",
        "667": "gradient checking so when you're when\n",
        "669": "you're ready once you verify the\n",
        "670": "implementation of backpropagation is\n",
        "673": "correct make sure you turn off or you\n",
        "674": "disable your gradient checking code\n",
        "676": "while you train your algorithm or else\n",
        "679": "and kokumo very slowly\n",
        "682": "so that's how you take gradients\n",
        "683": "numerically and that's how you can\n",
        "685": "verify that your implementation of\n",
        "687": "backpropagation is correct whenever I\n",
        "689": "influence back propagation or similar\n",
        "691": "gradient descent algorithm for a\n",
        "693": "complicated model I always use gradient\n",
        "694": "checking and this really helps me make\n"
    },
    "PPLop4L2eGk": {
        "0": "What this machine learning?\n",
        "2": "in this video, we will try to define what it is and also try to give you a sense of when you want to use machine learning\n",
        "8": "Even among machine learning practitioners there isn't a well accepted definition of what is and what isn't machine learning?\n",
        "15": "But let me show you a couple of examples of the ways that people have tried to Define it\n",
        "20": "Here's the definition of what is machine learning that is due to author Samuel?\n",
        "24": "He defined machine learning as the field of study that gives computers the ability [to] learn without being explicitly programmed\n",
        "32": "Samuels claim to fame was that back in the 1950s\n",
        "35": "He wrote a checkers playing program and the amazing thing about this checkers playing program\n",
        "41": "Was that [officer] samuel [himself] wasn't a very good checkers player?\n",
        "45": "but what he did was he had the program play tens of thousands of games against itself and\n",
        "49": "By watching what sorts of board [positions] tended to lead to wins and what sort of board positions?\n",
        "55": "Tended to Veto losses the checkers playing program learned over time\n",
        "59": "What are good board positions and what a bad board positions and eventually learned to play checkers better than arthur Samuel himself was able [to]\n",
        "67": "This was a remarkable result also samuel [himself] turned out not to be a very good checkers player\n",
        "73": "But because the computer has the patience to play tens of thousands of games against itself\n",
        "79": "No, human has the patience to play that many games\n",
        "82": "By doing this the computer was able to get so much checkers playing experience that it eventually\n",
        "88": "Became a better checkers player than offered Samuel himself\n",
        "92": "This is [somewhat] informal definition and an older one here's a slightly more recent definition by Tom Mitchell\n",
        "98": "Who is a friend out in Committee Mellon?\n",
        "101": "So Tom defines machine learning by saying that a well-posed learning problem is defined as follows\n",
        "107": "He says a computer program is said to learn from experience II with respect to some [tosti] and some performance measure key\n",
        "115": "Evidence Performance on t as measured by P\n",
        "118": "Improves of experience II I actually think he came up with this definition just to make it wrong\n",
        "124": "For the checkers playing examples the experience II would be the experience of having the program play tens of thousands of games against itself\n",
        "132": "The task t would be the task of playing checkers and the performance measure p would be the probability\n",
        "139": "That wins the next game of checkers against some new opponent\n",
        "143": "Throughout these [videos] besides me trying to teach you stuff\n",
        "146": "I'll occasionally ask you a question to make sure you understand the content. Here's one on\n",
        "153": "top of the definition of machine learning by Tom mcCullar\n",
        "157": "Let's say your email program watches which emails you do or do not [value] spam\n",
        "162": "so in an email client like [this] you might click [this] spam button to report some email as spam but not other emails and\n",
        "170": "Based on which emails you marcus them say your email program\n",
        "174": "learns better how to filter a spam email\n",
        "178": "what is the task of [t] in the setting in a few seconds the video will pause and when it does so\n",
        "184": "you can use your mouse to select one [of] these four radio buttons to\n",
        "188": "Let to let me know which of these four you think is the right answer to this question\n",
        "195": "So hopefully you got that this is the right answer classifying emails is a toss t in fact\n",
        "201": "this definition defines the task t a performance measure p and the some experience [E] and\n",
        "209": "so watching you label emails as spam or non-sPam this would be the experience E and\n",
        "216": "the [fraction] of emails correctly classified that might be our performance measure p and\n",
        "222": "So our task performance on the [task] [about] systems performance on the task t on\n",
        "229": "the performance measure p will improve after the experience e\n",
        "234": "In this course I hope to teach you about various different types of learning algorithms\n",
        "239": "there are several different types of learning algorithms the main two types are what we call supervised learning and\n",
        "245": "unsupervised learning\n",
        "246": "I'll define what these terms mean more in the next couple videos\n",
        "250": "But it turns out that in supervised learning the idea is we're going to teach the how to do something\n",
        "257": "Whereas in unsupervised learning\n",
        "259": "We're going to let it learn by itself don't worry if these two terms. Don't make sense yet in the next two videos\n",
        "265": "I'm going to say exactly what these two types of learning on\n",
        "269": "you will also hear all the buzz terms such as\n",
        "272": "reinforcement learning and\n",
        "274": "recommender systems\n",
        "276": "These are other types of machine learning algorithms that we'll talk about later\n",
        "280": "but the two most used types of learning algorithms are probably supervised learning and\n",
        "285": "Unsupervised learning and I'll define them in the next two videos and we'll spend most as far as talking about these two types of learning\n",
        "291": "algorithms\n",
        "292": "It turns out one of the other things we'll spend a lot of time on in this class is practical advice for applying learning algorithms\n",
        "300": "This is something [that] I feel pretty strongly about and exactly something that I don't know of any other [university] teachers\n",
        "306": "Teaching about learning algorithms. It's like giving you a set of tools and\n",
        "310": "Equally important or more important than giving you the tools is to teach you how to apply these tools\n",
        "316": "I like to make an analogy to learning to become a carpenter\n",
        "320": "Imagine that someone is teaching you how to be a carpenter, and they say here's a hammer\n",
        "325": "Here's a drive screwdriver is a saw good luck well\n",
        "329": "That's no good right you have all these tools, but the more important thing is to learn how to use these tools properly\n",
        "336": "There's a huge difference\n",
        "337": "We can keep between people that know how to use these machine learning algorithms versus people that don't know how to use these tools well\n",
        "344": "here in Silicon Valley when I live\n",
        "346": "When I go visit different companies even at the top silicon Valley companies very often I see people trying to apply Machine learning\n",
        "354": "Algorithms to some problem and sometimes there have been going at it for six months\n",
        "358": "But sometimes when I look at what they're [doing]\n",
        "360": "I say you know, I could have told them like gee I could have told you six months ago\n",
        "364": "or that you should be taking a learning algorithm, and applying it in like the slightly modified way and\n",
        "372": "Your chance of success would have been much higher um\n",
        "376": "So what we're going to do in this class is [actually] spend a lot of time talking about how if you're actually trying to develop\n",
        "381": "a machine learning system\n",
        "383": "How to make those best practices types decisions about the way in which you build your system, so that when you apply learning algorithm\n",
        "390": "You're less likely to end up one of those people who end up pursuing somehow for six months that you know\n",
        "395": "Someone else could have figured out [just] wasn't gonna work at all and it's a waste of time for six months\n",
        "400": "So I'm actually spend a lot of time teaching you\n",
        "403": "Those sorts of best practices in Machine learning and AI and how to get [the] stuff to work\n",
        "408": "And how we do it or how the best people do it in Silicon Valley and around the world\n",
        "412": "I hope to make you one of the best people and knowing how to design and build serious machine learning and Ai systems\n",
        "419": "So that's machine learning and these are the main [topics]. I hope to teach in the next video. I'm going to Define\n",
        "426": "What is supervised learning and after that what is unsupervised learning and also start to talk about when you will use each of them?\n"
    },
    "PpH_hv55GNQ": {
        "0": " \n",
        "2": "in this video I'd like to talk about how\n",
        "4": "to initialize k-means and more\n",
        "6": "importantly this will lead into a\n",
        "9": "discussion of how to make k-means avoid\n",
        "11": "local optima as well here's the k-means\n",
        "14": "clustering algorithm that we talked\n",
        "16": "about earlier one step that we never\n",
        "18": "really talked much about was this step\n",
        "20": "of how you randomly initialize the\n",
        "22": "cluster centroids there are a few\n",
        "24": "different ways that one could imagine\n",
        "26": "using to randomly initialize the cluster\n",
        "28": "centroids but it turns out that there's\n",
        "31": "one method that's much more recommended\n",
        "33": "than most of the other options one might\n",
        "35": "think about so let me tell you about\n",
        "37": "that option so this is what often seems\n",
        "40": "the world's best here's how I usually\n",
        "43": "initialize my cluster centroids when\n",
        "45": "running k-means you should have the\n",
        "47": "number of cluster centroids K is set to\n",
        "49": "be less than the number of training\n",
        "51": "examples M it would be really weird to\n",
        "53": "run k-means with a number of cluster\n",
        "54": "centroids this you know equal or greater\n",
        "56": "than the number of examples you have\n",
        "59": "right so the way I usually initialize\n",
        "62": "k-means is i would randomly pick kt\n",
        "66": "training examples so and what I do is\n",
        "68": "then set me one up to MU k equal to\n",
        "71": "these K examples let me show you a\n",
        "74": "concrete example let's say that K is\n",
        "77": "equal to 2 and so on this example right\n",
        "81": "let's say I want to find 2 clusters so\n",
        "83": "what I'm going to do in order to\n",
        "85": "initialize my cluster centroids is I'm\n",
        "87": "going to randomly pick a couple of\n",
        "89": "examples and let's say I pick this one\n",
        "92": "and I pick that one and the way I'm\n",
        "94": "going to initialize my cluster centroids\n",
        "96": "is I'm going to just going to initialize\n",
        "98": "my cluster centroids to be right on top\n",
        "100": "of those examples so that's my first\n",
        "102": "cluster centroid and that's my second\n",
        "104": "cluster centroid and that's one two\n",
        "108": "random initialization of k-means the one\n",
        "110": "I drew looks like a particularly good\n",
        "112": "one sometimes I might get less lucky and\n",
        "114": "maybe I'll end up picking that as my\n",
        "117": "first randomly chosen example and that\n",
        "118": "as my sec\n",
        "120": "one and here I'm picking two examples\n",
        "123": "because K equals two so I randomly pick\n",
        "125": "two training examples and if I chose\n",
        "128": "those two then I'll end up with maybe\n",
        "129": "this is my first cluster centroid and\n",
        "132": "that as my second initial location of\n",
        "134": "the cluster centroid so that's how you\n",
        "136": "can randomly initialize the cluster\n",
        "139": "centroids and so at initialization your\n",
        "142": "first cluster centroid v1 will be equal\n",
        "146": "to X I for some random e value of I and\n",
        "149": "mu 2 will be equal to XJ for some\n",
        "151": "different randomly chosen value of J and\n",
        "154": "so on if you have more clusters in work\n",
        "157": "versus employees as sort of a side\n",
        "159": "comment I should say that in the earlier\n",
        "162": "video where I first illustrated k-means\n",
        "165": "with an it with an animation in that set\n",
        "166": "of slides only for the purpose of\n",
        "168": "illustration I actually use a different\n",
        "171": "method of initialization for my cluster\n",
        "173": "centroids but the method described on\n",
        "175": "this slide this is really edia\n",
        "177": "recommended way that the way that you\n",
        "178": "should probably use when you implement\n",
        "182": "k-means so as i suggested perhaps by\n",
        "185": "these two illustrations on the right you\n",
        "187": "might really guess that k-means can end\n",
        "189": "up converging to different solutions\n",
        "192": "depending on exactly how the clusters\n",
        "194": "were initialized and so depending on the\n",
        "197": "random initialization k-means can end up\n",
        "200": "at different solutions and in particular\n",
        "202": "k-means can actually end up at local\n",
        "204": "optima if you're given the data set like\n",
        "206": "this one looks like you know there are\n",
        "209": "three clusters and so if you run k-means\n",
        "212": "and if it ends up at a good local firm\n",
        "214": "this might be really the global optimum\n",
        "215": "you might end up with that clustering\n",
        "219": "but if you have a particularly unlucky\n",
        "222": "random initialization k-means can also\n",
        "225": "get stuck at different local Optima so\n",
        "227": "in this example on the left you know it\n",
        "229": "looks like the blue cluster has captured\n",
        "230": "a lot of points on the left\n",
        "232": "and then I ran down the green clusters\n",
        "234": "each is capturing only a relatively\n",
        "235": "small number of points and so this\n",
        "238": "corresponds to a bad local optima\n",
        "240": "because it has basically taken these two\n",
        "242": "clusters and merge them into one and\n",
        "245": "furthermore what it has split the second\n",
        "249": "cluster into two separate sub clusters\n",
        "251": "like so and it has also taken the second\n",
        "254": "cluster and split it into two separate\n",
        "257": "sub clusters like so and so both of\n",
        "259": "these examples on the lower right\n",
        "262": "correspond to different local optima of\n",
        "264": "k-means and in fact in this example here\n",
        "266": "the cluster the vague cluster has\n",
        "269": "captured only a single unlabeled example\n",
        "271": "and the local optima by the way refers\n",
        "273": "to local optima of this distortion\n",
        "277": "function J and what these solutions on\n",
        "280": "the lower left what these local optima\n",
        "282": "correspond to is really solutions where\n",
        "284": "Keeney's has gotten stuck to the local\n",
        "286": "optima and is not doing a very good job\n",
        "290": "minimizing this distortion function J so\n",
        "292": "if you're worried about k-means getting\n",
        "294": "stuck in local optima if you want to\n",
        "296": "increase the odds of k-means finding the\n",
        "298": "best possible clustering like that shown\n",
        "301": "on top here what we can do is try\n",
        "303": "multiple random initializations so\n",
        "305": "instead of just initializing k means\n",
        "307": "once and hoping that that works what we\n",
        "309": "can do is initialize k-means lots of\n",
        "311": "times in run k-means lots of times and\n",
        "314": "use that to try to make sure we get as\n",
        "316": "good a solution as good a local or\n",
        "320": "global optimum as possible concretely\n",
        "321": "here's how you could go about doing that\n",
        "324": "let's say I decide to run k-means a\n",
        "326": "hundred times so I'm going to execute\n",
        "329": "this loop a hundred times and a fairly\n",
        "331": "typical number of times I run k-means\n",
        "333": "would be something from 50 up to maybe a\n",
        "335": "thousand so but let's say let's say you\n",
        "338": "decide to write k ds 100 times so what\n",
        "340": "that means is we were randomly initial\n",
        "343": "initialize k-means and for each of these\n",
        "345": "100 random initializations we rerun\n",
        "348": "k-means and now you give us a set of\n",
        "349": "clustering\n",
        "352": "cluster centroids and we will then\n",
        "355": "compute the distortion J that is compute\n",
        "358": "this cost function on the set of cluster\n",
        "359": "assignments and cluster centroids that\n",
        "362": "we got finally having done this whole\n",
        "364": "procedure 100 times you will have a\n",
        "367": "hundred different ways of clustering the\n",
        "371": "data and then finally what you do is out\n",
        "372": "of these hundred ways we have found if\n",
        "375": "clustering the data just pick one that\n",
        "376": "gives us the lowest cost if it does\n",
        "380": "little less distortion and it turns out\n",
        "382": "that if you're running k-means with a\n",
        "384": "fairly small number of clusters so you\n",
        "386": "know if the number of clusters is\n",
        "389": "anywhere from 2 up to maybe 10 then\n",
        "390": "doing multiple random initializations\n",
        "393": "can often can sometimes make sure that\n",
        "395": "you find a better local offer make sure\n",
        "396": "you find a better clustering of the data\n",
        "399": "but if K is very large so if K is much\n",
        "402": "greater than 10 certainly of K where if\n",
        "403": "you're trying to find hundreds of\n",
        "406": "clusters then having multiple random\n",
        "408": "initializations is less likely to make a\n",
        "410": "huge difference and there's much higher\n",
        "412": "chance that your first random\n",
        "414": "initialization will give you a pretty\n",
        "417": "decent solution already and doing doing\n",
        "419": "multiple random initializations\n",
        "420": "will probably give you a slightly better\n",
        "422": "solution but but maybe not that much but\n",
        "424": "it's really in the regime of when you\n",
        "426": "have a relatively small number of\n",
        "428": "clusters especially if we have maybe two\n",
        "430": "or three or four clusters that's a\n",
        "432": "random initialization could make a huge\n",
        "434": "difference in terms of making sure you\n",
        "436": "do a good job minimizing the distortion\n",
        "438": "function and giving you a good\n",
        "440": "clustering\n",
        "443": "so that's k-means with random\n",
        "445": "initialization if you're trying to learn\n",
        "447": "a clustering with a relatively small\n",
        "449": "number of clusters two three four five\n",
        "452": "maybe six seven using multiple random\n",
        "455": "initializations can sometimes help you\n",
        "457": "find much better clusterings of the data\n",
        "459": "but even if you are learning a large\n",
        "460": "number of clusters the initialization\n",
        "462": "the random initialization method I\n",
        "464": "described here that should give k-means\n",
        "466": "a reasonable starting point to start\n"
    },
    "Q4GNLhRtZNc": {
        "0": " \n",
        "1": "in this video we will start to talk\n",
        "4": "about a new version of linear regression\n",
        "6": "that is more powerful one that works\n",
        "9": "with multiple variables or with multiple\n",
        "12": "features here's what I mean in the\n",
        "14": "original version of linear regression\n",
        "16": "that we developed we had a single\n",
        "19": "feature X the size of the house and we\n",
        "21": "wanted to use that to predict Y the\n",
        "25": "price of the house and this was our form\n",
        "29": "of our hypothesis but now imagine what\n",
        "31": "if we had not only the size of the house\n",
        "34": "as a feature or as a variable with which\n",
        "37": "to try to predict the price but that we\n",
        "39": "also knew the number of bedrooms the\n",
        "42": "number of house and the age of the home\n",
        "44": "and years it seems like this would give\n",
        "46": "us a lot more information with which to\n",
        "49": "predict the price to introduce a little\n",
        "51": "bit of notation and we sort of started\n",
        "53": "to talk about this earlier I'm going to\n",
        "55": "use the variables X subscript 1 X\n",
        "60": "subscript 2 and so on to denote my in\n",
        "63": "this case for features and I'm going to\n",
        "66": "continue to use Y to denote the variable\n",
        "69": "the output variable price that we're\n",
        "71": "trying to predict let's introduce a\n",
        "74": "little bit more notation now that we\n",
        "77": "have four features I'm going to use\n",
        "80": "lowercase n to denote the number of\n",
        "82": "features so in this example we have N\n",
        "85": "equals four because we have you know one\n",
        "90": "two three four features and n is\n",
        "92": "different from our earlier notation\n",
        "96": "where we were using M to denote the\n",
        "98": "number of examples so if you have 47\n",
        "102": "rows M is the number of rows in this\n",
        "104": "table or the number of training examples\n",
        "109": "so I'm also going to use X superscript I\n",
        "112": "to denote the input features of the I\n",
        "116": "training example as a compute example\n",
        "122": "let's say x2 is going to be a vector of\n",
        "125": "the features for my second training\n",
        "128": "example and so x2 here is going to be a\n",
        "129": "vector\n",
        "135": "1 4 1 6 3 2 40 since those are my 4\n",
        "138": "features that have to try to predict the\n",
        "141": "price of the second house so in this\n",
        "144": "notation the superscript the superscript\n",
        "145": "2 here\n",
        "148": "that's an index into my training set\n",
        "151": "right this is not X to the power of 2\n",
        "154": "instead this is you know an index that\n",
        "156": "says look at the second row of this\n",
        "158": "table this refers to my second training\n",
        "162": "example with this notation x2 is a 4\n",
        "164": "dimensional vector in fact more\n",
        "167": "generally this is an N dimensional\n",
        "173": "feature vector with this notation x2 is\n",
        "176": "now a vector and so I'm going to use\n",
        "181": "also X I subscript J to denote the value\n",
        "185": "of the J feature number J in the I've\n",
        "189": "training example so concretely x2\n",
        "193": "subscript 3 let's say we refer to\n",
        "196": "feature number 3 in this vector which is\n",
        "199": "equal to 2 right and those are three\n",
        "201": "over there just to fix my handwriting so\n",
        "204": "x2 subscript 3 is going to be equal to 2\n",
        "207": "now that we have multiple features let's\n",
        "209": "talk about what the form of our\n",
        "213": "hypothesis should be previously this was\n",
        "215": "the form of our hypothesis where X was\n",
        "218": "our single feature but now that we have\n",
        "220": "multiple features we aren't going to use\n",
        "222": "this simple representation anymore\n",
        "226": "instead our form of the hypothesis in\n",
        "228": "linear regression is going to be this\n",
        "232": "going to be theta 0 plus theta 1 x1 plus\n",
        "233": "going to be theta 0 plus theta 1 x1 plus\n",
        "238": "theta 2 x2 plus theta 3 x3 plus theta 4\n",
        "241": "x4 and if we have n features then rather\n",
        "243": "than summing up over all four features\n",
        "246": "we would have a sum over our n features\n",
        "250": "concretely for a particular setting of\n",
        "252": "our parameters we may have H of x equals\n",
        "260": "80 plus 0.1 x1 plus 0.01 x2 plus 3x\n",
        "265": "3 - 2 X 4 this will be one example of\n",
        "268": "hypothesis and you remember our\n",
        "269": "hypothesis is trying to predict the\n",
        "271": "price of the house in thousands of\n",
        "273": "dollars - saying that you know the base\n",
        "276": "price of houses maybe on 80 thousand\n",
        "281": "plus another 0.1 so that's an extra 100\n",
        "283": "dollars per square feet you know plus\n",
        "285": "the price goes up a little bit for each\n",
        "286": "the price goes up a little bit for each\n",
        "289": "additional floor that the house has so\n",
        "291": "it's X 2 s number of floors and it goes\n",
        "293": "out further for each additional bedroom\n",
        "296": "the house has because X 3 was the number\n",
        "298": "of bedrooms and the price goes down a\n",
        "301": "little bit which for each additional age\n",
        "304": "of the house with with each additional\n",
        "307": "year of the age of the house here's the\n",
        "310": "form of our hypothesis we written on the\n",
        "312": "slide and what I'm going to do is\n",
        "313": "introduce a little bit of notation to\n",
        "317": "simplify this equation for convenience\n",
        "320": "of notation let me define x subscript 0\n",
        "323": "to be equal to 1 concretely this means\n",
        "326": "that for every example I I have a\n",
        "329": "feature vector X superscript I and X\n",
        "332": "superscript I subscript 0 is going to be\n",
        "334": "equal to 1 you can think of this as\n",
        "337": "defining an additional 0 feature so\n",
        "340": "whereas previously I had n features each\n",
        "343": "as X 1 X 2 through xn I'm now defining\n",
        "346": "an additional so that 0 feature vector\n",
        "350": "that always takes on the value of 1 so\n",
        "354": "now my feature vector X becomes this n\n",
        "359": "plus 1 dimensional vector that is 0\n",
        "363": "index so this is now a n plus 1\n",
        "364": "dimensional feature vector but I'm going\n",
        "368": "to index it from 0 and I'm also going to\n",
        "372": "think of my parameters as a vector so my\n",
        "374": "parameters here right that would be a\n",
        "376": "state of 0 theta 1 theta 2 and so on up\n",
        "379": "to theta n I'm going to gather them up\n",
        "382": "into a parameter vector written theta 0\n",
        "385": "theta 1 theta 2 and so on down to theta\n",
        "389": "n this is another zero index vectors of\n",
        "390": "index on\n",
        "393": "zero that is another n plus one\n",
        "398": "dimensional vector so my hypothesis can\n",
        "400": "now be written theta zero X zero plus\n",
        "405": "theta 1 X 1 plus dot dot dot up to theta\n",
        "409": "n xn and this equation is the same as\n",
        "413": "this one on top because you know X 0 is\n",
        "417": "equal to 1 and the neat thing is I can\n",
        "420": "now take this form of the hypothesis and\n",
        "425": "write this as theta transpose X depend\n",
        "426": "on how familiar you are with inner\n",
        "430": "products of vectors if you write out\n",
        "433": "what theta transpose X is what theta\n",
        "436": "transpose X is is there's a theta 0\n",
        "442": "theta 1 up to theta n so this thing here\n",
        "445": "is Theta transpose and this is actually\n",
        "450": "a n plus 1 by 1 matrix it's also called\n",
        "453": "a row vector and we take that 2 and\n",
        "454": "a row vector and we take that 2 and\n",
        "457": "multiply it with the vector X which is X\n",
        "462": "0 X 1 and so on down to X N and so the\n",
        "465": "inner product that is Theta transpose X\n",
        "468": "is just equal to this this gives us a\n",
        "470": "convenient way to write the form of the\n",
        "472": "hypothesis as just the inner product\n",
        "475": "between our parameter vector theta and\n",
        "478": "our feature vector X and it is this\n",
        "480": "little bit of notation this little extra\n",
        "482": "bit of notational convention that let us\n",
        "484": "write this in this compact form so\n",
        "487": "that's the form of our hypothesis when\n",
        "489": "we have multiple features and just to\n",
        "490": "give this another name this is also\n",
        "492": "called multivariate linear regression\n",
        "495": "and deter multivariate is just maybe a\n",
        "497": "fancy term for saying that we have\n",
        "499": "multiple features or multiple variables\n"
    },
    "QKc3Tr7U4Xc": {
        "0": " \n",
        "2": "in this video I'd like to tell you a bit\n",
        "4": "about the math behind large margin\n",
        "7": "classification this video is optional so\n",
        "9": "please feel free to skip it but they may\n",
        "11": "also give you better intuition about how\n",
        "14": "the optimization problem of the support\n",
        "17": "vector machine how that leads to large\n",
        "21": "margin classifiers in order to get\n",
        "23": "started let me first remind you of a\n",
        "26": "couple properties of what vector inner\n",
        "29": "products look like let's say I have two\n",
        "33": "vectors U and V that look like this so\n",
        "35": "both two dimensional vectors then let's\n",
        "36": "both two dimensional vectors then let's\n",
        "40": "see what u transpose D looks like and nu\n",
        "43": "transpose D is also called the inner\n",
        "48": "product between the vectors U and V use\n",
        "51": "a two dimensional vector so I can plot\n",
        "54": "it on this figure so let's say that the\n",
        "57": "vector U and what I mean by that is if\n",
        "60": "on the horizontal axis that value takes\n",
        "63": "u whatever value u 1 is and on the\n",
        "65": "vertical axis the height of that is\n",
        "68": "whatever u 2 u 2 is the second component\n",
        "73": "of the vector u now one quantity that'll\n",
        "76": "be nice to have is the norm of the\n",
        "79": "vector u so these are your double bars\n",
        "80": "on the left and right that denotes the\n",
        "83": "norm or the length of U so this just\n",
        "87": "means really the Euclidean length of the\n",
        "92": "vector U and this will by Pythagoras\n",
        "96": "theorem is just equal to u1 squared plus\n",
        "100": "u2 squared square root right and this is\n",
        "102": "the length of a vector u that's a real\n",
        "104": "numbers just as you know what is what is\n",
        "106": "the length of this what is the length of\n",
        "109": "this vector down here the length what is\n",
        "112": "the length of this arrow will I just\n",
        "117": "drew is the norm of U now let's go back\n",
        "118": "and look at the vector V because we want\n",
        "121": "to compute the inner product so V will\n",
        "123": "be some other vector with some you know\n",
        "126": "value V 1\n",
        "132": "me too and so the vector V will look\n",
        "138": "like that draw me like so now let's go\n",
        "140": "back and look at how to compute the\n",
        "142": "inner product between you and me here's\n",
        "144": "how you can do it I'm going to take the\n",
        "148": "vector V and project it down onto the\n",
        "150": "vector U so I'm going to take a\n",
        "152": "orthogonal projection or a 90-degree\n",
        "155": "projection to project it down onto you\n",
        "157": "like so and what I'm going to do is\n",
        "160": "measure the length of this red line that\n",
        "161": "measure the length of this red line that\n",
        "162": "I just drew here's one call the length\n",
        "166": "of that red line P so P is the length or\n",
        "169": "is the magnitude of the projection of\n",
        "173": "the vector V onto the vector U and then\n",
        "175": "let me just write that down so P is the\n",
        "182": "length of the projection of the vector V\n",
        "189": "onto the vector U and it's possible to\n",
        "191": "show that the inner product u transpose\n",
        "194": "D that this is going to be equal to P\n",
        "198": "times the norm or the length of the\n",
        "202": "vector u so this is one way to compute\n",
        "204": "the inner product and if you actually\n",
        "206": "know if you actually do the geometry and\n",
        "208": "figure out what P is and figure out what\n",
        "210": "the norm of U is this should give you\n",
        "212": "the same way the same answer as the\n",
        "214": "other way of computing inner product\n",
        "216": "right which is if you take u transpose V\n",
        "221": "then u transpose is this u 1 u 2 is a 1\n",
        "226": "by 2 matrix n times V and so this should\n",
        "232": "actually give you u1 v1 + u2 v2 and so\n",
        "234": "the theorem of linear algebra that these\n",
        "237": "two formulas give you the same answer\n",
        "239": "and by the way u transpose V is also\n",
        "244": "equal to V transpose u so if you were to\n",
        "245": "do the same process in Reverse\n",
        "248": "instead of projecting V onto u you could\n",
        "249": "reject you want to be\n",
        "251": "you know do the same process but with\n",
        "253": "the rows of you in the reverse and you\n",
        "255": "actually you should actually get the\n",
        "257": "same number one in that number right and\n",
        "259": "just to clarify what's going on in this\n",
        "262": "equation the norm of U is a real number\n",
        "266": "and P is also a real number and so u\n",
        "269": "transpose V is the interpret is the is\n",
        "271": "the same in regular multiplication of\n",
        "274": "two row numbers of the length of P times\n",
        "276": "of mode with you just one last detail\n",
        "279": "which is if we look at the norm of P P\n",
        "282": "is actually signed so right and it can\n",
        "284": "be either positive or negative so let me\n",
        "287": "say what I mean by that if u is a vector\n",
        "290": "that looks like this and if V is a\n",
        "292": "vector that looks like this so that the\n",
        "295": "angle between U and V is greater than 90\n",
        "299": "degrees then if I project V onto u what\n",
        "301": "I'll get is a projection that looks like\n",
        "305": "this and so now that like P and in this\n",
        "308": "case I will still have that u transpose\n",
        "312": "V is equal to P times the norm of U\n",
        "316": "except that in this example P will be\n",
        "319": "negative so you know in inner products\n",
        "322": "if the angle between U and V is less\n",
        "325": "than 90 degrees then P is the positive\n",
        "327": "length of that red line whereas at the\n",
        "330": "angle this angle here is greater than 90\n",
        "333": "degrees then P here will be negative of\n",
        "335": "the length of the single-line a bad\n",
        "337": "little line segments over there and so\n",
        "338": "the inner product between two vectors\n",
        "341": "can also be negative and the angle\n",
        "343": "between them is greater than 90 degrees\n",
        "345": "so that's how vector inner products work\n",
        "347": "we're going to use these properties of\n",
        "348": "we're going to use these properties of\n",
        "349": "vector inner product so you try to\n",
        "352": "understand the support vector machine\n",
        "354": "optimization objective a little better\n",
        "357": "here's the optimization objective for\n",
        "358": "the support vector machine that we\n",
        "361": "worked out earlier just for the purpose\n",
        "363": "of this slide I'm going to make one\n",
        "366": "simplification or once just to make the\n",
        "370": "objective easy to analyze and what I'm\n",
        "371": "going to do is ignore the intercept\n",
        "373": "terms and we'll just ignore theta 0 and\n",
        "375": "set that to be equal to 0\n",
        "378": "to make things easy to plot I'm also\n",
        "380": "going to set n the number of features to\n",
        "382": "be equal to two so we have only two\n",
        "387": "features x1 and x2 now let's look at the\n",
        "389": "objective function the optimization\n",
        "391": "objective of the SEM when we have only\n",
        "394": "two features when n is equal to 2 this\n",
        "398": "is this can be written 1/2 of theta 1\n",
        "400": "squared plus theta 2 squared because we\n",
        "402": "only have two parameters theta 1 and\n",
        "406": "theta 2 and what I'm going to do is\n",
        "407": "rewrite this a bit in the write this as\n",
        "411": "1/2 of theta 1 squared plus theta 2\n",
        "414": "squared and then square root squared and\n",
        "417": "the reason I can do that is because for\n",
        "421": "any number you know W write the square\n",
        "423": "roots of W and then squared that's just\n",
        "425": "equal to W so till square roots and\n",
        "427": "squared should give me the same thing\n",
        "429": "and what you may notice is that this\n",
        "432": "term inside the parentheses that's equal\n",
        "436": "to the norm or the length of the vector\n",
        "440": "theta and what I mean by that is that if\n",
        "442": "we write out the vector theta like this\n",
        "445": "as you know theta 1 theta 2 then this\n",
        "447": "term that just underlined in red that's\n",
        "450": "exactly the length for the norm of the\n",
        "453": "vector theta following the definition of\n",
        "454": "the norm of a vector that we have on the\n",
        "456": "previous line and in fact this is\n",
        "458": "actually equal to the length of the\n",
        "460": "vector theta whether you write it as\n",
        "463": "theta 0 theta 1 theta 2 that is if theta\n",
        "466": "0 is equal to 0 as I see in here or just\n",
        "468": "the length of theta 1 theta 2 but for\n",
        "471": "this line I'm going ignore theta 0 so\n",
        "473": "that just your treat later as this up\n",
        "476": "right let me just write theta theta the\n",
        "478": "norm of theta is this theta 1 theta 2\n",
        "481": "only but the math works out either way\n",
        "483": "whether we include you know theta 0 here\n",
        "484": "or not so it doesn't sense not for the\n",
        "485": "or not so it doesn't sense not for the\n",
        "486": "matter for the rest of our derivation\n",
        "489": "and so finally this means that my\n",
        "491": "optimization objective is equal to\n",
        "496": "one-half of the norm of theta squared so\n",
        "498": "our support vector machine is doing in\n",
        "500": "the optimization objective this is\n",
        "503": "minimizing the squared norm of the\n",
        "504": "squared length of the parameter vector\n",
        "507": " \n",
        "510": "now what I'd like to do is look at these\n",
        "513": "terms theta transpose X and understand\n",
        "515": "better what they're doing so given the\n",
        "517": "parameter vector theta and given an\n",
        "520": "example X what is this equal to\n",
        "522": "and on the previous slide we think about\n",
        "525": "what u transpose V looks like but\n",
        "528": "different vectors U and V and so we're\n",
        "529": "going to take those definitions you know\n",
        "533": "with theta and X I play the rules of U\n",
        "535": "and V let's see what that picture looks\n",
        "538": "like so let's say I plot let's say I\n",
        "539": "thought that just a single training\n",
        "541": "example let's say I have a positive\n",
        "544": "example drawing us across there and\n",
        "549": "let's say that's my example X I right so\n",
        "551": "what that really means is that I've\n",
        "554": "plotted on the horizontal axis some\n",
        "561": "value X i1 and on the vertical axis x i2\n",
        "564": "that's how I plot my training examples\n",
        "567": "and although we haven't been really\n",
        "568": "thinking of this as a vector what this\n",
        "570": "really is this is a vector from the\n",
        "575": "origin from 0 0 out to the location of\n",
        "578": "this training example and now let's say\n",
        "583": "we have a parameter vector theta and I'm\n",
        "585": "going to plug that as vector as well\n",
        "587": "what I mean by that is it would plot\n",
        "596": "theta 1 here and theta 2 there so what\n",
        "599": "is the inner product theta transpose X I\n",
        "602": "well using our earlier method the way we\n",
        "606": "compute that as we take my example and\n",
        "608": "project it onto my parameter vector\n",
        "611": "theta and then I'm going to look at the\n",
        "614": "length of this segment I'm coloring it\n",
        "617": "in red and I'm going to call that P\n",
        "621": "superscript I to denote that this is a\n",
        "622": "projection of the eye for training\n",
        "624": "example onto the parameter parameter\n",
        "628": "vector theta and so what we have is that\n",
        "631": "theta transpose X I is equal to\n",
        "633": "following what we had on the previous\n",
        "637": "slide this is going to be equal to P\n",
        "641": "times the length of the norm of the\n",
        "644": "vector theta and this is of course also\n",
        "650": "equal to theta 1 X 1 plus theta 2 X 2 so\n",
        "652": "if each of these you know is an equally\n",
        "654": "valid way of computing the inner product\n",
        "658": "between later and Exxon ok\n",
        "659": "so where does this leave us what this\n",
        "661": "means is that these constraints that\n",
        "663": "theta transpose X I'd be greater legal\n",
        "665": "one holders less than to the minus 1\n",
        "667": "what this means is it can replace these\n",
        "672": "with constraints that P I times X be\n",
        "674": "greater than equal to 1 because theta\n",
        "678": "transpose X I is equal to P I times the\n",
        "682": "norm of theta so writing that into our\n",
        "684": "optimization objective this is what we\n",
        "687": "get where I have instead of theta\n",
        "689": "transpose X I I now have this P I times\n",
        "693": "the norm of theta and just remind you we\n",
        "694": "worked out earlier too that this\n",
        "696": "optimization objective that can be\n",
        "699": "written as 1/2 times the norm of the\n",
        "704": "data squared so now let's consider the\n",
        "706": "training example that we have at the\n",
        "708": "bottom and for now continuing to use the\n",
        "711": "simplification that theta 0 is equal to\n",
        "713": "0 let's see what decision boundary the\n",
        "715": "support vector machine will choose\n",
        "718": "here's one option let's say the support\n",
        "720": "vector machine were to choose this\n",
        "723": "decision boundary this is not a very\n",
        "724": "good choice because it has very small\n",
        "727": "margin this decision boundary comes very\n",
        "730": "close to the training examples let's see\n",
        "732": "why the support vector machine would not\n",
        "736": "do this for this choice of parameters is\n",
        "739": "possible to show that the parameter\n",
        "742": "vector theta is actually at 90 degrees\n",
        "744": "to the decision boundary so that green\n",
        "747": "decision boundary corresponds to a\n",
        "749": "parameter vector theta that points in\n",
        "751": "that direction and by the way the\n",
        "753": "simplification that theta 0 equals 0\n",
        "755": "that just means that the decision\n",
        "756": "boundary has to pass through the origin\n",
        "757": "boundary has to pass through the origin\n",
        "761": "0 0 over there so now let's look at what\n",
        "763": "this implies for the optimization\n",
        "764": "objective\n",
        "767": "let's say that this example here let's\n",
        "770": "say that's my first example you know x1\n",
        "773": "if we look at the projection of this\n",
        "776": "example onto my parameters theta that's\n",
        "778": "the projection and so that little line\n",
        "782": "red segment that is equal to p1 that's\n",
        "786": "going to be pretty small right and\n",
        "790": "similarly if this example here if this\n",
        "792": "happens to be x2 that's my second\n",
        "795": "example then if I look the projection of\n",
        "798": "this example onto theta you know then\n",
        "801": "let me draw this one the magenta this\n",
        "804": "lower magenta line segment that's going\n",
        "806": "to be p2 that's the projection of the\n",
        "808": "second example onto my onto the\n",
        "811": "direction of my parameter vector theta\n",
        "814": "which goes like this and so this law\n",
        "816": "projection line segments will increase\n",
        "818": "while p2 will actually be a negative\n",
        "821": "number ring so p2 epdm because is in the\n",
        "825": "opposite direction this vector has\n",
        "827": "created a 90 degree angle with my\n",
        "828": "parameter vector theta is going to be\n",
        "831": "less than zero and so what we're finding\n",
        "836": "is that these terms P I and are going to\n",
        "839": "be pretty small numbers and so if we\n",
        "840": "look at the optimization objective and\n",
        "843": "see well for positive examples we need P\n",
        "847": "I times the norm of theta to be bigger\n",
        "851": "than either one but if P I over here if\n",
        "854": "p1 over here is pretty small that means\n",
        "858": "that we need the norm of theta to be\n",
        "860": "pretty large right because if p1 of\n",
        "863": "theta is small and we want p1 you know\n",
        "865": "times the norm of theta to be bigger\n",
        "867": "than angular 1 well the only way for\n",
        "868": "that be true for the product of these\n",
        "870": "two numbers to be large if p1 is small\n",
        "873": "is we set normal state set to be large\n",
        "877": "and similarly for our negative example\n",
        "884": " \n",
        "887": "to be less than or equal to minus one\n",
        "890": "and we saw in this example right that p2\n",
        "891": "is going to be a pretty small negative\n",
        "893": "number and so the only way for that to\n",
        "895": "happen as well is for the norm of theta\n",
        "900": "to be large but what we're doing and the\n",
        "902": "optimization objective is we're trying\n",
        "905": "to find a setting of parameters where\n",
        "907": "the norm of theta is small and so you\n",
        "909": "know if so this doesn't seem like such a\n",
        "911": "good direction for the parameter vector\n",
        "912": "theta\n",
        "915": "in contrast let's look at the different\n",
        "918": "decision boundary here let's say this\n",
        "923": "SVM chooses that decision boundary now\n",
        "926": "different if that's a decision boundary\n",
        "928": "here's the corresponding direction for\n",
        "931": "theta and so with the decision boundary\n",
        "933": "you're being that vertical line that\n",
        "936": "corresponds to is possible to show using\n",
        "938": "linear algebra that the way to get that\n",
        "940": "green decision boundary is have the\n",
        "943": "vector theta be at 90 degrees to it and\n",
        "945": "now if you look at the projection of\n",
        "948": "your data onto the vector X let's say as\n",
        "949": "your data onto the vector X let's say as\n",
        "951": "before that this example is my example\n",
        "955": "x1 so when I project this onto X onto\n",
        "959": "theta what I find is that this is p1\n",
        "964": "that length there is p1 and some other\n",
        "967": "example you know if that example is x2\n",
        "971": "and I do the same projection and what I\n",
        "975": "find is that this length here is a p2\n",
        "977": "really that's the gate this could be\n",
        "979": "left less than zero and you notice that\n",
        "984": "now p1 right MP to these lengths of the\n",
        "987": "projections will be much bigger and so\n",
        "989": "if we still need to enforce these\n",
        "992": "constraints that p1 of the norm of theta\n",
        "995": "is very simple one because p1 is so much\n",
        "999": "bigger now the norm of theta can be\n",
        "1001": "smaller\n",
        "1003": "and so what this means is that by\n",
        "1005": "choosing the decision boundary shown on\n",
        "1008": "the right instead of on the left the SVM\n",
        "1010": "can make the norm of the parameters\n",
        "1012": "theta much smaller so if you can make\n",
        "1014": "the norm of theta smaller and therefore\n",
        "1016": "make the squared norm of theta smaller\n",
        "1019": "which is why the SVM will choose this\n",
        "1023": "hypothesis on the right instead and this\n",
        "1028": "is how the SVM gives rise to this large\n",
        "1031": "margin classification effect namely if\n",
        "1033": "you look at this green line if you look\n",
        "1035": "at this green hypothesis we want the\n",
        "1037": "projections of my positive negative\n",
        "1039": "examples onto theta to be large and the\n",
        "1042": "only way for that to hold true is if you\n",
        "1045": "know surrounding this green line this is\n",
        "1050": "a large margin there's this large gap\n",
        "1054": "that separates the positive and negative\n",
        "1059": "examples and this is is really the the\n",
        "1061": "magnitude of this gap the magnitude of\n",
        "1065": "this margin is exactly the values of P 1\n",
        "1068": "P 2 P 3 and so on and this though is by\n",
        "1070": "making the margin large about making\n",
        "1073": "these terms P 1 P 2 P 3 and so on that\n",
        "1076": "the SVM can end up with a smaller value\n",
        "1078": "for the norm of theta which is what is\n",
        "1080": "trying to do the objective and this is\n",
        "1082": "why this support vector machine ends up\n",
        "1085": "with a large margin classifiers because\n",
        "1087": "it's trying to maximize the norm of\n",
        "1089": "these peons which is a distance from the\n",
        "1090": "training examples to your decision\n",
        "1095": "boundary finally we did this whole\n",
        "1098": "derivation using this simplification\n",
        "1101": "that the parameter theta 0 must be equal\n",
        "1103": "to 0 the effect of that as I mentioned\n",
        "1104": "to 0 the effect of that as I mentioned\n",
        "1106": "briefly is that if they to 0 is equal to\n",
        "1108": "0 what that means is that we're\n",
        "1110": "entertaining only decision boundaries\n",
        "1112": "that pass through the origins of\n",
        "1113": "decision boundaries that pass through\n",
        "1116": "the origin like that if you allow theta\n",
        "1120": "0 to be nonzero then what that means is\n",
        "1121": "that you entertain decision boundaries\n",
        "1124": "that do not pass through the origin like\n",
        "1127": "that one I just drew and I'm not going\n",
        "1129": "to do the full derivation but it turns\n",
        "1131": "out that the same large margin proof\n",
        "1134": "we're pretty much in exactly the same\n",
        "1136": "and there's a generalization of this\n",
        "1138": "argument that we just went through them\n",
        "1140": "not to go through that shows that song\n",
        "1144": "even when theta zero is nonzero what the\n",
        "1146": "SVM is trying to do when you have this\n",
        "1148": "optimization objective which again\n",
        "1151": "corresponds to the case of when C is\n",
        "1155": "very large but is possible to show that\n",
        "1157": "you know when theta zero is not equal to\n",
        "1160": "zero the support vector machine is still\n",
        "1162": "finding is really trying to find freeing\n",
        "1164": "large margin separator between the\n",
        "1167": "positive and negative examples\n",
        "1169": "so that explains how the support vector\n",
        "1172": "machine is a large margin classifier in\n",
        "1174": "the next video we'll start to talk about\n",
        "1177": "how to take some of these SVM ideas and\n",
        "1179": "start to apply them to building complex\n"
    },
    "R8zHEyT2R4E": {
        "0": " \n",
        "1": "in some of the earlier videos I was\n",
        "4": "talking about PCA as a compression\n",
        "6": "algorithm where you may have say a\n",
        "8": "thousand dimensional data and compress\n",
        "10": "it to a hundred dimensional feature\n",
        "13": "vector or have three dimensional data\n",
        "14": "and compress it to a two dimensional\n",
        "17": "representation so if this is a\n",
        "19": "compression algorithm there should be a\n",
        "21": "way to go back from this compressed\n",
        "23": "representation back to an approximation\n",
        "26": "of your original high dimensional data\n",
        "28": "so given Zi which may be a hundred\n",
        "31": "dimensional how do you go back to your\n",
        "33": "original representation X I which was\n",
        "36": "maybe a thousand dimensional in this\n",
        "37": "video I'd like to describe how to do\n",
        "42": "that in the PCA algorithm we may have an\n",
        "44": "example like this so maybe that's one\n",
        "48": "example x1 maybe that's my example x2\n",
        "50": "and what we do is we take these examples\n",
        "53": "and we project them onto this one\n",
        "56": "dimensional surface and then now we need\n",
        "60": "to use only a real number say z1 to\n",
        "61": "specify the location of these points\n",
        "64": "after they've been projected onto this 1\n",
        "66": "dimensional surface so give it a point\n",
        "70": "like this given the point Z 1 how can we\n",
        "72": "go back to this original two dimensional\n",
        "76": "space and in particular given a point Z\n",
        "79": "which is an R can we map this back to\n",
        "82": "some approximate representation X and r2\n",
        "84": "of whatever the original value of the\n",
        "88": "data was so whereas Z equals you reduce\n",
        "90": "transpose X if you want to go in the\n",
        "93": "opposite direction the equation for that\n",
        "97": "is we're going to write X aprox\n",
        "104": "equals you reduce times Z and again just\n",
        "107": "to check the dimensions here you reduce\n",
        "110": "is going to be an N by K dimensional\n",
        "112": "vector Z is going to be a K by 1\n",
        "114": "dimensional vector so when you multiply\n",
        "116": "these out that's going to be n by 1 and\n",
        "117": "so x aprox\n",
        "120": "is going to be an N dimensional vector\n",
        "122": "and so the intent of PCA that is if the\n",
        "124": "square projection error is not too big\n",
        "126": "it is that this extra prompt will be\n",
        "129": "close to whatever was the original value\n",
        "132": "of x that you had used to derive Z in\n",
        "134": "the first place the show picture what\n",
        "136": "this looks like this is what it looks\n",
        "138": "like what you get back of this procedure\n",
        "141": "are points that lie on the projection of\n",
        "144": "that onto the Green Line so to take our\n",
        "146": "early example if we started off with\n",
        "149": "this value of x 1 and we got this value\n",
        "152": "of Z 1 if you plug Z 1 through this\n",
        "155": "formula to get X 1 and props then this\n",
        "159": "point here that would be X 1 a props\n",
        "163": "which is going to be in R 2 and so and\n",
        "164": "similarly if you do the same procedure\n",
        "170": "this would be X 2 approach and you know\n",
        "172": "that's a pretty decent approximation to\n",
        "175": "the original data so that's how you go\n",
        "176": "back from your low dimensional\n",
        "179": "representation Z back to an uncompressed\n",
        "181": "representation of the data you get back\n",
        "184": "an approximation to your original data X\n",
        "187": "and we also call this process\n",
        "189": "reconstruction of the original data when\n",
        "191": "we think of trying to reconstruct the\n",
        "193": "original value of x from the compressed\n",
        "196": " \n",
        "199": "so given an unlabeled data set you now\n",
        "202": "know how to apply PCA and take your high\n",
        "204": "dimensional features X and map it to\n",
        "206": "this lower dimensional representations D\n",
        "208": "and from this video hopefully you now\n",
        "210": "also know how to take these low\n",
        "212": "representations D and map it back up to\n",
        "215": "an approximation of your original high\n",
        "218": "dimensional data now that you know how\n",
        "220": "to implement and apply PCA would like to\n",
        "222": "do Nyx's talk about some of the\n",
        "225": "mechanics of how to actually use PCA\n",
        "227": "well and in particular in the next video\n",
        "229": "like to talk about how to choose K which\n",
        "231": "is how to choose the dimension of this\n"
    },
    "T-B8muDvzu0": {
        "0": " \n",
        "1": "for the problem of dimensionality\n",
        "4": "reduction by far the most popular by far\n",
        "6": "the most commonly used algorithm is\n",
        "8": "something called principal components\n",
        "11": "analysis or PCA in this video like to\n",
        "13": "start talking about the problem\n",
        "16": "formulation for PCA in other words let's\n",
        "18": "try to formulate precisely exactly what\n",
        "21": "we would like PCA to do let's say we\n",
        "22": "have a data set like this so this is a\n",
        "25": "data set of examples X in R 2 and let's\n",
        "27": "say I want to reduce the dimension of\n",
        "29": "the data from two-dimensional to\n",
        "31": "one-dimensional in other words I would\n",
        "33": "like to find a line onto which to\n",
        "35": "project the data so what seems like a\n",
        "37": "good line to put onto which the\n",
        "38": "projected data it seems like it's line\n",
        "40": "like this might be a pretty good choice\n",
        "43": "and the reason we think this might be a\n",
        "46": "good choice is that if you look at where\n",
        "48": "the projected versions of the points go\n",
        "50": "so I take this point and you projected\n",
        "52": "down here get that this point is\n",
        "55": "projected here to here to here to here\n",
        "57": "what we find is that the distance\n",
        "60": "between each point and the projected\n",
        "65": "version is pretty small that is the\n",
        "68": "these are blue line segments are pretty\n",
        "71": "short so what PCA does formally is it\n",
        "73": "tries to find a lower dimensional\n",
        "76": "surface really align in this case onto\n",
        "78": "which to project the data so that the\n",
        "81": "sum of squares of these little blue line\n",
        "84": "segments is minimized the length of\n",
        "86": "those blue line segments that sometimes\n",
        "89": "also called the projection error and so\n",
        "90": "what PCA does is it tries to find the\n",
        "92": "surface onto which to project the data\n",
        "95": "so as to minimize that as an aside on\n",
        "99": "before applying PCA is good standard\n",
        "100": "practice to first perform mean\n",
        "102": "normalization and feature scaling so\n",
        "105": "that the features x1 and x2 should have\n",
        "107": "you know 0 mean and should have a\n",
        "109": "comparable ranges of values I've already\n",
        "111": "done this for this example well come\n",
        "113": "back to this later and talk more about\n",
        "115": "feature scaling and mean normalization\n",
        "116": "in a Khan\n",
        "119": "the PC Anita but coming back to this\n",
        "122": "example in contrast to the red line that\n",
        "124": "I just drew here's a different line onto\n",
        "126": "which I could project my data it's this\n",
        "129": "magenta line and as you can see you know\n",
        "131": "this magenta line is a much worse\n",
        "133": "direction onto which to project my data\n",
        "136": "right so to project my data onto the\n",
        "137": "magenta line the other set of points\n",
        "141": "like that and the projection errors that\n",
        "144": "is DS on blue line segments will be huge\n",
        "146": "so these points have to move a huge\n",
        "151": "distance in order to get onto the UM in\n",
        "153": "order to get projected onto the magenta\n",
        "156": "line and so that's why our pca principal\n",
        "158": "components analysis we choose something\n",
        "160": "like the red line rather than like the\n",
        "163": "magenta line down here let's write out\n",
        "165": "the pca problem a little more formally\n",
        "168": "the goal of PCA if we want to reduce\n",
        "169": "data from two-dimensional to\n",
        "172": "one-dimensional is we're going to try to\n",
        "177": "find a vector that is a vector UI which\n",
        "179": "is going to be an RN so there will be an\n",
        "182": "r2 in this case but find the direction\n",
        "184": "onto which to project the data so to\n",
        "186": "minimize projection everyone so so in\n",
        "188": "this example I'm hoping that PCA will\n",
        "190": "find this vector which I'm going to call\n",
        "194": "you one so that when I project the data\n",
        "198": "onto the line that you know I define by\n",
        "201": "extending out this vector I end up with\n",
        "203": "pretty small reconstruction errors and\n",
        "206": "- that looks like this and by the way I\n",
        "208": "should mention that I'm where the PCA\n",
        "212": "gives me u 1 or negative u 1 doesn't\n",
        "213": "answer so if it gives me a positive\n",
        "215": "actor in this direction that's fine if\n",
        "217": "it gives me so the opposite vector\n",
        "220": "facing in the opposite direction like so\n",
        "222": "that would be like minus u 1 straw the\n",
        "224": "influence day if whether it gives me\n",
        "226": "positive u 1 or negative zu do you want\n",
        "228": "it doesn't matter because each of these\n",
        "231": "vectors defines the same red line onto\n",
        "234": "which I'm projecting my data so this is\n",
        "236": "a case of reducing data from\n",
        "238": "two-dimensional to one-dimensional in\n",
        "240": "the more general case we have n\n",
        "242": "dimensional data and we'll want to\n",
        "245": "reduce it to K dimensions in that case\n",
        "247": "we want to find not just a single vector\n",
        "249": "onto which to project the data we want\n",
        "251": "to find K dimensions onto which to\n",
        "254": "project the data so as to minimize this\n",
        "257": "projection error so here's an example if\n",
        "260": "I have a 3d point cloud like this then\n",
        "264": "maybe what I want to do is find vectors\n",
        "267": "so find the pair of vectors and I'm\n",
        "269": "going to call these vectors let's draw\n",
        "271": "these in red I'm going to find a pair of\n",
        "275": "vectors since I'm from the origin here's\n",
        "280": "u1 and here's my second vector u 2 and\n",
        "283": "together these two vectors define a\n",
        "286": "plane or they define a 2d surface right\n",
        "289": "like this sort of 2d surface onto which\n",
        "292": "I'm going to project my data for the\n",
        "294": "Steve they're not familiar with linear\n",
        "296": "algebra photos the other really experts\n",
        "298": "in linear algebra the formal definition\n",
        "300": "of this is that we're going to find the\n",
        "302": "other set of vectors u 1 u 2 maybe up to\n",
        "304": "UK and what we're going to do is project\n",
        "307": "the data onto the linear subspace\n",
        "310": "spanned by this set of K vectors but if\n",
        "312": "you're not from the familiar with linear\n",
        "314": "algebra just think of it as finding K\n",
        "316": "directions instead of just one direction\n",
        "318": "onto which to project the data so\n",
        "320": "finding a K dimensional surface really\n",
        "322": "finding a 2d plane in\n",
        "324": "this case I've shown in this figure\n",
        "327": "we're wrong we can define the position\n",
        "329": "of the points in a plane using K\n",
        "332": "directions that's why for PCA we want to\n",
        "334": "find K vectors onto which to project the\n",
        "338": "data and so more formally in PCA what we\n",
        "340": "want to do is find this way to project\n",
        "343": "the data so as to minimize the sort of a\n",
        "344": "projection distance which is the\n",
        "346": "distance between points and projections\n",
        "349": "and so in this video example 2 given the\n",
        "351": "point we would take the point and\n",
        "355": "project it onto this on 2d surface we\n",
        "358": "end up with that and so the projection\n",
        "360": "error would be you know the distance\n",
        "362": "between the point and where it gets\n",
        "366": "projected down to my 2d surface and so\n",
        "367": "what PCA does is I'll try to find the\n",
        "370": "line or a plane or whatever onto which\n",
        "372": "the projected data to try to minimize\n",
        "376": "that square projection that 90 degree or\n",
        "378": "that orthogonal projection error finally\n",
        "380": "one question that sometimes get asked is\n",
        "382": "how does PCA relate to linear regression\n",
        "385": "because when explaining PCA I sometimes\n",
        "387": "in the drawing diagrams like these and\n",
        "388": "that looks a little bit like linear\n",
        "392": "regression it turns out PCA is not the\n",
        "394": "linear regression and despite some\n",
        "396": "cosmetic similarity these are actually\n",
        "399": "totally different algorithms if we were\n",
        "401": "doing linear regression what we would do\n",
        "403": "would be on the Left we will be trying\n",
        "405": "to predict the value of some variable Y\n",
        "407": "given some input features X and so\n",
        "409": "linear regression what we're doing is\n",
        "412": "we're fitting a straight line so as to\n",
        "414": "minimize the squared error between a\n",
        "416": "point and the straight line and so we're\n",
        "418": "minimizing would be the squared\n",
        "421": "magnitude of these blue lines and notice\n",
        "424": "I'm drawing these blue lines vertically\n",
        "425": "director these blue lines are the\n",
        "427": "vertical distance between a point and\n",
        "429": "the value predicted by the hypothesis\n",
        "433": "whereas in contrast in PCA what it does\n",
        "436": "is it tries to minimize the magnitude of\n",
        "438": "these blue lines which are you know\n",
        "440": "drawn at an angle these are really the\n",
        "442": "shortest orthogonal distances the\n",
        "444": "shortest distance between the point X\n",
        "448": "and this red line and this gives very\n",
        "451": "different effects depending on the data\n",
        "454": "set and more generally and more\n",
        "456": "generally when you're doing linear\n",
        "458": "regression there is this distinguish\n",
        "460": "variable Y they were trying to predict\n",
        "462": "that all that linear regression is about\n",
        "465": "taking all the values of X and trying to\n",
        "468": "use that to predict Y whereas in PCA you\n",
        "470": "know there is no distinguish or there is\n",
        "472": "no special variable Y that we're trying\n",
        "473": "to predict and instead we have a list of\n",
        "477": "features x1 x2 and so on up to xn and\n",
        "479": "all of these features are treated\n",
        "482": "equally so now one of them is special as\n",
        "486": "one last example if I have three\n",
        "488": "dimensional data and I want to reduce\n",
        "490": "data from 3d to 2d so we really want to\n",
        "492": "find two directions\n",
        "495": "you know u1 and u2 onto which to project\n",
        "498": "my data then what I have is I have three\n",
        "502": "features x1 x2 x3 and all of these are\n",
        "503": "treated alike all of these are treated\n",
        "506": "so symmetrically and there's no special\n",
        "508": "variable wide and I'm trying to predict\n",
        "511": "and so PCA is not a linear regression\n",
        "514": "and when you and even though at some\n",
        "516": "cosmetic level they might look related\n",
        "518": "these are actually very different\n",
        "520": "algorithms\n",
        "524": "so hopefully you now understand what pca\n",
        "526": "is doing is trying to find a lower\n",
        "528": "dimensional surface onto which to\n",
        "530": "project the data so as to minimize this\n",
        "533": "a squared projection error to minimize\n",
        "534": "the squared distance between each point\n",
        "536": "and the location of where it gets\n",
        "539": "projected in the next video we'll start\n",
        "541": "to talk about how to actually find this\n",
        "543": "lower dimensional surface onto which to\n"
    },
    "TCA2VuHTHcM": {
        "0": " \n",
        "1": "in the last few videos we talked about\n",
        "3": "stochastic gradient descent and you know\n",
        "5": "other variations of the stochastic\n",
        "7": "gradient descent algorithm including us\n",
        "10": "adaptations online learning but all of\n",
        "12": "those algorithms could be run on one\n",
        "14": "machine could be run on one computer and\n",
        "16": "some machine learning problems are just\n",
        "18": "too big to run on one machine and\n",
        "20": "sometimes maybe you just have so much\n",
        "22": "data you just don't ever want to run all\n",
        "25": "that data through a single computer no\n",
        "27": "matter what algorithm you would use on\n",
        "29": "that computer so in this video I'd like\n",
        "31": "to talk about a different approach to\n",
        "34": "large scale machine learning called the\n",
        "38": "MapReduce approach and even though we\n",
        "39": "had quite a few videos on stochastic\n",
        "42": "gradient descent and to spend relatively\n",
        "45": "less time on MapReduce don't judge the\n",
        "47": "relative importance of MapReduce versus\n",
        "49": "the costly penderson based on the amount\n",
        "51": "of time I spend on these ideas in\n",
        "54": "particular many people will say that Mac\n",
        "56": "reduces and leaves an equally important\n",
        "57": "and some would say an even more\n",
        "59": "important idea compared to stochastic\n",
        "62": "gradient descent only it is relatively\n",
        "64": "simpler to explain which is why I'm\n",
        "66": "going to spend less time on it but using\n",
        "68": "these ideas you might be with a scale\n",
        "71": "learning algorithms to even far larger\n",
        "73": "problems than is possible using\n",
        "77": " \n",
        "81": "here's the idea let's say we want to fit\n",
        "83": "a linear regression model or logistic\n",
        "85": "regression model or somesuch and let's\n",
        "87": "start again with batch gradient descent\n",
        "89": "so that's our batch gradient descent\n",
        "92": "learning rule and to keep the writing on\n",
        "94": "this slide tractable I'm going to assume\n",
        "97": "throughout that we have M equals 400\n",
        "100": "examples of course by our standards in\n",
        "101": "terms of large scale machine learning\n",
        "103": "you know n might be pretty small and so\n",
        "106": "this might be more commonly applied to\n",
        "108": "problems where you have maybe closer to\n",
        "110": "400 million examples or some such but\n",
        "112": "just to make the writing on this slide\n",
        "115": "simpler and then to pretend we have 100\n",
        "117": "examples so in that case the batch\n",
        "118": "examples so in that case the batch\n",
        "119": "gradient descent learning rule has this\n",
        "122": "you have 1 over 400 and this sum from I\n",
        "124": "equals 1 through 400 through 400\n",
        "127": "examples here and if M is large then\n",
        "130": "this is a computationally expensive step\n",
        "133": "so what the MapReduce idea does is the\n",
        "136": "following and I should say the MapReduce\n",
        "140": "idea is due to two researchers Jeff Dean\n",
        "143": "and Sanjay Ghemawat Jeff Dean by the way\n",
        "145": "is also one of the most legendary\n",
        "147": "engineers in all of Silicon Valley and\n",
        "150": "he kind of a belt a large fraction of\n",
        "152": "the architecture or the infrastructure\n",
        "156": "that all of Google runs on today oh but\n",
        "158": "here's the MapReduce idea so let's say I\n",
        "160": "have some training set yeah denote by\n",
        "163": "this box here of X Y here it says you\n",
        "167": "know x1 y1\n",
        "172": "down to my 400 examples XM YM so that's\n",
        "175": "my training sample 400 ring examples in\n",
        "177": "the MapReduce idea what I'm going to do\n",
        "179": "is split this training set into\n",
        "183": "different subsets and I'm going to see\n",
        "185": "you for this example that I have for\n",
        "187": "computers or for machines to run in\n",
        "189": "parallel on my training set which is not\n",
        "191": "splitting this into four machines if you\n",
        "193": "have ten machines or 100 machines that\n",
        "194": "you wouldn't you know split your\n",
        "196": "training set into ten pieces or 100\n",
        "198": "pieces or what have you and what the\n",
        "201": "first of my four machines going to do\n",
        "205": "say is use just the first 1/4 of my\n",
        "208": "training sets reduce just the first 100\n",
        "211": "training examples and in particular what\n",
        "212": "it's going to do is look at this\n",
        "216": "summation and compute that summation for\n",
        "220": "just the first 100 training examples so\n",
        "222": "let me write that out and computer in\n",
        "225": "the computer variable temp1\n",
        "226": "the superscript 1 denotes is the first\n",
        "231": "machine j equals sum from I equals 1\n",
        "234": "through 100 and then I'm going to plug\n",
        "236": "in here exactly that term and so I have\n",
        "244": "a dictator X I - why I times X I J right\n",
        "246": "so that's just a that green in to center\n",
        "250": "up there and then similarly I'm going to\n",
        "252": "take the second quarter of my data and\n",
        "254": "send it to my second machine and my\n",
        "256": "second machine would use training\n",
        "259": "examples 101 through 200 and you know\n",
        "262": "computer similar their boss of at m2j\n",
        "265": "which is the same sum the index from\n",
        "268": "examples 101 through 200 and similarly\n",
        "270": "machines to be\n",
        "273": "and for we'll use the third quarter and\n",
        "277": "the fourth quarter of my training set so\n",
        "280": "now each machine has to sum over 100\n",
        "282": "instead of over 400 examples and so has\n",
        "284": "to do only a quarter of the work and\n",
        "286": "thus presumably it could do it about\n",
        "290": "four times as fast finally after all\n",
        "291": "these machines have done this work I'm\n",
        "295": "going to take these temp variables and\n",
        "297": "put them back together so we'll take\n",
        "299": "these variables and send them all to a\n",
        "302": "you know centralized master server and\n",
        "305": "what the master server will do is\n",
        "307": "combine these results together and in\n",
        "310": "particular it will update my parameters\n",
        "313": "theta J according to theta J gets\n",
        "318": "updated as theta J minus the learning\n",
        "324": "rate alpha times 1 over 400 times temp 1\n",
        "333": "J plus temp 2 J plus temp 3 J plus temp\n",
        "336": "4 J and of course that we have to do\n",
        "339": "this separately for J equals 0 you know\n",
        "341": "up to and within this number of inches\n",
        "344": "so sorry about breaking this equation\n",
        "345": "into multiple lines but hope is clear so\n",
        "350": "what this is what this equation is what\n",
        "354": "it's doing is exactly the same as that\n",
        "355": "when you have a centralized master\n",
        "358": "server that takes the results at 10 1 J\n",
        "361": "10 2 J 10 3 J 10 4 J and has them up and\n",
        "364": "so of course the sum of these four\n",
        "368": "things right that's just the sum of this\n",
        "371": "plus the sum of this versus some of this\n",
        "373": "let's\n",
        "374": "some of that and those four things just\n",
        "377": "cut up to be equal to this some that\n",
        "379": "were originally computing a batch\n",
        "381": "gradient descent and then we have the\n",
        "383": "alpha times when the 400 alpha times 100\n",
        "386": "and thus distance exactly equivalent to\n",
        "388": "the batch gradient descent algorithm\n",
        "391": "only instead of needing to sum over all\n",
        "394": "400 training examples on just one\n",
        "396": "machine we can instead divide up the\n",
        "399": "workload on four machines so here's what\n",
        "402": "the general picture of the MapReduce\n",
        "405": "technique feels like we have some\n",
        "407": "training sets and if we want to paralyze\n",
        "409": "across four machines willing to take the\n",
        "411": "training set and split it you know\n",
        "413": "equally or splitted as evenly as we can\n",
        "416": "into four subsets then we're going to\n",
        "417": "take the four subsets the training data\n",
        "419": "and send them to four different\n",
        "421": "computers and each of the four computers\n",
        "424": "can compute a summation over just one\n",
        "426": "quarter of the training set and then\n",
        "428": "finally it takes each of the computers\n",
        "430": "take the result sends them to a\n",
        "432": "centralized server which then combines\n",
        "434": "the results together so on the previous\n",
        "436": "slide in that example the bulk of the\n",
        "439": "work in gradient descent was computing\n",
        "442": "the sum from I equals 1 to 400 or\n",
        "444": "something so more generally sum from I\n",
        "447": "equals 1 to M of that you know formula\n",
        "449": "for gradient descent and now because\n",
        "450": "for gradient descent and now because\n",
        "452": "each of the 4 computers can do just a\n",
        "454": "porter of the work potentially you can\n",
        "459": "get up to a 4x speed up in particular if\n",
        "462": "there were no network latencies and no\n",
        "464": "cost of the network communication to\n",
        "465": "send the data back and forth you can\n",
        "468": "potentially get up to a 4x speed up of\n",
        "471": "course in practice because of network\n",
        "474": "latency is the overhead of combining the\n",
        "476": "results afterwards and other factors in\n",
        "478": "practice you get a slightly less than\n",
        "481": "the 4x speed up but nonetheless this\n",
        "483": "sort of MapReduce approach does offer us\n",
        "485": "a way to process much larger data sets\n",
        "488": "than is possible using a single computer\n",
        "490": "if you are thinking of applying\n",
        "493": "MapReduce to some learning algorithm in\n",
        "495": "order to speed it up by parallelizing\n",
        "497": "the computation over different computers\n",
        "500": "the key question to ask yourself is can\n",
        "502": "your learning algorithm be expressed as\n",
        "505": "a summation over the training set and it\n",
        "507": "turns out that many learning algorithms\n",
        "509": "can actually be expressed as computing\n",
        "511": "sums of functions over the training set\n",
        "514": "and the computational expense of running\n",
        "516": "them on large data sets is because they\n",
        "517": "need to sum over a very large training\n",
        "520": "set so whenever your learning algorithm\n",
        "522": "can be expressed as a sum over the\n",
        "523": "training set whenever the bulk of the\n",
        "525": "work of a learning algorithm can be\n",
        "527": "expressed as a sum of the training set\n",
        "529": "then map reviews might be a good\n",
        "531": "candidate for scaling your learning\n",
        "533": "algorithms to very very big data sets\n",
        "536": "let's just look at one more example last\n",
        "537": "thing that we want to use one of the\n",
        "539": "advanced optimization algorithms so\n",
        "540": "things on your l-bfgs conjugate\n",
        "543": "gradients and so on and let's say we\n",
        "544": "want to train in logistic regression\n",
        "547": "learning algorithm for that we need to\n",
        "550": "compute two main quantities one is for\n",
        "551": "the advanced optimization algorithms\n",
        "553": "like you know LPF GS and conjugate\n",
        "555": "gradient we need to provide it a routine\n",
        "558": "to compute the cost function of the\n",
        "560": "optimization objective and so for\n",
        "561": "logistic regression\n",
        "563": "you remember that the cost function has\n",
        "565": "this sort of sum over the training set\n",
        "568": "again and so if you're paralyzing over\n",
        "570": "ten machines you would split up the\n",
        "573": "training set onto ten machines and have\n",
        "575": "each of the ten machines compute the sum\n",
        "578": "of this quantity over just one tenth of\n",
        "581": "the training data then the other thing\n",
        "582": "that the advanced optimization\n",
        "585": "algorithms need is routine to compute\n",
        "587": "these partial derivative terms and once\n",
        "589": "again these derivative terms for\n",
        "591": "logistic regression can be expressed as\n",
        "593": "a sum over the training set and so once\n",
        "595": "again similar to our earlier example you\n",
        "597": "would have each machine compute that\n",
        "600": "summation over just some small fraction\n",
        "604": "of your training data and finally having\n",
        "606": "computed all of these things they could\n",
        "608": "then send their results to a centralized\n",
        "611": "server which can then add up the local\n",
        "613": "add up the partial sums this corresponds\n",
        "614": "to adding up\n",
        "621": "you know temp I or the temp IJ variables\n",
        "623": "which were computer locally on machine\n",
        "626": "number I and so the centralized server\n",
        "628": "can sum these things up and get the\n",
        "631": "overall cost function and get the\n",
        "633": "overall partial diversity which you can\n",
        "635": "then pass through the advanced\n",
        "638": "optimization atom so more broadly by\n",
        "640": "taking other learning algorithms and\n",
        "642": "expressing them in sort of summation\n",
        "645": "form or by expressing them in terms of\n",
        "646": "computing sums of functions over the\n",
        "648": "training set you can use the MapReduce\n",
        "650": "technique to parallelize other learning\n",
        "653": "adverbs as well and scale them to very\n",
        "655": "large training sets finally as one last\n",
        "657": "comments so far I've been discussing\n",
        "660": "MapReduce algorithms as allowing you to\n",
        "663": "parallelize over multiple computers that\n",
        "665": "may be multiple computers and a computer\n",
        "667": "cluster all the multiple computers in\n",
        "669": "the data center it turns out that\n",
        "672": "sometimes even if you have just a single\n",
        "674": "computer MapReduce can also be\n",
        "677": "applicable in particular on many single\n",
        "678": "computers now you can have multiple\n",
        "681": "processing cores we have multiple CPUs\n",
        "683": "and within the CPU can have multiple\n",
        "687": "protocols and so if you have a large\n",
        "689": "training set what you can do if say you\n",
        "691": "have a computer with four cores with\n",
        "694": "four computing cores we can do is even\n",
        "695": "on a single computer you can split the\n",
        "698": "training sense multiple pieces and send\n",
        "699": "the training set to different cores\n",
        "701": "within a single box like within a single\n",
        "703": "desktop computer or in a single server\n",
        "704": "desktop computer or in a single server\n",
        "706": "and use MapReduce this way to divvy up\n",
        "709": "the workload each of the cores can then\n",
        "711": "carry out the sum over say one quarter\n",
        "713": "of your training set and then they can\n",
        "715": "take the partial sums and you know\n",
        "717": "combine them in order to get the\n",
        "719": "summation over the entire training set\n",
        "721": "the advantage of thinking about\n",
        "724": "MapReduce this way as paralyzing over\n",
        "725": "cause within a single machine rather\n",
        "727": "than paralyzing across multiple machines\n",
        "730": "is that this way you don't have to worry\n",
        "732": "about network latency because all the\n",
        "734": "communication all the sending\n",
        "736": "of the temp jayveer Vols back and forth\n",
        "738": "all that happens within a single machine\n",
        "740": "and so network latency becomes much less\n",
        "742": "of an issue compared to if you were\n",
        "744": "using this to paralyze over different\n",
        "747": "computers within the data center finally\n",
        "749": "one last caveat on parallelizing within\n",
        "752": "a multi-core machine depending on the\n",
        "754": "details of implementation if you have a\n",
        "756": "multi-core machine and if you have\n",
        "757": "certain numerical linear algebra\n",
        "760": "libraries it turns out that the some\n",
        "761": "numerical in the area libraries that can\n",
        "764": "automatically paralyze their linear\n",
        "767": "algebra operations across multiple holes\n",
        "769": "within the machine so if you're\n",
        "770": "unfortunate enough to be using one of\n",
        "772": "those numerical than the arcuate\n",
        "773": "libraries and certainly this does not\n",
        "776": "apply to every single library but I'll\n",
        "778": "be using one of those libraries and if\n",
        "779": "you have a very good vectorized\n",
        "781": "implementation of a learning algorithm\n",
        "783": "sometimes you can just implement your\n",
        "785": "standard learning algorithm in a\n",
        "787": "vectorized fashion and not worry about\n",
        "789": "parallelization and your numerical\n",
        "790": "linearity library could take care of\n",
        "792": "some of it for you so that you don't\n",
        "795": "need to implement MapReduce but for\n",
        "797": "other learning problems taking advantage\n",
        "798": "of this sort of MapReduce implementation\n",
        "801": "while finding a using this MapReduce\n",
        "803": "formalism to paralyze across course\n",
        "805": "explicitly yourself might be a good idea\n",
        "807": "as well and could that you speed up your\n",
        "809": "learning algorithm\n",
        "812": "in this video we talked about the Map\n",
        "814": "Reduce approach to parallelizing machine\n",
        "816": "learning by taking your data and\n",
        "818": "spreading them across maybe many\n",
        "819": "computers in the data center\n",
        "822": "although these years are applicable to\n",
        "825": "paralyzing across multiple cores within\n",
        "827": "a single computer as well and today\n",
        "829": "there are some good open-source\n",
        "832": "implementations of MapReduce so there\n",
        "833": "are many users of an open-source system\n",
        "836": "called Hadoop and using either your own\n",
        "837": "implementation or using someone else's\n",
        "840": "open-source implementation you can use\n",
        "842": "these ideas to paralyze learning\n",
        "844": "algorithms and get them to run on much\n",
        "846": "larger data sets than is possible\n"
    },
    "TTdcc21Ko9A": {
        "0": " \n",
        "2": "in this video we'll figure out a\n",
        "4": "slightly simpler way to write the cost\n",
        "6": "function than we have been using so far\n",
        "9": "and we'll also figure out how to apply\n",
        "11": "gradient descent to fit the parameters\n",
        "14": "of logistic regression so by the end of\n",
        "16": "this video you know how to implement a\n",
        "18": "fully working version of logistic\n",
        "24": "regression here's our cost function for\n",
        "25": "logistic regression\n",
        "28": "our overall cost function is 1 over m\n",
        "30": "times sum over the training set of the\n",
        "33": "cost of making different predictions on\n",
        "35": "the different examples of labels Y I and\n",
        "38": "this is the cost for a single example\n",
        "40": "that we worked out earlier and just want\n",
        "42": "to remind you that for classification\n",
        "45": "problems in our training sets and in\n",
        "47": "fact you know even for examples now that\n",
        "50": "our training set Y is always equal to 0\n",
        "51": "or 1 right that's sort of part of the\n",
        "56": "mathematical definition of Y because Y\n",
        "59": "is either 0 1 we'll be able to come up\n",
        "62": "with a simpler way to write this cost\n",
        "64": "function and in particular rather than\n",
        "66": "writing out this cost function on two\n",
        "68": "separate lines with two separate cases\n",
        "70": "so y equals 1 and y equals 0 I'm going\n",
        "72": "to show you a way to take these two\n",
        "75": "lines and compress them into one\n",
        "77": "equation and this will make it more\n",
        "79": "convenient to write our cost function\n",
        "82": "and derive gradient descent concretely\n",
        "83": "we can write out the cost function as\n",
        "87": "follows we say that cost of H of X comma\n",
        "91": "Y I'm going to write this as minus y\n",
        "99": "times log H of X minus 1 minus y times\n",
        "105": "log 1 minus H of X and I'll show you in\n",
        "108": "a second that this expression or this\n",
        "111": "equation is a equivalent way or more\n",
        "113": "compact way of writing out this\n",
        "115": "definition of the cost function that we\n",
        "117": "had up here let's see why that's the\n",
        "122": " \n",
        "125": "we know that there are only two possible\n",
        "129": "cases y must be 0 or Y or 1 so let's\n",
        "132": "suppose y equals 1 if Y is equal to 1\n",
        "135": "then this equation is saying that the\n",
        "141": "cost is equal to well if y is equal to 1\n",
        "144": "then this thing here is equal to 1 and 1\n",
        "146": "minus y is going to be equal to 0 right\n",
        "149": "so if y is equal to 1 then 1 minus y is\n",
        "152": "1 minus 1 which is therefore 0 so the\n",
        "154": "second term gets multiplied by 0 and\n",
        "157": "goes away and we're left with only this\n",
        "160": "first term which is y times log minus y\n",
        "162": "times log of H of X y is 1 so that's\n",
        "167": "equal to minus log H of X and this this\n",
        "170": "equation is exactly what we have up here\n",
        "174": "for if y y is equal to 1 the other case\n",
        "178": "is if Y is equal to 0 and if that's the\n",
        "181": "case then all the writing of the cost\n",
        "184": "function is saying that well Y is equal\n",
        "186": "equal to 0 then this term here would be\n",
        "190": "equal to 0 where as 1 minus y if y is\n",
        "192": "equals 0 would be we'll want because 1\n",
        "195": "minus y becomes 1 minus 0 which is just\n",
        "198": "equal to 1 and so the cost function\n",
        "201": "simplifies to just this last term here\n",
        "204": "right because on the first term over\n",
        "206": "here it gets multiplied by 0 and so\n",
        "208": "disappears so we're just left with this\n",
        "212": "last term which is minus log 1 minus H\n",
        "215": "of X and you can verify that this term\n",
        "218": "here is just exactly what we had for\n",
        "221": "when Y is equal to 0 so this shows that\n",
        "223": "this definition for the cost is just a\n",
        "226": "more compact way of taking both of these\n",
        "228": "expressions the cases y equals 1 and y\n",
        "232": "equals 0 and writing them in 1 in a more\n",
        "235": "convenient form with just one line we\n",
        "236": "can therefore write all our cost\n",
        "238": "function for logistic regression as\n",
        "241": "follows it is this 1 over m of the sum\n",
        "244": "of these cost functions and plugging in\n",
        "245": "the definition for the cost that we\n",
        "246": "worked on earlier\n",
        "247": "we end up with this and it just brought\n",
        "250": "the minus sign outside and why do we\n",
        "251": "choose this particular\n",
        "253": "well looks like there could be other\n",
        "255": "cost functions we could have chosen\n",
        "257": "although I won't have time to go into\n",
        "260": "great detail of this in this course this\n",
        "262": "cost function can be derived from\n",
        "264": "statistics using the principle of\n",
        "266": "maximum likelihood estimation which is\n",
        "268": "an idea in statistics for how to\n",
        "271": "efficiently find parameters data for\n",
        "274": "different models and it also has a nice\n",
        "276": "property that it is convex so this is\n",
        "278": "the cost function that you know\n",
        "280": "essentially everyone uses when fitting\n",
        "283": "logistic regression models if you don't\n",
        "285": "understand the terms that I just said so\n",
        "286": "that if you don't know what the\n",
        "287": "principle of maximum likelihood\n",
        "289": "estimation is don't worry about it but\n",
        "291": "it just there's just a deeper rationale\n",
        "293": "and justification behind this particular\n",
        "296": "cost function then I have time to go\n",
        "298": "into in this class given this cost\n",
        "301": "function in order to fit the parameters\n",
        "304": "what we're going to do then is try to\n",
        "306": "find the parameters theta that minimizes\n",
        "309": "J of theta so if we you know try to\n",
        "312": "minimize this this would give us some\n",
        "316": "set of parameters theta finally if we're\n",
        "318": "given a new example with some set of\n",
        "320": "features X we can then take the Thetas\n",
        "322": "that we fit to our training set and\n",
        "325": "output our prediction as this and just\n",
        "327": "to remind you the output of my\n",
        "329": "hypothesis I'm going to interpret as the\n",
        "332": "probability that Y is equal to 1 and\n",
        "335": "this is given the in product and\n",
        "336": "parameterize by theta\n",
        "339": "but just in think of this as just my\n",
        "341": "hypothesis is estimating the probability\n",
        "345": "that Y is equal to 1 so all that remains\n",
        "348": "to be done is figure out how to actually\n",
        "351": "minimize J of theta as a function of\n",
        "352": "theta so that can actually fit the\n",
        "356": "parameters to our training set the way\n",
        "357": "we're going to minimize the cost\n",
        "358": "we're going to minimize the cost\n",
        "359": "function is using gradient descent\n",
        "363": "here's our cost function and if we want\n",
        "364": "to minimize it as a function of theta\n",
        "367": "here's our usual template for gradient\n",
        "369": "descent where we repeatedly update each\n",
        "372": "parameter by taking updating it as\n",
        "375": "itself - in learning rate alpha times\n",
        "378": "this derivative term if you know some\n",
        "380": "calculus feel free to take this term and\n",
        "382": "try to compute the derivative yourself\n",
        "383": "and see a\n",
        "385": "can simplify it to the same answer that\n",
        "387": "I get but even if you don't know\n",
        "390": "calculus don't worry about it if you\n",
        "393": "actually compute this what you get is\n",
        "397": "this equation and I'll just write it out\n",
        "399": "here and sum from I equals 1 through m\n",
        "405": "of the essentially the error times X IJ\n",
        "408": "so if you take this partial derivative\n",
        "410": "term and plug it back in here we can\n",
        "412": "then write out our gradient descent\n",
        "415": "algorithm as follows and all I've done\n",
        "417": "is I took the derivative term from the\n",
        "419": "previous slide and plugged it in there\n",
        "422": "so if you have n features you would have\n",
        "425": "you know a parameter vector theta which\n",
        "428": "would parameters theta 0 theta 1 theta 2\n",
        "431": "down to theta N and you would use this\n",
        "434": "update to simultaneously update all of\n",
        "436": "your values of theta now if you take\n",
        "439": "this update rule and compare it to what\n",
        "443": "we were doing for linear regression you\n",
        "445": "might be surprised to realize that well\n",
        "449": "this equation was exactly what we had\n",
        "451": "for linear regression in fact if you\n",
        "453": "look in the earlier videos and look at\n",
        "455": "the update rule the gradient descent\n",
        "457": "rule for linear regression it looked\n",
        "459": "exactly like what I drew here inside the\n",
        "463": "blue box so on linear regression and\n",
        "464": "logistic regression different algorithms\n",
        "467": "or not well this is resolved by\n",
        "469": "observing that for logistic regression\n",
        "472": "what has changed is that the definition\n",
        "474": "for this hypothesis has changed so\n",
        "475": "for this hypothesis has changed so\n",
        "477": "whereas for linear regression we have H\n",
        "481": "of x equals theta transpose X now the\n",
        "483": "definition of H of X has changed and is\n",
        "484": "definition of H of X has changed and is\n",
        "486": "instead now 1 over 1 plus e to the\n",
        "488": "negative theta transpose X so even\n",
        "489": "though the update rule looks\n",
        "492": "cosmetically identical because the\n",
        "494": "definition of the hypothesis has changed\n",
        "496": "this is actually not the same thing as\n",
        "498": "gradient descent for linear regression\n",
        "501": "in an earlier video when we were talking\n",
        "503": "about gradient descent for linear\n",
        "505": "regression we had talked about how to\n",
        "507": "monitor gradient descent to make sure\n",
        "510": "that it is converging I usually apply\n",
        "512": "that same method to logistic regression\n",
        "515": "to to monitor gradient descent\n",
        "516": "to make sure this conversion correctly\n",
        "518": "and hopefully you can figure out how to\n",
        "519": "and hopefully you can figure out how to\n",
        "520": "apply that technique to logistic\n",
        "523": "regression yourself\n",
        "526": "when implementing logistic regression\n",
        "528": "with gradient descent we have all of\n",
        "531": "these different parameter values you\n",
        "533": "know theta zero down to theta n that we\n",
        "535": "need to update using this expression and\n",
        "538": "one thing we could do is have a for loop\n",
        "543": "so for I equals 0 to n or for I equals 1\n",
        "545": "to n plus 1 to update each of these\n",
        "547": "parameter values in turn but of course\n",
        "550": "rather than using a for loop ideally we\n",
        "551": "would also use a vectorized\n",
        "554": "implementation and so that a vectorized\n",
        "557": "implementation can update you know all\n",
        "559": "of these n plus 1 parameters all in one\n",
        "560": "fell swoop\n",
        "563": "and to check your own understanding you\n",
        "565": "might see if you can figure out how to\n",
        "567": "do the vectorized implementation of this\n",
        "568": "do the vectorized implementation of this\n",
        "570": "algorithm as well\n",
        "572": "so now you know how to implement\n",
        "574": "gradient descents for logistic\n",
        "576": "regression there was one last idea that\n",
        "578": "we had talked about earlier for linear\n",
        "580": "regression which was feature scaling so\n",
        "583": "how feature scaling can help gradient\n",
        "585": "descents converge faster for linear\n",
        "586": "regression\n",
        "588": "the idea of feature scaling also applies\n",
        "590": "to gradient descent for logistic\n",
        "593": "regression and if you have features that\n",
        "595": "are on very different scales then\n",
        "596": "applying feature scaling can also make\n",
        "599": "it gradient descent run faster for\n",
        "602": "logistic regression so that's it you now\n",
        "604": "know how to implement logistic\n",
        "608": "regression and this is a very powerful\n",
        "610": "and probably the most widely used\n",
        "612": "classification algorithm the world and\n",
        "613": "you now know how we get to work with\n"
    },
    "W5meQnGACGo": {
        "0": " \n",
        "1": "in the last video we talked about\n",
        "4": "precision and recall as an evaluation\n",
        "7": "metric for classification problems with\n",
        "10": "skew classes for many applications we'll\n",
        "13": "want to somehow control the trade-off\n",
        "17": "between precision and recall let me tell\n",
        "19": "you how to do that and also show you\n",
        "21": "some even more effective ways to use\n",
        "24": "precision and recall as an evaluation\n",
        "28": "metric for learning algorithms as a\n",
        "31": "reminder here are the definitions of\n",
        "33": "precision and recall from the previous\n",
        "37": "video let's continue our cancer\n",
        "40": "classification example where y equals 1\n",
        "43": "if a patient has cancer and y equals 0\n",
        "46": "otherwise and let's say we've trained a\n",
        "48": "logistic regression classifier which\n",
        "51": "outputs probabilities between 0 and 1 so\n",
        "54": "as usual we're going to predict 1 y\n",
        "57": "equals 1 if P of X is greater than equal\n",
        "60": "to 0.5 and predict 0 if the hypothesis\n",
        "63": "offers a value less than 0.5 and this\n",
        "66": "classifier may give us some value for\n",
        "70": "precision and some value for recall but\n",
        "73": "now suppose we want to predict that the\n",
        "75": "patient has cancer only if we're very\n",
        "78": "confident that they really do because\n",
        "80": "you know if you go to a patient and you\n",
        "81": "tell them that they have cancer it's\n",
        "83": "gonna give them a huge shock gives us a\n",
        "86": "seriously bad news and they may end up\n",
        "88": "going through a pretty painful treatment\n",
        "90": "process and so on and so maybe we want\n",
        "92": "to tell someone that we think they have\n",
        "95": "cancer only if they are very confident\n",
        "97": "one way to do this would be to modify\n",
        "100": "the algorithm so that instead of setting\n",
        "103": "this threshold at 0.5 we might instead\n",
        "105": "say they will predict that Y is equal to\n",
        "109": "1 only of H of X is greater than or\n",
        "112": "equal to 0.7 so this is my saying well\n",
        "114": "tell someone they have cancer only if we\n",
        "116": "think there's a greater than you know\n",
        "118": "greater than or equal to 70% chance that\n",
        "122": "they have cancer and if you do this then\n",
        "124": "you're predicting someone has cancer\n",
        "127": "only when you're more confident and so\n",
        "129": "you end up with a classifier\n",
        "134": "that has higher precision because all of\n",
        "136": "the patients that you're going to and\n",
        "137": "say you know we think you have cancer\n",
        "140": "although those patients are now pretty\n",
        "142": "ones that you're pretty confident\n",
        "144": "actually have cancer and so a higher\n",
        "147": "fraction of the patients that you\n",
        "149": "predict have cancer will actually turn\n",
        "151": "out to have cancer because we're making\n",
        "152": "those predictions only if we're pretty\n",
        "155": "confident but in contrast this\n",
        "158": "classifier will have lower recall\n",
        "160": "because now we're going to make\n",
        "162": "predictions we're going to predict y\n",
        "164": "equals 1 on a smaller number of patients\n",
        "166": "and we can even take this further\n",
        "168": "instead of setting the threshold at 0.7\n",
        "170": "we could set this at all point nine and\n",
        "173": "we'll predict y equals one earlier where\n",
        "176": "more than 90% certain that the patient\n",
        "178": "has cancer and so you know a large\n",
        "179": "fraction of those patients will turn out\n",
        "181": "to have cancer and so this would be a\n",
        "184": "high precision classifier will have\n",
        "186": "lower recall because we want to\n",
        "188": "correctly detect that those patients\n",
        "190": "have cancer now consider a different\n",
        "193": "example suppose we want to avoid missing\n",
        "196": "too many actual cases of cancer so we\n",
        "198": "want to avoid false negatives in\n",
        "200": "particular if a patient actually has\n",
        "203": "cancer but we fail to tell them that\n",
        "205": "they have cancer then that could be\n",
        "207": "really bad because you know if we tell a\n",
        "210": "patient that they don't have cancer then\n",
        "212": "they they're not going to go for\n",
        "213": "treatment and if it turns out that they\n",
        "214": "treatment and if it turns out that they\n",
        "216": "have cancer but we fail to tell them\n",
        "218": "they have cancer well they may not get\n",
        "220": "treated at all and so that that would be\n",
        "222": "a really bad outcome they just die\n",
        "223": "because we told them they don't have\n",
        "225": "cancer they fail to get treated but it\n",
        "227": "turns out they actually have cancer so\n",
        "230": "suppose that when in doubt we want to\n",
        "233": "predict that y equals 1 so when in doubt\n",
        "236": "we want to predict that they have cancer\n",
        "238": "so that at least they look further into\n",
        "240": "it and the data and these can get\n",
        "242": "treated in case they do turn out to have\n",
        "246": "cancer in this case rather than setting\n",
        "249": "higher probability threshold we might\n",
        "252": "instead take this particular value and\n",
        "255": "instead set in to a lower value so maybe\n",
        "256": "0.3\n",
        "259": "like so right time by doing so we're\n",
        "261": "saying that you know what if we think\n",
        "263": "there's more than a 30% chance that they\n",
        "265": "have cancer we better be more\n",
        "267": "conservative and tell them that they may\n",
        "268": "have cancer so that they can seek\n",
        "272": "treatment if necessary and in this case\n",
        "274": "what we would have is going to be a\n",
        "281": "higher recall classifier because we're\n",
        "283": "going to be correctly flagging a higher\n",
        "285": "fraction of all of the patients to\n",
        "287": "actually do have cancer but we're going\n",
        "292": "to end up with lower precision because a\n",
        "294": "higher fraction of the patients that we\n",
        "296": "said have cancer your higher fraction of\n",
        "298": "them will turn out not to have cancer\n",
        "301": "after all and by the way just as an\n",
        "304": "aside when I talk about this in to two\n",
        "307": "other students I've been told before you\n",
        "309": "know it's pretty amazing some of my\n",
        "311": "students say is how I can tell the\n",
        "313": "stories both ways to know why we might\n",
        "315": "want to you know have higher position or\n",
        "317": "Harvey Cole and the story actually seems\n",
        "318": "to work both ways\n",
        "320": "but I hope the details that the\n",
        "323": "algorithm is true and the more general\n",
        "325": "principle is depending on where you want\n",
        "328": "whether you want higher position Lord we\n",
        "330": "call a Harvey caller position you can\n",
        "334": "end up predicting y equals 1 when H of X\n",
        "337": "is greater than some threshold and so in\n",
        "340": "general there for most classifiers\n",
        "342": "there's going to be a trade-off between\n",
        "346": "precision and recall and as you vary the\n",
        "349": "value of this threshold this value of\n",
        "351": "this special that when drawing here you\n",
        "353": "can actually plot out some curve that\n",
        "357": "trains off precision and recall where a\n",
        "361": "value up here this would correspond to a\n",
        "363": "very high value of the threshold maybe\n",
        "366": "threshold equals 0.99 so that's saying\n",
        "368": "predict y equals 1 only if we're you\n",
        "371": "know more than 99% confident at least\n",
        "373": "99% probability this one so that'd be a\n",
        "376": "high precision relatively low recall\n",
        "379": "whereas a point down here will\n",
        "381": "correspond to a value of the threshold\n",
        "382": "that's my\n",
        "384": "lower the vehicle you know 0.01 meaning\n",
        "386": "home when it doubt at all predict y\n",
        "388": "equals one and if you do that you end up\n",
        "391": "with a much lower precision higher\n",
        "394": "recall classifier and as you vary the\n",
        "396": "threshold you can if you want you can\n",
        "397": "actually trace all the current for your\n",
        "400": "classifier to see the range of different\n",
        "402": "values you can get for precision recall\n",
        "404": "and by the way the precision recall\n",
        "406": "curve can look like a many different\n",
        "408": "shapes sometimes they look like this\n",
        "412": "sometimes look like that you know there\n",
        "414": "are many different possible shapes from\n",
        "416": "the position recall curve depending on\n",
        "417": "the details of the collimator\n",
        "419": "so this raises another interesting\n",
        "423": "question which is this era way to choose\n",
        "426": "this threshold automatically or more\n",
        "429": "generally if we have a few different\n",
        "430": "algorithms there are a few different\n",
        "433": "ideas for algorithms how do we compare\n",
        "435": "different precision recall numbers\n",
        "438": "concretely suppose we have three\n",
        "439": "different learning algorithms actually\n",
        "440": "maybe these are three different learning\n",
        "443": "algorithms maybe these are the same\n",
        "444": "algorithm but just with different values\n",
        "447": "for the threshold how do we decide which\n",
        "450": "of these algorithms is best one of the\n",
        "451": "things we talked about earlier is the\n",
        "453": "importance of a single real number\n",
        "456": "evaluation metric and that is the idea\n",
        "458": "of having a number they just tells you\n",
        "461": "how well is your classifier doing but by\n",
        "463": "switching to the precision recall metric\n",
        "465": "we've actually lost that we now have two\n",
        "468": "real numbers and so we often may end up\n",
        "471": "facing situations like if we try to\n",
        "472": "compare algorithm one in algorithm to\n",
        "473": "compare algorithm one in algorithm to\n",
        "474": "you know we have them asking ourselves\n",
        "477": "as a precision of 0.5 and recall point\n",
        "478": "four well is that better or worse than\n",
        "480": "the position of point seven and recall\n",
        "483": "of 0.1 and if every time you try out a\n",
        "485": "new algorithm you end up having to sit\n",
        "487": "around and think well maybe point five\n",
        "489": "point four is better than point seven\n",
        "491": "point one or maybe known I don't know if\n",
        "492": "you end up having to sit around and\n",
        "494": "think and make these decisions that\n",
        "496": "really slows down your decision-making\n",
        "499": "process for what thinks what changes are\n",
        "501": "useful to incorporate into your\n",
        "504": "algorithm whereas in contrast if we have\n",
        "506": "a single real number evaluation metric\n",
        "509": "like a number that just tells us is\n",
        "511": "everyone or is algorithm together\n",
        "513": "that that helps us to much more quickly\n",
        "515": "decide which algorithm to go with and\n",
        "517": "house us as well to much more quickly\n",
        "519": "evaluate different changes that we may\n",
        "520": "evaluate different changes that we may\n",
        "522": "be contemplating for an algorithm so how\n",
        "524": "can we get a single real number\n",
        "528": "evaluation metric one natural thing that\n",
        "530": "you might try is to look at the average\n",
        "533": "between precision and recall so using P\n",
        "534": "and R to denote precision recall one\n",
        "536": "thing could do is just compute the\n",
        "537": "average and look at what classifier has\n",
        "541": "the highest average value but this turns\n",
        "543": "out not to be such a good solution\n",
        "546": "because similar to the example we had\n",
        "549": "earlier it turns out that if we have a\n",
        "551": "classifier that predicts y equals 1 all\n",
        "554": "the time then if you do that you can get\n",
        "557": "a very high recall but you end up with a\n",
        "560": "very low value of precision conversely\n",
        "563": "if you have a classifier that predicts y\n",
        "566": "equals 0 almost all the time that is if\n",
        "568": "it predicts y equals 1 very sparingly\n",
        "571": "this corresponds to setting a very high\n",
        "573": "threshold using the notation of the\n",
        "575": "previous line then you can actually end\n",
        "577": "up with a very hard position to the very\n",
        "580": "low recall so the two extremes of either\n",
        "582": "a very high threshold or a very low\n",
        "585": "threshold neither of that would give a\n",
        "586": "particularly good classifier and the way\n",
        "588": "we recognize that is by seeing if we end\n",
        "592": "up with a very low precision or or a\n",
        "594": "very low recall and if you just take the\n",
        "597": "average of people as are our two ones\n",
        "600": "this example the average is actually\n",
        "602": "highest for however three even though\n",
        "604": "you can get that sort of performance by\n",
        "606": "protecting y equals 1 all the time and\n",
        "608": "that's just not a very good classifier\n",
        "609": "right if you predict y equals 1 all the\n",
        "612": "time it's just not a useful cause 5 it\n",
        "614": "if all does this prints out y equals 1\n",
        "618": "and so every one or algorithm 2 would be\n",
        "620": "more useful than other than 3 but in\n",
        "623": "this example algorithm 3 has a higher\n",
        "626": "average value of precision recall than\n",
        "629": "algorithm 1 and 2 so we usually think of\n",
        "632": "this average of position of Iko as not a\n",
        "635": "particularly good way to evaluate our\n",
        "637": "learning\n",
        "640": "in contrast just a different way if a\n",
        "642": "combining precision and we call is\n",
        "645": "called the F score and uses that formula\n",
        "648": "and so in this example here are the F\n",
        "651": "scores and so we would tell from from\n",
        "653": "these F schools and looks like everyone\n",
        "656": "has the highest F score out of 200\n",
        "657": "second highest in thousand three has the\n",
        "660": "lowest and so you know if we go by the F\n",
        "662": "scope we would pick per the album one\n",
        "666": "over the others the F score which is\n",
        "668": "also called the f1 score is usually\n",
        "669": "written F one school that I have here\n",
        "672": "but often people will just say F scores\n",
        "674": "you needn't either term us use is a\n",
        "676": "little bit like taking the average of\n",
        "679": "precision V call but to gives the lower\n",
        "681": "value of precision in V column whichever\n",
        "684": "it is it gives it a higher weight and so\n",
        "687": "you see in the numerator here that the F\n",
        "690": "score takes a product or position and we\n",
        "692": "call and so if either precision is zero\n",
        "694": "or recall is equal to zero the F school\n",
        "696": "will be equal to zero so in that sense\n",
        "699": "it kind of combines precision recall but\n",
        "701": "you know for the F school to be large\n",
        "703": "both precision and V code have to be\n",
        "705": "pretty large I should say that there are\n",
        "708": "many different possible formulas for\n",
        "710": "combining precision we call this F score\n",
        "713": "formula is really maybe a just one out\n",
        "715": "of a much larger number of possibilities\n",
        "718": "but historically our traditionally this\n",
        "719": "is what people in machine learning seem\n",
        "723": "to use and the term F score you know it\n",
        "725": "doesn't really mean anything so don't\n",
        "727": "worry about why it's called F score or\n",
        "731": "f1 score but this usually gives you the\n",
        "733": "effect that you want because if either a\n",
        "735": "precision the zero or we call a zero\n",
        "738": "this gives you a very low F score and so\n",
        "739": "to have a high F score\n",
        "741": "you kind of need a prisoner we call to\n",
        "745": "be one and concretely um if P equals 0\n",
        "748": "or R equals zero then this gives you\n",
        "750": "that the F score\n",
        "755": "equals zero whereas a perfect f school\n",
        "758": "so if precision equals one and we call\n",
        "761": "equals one that would give you an F\n",
        "765": "score that's equal to one times one over\n",
        "767": "two times two so the F school will be\n",
        "769": "equal to one if you have perfect\n",
        "770": "precision and perfect recall\n",
        "772": "and intermediate values between zero and\n",
        "774": "one you know this usually gives a\n",
        "777": "reasonable back ordering of different\n",
        "779": "classifiers\n",
        "781": "so in this video we talked about the\n",
        "784": "notion of trading off between precision\n",
        "787": "and recall and how we can vary the\n",
        "789": "threshold that we use to decide whether\n",
        "792": "to predict y equals 1 or y equals 0 this\n",
        "793": "is a special and it says you know that\n",
        "796": "we need to be at least by 70% confident\n",
        "799": "or 90% confident or whatever before we\n",
        "801": "predict y equals 1 and by varying the\n",
        "802": "predict y equals 1 and by varying the\n",
        "803": "threshold you can control a trade-off\n",
        "806": "between precision and recall we also\n",
        "808": "talked about the F score which takes\n",
        "810": "precision and recall and again gives you\n",
        "812": "a single real number evaluation metric\n",
        "814": "and of course if you go is to\n",
        "817": "automatically set that threshold to\n",
        "819": "decide what to many y equals 1 and y\n",
        "822": "equals 0 one pretty means no way to do\n",
        "825": "that would also be to try a range of\n",
        "827": "different values of threshold so to try\n",
        "829": "a range of values of threshold and\n",
        "831": "evaluate these different thresholds on\n",
        "834": "stereo cross-validation set and then to\n",
        "836": "pick whatever value of threshold gives\n",
        "838": "you the highest F score on your cross\n",
        "840": "validation set that'd be a pretty\n",
        "842": "reasonable way to automatically choose\n",
        "844": "the threshold for your classifier as\n"
    },
    "W9iWNJNFzQI": {
        "0": " \n",
        "2": "for many learning algorithms among the\n",
        "4": "linear regression logistic regression\n",
        "6": "and neural networks the way we derive\n",
        "8": "the algorithm was by coming up with a\n",
        "10": "cost function or coming up with an\n",
        "13": "optimization objective and then using an\n",
        "14": "algorithm like gradient descent to\n",
        "17": "minimize that cost function when you\n",
        "19": "have a very large training set gradient\n",
        "21": "descent becomes a computationally very\n",
        "23": "expensive procedure in this video we'll\n",
        "26": "talk about a modification to the basic\n",
        "27": "gradient descent algorithm called\n",
        "29": "stochastic gradient descent which will\n",
        "31": "allow us to scale these algorithms to\n",
        "38": "much bigger training sets suppose you\n",
        "39": "are training a linear regression model\n",
        "42": "using gradient descent as the quick\n",
        "45": "recap the hypothesis will look like this\n",
        "47": "and the cost function will look like\n",
        "49": "this which is this sort of one-half of\n",
        "52": "the average squared error of your\n",
        "53": "hypothesis on your M training examples\n",
        "57": "and the cost function we've already seen\n",
        "58": "looks like this sort of bow shape\n",
        "60": "functions to plot it as a function of\n",
        "63": "the parameters theta0 and theta1 the\n",
        "65": "cost function J is the sort of a bowl\n",
        "67": "shape function and gradient descent\n",
        "70": "looks like this where in the inner loop\n",
        "72": "of gradient descent you repeatedly\n",
        "74": "update the parameters theta using that\n",
        "78": "expression now in the rest of this video\n",
        "80": "I'm going to keep using linear\n",
        "82": "regression as the running example but\n",
        "84": "the idea is here the idea of stochastic\n",
        "87": "gradient descent is fully general and\n",
        "88": "also applies to other learning\n",
        "90": "algorithms like logistic regression in\n",
        "92": "neural networks and other algorithms\n",
        "94": "that are based on training gradient\n",
        "98": "descent on the specific training set so\n",
        "99": "here's a picture of what gradient\n",
        "101": "descent does if the parameters are\n",
        "104": "initialized at a point there then as you\n",
        "105": "run gradient descent different\n",
        "107": "iterations of gradient descent will take\n",
        "109": "the parameters to the global minimum so\n",
        "111": "take a trajectory that looks like that\n",
        "114": "and heads pretty directly to the Google\n",
        "117": "now the problem with gradient descent is\n",
        "121": "that if M is large then computing this\n",
        "124": "derivative term can be very expensive\n",
        "127": "because this requires summing over all M\n",
        "133": "examples so if M is 300 million so in\n",
        "135": "the United States there are about 300\n",
        "136": "million people and so the US would be\n",
        "139": "united states census data we have on the\n",
        "141": "order of that many records so if you\n",
        "142": "want to fit the linear regression model\n",
        "145": "to that then you need to sum over 300\n",
        "147": "million records and that's very\n",
        "149": "expensive to give the algorithm a name\n",
        "151": "this particular version of gradient\n",
        "154": "descent is also called batch gradient\n",
        "158": "descent and the term that refers to the\n",
        "159": "fact that we're looking at all of the\n",
        "161": "training examples at the time the cause\n",
        "163": "of a batch of all of the training\n",
        "166": "examples it really isn't maybe the best\n",
        "168": "thing but this is one machine learning\n",
        "170": "people call this particular version of\n",
        "172": "being in the center and if you imagine\n",
        "174": "really that you know you have 300\n",
        "176": "million census records stored away on\n",
        "179": "disk the way those album works is you\n",
        "181": "will need to read into your computer\n",
        "184": "memory all 300 million records in order\n",
        "186": "to compute this derivative term you need\n",
        "188": "to stream all of these records through\n",
        "190": "computer because you can't store all\n",
        "191": "your records and computer memories you\n",
        "193": "need to be through them and slowly you\n",
        "195": "know accumulate the sum in order to\n",
        "197": "compute the derivative and then having\n",
        "199": "done all that work that allows you to\n",
        "201": "take one step of gradient descent and\n",
        "204": "now you need to do the whole thing again\n",
        "206": "you know scan through all 300 million\n",
        "209": "records accumulate these sums and having\n",
        "211": "done all that work you can take another\n",
        "212": "little step using getting this end and\n",
        "214": "then do that again and then as you take\n",
        "215": "then do that again and then as you take\n",
        "217": "you had a third step and so on and so\n",
        "219": "it's gonna take a long time in order to\n",
        "221": "get the Apple to converge in contrast to\n",
        "223": "batch gradient descent what we're going\n",
        "224": "to do is come up with a different\n",
        "226": "algorithm that doesn't need to look at\n",
        "229": "all of the training examples you know in\n",
        "231": "every single iteration but that needs to\n",
        "233": "look at only a single training example\n",
        "235": "one iteration before moving on to the\n",
        "237": "new algorithm here's just a batch\n",
        "239": "gradient descent album written on the\n",
        "241": "game with that being the cost function\n",
        "244": "and that being the update and of course\n",
        "247": "this term here that's used in the\n",
        "249": "gradient descent rule that is the\n",
        "252": "partial derivative with respect to the\n",
        "255": "parameter theta J of our optimization\n",
        "258": "objective J train of theta now let's\n",
        "260": "look at the more efficient algorithm\n",
        "263": "that scales better to largely assess in\n",
        "265": "order to work out the algorithms cost\n",
        "267": "the Casagrande sent this write out the\n",
        "268": "cost function in a slightly different\n",
        "271": "way when I defined the cost of a\n",
        "273": "parameter theta with respect to a\n",
        "277": "training example X I comma Y to be equal\n",
        "280": "to 1/2 times the squared error that my\n",
        "283": "hypothesis incurs on that example exact\n",
        "286": "on the Y I so this cost function term\n",
        "288": "really measures how well is my\n",
        "291": "hypothesis doing on a single example X I\n",
        "295": "comma Y I now you notice that the\n",
        "298": "overall cost function J train can now be\n",
        "301": "written in this equivalent form so J\n",
        "304": "train is just the average over my M\n",
        "306": "training examples of the cost of my\n",
        "309": "hypothesis on that example X I comma Y\n",
        "311": "light island put this view of the cost\n",
        "314": "function for linear regression let me\n",
        "316": "now write out what stochastic gradient\n",
        "319": "descent does the first step of\n",
        "322": "stochastic gradient descent is to\n",
        "327": "randomly shuffle the data set so by that\n",
        "329": "I just mean randomly shuffle or randomly\n",
        "332": "reorder your M training examples\n",
        "334": "sort of a standard pre-processing step\n",
        "337": "come back to this in a minute\n",
        "339": "but the main work of stochastic gradient\n",
        "342": "descent is then done in the following\n",
        "346": "we're going to repeat for I equals 1\n",
        "349": "through m so repeatedly scan through my\n",
        "352": "training examples and perform the\n",
        "353": "following update to update the parameter\n",
        "358": "theta J as theta J minus alpha times H\n",
        "366": "of X I minus y I times thanks I okay and\n",
        "370": "we're going to do this update as usual\n",
        "374": "for all values of J now you notice that\n",
        "381": "this term over here this is exactly what\n",
        "383": "we have inside the summation for batch\n",
        "386": "gradient descent in fact but they'll see\n",
        "387": "you there are familiar of calculus as\n",
        "390": "possible to show that that term here is\n",
        "393": "this term here is equal to the partial\n",
        "396": "derivative respect my parameter theta J\n",
        "401": "of the cost of the parameters theta on X\n",
        "404": "I comma Y only where cost is of course\n",
        "406": "this thing that was defined previously\n",
        "410": "and just the rap of the algorithm let me\n",
        "413": "close my curly braces over there so\n",
        "415": "what's the cost of gradient descent is\n",
        "418": "doing is is actually scanning through\n",
        "420": "the training examples and first it's\n",
        "421": "going to look at my first training\n",
        "425": "example x1 comma y1 and then looking at\n",
        "427": "only this first example is going to take\n",
        "429": "like they see a little gradient descent\n",
        "432": "step with respect to the cost of justice\n",
        "434": "first training example so in other words\n",
        "435": "it's going to look at the first example\n",
        "438": "and modify the parameters a little bit\n",
        "440": "to fit just the first training example a\n",
        "442": "little bit better having done this\n",
        "445": "inside this in the for loop is then\n",
        "447": "going to go on to the second training\n",
        "448": "example\n",
        "451": "and what is going to do there is take\n",
        "453": "another little step in parameter space\n",
        "455": "so modify the parameters just a little\n",
        "457": "bit to try to fit just the second\n",
        "459": "training example a little bit better\n",
        "462": "having done that is then going to go on\n",
        "466": "to my third training example and modify\n",
        "468": "the parameters to fit to try to fit just\n",
        "470": "the third training example a little bit\n",
        "471": "better and so on\n",
        "473": "until you get through the entire\n",
        "476": "training set and then this outer repeat\n",
        "478": "loop may cause it to take multiple\n",
        "481": "passes over the entire training set this\n",
        "483": "view of stochastic gradient descent also\n",
        "485": "motivates why we wanted to start by\n",
        "487": "randomly shuffling the data set this\n",
        "489": "just ensures that when we scan through\n",
        "491": "the training set here that we end up\n",
        "493": "visiting the training examples and some\n",
        "495": "sort of randomly sorted order depending\n",
        "497": "on whether your data already came\n",
        "499": "randomly sorted or whether it came\n",
        "501": "originally sorted in some strange order\n",
        "503": "in practice this would just speed up the\n",
        "505": "convergence is the cost of gradient\n",
        "506": "descent just a little bit so in the\n",
        "508": "interest of safety is usually better to\n",
        "510": "randomly shuffle the data set if you\n",
        "511": "aren't sure if it came to you in the\n",
        "514": "randomly sorted order or not but more\n",
        "516": "importantly another view of stochastic\n",
        "518": "you in descent is that is a lot like\n",
        "520": "batch gradient descent they rather than\n",
        "523": "waiting to sum up these gradient terms\n",
        "526": "over all every training examples what\n",
        "527": "we're doing is we're taking this\n",
        "529": "gradient term using just one single\n",
        "531": "training example and we're starting to\n",
        "533": "make progress in improving the\n",
        "535": "parameters already so rather than you\n",
        "537": "know waiting till we've taken apart\n",
        "541": "through all 300,000 united states census\n",
        "543": "records say rather than needing to scan\n",
        "545": "through all of the training examples\n",
        "547": "before we can modify the parameters a\n",
        "549": "little bit and make progress towards a\n",
        "551": "global minimum for stochastic gradient\n",
        "553": "descent instead we just need to look at\n",
        "555": "a single training example and we'll\n",
        "557": "already start making progress in the\n",
        "560": "space of parameters to of moving the\n",
        "562": "parameters to us the global minima\n",
        "564": "so here's the algorithm written I'll\n",
        "566": "begin where the first step is the\n",
        "568": "randomly shuffle the data and the second\n",
        "569": "step is where the real work is done\n",
        "571": "where that's the update with respect to\n",
        "574": "a single training example X I comma Y I\n",
        "579": "so let's see what this algorithm does\n",
        "581": "the parameters previously we saw that\n",
        "583": "when we're using batch gradient descent\n",
        "585": "that is the algorithm that looks at all\n",
        "586": "the training examples of the time factor\n",
        "588": "in descent would tend to you know take a\n",
        "591": "reasonably straight line trajectory to\n",
        "594": "get to the global minimum like that in\n",
        "597": "contrast with stochastic gradient\n",
        "599": "descent every innovation is going to be\n",
        "601": "much faster because we don't need to sum\n",
        "602": "up over all the trainings of the\n",
        "605": "examples but every innovation is just\n",
        "606": "trying to fit a single training example\n",
        "609": "better so if we were to start stochastic\n",
        "610": "gradient descent\n",
        "612": "oh let's pass the cross between descend\n",
        "615": "at a point like that the first iteration\n",
        "618": "you know may take the parameters in that\n",
        "620": "direction and maybe the second iteration\n",
        "622": "looking at just a second example may be\n",
        "624": "just by chance we get a little unlucky\n",
        "627": "and actually head in a bad direction and\n",
        "629": "with two prongs like that in the third\n",
        "631": "iteration where we try to modify the\n",
        "632": "parameters to fit just the third\n",
        "634": "training examples better maybe we'll end\n",
        "636": "up heading in that direction and then we\n",
        "637": "look at the fourth training example and\n",
        "640": "we will do that the fifth example six\n",
        "642": "example seven and so on\n",
        "644": "and as you run the stochastic gradient\n",
        "647": "descent what you find is that it will\n",
        "650": "generally move the parameters in the\n",
        "652": "direction of the global minimum but not\n",
        "656": "always and so take a some more random\n",
        "659": "looking and circuited path towards the\n",
        "662": "global minimum and in fact as he runs to\n",
        "664": "Casa Grande descent it doesn't actually\n",
        "666": "converge in the same sense as batch\n",
        "668": "gradient descent us and what events are\n",
        "670": "doing is wandering around continuously\n",
        "673": "in some region that's in some region\n",
        "675": "close to the global minimum but it\n",
        "676": "doesn't actually just get to the global\n",
        "677": "doesn't actually just get to the global\n",
        "679": "and stay there but the practice this\n",
        "681": "isn't a problem because you know so long\n",
        "684": "as the parameters end up in some region\n",
        "685": "there maybe it is pretty close to the\n",
        "688": "global minimum so lost parameters ends\n",
        "689": "up pretty close to the global minimum\n",
        "692": "that will be a pretty good hypothesis\n",
        "695": "and so usually running so costly in this\n",
        "698": "end we get the parameter near the global\n",
        "700": "minimum and that's good enough but you\n",
        "702": "know I'm always actually in most\n",
        "705": "practical purposes just one final detail\n",
        "707": "in stochastic drain descent we had this\n",
        "709": "outer loop repeat which says to do this\n",
        "712": "in a loop multiple times so how many\n",
        "713": "times do we repeat this outer loop\n",
        "715": "depending on the size of the training\n",
        "718": "set doing this just a single time may be\n",
        "721": "enough and up to you know maybe 10 times\n",
        "722": "maybe typical so you may end up\n",
        "724": "repeating this inner loop anywhere from\n",
        "728": "once to 10 times so if you have a you\n",
        "730": "know a truly massive data set like this\n",
        "732": "u.s. census data set example that I've\n",
        "735": "been talking about with 300 Minneapolis\n",
        "737": "it is possible that by the time you've\n",
        "738": "taken just a single pass through your\n",
        "742": "training set for I equals 1 through 300\n",
        "744": "million is possible but by the time you\n",
        "745": "take on a single pass for your dataset\n",
        "748": "you might already have a perfectly good\n",
        "751": "hypothesis in which case you know this\n",
        "754": "inner loop you might need to do only\n",
        "757": "once if M is very very large but in\n",
        "759": "general taking anywhere from 1 through\n",
        "761": "10 pulses through your data set you know\n",
        "763": "may be fairly common but they're really\n",
        "764": "it depends on the size of your training\n",
        "765": "it depends on the size of your training\n",
        "768": "set and if you contrast this to batch\n",
        "770": "gradient descent where battery and\n",
        "772": "descend after taking a pass through your\n",
        "774": "entire training set you would have taken\n",
        "777": "just one single gradient descent step so\n",
        "778": "one of these little baby steps of\n",
        "780": "gradient descent where you just take one\n",
        "781": "gradient descent where you just take one\n",
        "783": "small gradient to sensor and this is why\n",
        "787": "so Casagrande descent can be much faster\n",
        "789": "so that was the stochastic gradient\n",
        "791": "descent algorithm and if you implement\n",
        "793": "it hopefully that will allow you to\n",
        "795": "scale what many of your learning\n",
        "797": "algorithms do much bigger data sets and\n"
    },
    "XfyR_49hfi8": {
        "0": " \n",
        "2": "in the last video we started to talk\n",
        "4": "about the kernels idea and how it can be\n",
        "6": "used to define new features for support\n",
        "8": "vector machine in this video I'd like to\n",
        "10": "fill in some of the missing details and\n",
        "13": "also say a few words about how to use\n",
        "15": "these ideas in practice such as how they\n",
        "18": "pertain to for example the bias-variance\n",
        "22": "tradeoff in support vector machines in\n",
        "24": "the last video I talked about the\n",
        "26": "process of picking a few landmarks in\n",
        "29": "your l1 l2 l3 and that allowed us to\n",
        "31": "define the similarity function also\n",
        "34": "called a kernel or in this example if\n",
        "36": "you have this particular similarity\n",
        "37": "function this is a Gaussian kernel and\n",
        "38": "function this is a Gaussian kernel and\n",
        "41": "that allowed us to build this form of a\n",
        "43": "hypothesis function but where do we get\n",
        "45": "these landmarks from where do we get l1\n",
        "48": "l2 l3 from and it seems also that for\n",
        "50": "complex learning problems maybe we want\n",
        "52": "a lot more landmarks than just three of\n",
        "55": "them that we might choose my hand so in\n",
        "57": "practice this is how the landmarks are\n",
        "60": "chosen which is that given a machine\n",
        "61": "learning problem we have some data set\n",
        "63": "of some positive and negative examples\n",
        "65": "so here's your year which is that we're\n",
        "68": "going to take the examples and for every\n",
        "70": "training example that we have we're just\n",
        "74": "going to call it we're just going to put\n",
        "76": "landmarks as exactly the same locations\n",
        "79": "as the training examples so if I have\n",
        "83": "one training example if that's x1 well\n",
        "84": "then I'm going to choose my first\n",
        "86": "landmark to be at exactly the same\n",
        "88": "location as my first training example\n",
        "90": "and if I have a different training\n",
        "94": "example x2 well we will set X the second\n",
        "96": "landmark to be the location of my second\n",
        "99": "training example on the finger on the\n",
        "101": "right I used red and blue dots just to\n",
        "103": "illustration the color of this figure\n",
        "105": "the color of the dots on the figure on\n",
        "107": "the right is not significant but what\n",
        "108": "I'm going to end up with using this\n",
        "111": "method is I'm going to end up with M\n",
        "116": "landmarks of l1 l2 down to M if I have M\n",
        "119": "training examples with one landmark per\n",
        "120": "location\n",
        "123": "my per location of each of my training\n",
        "125": "examples and this is nice because the\n",
        "127": "saying that my features are basically\n",
        "130": "going to measure how close an example is\n",
        "131": "to one of the things I saw in my\n",
        "132": "to one of the things I saw in my\n",
        "134": "training set so just to write this out a\n",
        "135": "little more concretely\n",
        "138": "given M training examples I'm going to\n",
        "140": "choose the location of my landmarks to\n",
        "143": "be exactly the locations of my M\n",
        "146": "training examples when you're given an\n",
        "148": "example X and this example X can be\n",
        "149": "something in the training set it can be\n",
        "151": "something in the cross-validation set up\n",
        "153": "can be something the test set given an\n",
        "155": "example X I'm going to compute you know\n",
        "158": "these features as so f1 f2 and so on\n",
        "161": "where L where L 1 is actually equal to\n",
        "165": "x1 and so on and these then give me a\n",
        "166": "feature vector\n",
        "169": "so let me write f as the feature vector\n",
        "172": "so I'm going to take these f1 f2 and so\n",
        "175": "on and just group them into the feature\n",
        "180": "vector that goes down to FM and you're\n",
        "182": "just by a convention if we want we can\n",
        "185": "add an extra feature f0 which is always\n",
        "187": "equal to 1 so this plays a role similar\n",
        "190": "to what we had previously for x0 which\n",
        "194": "one's our intercept term so for example\n",
        "197": "if we have a training example X I comma\n",
        "199": "Y I the features we will compute for\n",
        "202": "this training example will be as follows\n",
        "205": "given X i we will then map it to you\n",
        "209": "know f1 I which is the similarity I'm\n",
        "212": "going to abbreviate it as si M instead\n",
        "213": "of writing out the whole word similarity\n",
        "219": "right and f2 I equals the similarity\n",
        "227": "between X and L 2 and so on down to FM I\n",
        "233": "equals the similarity between X I\n",
        "238": "and l/m and somewhere in the middle\n",
        "241": "somewhere in this list you know at the I\n",
        "245": "component I will actually have one\n",
        "248": "feature component which is F subscript I\n",
        "251": "I which is going to be the similarity\n",
        "257": "between X and Li where Li is equal to X\n",
        "260": "I and so you know fi I is just going to\n",
        "262": "be the similarity between X and itself\n",
        "264": "and if you're using the Gaussian kernel\n",
        "266": "this is essentially e to the minus 0\n",
        "267": "this is essentially e to the minus 0\n",
        "268": "over 2 Sigma squared so this will be\n",
        "270": "equal to 1 that's ok so one of my\n",
        "272": "features for this training example is\n",
        "274": "going to be equal to 1 and that's\n",
        "276": "similar to what I had above I can take\n",
        "279": "all of these M features and group them\n",
        "280": "into a feature vector so instead of\n",
        "283": "representing my example using you know X\n",
        "286": "I which is what RN or RN plus 1\n",
        "289": "dimensional vector dependent value\n",
        "292": "County in September Z to RN to RN plus 1\n",
        "295": "we can now instead represent my training\n",
        "298": "example using this feature vector F I'm\n",
        "300": "going to write this F superscript I\n",
        "303": "which is going to be taking all of these\n",
        "306": "things and stacking them into the\n",
        "313": "vectors at F 1 I down to F M I and if\n",
        "315": "you want and what usually will also add\n",
        "319": "this F 0 I where FC Rho I is equal to 1\n",
        "323": "and so this vector here gives me my new\n",
        "325": "feature vector with which to represent\n",
        "330": "my training example so given these\n",
        "333": "kernels and similarity functions here's\n",
        "335": "how we use a support vector machine if\n",
        "337": "you already have a lower set of\n",
        "339": "parameters theta then if you given a\n",
        "341": "value of x and you want to make a\n",
        "342": "prediction what we do is we compute the\n",
        "345": "features F which is now a our M plus 1\n",
        "349": "dimensional feature vector and we have M\n",
        "352": "here because we have M training examples\n",
        "357": "and thus M landmarks and what we do is\n",
        "360": "we predict 1 if theta transpose F is\n",
        "362": "greater than equal to 0\n",
        "363": "right so theta transpose F of course\n",
        "366": "that's just equal to theta 0 F 0 plus\n",
        "372": "theta 1 f1 plus dot dot plus theta M F M\n",
        "376": "and so my parameter vector theta is also\n",
        "378": "now going to be an ever plus 1\n",
        "382": "dimensional vector and we have M here\n",
        "384": "because right the number of landmarks is\n",
        "387": "equal to the training set size so M was\n",
        "389": "the training set size and now the\n",
        "391": "parameter vector theta is going to be M\n",
        "393": "plus 1 dimensional so that's how you\n",
        "395": "make a prediction if you already have a\n",
        "398": "setting for the parameters theta how do\n",
        "400": "you get the parameters theta well you do\n",
        "402": "that using the SVM learning algorithm\n",
        "405": "and specifically what you do is you\n",
        "407": "would solve this minimization problem\n",
        "409": "you minimize the parameters theta of C\n",
        "411": "times this cost function which you had\n",
        "414": "before only now instead of them looking\n",
        "417": "there instead of making predictions\n",
        "421": "using theta transpose X I using our\n",
        "423": "original features X I instead we've\n",
        "425": "taken the features X I and replace them\n",
        "427": "with the new features so we're using\n",
        "431": "theta transpose F I to make a prediction\n",
        "434": "on the training example you see that in\n",
        "437": "both places here and this by solving\n",
        "439": "this minimization problem that you get\n",
        "442": "the parameters for your support vector\n",
        "445": "machine and one last detail is because\n",
        "449": "for this optimization problem we really\n",
        "454": "have N equals M features that is here\n",
        "457": "the number of features we have in really\n",
        "458": "the effective number of features we have\n",
        "461": "is a dimension of F so that n is\n",
        "463": "actually going to be equal to M so if\n",
        "465": "you want to you can think of this as sum\n",
        "468": "this this really is a sum from J equals\n",
        "470": "1 through m and then one way to think\n",
        "472": "about this is you can think of it as n\n",
        "475": "being equal to M because\n",
        "478": "if F is a new features that we have you\n",
        "481": "know n plus one features with the plus\n",
        "485": "one coming from the Interceptor and here\n",
        "488": "we still do some from J equals 1 through\n",
        "491": "m because similar to our earlier videos\n",
        "493": "on regularization we still do not\n",
        "495": "regular eyes the parameter theta 0 which\n",
        "497": "is why this is sum from J equals 1\n",
        "500": "through m instead of J equals 0 through\n",
        "503": "n so that's the support vector machine\n",
        "505": "learning algorithm that's one sort of\n",
        "508": "mathematical detail aside that I should\n",
        "510": "mention which is that in the way a\n",
        "512": "support vector machine is implemented\n",
        "514": "this last term is that he done a little\n",
        "516": "bit differently so you don't really need\n",
        "518": "to know about this last detail in order\n",
        "519": "to use support vector machines and in\n",
        "520": "to use support vector machines and in\n",
        "522": "fact the equation that written down here\n",
        "523": "should give you all the intuitions that\n",
        "526": "you need but in the way a support vector\n",
        "527": "machine is implemented you know that\n",
        "531": "term the sum of a J of theta J squared\n",
        "534": "right another way to write this is this\n",
        "538": "is a can be written as theta transpose\n",
        "541": "theta if we ignore the parameter theta 0\n",
        "545": "so if theta were you know theta 1 down\n",
        "552": "to theta M ignoring theta 0 then this\n",
        "555": "sum sum from sum will be J of theta J\n",
        "556": "squared that this can also be written\n",
        "560": "theta transpose theta and what most\n",
        "562": "support vector machine implementations\n",
        "564": "do is actually replace this theta\n",
        "567": "transpose theta would instead theta a\n",
        "570": "transpose times some matrix inside that\n",
        "571": "depends on the kernel you use times\n",
        "574": "theta and so this gives us a slightly\n",
        "576": "different distance metric like this was\n",
        "579": "a study different measure of minima\n",
        "581": "instead of minimizing exactly the norm\n",
        "584": "of theta squared instead minimize\n",
        "586": "something slightly similar to it that's\n",
        "587": "like a rescale version with the\n",
        "589": "parameter vector theta there depends on\n",
        "590": "the\n",
        "592": "but this is kind of a mathematical\n",
        "594": "detail that allows the support vector\n",
        "596": "machine software to run much more\n",
        "599": "efficiently and the reason the support\n",
        "601": "vector machine does this is with with\n",
        "603": "this modification it allows it to scale\n",
        "606": "to much bigger training sets because for\n",
        "609": "example if you have a training set with\n",
        "613": "10,000 training examples then you know\n",
        "614": "the way we define landmarks we end up\n",
        "617": "with 10,000 landmarks and so theta\n",
        "619": "becomes 10,000 dimensional maybe that\n",
        "621": "were but when n becomes really really\n",
        "624": "big than solving for the all of these\n",
        "626": "parameters you have and we're 50,000\n",
        "628": "100,000 then solving for all of these\n",
        "631": "parameters can become expensive for the\n",
        "632": "support vector machine optimization\n",
        "634": "software they're solving the\n",
        "636": "minimization problem do I drew here so\n",
        "638": "kind of as a mathematical detail which\n",
        "639": "again you really don't need to know\n",
        "642": "about it actually modified it actually\n",
        "644": "modifies that last term a little bit to\n",
        "645": "optimize something slightly different\n",
        "648": "than just minimizing the norm squared of\n",
        "651": "theta screw of data but if you want you\n",
        "653": "can think feel free to think of this as\n",
        "655": "kind of an implementational detail that\n",
        "657": "does change the objective a bit but is\n",
        "659": "done primarily for reasons of\n",
        "662": "computationally computational efficiency\n",
        "664": "so usually don't really have to worry\n",
        "668": "about this and by the way if you're\n",
        "670": "wondering in case you're wondering why\n",
        "672": "we don't apply the kernels idea to other\n",
        "674": "algorithms as well languages regression\n",
        "677": "it turns out that if you want you can\n",
        "679": "actually apply the kernels idea and\n",
        "681": "define these sorts of features using\n",
        "682": "landmarks and so on for logistic\n",
        "683": "regression\n",
        "686": "but the computational tricks that apply\n",
        "688": "for support vector machines don't\n",
        "690": "generalize well to other algorithms like\n",
        "693": "logistic regression and so using kernels\n",
        "694": "with logistic regression is going to be\n",
        "696": "very slow whereas because of\n",
        "698": "computational tricks like that embodied\n",
        "700": "and how it multiplies this and the\n",
        "702": "details of how the support vector\n",
        "704": "machine software is implemented support\n",
        "706": "vector machines and kernels tend to go\n",
        "707": "particularly well together\n",
        "709": "whereas logistic regression and kernels\n",
        "711": "you know you can do a person will run\n",
        "713": "very slowly and\n",
        "714": "won't be able to take advantage of\n",
        "716": "advanced optimization techniques that\n",
        "717": "people have figured out for the\n",
        "720": "particular case running a support vector\n",
        "722": "machine with a kernel but all this\n",
        "724": "pertains only to how you actually\n",
        "726": "implement software to minimize the cost\n",
        "729": "function I'll say more about that in the\n",
        "731": "next video but you really don't need to\n",
        "733": "know about how to write software to\n",
        "735": "minimize this cost function because you\n",
        "736": "can find very good off-the-shelf\n",
        "739": "software for doing so and just as you\n",
        "741": "know I wouldn't recommend writing code\n",
        "743": "to invert a matrix or to compute a\n",
        "745": "square root I actually do not recommend\n",
        "747": "writing software to minimize this cost\n",
        "749": "function yourself but instead to use\n",
        "751": "off-the-shelf software packages that\n",
        "753": "people have developed and so those\n",
        "755": "software packages already embody this\n",
        "758": "these numerical optimization tricks and\n",
        "761": "so you don't really have to worry about\n",
        "763": "them but one other thing that is worth\n",
        "766": "knowing about is when you're applying a\n",
        "768": "support vector machine how do you choose\n",
        "770": "the parameters of the support vector\n",
        "772": "machine and the last thing I want to do\n",
        "774": "this video is say a little bit about the\n",
        "776": "bias and variance trade-offs when using\n",
        "778": "a support vector machine when using an\n",
        "780": "SVM one of the things you need to choose\n",
        "784": "is the parameter C which was in\n",
        "786": "optimization objective and you recall\n",
        "790": "that C played a role similar to 1 over\n",
        "791": "lambda where lambda was the\n",
        "793": "regularization parameter we have the\n",
        "796": "logistic regression so if you have a\n",
        "799": "large value of C this corresponds to\n",
        "801": "what we had back in logistic regression\n",
        "804": "of a small value of lambda meaning of\n",
        "806": "not using much regularization and if you\n",
        "808": "do that you tend to have a hypothesis\n",
        "809": "with lower bias and higher variance\n",
        "812": "whereas if you use a smaller value of C\n",
        "814": "that this corresponds to when we're\n",
        "816": "using logistic regression with a large\n",
        "818": "value of lambda and that corresponds to\n",
        "821": "a hypothesis with higher back bias and\n",
        "824": "lower variance and so hypothesis with\n",
        "828": "large C has a higher variance and it's\n",
        "829": "more prone to overfitting\n",
        "832": "whereas hypotheses of small C has higher\n",
        "834": "bias and is thus more prone to\n",
        "836": "underfitting\n",
        "839": "so this parameter C is one of the\n",
        "840": "Frances we need to choose the other one\n",
        "843": "is the parameter Sigma squared which\n",
        "846": "appeared in the Gaussian kernel so if\n",
        "848": "the Gaussian kernel Sigma squared is\n",
        "851": "large then in the similarity function\n",
        "854": "which was this you know e to the minus X\n",
        "857": "minus Landmark right I squared over 2\n",
        "861": "Sigma squared in this 1d example if I\n",
        "864": "have only one feature X 1 if I have a\n",
        "868": "landmark there at that location if Sigma\n",
        "869": "squared is large then you know the\n",
        "871": "Gaussian kernel will tend to fall off\n",
        "874": "relatively slowly and so this will be my\n",
        "877": "feature F I and so this would be a\n",
        "879": "smoother function that varies more\n",
        "881": "smoothly and so this would give me a\n",
        "883": "hypothesis with higher bias and lower\n",
        "885": "variance because the Gaussian kernel\n",
        "887": "that falls off smoothly you tend to get\n",
        "889": "the hypothesis that very slowly or very\n",
        "892": "smoothly as you change the input X\n",
        "895": "whereas in contrast if Sigma squared was\n",
        "898": "small then if that's my landmark given\n",
        "901": "my 1 feature x1 you know my Gaussian\n",
        "904": "kernel my similarity function will vary\n",
        "907": "more abruptly in both cases of PK 1 and\n",
        "910": "so if Sigma squared is small then my\n",
        "913": "feature is very less movies of just a\n",
        "915": "higher slope so higher derivatives here\n",
        "918": "and using this you end up fitting\n",
        "921": "hypotheses with lower bias and you can\n",
        "924": "have higher variance and if you look at\n",
        "925": "this week's program exercise you\n",
        "927": "actually get to play around with some of\n",
        "929": "these ideas yourself and see these\n",
        "930": "effects yourself\n",
        "933": "so that was the support vector machine\n",
        "935": "with kernels algorithm and hopefully\n",
        "938": "this discussion of fighters and variants\n",
        "940": "will give you some sense of how you can\n"
    },
    "YRS-IB3vCow": {
        "0": " \n",
        "2": "in the last video we talked about the\n",
        "4": "multivariate Gaussian distribution and\n",
        "6": "saw some examples of the sorts of\n",
        "9": "distributions you can model as you vary\n",
        "12": "the parameters mu and Sigma in this\n",
        "14": "video let's take those ideas and apply\n",
        "17": "them to develop a different anomaly\n",
        "21": "detection algorithm to recap the\n",
        "23": "multivariate Gaussian distribution of\n",
        "24": "the multivariate normal distribution has\n",
        "27": "two parameters mu and Sigma where mu is\n",
        "32": "an n-dimensional vector and Sigma the\n",
        "35": "covariance matrix is an N by n matrix\n",
        "39": "and here's the formula for the property\n",
        "42": "of X as parametrized by mu and Sigma and\n",
        "45": "as you vary mu and Sigma you can get a\n",
        "47": "range of different distributions like\n",
        "48": "you know these are to be examples that\n",
        "50": "the ones that we saw in the previous\n",
        "53": "video so let's talk about the parameter\n",
        "54": "video so let's talk about the parameter\n",
        "55": "fitting or the parameter estimation\n",
        "58": "problem the question as usual is if I\n",
        "61": "have a set of examples x1 through XM and\n",
        "63": "here are each of these examples is an n\n",
        "65": "dimensional vector and I think my\n",
        "67": "examples come from a multivariate\n",
        "70": "Gaussian distribution how do I try to\n",
        "73": "estimate my parameters mu and Sigma well\n",
        "75": "the standard formulas for estimating\n",
        "78": "them is you sent you to be just the\n",
        "80": "average of your training examples and\n",
        "83": "you set Sigma to be equal to this and\n",
        "84": "this is actually just like the Sigma\n",
        "87": "that we have written out when we were\n",
        "89": "using the PCA or their principal\n",
        "91": "components\n",
        "94": "so you just planted these two formulas\n",
        "96": "and this would give you your estimated\n",
        "98": "parameter mu and your estimated\n",
        "102": "parameter Sigma so given the data set\n",
        "104": "here's how you has been real Sigma let's\n",
        "107": "take this method and just plug it into\n",
        "110": "an anomaly detection algorithm so how do\n",
        "113": "we put all this together to develop an\n",
        "115": "anomaly detection algorithm yes what we\n",
        "118": "do first we take our training set and we\n",
        "121": "fit the model we fit P of x by setting\n",
        "124": "mu and Sigma as described on the\n",
        "128": "previous line next when you're given a\n",
        "131": "new example X so if you're given the\n",
        "133": "test example let's take an earlier\n",
        "135": "example let's say have a new example out\n",
        "138": "here and that's my test example during\n",
        "140": "the new example X we're going to do is\n",
        "144": "compute P of X using this formula for\n",
        "146": "the multivariate Gaussian distribution\n",
        "150": "and then if P of X is very small then we\n",
        "152": "flagged it as an anomaly whereas if P of\n",
        "155": "X is greater than that parameter epsilon\n",
        "158": "that we don't like it as an anomaly so\n",
        "160": "it turns out if you were to fit a\n",
        "161": "multivariate Gaussian distribution to\n",
        "163": "this data set to just a red crosses\n",
        "165": "right not the green example you end up\n",
        "167": "with a Gaussian distribution that places\n",
        "169": "lots of probability in the central\n",
        "172": "region slightly less probability here\n",
        "174": "study less probably here slightly less\n",
        "177": "probably here and very low probability\n",
        "180": "at the point that's way out here\n",
        "183": "and so if you apply the multivariate\n",
        "186": "Gaussian distribution to this example it\n",
        "188": "will actually correctly flag that\n",
        "196": " \n",
        "198": "finally it's worth saying a few words\n",
        "200": "about what is the relationship between\n",
        "202": "the multivariate Gaussian distribution\n",
        "205": "model and the original model where we\n",
        "208": "were modeling P of X as a product of\n",
        "211": "missus P of X 1 P of X 2 up to P of X n\n",
        "214": "it turns out that you can prove\n",
        "216": "mathematically I'm not going to do the\n",
        "217": "proof yet we can prove mathematically\n",
        "219": "that this relationship between the\n",
        "223": "multivariate Gaussian dismiss original\n",
        "225": "one and in particular turns out that the\n",
        "228": "original model corresponds to\n",
        "231": "multivariate gaussians where the\n",
        "233": "contours of the Gaussian are always acts\n",
        "238": "as aligned and so all three of these are\n",
        "241": "examples of Gaussian distributions that\n",
        "243": "you can fit using the original model\n",
        "244": "turns out that that corresponds to\n",
        "247": "multivariate Gaussian where you know the\n",
        "251": "ellipse this year.the the contours of\n",
        "254": "this distribution it turns out that this\n",
        "256": "model actually corresponds to a special\n",
        "259": "case of a multivariate Gaussian\n",
        "261": "distribution and in particular the\n",
        "263": "special case is defined by constraining\n",
        "266": "the distribution of P of X the\n",
        "268": "multivariate Gaussian distribution of P\n",
        "271": "of X so that the contours of the prior\n",
        "273": "density function of the probability\n",
        "275": "distribution function are axis aligned\n",
        "278": "and so you can get you know a P of X\n",
        "281": "with a multivariate Gaussian that looks\n",
        "284": "like this or like this or like this and\n",
        "285": "you notice that in all three of these\n",
        "289": "examples these ellipses are these ovals\n",
        "292": "not drawing have their axes aligns with\n",
        "295": "the x1 x2 axis and what we do not have\n",
        "299": "is a set of contours that I had enough\n",
        "301": "at an angle right and this correspond to\n",
        "304": "examples where a Sigma is equal to one\n",
        "307": "one 0.8 0.8 let's say with nonzero\n",
        "311": "elements on the off diagonals so it\n",
        "313": "turns out that it is possible to show\n",
        "315": "mathematically that this model actually\n",
        "318": "is the same as a multivariate Gaussian\n",
        "321": "distribution but with a constraint and\n",
        "324": "the constraint is that the covariance\n",
        "328": "matrix Sigma must have zeros on the off\n",
        "330": "diagonal elements in particular of the\n",
        "331": "covariance matrix Sigma this thing here\n",
        "334": "it would be sigma squared 1 sigma square\n",
        "338": "2 down 2 Sigma squared n and then\n",
        "341": "everything on the off diagonal entries\n",
        "344": "in all of these elements above and below\n",
        "345": "the diagonal of that matrix\n",
        "348": "all of those are going to be 0 and in\n",
        "350": "fact if you take these values of Sigma\n",
        "353": "Sigma square ones who put two dancers in\n",
        "355": "the screen and and plug them into here\n",
        "357": "and you know plug them into this\n",
        "360": "covariance matrix then the two models\n",
        "362": "are actually identical that is this new\n",
        "365": " \n",
        "367": "using the multivariate Gaussian\n",
        "370": "distribution corresponds exactly to the\n",
        "372": "old model if the covariance matrix Sigma\n",
        "376": "has only zero elements of the diagonals\n",
        "378": "and then pictures that corresponds to\n",
        "381": "having Gaussian distributions where the\n",
        "383": "contours of this distribution function\n",
        "386": "are excess aligned so you aren't allowed\n",
        "388": "to model the correlations between the\n",
        "391": "different features so in that sense the\n",
        "394": "original model is actually a special\n",
        "397": "case of this multivariate Gaussian model\n",
        "400": "so what would you use each of these two\n",
        "401": "models so what would you use the\n",
        "404": "original model or use the multivariate\n",
        "411": " \n",
        "414": "the original model is probably used\n",
        "421": "somewhat more often and and whereas the\n",
        "423": "multivariate Gaussian distribution you\n",
        "425": "see you know some of this but it has the\n",
        "427": "advantage of being able to capture\n",
        "431": "correlations between features so suppose\n",
        "433": "you want to capture anomalies where you\n",
        "434": "have different features say where\n",
        "437": "features x1 x2 take on unusual\n",
        "441": "combinations of values so in the earlier\n",
        "443": "example we had an example where you know\n",
        "446": "the anomaly was with the cpu load and\n",
        "448": "the memory use taking on unusual\n",
        "450": "combinations of values if you want to\n",
        "452": "use the original model to capture that\n",
        "454": "then what you need to do is create an\n",
        "457": "extra feature such as like 3 equals x 1\n",
        "463": "over X to the equals uh maybe the CPU\n",
        "464": "knows\n",
        "467": "you know / the memory uses I think and\n",
        "469": "you need to create extra features if\n",
        "471": "it's unusual combinations of values\n",
        "474": "where x1 and x2 take on an unusual\n",
        "476": "combination of values even though x1 by\n",
        "479": "itself and x2 by and x2 by itself those\n",
        "481": "like sticking a perfectly normal value\n",
        "483": "but if you're willing to spend a time to\n",
        "485": "manually create an extra feature like\n",
        "487": "this then the original model will work\n",
        "491": "fine whereas in contrast the\n",
        "493": "multivariate Gaussian model can\n",
        "495": "automatically capture correlations\n",
        "497": "between different features but the\n",
        "499": "original model has some other\n",
        "503": "minimalistic liftin advantages too and\n",
        "505": "one huge advantage that the original\n",
        "506": "model is that it is computationally\n",
        "509": "cheaper and another view on this is that\n",
        "512": "it scales better to very large values of\n",
        "515": "m to very large numbers of features and\n",
        "520": "so even if n were say 10,000 or even if\n",
        "521": "n when y equals to a hundred thousand\n",
        "524": "the original model will usually work\n",
        "526": "just fine whereas in contrast for the\n",
        "528": "multivariate Gaussian model notice here\n",
        "530": "for example that we need to compute the\n",
        "533": "inverse of the matrix Sigma where Sigma\n",
        "537": "is an N by n matrix and so computing\n",
        "539": "Sigma if you know Sigma is a hundred\n",
        "541": "thousand five hundred thousand matrix\n",
        "542": "that's going to be very computationally\n",
        "544": "expensive and so the multivariate\n",
        "547": "Gaussian model scales less well to wash\n",
        "548": "values of N\n",
        "552": "and finally for the original model it\n",
        "554": "turns out to work okay even if you have\n",
        "556": "a relatively smaller training set this\n",
        "558": "is a small unlabeled examples that we\n",
        "560": "use to model P of X of course it's just\n",
        "562": "what's fine even if M is you know give\n",
        "565": "me about fifty a hundred was fine\n",
        "567": "whereas for the multivariate Gaussian it\n",
        "569": "is sort of a mathematical property of\n",
        "571": "the algorithm that you must have n m\n",
        "573": "greater than n so that the number of\n",
        "575": "examples is greater than the number of\n",
        "578": "features you have and there's a\n",
        "580": "mathematical property of the way we\n",
        "582": "estimate the parameters that if this is\n",
        "584": "not true so if M is less than or equal\n",
        "587": "to n then this matrix isn't even\n",
        "588": "invertible that is this matrix a\n",
        "590": "singular and so you can't even use the\n",
        "592": "multivariate Gaussian model unless you\n",
        "595": "make some changes to it but typical rule\n",
        "598": "of thumb that I use is you know I will\n",
        "600": "use the multivariate Gaussian model only\n",
        "602": "of M is much greater than and so this\n",
        "603": "of M is much greater than and so this\n",
        "605": "saw the narrow mathematical requirement\n",
        "607": "but in practice I would use the\n",
        "609": "multivariate Gaussian model only um were\n",
        "612": "quite a bit bigger than n so if n were\n",
        "614": "greater than or equal to 10 times\n",
        "616": "endless they might be a reasonable row\n",
        "618": "of them and if it doesn't satisfy this\n",
        "620": "then you know the multivariate Gaussian\n",
        "623": "model has a lot of parameters right so\n",
        "626": "this covariance matrix Sigma is an N by\n",
        "627": "n matrix so it has you know roughly n\n",
        "630": "square parameters being because a\n",
        "632": "symmetric matrix is actually 12 32 and\n",
        "635": "squared over 2 parameters but this is a\n",
        "637": "lot of parameters and so you need to\n",
        "639": "make sure you have a fairly large value\n",
        "640": "for M make sure\n",
        "642": "have enough data to fit all these\n",
        "643": "have enough data to fit all these\n",
        "646": "parameters and M greater than or equal\n",
        "649": "to ten and would be a reasonable rule of\n",
        "651": "thumb to make sure that you can estimate\n",
        "653": "this covariance matrix Sigma is nothing\n",
        "657": "more so in practice the original model\n",
        "658": "shown on the left that is used more\n",
        "661": "often and if you suspect that you need\n",
        "662": "to capture correlations between features\n",
        "664": "what people will often do is just you\n",
        "666": "know manually design extra features like\n",
        "669": "these to capture specific unusual\n",
        "672": "combinations of values but in problems\n",
        "674": "where you have a very large training set\n",
        "677": "where m is very large and and yes it's\n",
        "679": "not too large then the multivariate\n",
        "682": "Gaussian model is well worth considering\n",
        "684": "that may work better as well and can\n",
        "686": "save you from having to spend you know\n",
        "688": "spend the time to manually create extra\n",
        "691": "features in case the anomaly is turned\n",
        "694": "out to be captured by unusual\n",
        "696": "combinations of values of the features\n",
        "698": "finally I just want to briefly mention\n",
        "702": "one somewhat technical property but if\n",
        "703": "you fitting multivariate Gaussian model\n",
        "705": "and if you find that the covariance\n",
        "708": "matrix Sigma is singular or found is\n",
        "710": "non-invertible there are usually two\n",
        "713": "cases for this one is if is failing to\n",
        "715": "satisfy this imprinted in name condition\n",
        "718": "and the second case is if you have\n",
        "720": "redundant features so by redundant\n",
        "722": "features I mean if you have two features\n",
        "724": "that are saying somehow you accidentally\n",
        "726": "made two copies in the future so X 1 is\n",
        "728": "just equal to X 2 or if you have\n",
        "730": "redundant features and maybe of like\n",
        "734": "maybe feature X 3 is equal to feature X\n",
        "736": "4 plus feature X 5 ok so if you have\n",
        "738": "highly redundant features like these you\n",
        "740": "know where if I X 3 is equal to X 1 plus\n",
        "743": "X 5 well X 3 doesn't contain any extra\n",
        "744": "information right you just you take\n",
        "745": "information right you just you take\n",
        "748": "we add them together and if you have\n",
        "749": "this sort of redundant features\n",
        "752": "duplicate the features or disorder\n",
        "753": "features then Sigma or maybe\n",
        "756": "non-invertible and so there's a\n",
        "758": "debugging set this very rarely\n",
        "760": "happened so you probably won't run than\n",
        "761": "it is it's very unlikely or to worry\n",
        "764": "about this but in case you implement a\n",
        "766": "multivariate Gaussian model and you find\n",
        "768": "that Sigma is not invertible what I\n",
        "770": "would do is first make sure that M is\n",
        "773": "you know quite a bit bigger than him and\n",
        "775": "if it is then the second thing I do is\n",
        "777": "just check for redundant features and so\n",
        "779": "if you have two features that equal just\n",
        "781": "get rid of one of them or if you have\n",
        "784": "redundant beast IDs x2 equals x plus y\n",
        "785": "just carefully load the redundant\n",
        "788": "feature an initial work fine again as an\n",
        "790": "aside for those of you that are\n",
        "791": "excellence in linear algebra by\n",
        "794": "redundant features what I mean is the\n",
        "796": "formal Turnus features that are in a\n",
        "798": "nearly dependent but in practice you\n",
        "800": "know what that really means is a is one\n",
        "802": "of these problems tripping up the Africa\n",
        "804": "and if you just make your features non\n",
        "806": "redundant that should solve the problem\n",
        "809": "of Sigma being non-invertible but once\n",
        "810": "again the author you're running into\n",
        "812": "this at all are pretty low so what\n",
        "814": "chances are you you can just apply the\n",
        "816": "multivariate Gaussian model without\n",
        "818": "having to worry about Sigma being\n",
        "821": "non-invertible so long as M is great in\n",
        "822": "any way\n",
        "825": "so that's it for anomaly detection with\n",
        "827": "the multivariate Gaussian distribution\n",
        "830": "and if you apply this method you'd be\n",
        "832": "able to have an anomaly detection\n",
        "834": "algorithm that automatically captures\n",
        "836": "positive and negative correlations\n",
        "837": "between your different features and\n",
        "840": "Flags an anomaly ever sees an unusual\n",
        "842": "combination of the values of your\n"
    },
    "YW2b8La2ICo": {
        "0": " \n",
        "2": "in the last couple videos we talked\n",
        "4": "about the ideas of how first if you give\n",
        "6": "it features for movies you can use that\n",
        "9": "to learn parameters data for users and\n",
        "11": "second if you give it parameters for the\n",
        "13": "users you can use that to learn features\n",
        "15": "for the movies in this video we're going\n",
        "17": "to take those ideas and put them\n",
        "19": "together to come up with a collaborative\n",
        "22": "filtering algorithm so one of the things\n",
        "23": "we worked out earlier is that if you\n",
        "25": "have features for the movies then you\n",
        "27": "can solve this minimization problem to\n",
        "30": "find the parameters theta for your users\n",
        "34": "and then we also worked out that if you\n",
        "38": "are given the parameters theta you can\n",
        "40": "also use that to estimate the features X\n",
        "42": "and you can do that by solving this\n",
        "45": "minimization problem so one thing you\n",
        "47": "could do is actually go back and forth\n",
        "49": "you know maybe randomly initialize\n",
        "51": "parameters and then solve for theta it's\n",
        "54": "off X off with a that's all for X but it\n",
        "55": "turns out that there's a more efficient\n",
        "57": "algorithm that it doesn't need to go\n",
        "59": "back and forth between the X's and the\n",
        "62": "Thetas but that can solve for theta and\n",
        "65": "X simultaneously and here this what\n",
        "66": "we're going to do is basically take both\n",
        "69": "of these optimization objectives and put\n",
        "72": "them into the same objective so I'm\n",
        "73": "going to define a new optimization\n",
        "76": "objective J which is a cost function as\n",
        "79": "a function of my features X and a\n",
        "81": "function of my parameters theta it's\n",
        "83": "basically the two optimization\n",
        "84": "objectives that had on top but put\n",
        "88": "together so in order to explain this\n",
        "91": "first I want to point out that this term\n",
        "95": "over here this grant error term is the\n",
        "99": "same as this squared error term and the\n",
        "101": "summations look a little bit different\n",
        "102": "but let's see what the summations are\n",
        "105": "really doing the first summation is sum\n",
        "109": "over all users J and then sum over all\n",
        "112": "movies rated by that user right so this\n",
        "114": "is really something over all pairs IJ\n",
        "117": "that correspond to a movie that was\n",
        "119": "raised about you it's a sum over J says\n",
        "122": "for every user sum over all the movies\n",
        "123": "related by that\n",
        "126": "this summation down here just does\n",
        "128": "things in the opposite order this is for\n",
        "132": "every movie I sum over all the useless J\n",
        "134": "that have rated that movie and so you\n",
        "136": "know these summations both of these are\n",
        "141": "just summations over all pairs IJ for\n",
        "144": "which R of I J is equal to 1 is just\n",
        "148": "summing over you know all the user movie\n",
        "151": "pairs for which you have a rating and so\n",
        "154": "those two terms up there it's just\n",
        "156": "exactly this first term and I've just\n",
        "159": "written a summation here explicitly\n",
        "160": "where I'm just saying you know the sum\n",
        "163": "of all pairs IJ such that R IJ is equal\n",
        "167": "to 1 and so what we're going to do is\n",
        "170": "define a combined optimization objective\n",
        "173": "that we want to minimize in order to\n",
        "176": "solve simultaneously for X and theta and\n",
        "178": "then the other terms in the optimization\n",
        "181": "objective are this which is a\n",
        "183": "regularization in terms of theta and so\n",
        "187": "that came down here and the final piece\n",
        "191": "is this term which is my optimization\n",
        "194": "objective for the XS and that between\n",
        "198": "this and this optimization objective J\n",
        "200": "actually has an interesting property\n",
        "201": "that if you were to hold the X is\n",
        "203": "constant and just minimize with respect\n",
        "206": "to the Thetas then you'd be solving\n",
        "208": "exactly this problem whereas if you were\n",
        "209": "to do the opposite if you were to hold\n",
        "212": "the Thetas constant and minimize J only\n",
        "214": "with respect to the axis then it becomes\n",
        "216": "it closer to this because you know\n",
        "218": "either this term or this term is\n",
        "219": "constant if you're minimizing only\n",
        "222": "respective XS or your respective states\n",
        "225": "so here's an optimization objective that\n",
        "228": "puts together you know my cost functions\n",
        "231": "in terms of X and in terms of theta and\n",
        "234": "in order to come up with what's just one\n",
        "236": "optimization problem what we're going to\n",
        "239": "do is treat this cost function as a\n",
        "242": "function of my features X and of my user\n",
        "245": "per user parameters theta and just\n",
        "247": "minimize this whole thing as a function\n",
        "250": "of both the X's and a function of the\n",
        "252": "Thetas and really the only difference\n",
        "254": "between this and the are older algorithm\n",
        "257": "is that instead of going back and forth\n",
        "258": "you know previously we talked about\n",
        "261": "minimizing respect to theta then\n",
        "262": "minimizing respect to X were minimizing\n",
        "263": "respect to theta\n",
        "266": "minimizing respect to X and so on in\n",
        "268": "this new version instead of sequentially\n",
        "270": "going between the two sets of parameters\n",
        "273": "X and theta what we're going to do is\n",
        "275": "just minimize with respect to both sets\n",
        "280": "of parameters simultaneously finally one\n",
        "283": "last detail is that when we're learning\n",
        "285": "the features this way previously we've\n",
        "289": "have been using this convention that we\n",
        "291": "have a feature x0 equals 1 that\n",
        "294": "corresponds to an intercept term when\n",
        "296": "we're using this sort of formalism where\n",
        "297": "we're actually learning the features\n",
        "299": "we're actually going to do away with\n",
        "302": "this convention and so the features were\n",
        "305": "going to learn X will be in RN right\n",
        "307": "whereas previously we've had features X\n",
        "309": "in RN plus 1 including the intercept\n",
        "312": "term by getting rid of x0 we now have\n",
        "316": "just X in RN and so similarly because\n",
        "318": "the parameters theta is in the same\n",
        "321": "dimension we now also have theta in RN\n",
        "324": "because if there's no a 0 then there's\n",
        "326": "no need for a big parameter theta 0 as\n",
        "329": "well and the reason we do away with this\n",
        "331": "convention is because we're now learning\n",
        "333": "all the features right so that just\n",
        "334": "means that there's no need to hard-code\n",
        "335": "means that there's no need to hard-code\n",
        "336": "the feature there's always equal to one\n",
        "338": "because if you album really wants a\n",
        "340": "feature there's always equal to one they\n",
        "342": "can choose to learn one for itself so if\n",
        "344": "the average chooses it can set the\n",
        "346": "feature x1 equals to 1 and so\n",
        "348": "no need to hard to all the features over\n",
        "350": "one the algorithm now has effects\n",
        "356": "ability to just learn it by itself so\n",
        "358": "putting a being together here's our\n",
        "361": "collaborative filtering algorithm first\n",
        "366": "we're going to initialize X and theta to\n",
        "369": "small random values and this is a little\n",
        "370": "bit like neural network training where\n",
        "373": "there will also initializing all the\n",
        "374": "parameters of a neural network to small\n",
        "378": "random values Knicks were then going to\n",
        "380": "minimize the cost function using\n",
        "382": "gradient descent or one of the or one of\n",
        "384": "the advanced optimization algorithms so\n",
        "386": "if you take derivatives you find that\n",
        "388": "the gradient descent updates or like\n",
        "390": "this and so you know this term here is\n",
        "393": "the partial derivative of the cost\n",
        "396": "function I'm not gonna write that out\n",
        "398": "with respect to the feature value X I K\n",
        "399": "with respect to the feature value X I K\n",
        "402": "and similarly you know this term here is\n",
        "404": "also a partial derivative value of the\n",
        "406": "cost function with respect to the\n",
        "408": "parameter theta that were minimizing and\n",
        "412": "just as a reminder in this formulism we\n",
        "416": "no longer have this X 0 equals 1 and so\n",
        "419": "we have that X is in RN and theta is an\n",
        "422": "RN and in this new formulism we're\n",
        "424": "regularizing every one of our parameter\n",
        "426": "states and every one of our parameters\n",
        "428": "xn there's no longer there's no longer\n",
        "432": "this special case theta 0 which was\n",
        "434": "regular rise differently or which was\n",
        "435": "not regularized compared to at the\n",
        "437": "parameters theta 1 down to theta R\n",
        "440": "anything so there's no longer a theta 0\n",
        "442": "which is why these updates I did not\n",
        "445": "break up the special case for K equals 0\n",
        "447": "so we then use gradient descent to\n",
        "449": "minimize the cost function J with\n",
        "451": "respect to the features XM respective\n",
        "455": "parameters theta and finally given a\n",
        "458": "user if a user has some parameters theta\n",
        "461": "and if there's a movie with some sort of\n",
        "463": "learned features X we would then predict\n",
        "465": "that that movie will be given a star\n",
        "468": "rating by that user of theta transpose J\n",
        "470": "or just the photos in and then we're\n",
        "471": "saying that\n",
        "476": "if user J has not yet rated movie I than\n",
        "478": "what we do is predict that user J is\n",
        "482": "going to rate movie I according to this\n",
        "486": "of theta J transpose X I\n",
        "489": "so that's the collaborative filtering\n",
        "491": "algorithm and if you implement this\n",
        "492": "algorithm you actually get pretty decent\n",
        "495": "algorithm that was simultaneously learnt\n",
        "497": "good features for hopefully all the\n",
        "499": "movies as well as learn parameters for\n",
        "501": "all the users and hopefully give pretty\n",
        "503": "good predictions for how different users\n",
        "505": "will rate different movies that they\n"
    },
    "YovTqTY-PYY": {
        "0": " \n",
        "1": "in the previous video we gave a\n",
        "3": "mathematical definition of gradient\n",
        "6": "descent let's delve deeper and in this\n",
        "8": "video get better intuition about what\n",
        "10": "the algorithm is doing and why the steps\n",
        "11": "of the gradient descent algorithm might\n",
        "16": "make sense here's the gradient descent\n",
        "19": "algorithm that we saw last time and I\n",
        "22": "just remind you this parameter or this\n",
        "24": "term alpha is called the learning rate\n",
        "28": "and it controls how big a step we take\n",
        "30": "when updating my parameter theta J and\n",
        "35": "this second term here is the derivative\n",
        "40": "term and what I want to do in this video\n",
        "43": "is give you better intuition about when\n",
        "44": "each of these two terms doing and why\n",
        "46": "you know when put together this entire\n",
        "51": "update make sense in order to convey\n",
        "53": "these intuitions what I want to do is\n",
        "56": "use a slightly simpler example where we\n",
        "59": "want to minimize a function of just one\n",
        "61": "parameter so so we have say we have a\n",
        "63": "cost function J of just one parameter\n",
        "65": "theta one like we did you know a few\n",
        "68": "videos back when theta one is a real\n",
        "70": "number okay so so just again so we can\n",
        "73": "have 1d plots which are a little bit\n",
        "74": "simpler to look at let's try to\n",
        "76": "understand what gradient descent will do\n",
        "82": "on this function so let's say here's my\n",
        "87": "function J of theta 1 and so that's my\n",
        "92": "and where theta 1 is a real number right\n",
        "95": "now let's say I've initialized gradient\n",
        "99": "descent with theta 1 at this location so\n",
        "102": "imagine that we start off at that point\n",
        "104": "on my function what gradient descent\n",
        "105": "on my function what gradient descent\n",
        "110": "will do is it will update theta 1 gets\n",
        "115": "updated as theta 1 minus alpha times DD\n",
        "120": "theta one J of theta 1\n",
        "124": "and as an aside you know this this\n",
        "129": "derivative term right if you're\n",
        "130": "wondering why I change the notation from\n",
        "133": "these partial derivative symbols if you\n",
        "134": "don't know what the difference is\n",
        "136": "between these partial derivative symbols\n",
        "138": "in the DD theta don't worry about it\n",
        "140": "technically in mathematics we call this\n",
        "142": "a partial derivative we call this a\n",
        "144": "derivative depending on the number of\n",
        "147": "parameters in the function J but that's\n",
        "149": "a mathematical technicality so you know\n",
        "152": "for the purpose of this lecture think of\n",
        "154": "these partial symbols and D D theta one\n",
        "156": "is exactly the same thing and don't\n",
        "157": "worry about whether during the\n",
        "159": "difference is I'm going to try to use\n",
        "162": "the mathematically precise notation but\n",
        "164": "for our purposes these two notations are\n",
        "167": "really the same thing but so let's see\n",
        "169": "what this this equation will do so we're\n",
        "172": "going to compute this derivative an\n",
        "173": "option you've seen derivatives in\n",
        "175": "calculus before but what a derivative at\n",
        "177": "this point does is basically saying you\n",
        "179": "know let's take the tangent to that\n",
        "181": "point like that straight line there a\n",
        "183": "line is just touching this dysfunction\n",
        "186": "and let's look at the slope of this red\n",
        "187": "line that's what the derivative is it\n",
        "190": "says what's the slope of line then is\n",
        "192": "just tangent to the function okay and\n",
        "193": "the slope of a line of course is just\n",
        "196": "right now this height divided by this\n",
        "200": "horizontal thing now this line has a\n",
        "204": "positive slope so it has a positive\n",
        "208": "derivative and so my update to theta is\n",
        "211": "going to be theta one gets updated as\n",
        "215": "theta one minus alpha times some\n",
        "220": "positive number okay alpha the learning\n",
        "223": "rate is always a positive number and so\n",
        "225": "I'm going to take the one gets updated\n",
        "227": "as theta one minus something so I'm\n",
        "229": "going to end up moving theta one to the\n",
        "232": "left with a decrease a the one and we\n",
        "233": "can see this is the right thing to do\n",
        "234": "because I actually want to head in this\n",
        "237": "direction you know to get me closer to\n",
        "241": "the minimum over there so gradient\n",
        "242": "descent so far seems to be doing the\n",
        "243": "right thing\n",
        "246": "let's look at another example so let's\n",
        "249": "take my same function J for the draw the\n",
        "251": "fungal same function jail\n",
        "253": "and now let's say I had instead\n",
        "255": "initialize my parameter over there on\n",
        "257": "the left so theta one is here I'm going\n",
        "261": "to at that point on the surface now my\n",
        "263": "derivative term DD theta one J of theta\n",
        "266": "one when you validate it at this point\n",
        "270": "going to look at write the slope of that\n",
        "273": "line so this derivative term is a slope\n",
        "275": "of this line but this line is slanting\n",
        "280": " \n",
        "283": "right or alternatively I say that this\n",
        "285": "function has negative derivative just\n",
        "288": "means negative slope at that point so\n",
        "291": "this is less than equal to 0 so when I\n",
        "293": "update theta I'm going to have theta\n",
        "297": "gets updated as theta minus alpha times\n",
        "303": "a negative number and so I have theta 1\n",
        "306": "minus a negative number which means I'm\n",
        "308": "actually going to increase theta right\n",
        "310": "because this is minus of a negative\n",
        "312": "number means I'm adding something to\n",
        "314": "theta and what that means is that I'm\n",
        "316": "going to end up increasing theta and so\n",
        "318": "we'll start here and increase theta\n",
        "320": "which again seems like the thing I\n",
        "323": "wanted to do to try to get me closer to\n",
        "328": "the minimum so this hopefully explains\n",
        "330": "the intuition behind what the derivative\n",
        "332": "term is doing let's next take a look at\n",
        "335": "the learning rate term alpha and try to\n",
        "339": "figure out what that's doing so here's\n",
        "341": "my gradient descent update rule right\n",
        "345": "it's this equation and let's look at\n",
        "347": "what could happen if alpha is either too\n",
        "350": "small or if alpha is too large so in\n",
        "353": "this first example what happens if alpha\n",
        "358": "is too small so here's my function J J\n",
        "362": "of theta let's say I start here if alpha\n",
        "365": "is too small then what I'm going to do\n",
        "367": "is going to multiply my update by some\n",
        "369": "small number so I end up taking it looks\n",
        "371": "like a baby step like that ok so that's\n",
        "374": "one step reason then from this new point\n",
        "375": "I'm going to take another step but if\n",
        "377": "alpha is too small and take another\n",
        "381": " \n",
        "384": "and so if my learning rate is too small\n",
        "386": "I'm going to end up you know taking\n",
        "390": "these tiny tiny baby steps to try to get\n",
        "392": "to the minimum and I'm going to need a\n",
        "394": "lot of steps to get to the minimum and\n",
        "397": "so if alpha is too small gradient\n",
        "398": "descent can be slow because it's going\n",
        "400": "to take these tiny tiny baby steps and\n",
        "402": "so it's going to need a lot of steps\n",
        "404": "before it gets any way close to the\n",
        "408": "global minimum now how about if alpha is\n",
        "410": "to launch so here's my function J of\n",
        "414": "theta turns out if alpha is too large\n",
        "416": "then gradient descent can overshoot the\n",
        "419": "minimum and may even fail to converge or\n",
        "421": "even diverge so here's what I mean let's\n",
        "422": "say a solid fatal there it's actually\n",
        "424": "pretty close to minimum so the\n",
        "426": "derivative points to the right but if\n",
        "428": "alpha is too big I'm going to take a\n",
        "430": "huge step maybe take a huge step like\n",
        "432": "that whereas I end up taking a huge step\n",
        "434": "and now my cost functions I've gotten\n",
        "435": "worse because I start off with this\n",
        "438": "value but now my values have gotten\n",
        "440": "worse now my derivative\n",
        "441": "you know points to the left is a section\n",
        "444": "decrease theta but my learning rate is\n",
        "446": "too big I may take a huge step going\n",
        "448": "from here all the way out there so we\n",
        "451": "end up going on there right and if my\n",
        "452": "learning is to Vega can't take another\n",
        "455": "huge step on the next iteration and kind\n",
        "458": "of overshoot and overshoot and so on\n",
        "460": "until yo you notice I'm actually getting\n",
        "463": "further and further away from the\n",
        "465": "minimum and so if alpha is too large\n",
        "467": "they can fail to converge or even\n",
        "471": "diverge now have another question for\n",
        "473": "you so this is a tricky one and when I\n",
        "475": "was first learning this stuff it\n",
        "476": "actually took me a long time to figure\n",
        "478": "this out one of your parameter theta one\n",
        "480": "is already at a local minimum what do\n",
        "482": "you think one step of gradient descent\n",
        "485": " \n",
        "488": "so let's suppose you initialize theta 1\n",
        "491": "add two local minimum so you know\n",
        "492": "suppose this is your initial value of\n",
        "496": "theta one over here and is already at a\n",
        "499": "local optimum and the local minimum it\n",
        "501": "turns out the local optimum your\n",
        "503": "derivative will be equal to zero so look\n",
        "505": "at that slope where is that tangent\n",
        "509": "point so the slope of this line will be\n",
        "512": "equal to zero and thus this derivative\n",
        "517": "term is equal to zero and so in your\n",
        "518": "gradient descent update you have theta\n",
        "521": "one gets updated as theta one minus\n",
        "524": "alpha times zero and so what this means\n",
        "525": "alpha times zero and so what this means\n",
        "527": "is that if you're already at the local\n",
        "529": "optimum and leaves theta one unchanged\n",
        "532": "because you know this updates theta one\n",
        "535": "equals the other one so if your\n",
        "536": "parameter is already at the local\n",
        "538": "minimum one step of gradient descent\n",
        "540": "does absolutely nothing it doesn't\n",
        "541": "change a parameter which is which is\n",
        "542": "what you want because it keeps your\n",
        "546": "solution at the local optimum this also\n",
        "548": "explains why gradient descent can\n",
        "550": "converge to local minimum even with the\n",
        "553": "learning rate alpha fixed here's what I\n",
        "555": "mean by that let's look at an example so\n",
        "560": "here's a cost function J of theta that\n",
        "562": "maybe I want to minimize and let's say I\n",
        "565": "initialize my algorithm my Granderson\n",
        "567": "algorithm you know out there at that\n",
        "569": "magenta point if I take one step of\n",
        "571": "gradient descent you know maybe it'll\n",
        "573": "take me to that point because my\n",
        "574": "director is pretty steep out there right\n",
        "579": "now I'm at this green point and if I\n",
        "581": "take another step of great descent you\n",
        "583": "notice that my derivative meaning the\n",
        "586": "slope is less steep at the green point\n",
        "588": "then compared to at the magenta point\n",
        "590": "out there right because as I approach\n",
        "592": "the minimum my derivative gets closer\n",
        "594": "and closer to zero as I approach the\n",
        "598": "minimum so often one step of grand\n",
        "601": "ascend my new derivative is a little bit\n",
        "603": "smaller so I want to take another step\n",
        "604": "of great descent\n",
        "607": "I will naturally take a somewhat smaller\n",
        "609": "step from the screen point that I get\n",
        "611": "magenta point now with a new point the\n",
        "614": "red point and then now even closer to\n",
        "615": "global minimum so the derivative here\n",
        "618": "will be even smaller than it was at a\n",
        "620": "green point so when I take another step\n",
        "622": "of green descent you know now my\n",
        "625": "derivative term is even smaller and so\n",
        "628": "the magnitude of the update to theta one\n",
        "630": "is even smaller so take a small step\n",
        "636": "like so and as gradient descent runs you\n",
        "640": "will automatically take smaller and\n",
        "642": "smaller steps until eventually you're\n",
        "645": "taking very small steps you know and you\n",
        "648": "finally converge to the - to the local\n",
        "652": "minimum so just a recap in gradient\n",
        "654": "descent as we approach a local minimum\n",
        "657": "gradient descent will automatically take\n",
        "659": "smaller steps and that's because as we\n",
        "661": "approach the local minimum by definition\n",
        "663": "local minimum is when you know this\n",
        "666": "derivative is equal to zero so as we\n",
        "668": "approach the local minimum this\n",
        "670": "derivative term will automatically get\n",
        "672": "smaller and so gradient descent will\n",
        "676": "automatically take smaller steps so this\n",
        "677": "is what we understand looks like and\n",
        "679": "then so actually no need to decrease\n",
        "680": "then so actually no need to decrease\n",
        "684": "alpha over time so that's the gradient\n",
        "686": "descent algorithm and you can use it to\n",
        "688": "minimize to try to minimize any cost\n",
        "690": "function J not the cost function J that\n",
        "693": "we define for linear regression in the\n",
        "694": "next video we're going to take the\n",
        "696": "function J and set that back to be\n",
        "698": "exactly linear regressions cost function\n",
        "700": "the square cost function that we came up\n",
        "703": "with earlier and taking gradient descent\n",
        "705": "and the square cost function and putting\n",
        "707": "them together that will give us our\n",
        "709": "first learning algorithm they'll give us\n"
    },
    "ZKaOfJIjMRg": {
        "0": " \n",
        "2": "by now you've seen the anomaly detection\n",
        "4": "algorithm and we'll also talked about\n",
        "7": "how to evaluate an anomaly detection\n",
        "10": "algorithm turns out that when you're\n",
        "12": "applying anomaly detection one of the\n",
        "14": "things that has a huge effect on how\n",
        "17": "well it does is what features you use or\n",
        "19": "what features you choose to give the\n",
        "21": "anomaly detection algorithm so in this\n",
        "23": "video what I'd like to do is say a few\n",
        "25": "words give some suggestions and\n",
        "27": "guidelines for how to go about designing\n",
        "29": "or selecting features to give to your\n",
        "34": "anomaly detection algorithm in our\n",
        "36": "anomaly detection algorithm one of the\n",
        "38": "things we did was model the features\n",
        "40": "using this sort of Gaussian distribution\n",
        "44": "with you know exciting new I comma Sigma\n",
        "47": "squared I let's say and so one of the\n",
        "50": "things I often do well will be to plot\n",
        "52": "the data upload a histogram of the data\n",
        "54": "to make sure that the data are actually\n",
        "56": "locally oh vaguely Gaussian before\n",
        "58": "feeding it to my anomaly detection\n",
        "61": "algorithm and it will usually work okay\n",
        "63": "even if your data isn't Gaussian but\n",
        "65": "this is sort of a nice sanity check to\n",
        "67": "run but by the way in case of data loss\n",
        "69": "non Gaussian the Algrim will often work\n",
        "70": "just fine but um\n",
        "73": "concretely if I plot the data like this\n",
        "75": "and it looks like the histogram like\n",
        "77": "this and the way to plot a histogram is\n",
        "80": "to use the H is T on the hist command in\n",
        "82": "octave but it looks like this this looks\n",
        "85": "vaguely Gaussian so if my features look\n",
        "87": "like this I'd be pretty happy feeding it\n",
        "90": "to my album but if I were to plot a\n",
        "93": "histogram of my out of my data and if it\n",
        "94": "was it'd look like this well this\n",
        "96": "doesn't look at all like a bell-shaped\n",
        "98": "curve right this is a very a symmetric\n",
        "100": "distribution as a peak way off to one\n",
        "103": "side if this is what my data looks like\n",
        "105": "what I'll often do is play with\n",
        "107": "different transformations in the data in\n",
        "108": "all\n",
        "110": "to make the look more Gaussian and again\n",
        "112": "the album will usually work okay even if\n",
        "113": "you don't but if you use these\n",
        "115": "transformations to make your data more\n",
        "117": "Gaussian it might work a bit better so\n",
        "120": "given a data set that looks like this\n",
        "123": "what I might do is say take a log\n",
        "126": "transformation of the data and if I do\n",
        "128": "that andrey plot the histogram what I\n",
        "130": "end up with in this particular example\n",
        "132": "this histogram that look like this and\n",
        "134": "this looks much more Gaussian right this\n",
        "136": "smells much more like the classic\n",
        "138": "bell-shaped curve that we can fit you\n",
        "140": "know some mean and some various\n",
        "142": "parameters Sigma so what I mean by\n",
        "144": "ticular log transform is really that if\n",
        "147": "I have some feature x1 and then the\n",
        "150": "histogram of x1 looks like this then I\n",
        "153": "might take my feature x1 and replace it\n",
        "157": "with log of x1 and this is my new\n",
        "158": "exponent about plot to the histogram of\n",
        "160": "on the right this looks much more\n",
        "163": "Gaussian and rather than just a lot of\n",
        "165": "transform some other things you could do\n",
        "167": "might be a that's let's say I have a\n",
        "169": "different feature x2 maybe I replaced\n",
        "174": "that with log X plus 1 or more generally\n",
        "178": "with log X maybe that's x2 plus some\n",
        "181": "constant C and this constant could be\n",
        "183": "something that I play with to try to\n",
        "185": "make a look at Gaussian as possible or\n",
        "187": "for a different future x3 maybe I\n",
        "190": "replace it with you know x3 mighty the\n",
        "193": "square root square root is just X to me\n",
        "196": "to the power of 1/2 right and this 1/2\n",
        "199": "is another example the parameter name\n",
        "202": "play with so I might have you know x4\n",
        "203": "maybe I'll instead replace that with\n",
        "206": "explore to the power of something else\n",
        "210": "maybe to the power of 1/3 and these all\n",
        "214": "of these this one this exponent or the C\n",
        "216": "parameter all of these are examples of\n",
        "218": "parameters in play with in order to make\n",
        "220": "your data and look a little bit more\n",
        "224": " \n",
        "226": "so let me show you a live demo of how\n",
        "228": "I'm actually go about you know playing\n",
        "230": "around with my data to make her orgasm\n",
        "233": "so I have already loaded it into octave\n",
        "235": "here a set of features X I have a\n",
        "238": "thousand examples northern over there so\n",
        "240": "let's pop a histogram of my data when we\n",
        "244": "use the history X command so that's my\n",
        "246": "histogram by default I think this use\n",
        "248": "this 10 bins in the histogram but I want\n",
        "251": "to see a more fine-grained histogram so\n",
        "253": "history X comma 50 so this plots it in\n",
        "255": "50 different bins okay that looks better\n",
        "257": "now this does a little very Gaussian\n",
        "260": "doesn't so let's stop playing around the\n",
        "265": "data let's try just the X to the 0.5\n",
        "267": "it's going to take the square root of\n",
        "270": "the data and plug that as the ground and\n",
        "272": "okay it looks a little bit more Gaussian\n",
        "274": "we're not quite there so let's play it\n",
        "278": "the 0.5 parameter and set this to 0.2\n",
        "281": "what's a little bit more Gaussian let's\n",
        "284": "let's reduce a little bit more 0.1 yeah\n",
        "285": "that looks pretty good I can actually\n",
        "288": "just use 0.5 on this list reduce it to\n",
        "292": "0.05 and you know okay this looks pretty\n",
        "294": "Gaussian so I could define a new feature\n",
        "297": "which is X nu equals x element wise\n",
        "301": "exponentiation to the 0.05 and now my\n",
        "303": "new feature X nu looks more Gaussian\n",
        "305": "than my previous one and then I instead\n",
        "308": "use this new feature to feed into my\n",
        "310": "anomaly detection algorithm and of\n",
        "312": "course there's more than one way to do\n",
        "315": "this you can also have his log effects\n",
        "316": "that's another example for\n",
        "319": "transformation you could use and you\n",
        "321": "know that also looks pretty Gaussian so\n",
        "323": "I could also define X name equals log of\n",
        "325": "X and that would be another pretty good\n",
        "328": "choice of feature to use so to summarize\n",
        "331": "if you plot a histogram of the data and\n",
        "332": "find that that's pretty non Gaussian\n",
        "334": "this were playing around a little bit\n",
        "336": "with different transformations like\n",
        "338": "these to see if you can make your data\n",
        "340": "look a little bit more Gaussian before\n",
        "341": "you feed it to your own\n",
        "343": "although even if you don't it might work\n",
        "345": "okay but I usually do take this thing\n",
        "347": "now the second thing was talked about is\n",
        "350": "how do you come up with features for an\n",
        "353": "anomaly detection algorithm and the way\n",
        "356": "I often do so is are an error analysis\n",
        "359": "procedure so what I mean by that is that\n",
        "360": "this is really similar to the error\n",
        "362": "analysis procedure that we have for\n",
        "363": "analysis procedure that we have for\n",
        "366": "supervised learning where we would train\n",
        "368": "a complete algorithm and run the\n",
        "370": "algorithm on a cross validation set and\n",
        "372": "look at the examples it gets wrong and\n",
        "374": "see if we can come up with extra\n",
        "376": "features to help the algorithm do better\n",
        "379": "on the examples that it got wrong with\n",
        "382": "the cross-validation set so let's look\n",
        "385": "through a of let's try to reason through\n",
        "387": "an example of this process in anomaly\n",
        "389": "detection you know we're hoping that P\n",
        "390": "of X will be launched for the normal\n",
        "392": "examples and they'll be small for the\n",
        "395": "anomalous examples and so a pretty\n",
        "397": "common problem would be if P of X is\n",
        "400": "comparable maybe both are large for both\n",
        "402": "the normal and the anomalous examples\n",
        "404": "let's look at a specific example that\n",
        "406": "let's say that this is my unlabeled data\n",
        "409": "so here I have just one feature X 1 and\n",
        "411": "so I'm going to fit a Gaussian to this\n",
        "415": "and maybe my Gaussian that fits my data\n",
        "418": "looks like that and now let's say I have\n",
        "419": "an anomalous example let's say my\n",
        "420": "an anomalous example let's say my\n",
        "423": "anomalous example takes on an x value of\n",
        "426": "2.5 so plot my anomalous example there\n",
        "428": "and you know it's kind of buried in the\n",
        "431": "middle of a bunch of normal examples and\n",
        "435": "so this example this anomalous example\n",
        "436": "that I've drawn in green and gets a\n",
        "439": "pretty high probability as the heights\n",
        "441": "of the blue curve and the algorithm\n",
        "443": "fails to flag this as an anomalous\n",
        "446": "example now if this were maybe aircraft\n",
        "447": "example now if this were maybe aircraft\n",
        "449": "engine manufacturing or something what I\n",
        "451": "would do is I would actually look at my\n",
        "453": "training examples and look at what went\n",
        "454": "wrong\n",
        "456": "with that particular aircraft engine and\n",
        "459": "see if looking at that example can\n",
        "462": "inspire me to come up with a new feature\n",
        "464": "x2 that helps to distinguish between\n",
        "467": "this bad example compared to the\n",
        "470": "my reading samples compared to all of my\n",
        "473": "normal aircraft engines and if I managed\n",
        "475": "to do so that hope would be then that if\n",
        "479": "I can create a new feature x2 so that\n",
        "482": "when I read plot my data if I take all\n",
        "484": "my normal examples that my training set\n",
        "485": "hopefully I find it all my training\n",
        "488": "examples are these red crosses here and\n",
        "490": "hopefully if I find that for my\n",
        "492": "anomalous example the feature x2 takes\n",
        "495": "on a very unusual value so for my green\n",
        "498": "example here this dis anomaly right my\n",
        "502": "x1 value is still 2.5 that maybe my x2\n",
        "504": "value hopefully it takes on a very large\n",
        "508": "value like 3.5 over there or very small\n",
        "511": "value but now if I'm although my data\n",
        "513": "I'll find it my anomaly detection\n",
        "516": "algorithm gives high probability you\n",
        "518": "know to data in the central regions but\n",
        "520": "you know probably to that sale it'll\n",
        "522": "probably to that an example and then an\n",
        "523": "probably to that an example and then an\n",
        "525": "example that's all the way out there my\n",
        "526": "algorithm will now get very little\n",
        "529": "probabilities and so the process of this\n",
        "531": "is really you know look at the mistakes\n",
        "534": "is making look at the anomaly that the\n",
        "536": "album is failing to flag and see if that\n",
        "539": "inspires you to create some new feature\n",
        "541": "as I find something unusual about the\n",
        "543": "headcrab engine and use that to create a\n",
        "545": "new feature so that with this new\n",
        "547": "feature it becomes easier to distinguish\n",
        "550": "the anomalies for me good examples and\n",
        "553": "so that's the process of error analysis\n",
        "556": "and using that to create new features\n",
        "558": "for anomaly detection finally let me\n",
        "560": "share with you my thinking on how I\n",
        "562": "usually go about choosing features for\n",
        "566": "anomaly detection so usually the way I\n",
        "568": "think about choosing features is I want\n",
        "569": "to choose features that might take on\n",
        "571": "either very very large values or very\n",
        "575": "very small values for examples that you\n",
        "576": "know I think might turn out to be\n",
        "579": "anomalies so let's use our example game\n",
        "581": "of monitoring the computers in a data\n",
        "583": "center and so you know we've lots of\n",
        "585": "machines maybe thousands or tens of\n",
        "587": "thousands of machines in a data center\n",
        "589": "and we want to know if one of the\n",
        "591": "machines one of our computers is acting\n",
        "594": "up doing something strange so here are\n",
        "596": "examples of features human shoes\n",
        "598": "memory users number this access to the\n",
        "601": "CPU load network traffic but now let's\n",
        "603": "say that I suspect one of the failure\n",
        "606": "cases let's say that in my data set I\n",
        "608": "think that CPU load and network traffic\n",
        "610": "tend to grow linearly with each other\n",
        "612": "maybe I'm running a bunch of web servers\n",
        "615": "and so you know if one of my servers is\n",
        "617": "serving a lot of users I have very high\n",
        "619": "CPU load and have a very high network\n",
        "622": "traffic but let's say I think let's say\n",
        "623": "I have a suspicion that one of the\n",
        "626": "failure cases is if one of my computers\n",
        "629": "has a job that gets stuck in some\n",
        "630": "infinite loop so if I think one of the\n",
        "633": "failure cases is one of machines you\n",
        "635": "know one of my own web server server\n",
        "638": "code gets stuck in some infinite loop\n",
        "640": "and so the cpu load grows but the\n",
        "642": "network traffic doesn't because it's\n",
        "643": "just spinning as wheels and doing a lot\n",
        "645": "of CPU world you know stuck in some\n",
        "648": "infinity in that case to detect that\n",
        "650": "type of anomaly I might create a new\n",
        "656": "feature X 5 which might be CPU load\n",
        "662": "divided by network traffic and so here X\n",
        "665": "5 will take on an unusually large value\n",
        "667": "if one of the machines is very large CPU\n",
        "669": "dope and not that much network traffic\n",
        "672": "and so this will be a feature that will\n",
        "674": "help your anomaly detection capture your\n",
        "676": "certain type of enemy and you can also\n",
        "678": "get creative and come up with other\n",
        "679": "features as well that maybe I have a\n",
        "683": "feature xx that's CPU load squared\n",
        "687": "divided by network traffic and this\n",
        "689": "would be another variant for the future\n",
        "691": "like X 5 to try to capture anomalies\n",
        "693": "where one of your machines has a very\n",
        "695": "high CPU load that maybe doesn't have a\n",
        "699": "commensurate ivar network traffic and by\n",
        "700": "creating features like these you can\n",
        "703": "start to capture anomalies that\n",
        "706": "correspond to particular unusual\n",
        "710": "combinations of values of the features\n",
        "713": "so in this video we talked about how to\n",
        "715": "take a feature and maybe transform it in\n",
        "717": "a little bit so that it becomes a bit\n",
        "719": "more Gaussian before feeding into an\n",
        "721": "anomaly detection algorithm and also the\n",
        "723": "error analysis in this process of\n",
        "725": "creating features to try to capture\n",
        "728": "different types of anomalies and these\n",
        "730": "sorts of guidelines hopefully that will\n",
        "732": "help you to choose good features to give\n",
        "734": "you an omni detection algorithm to help\n"
    },
    "Zbr5hyJNGCs": {
        "0": " \n",
        "1": "in this video I'd like to start talking\n",
        "4": "about a second type of unsupervised\n",
        "6": "learning problem called dimensionality\n",
        "8": "reduction there are a couple different\n",
        "10": "reasons why one might want to do\n",
        "12": "dimensionality reduction one is data\n",
        "15": "compression and as we'll see later a few\n",
        "18": "videos later data compression not only\n",
        "20": "allows us to compress the data and have\n",
        "23": "it therefore use of less computer memory\n",
        "25": "or disk space but it will also allow us\n",
        "28": "to speed up our learning algorithms but\n",
        "30": "first let's start by talking about what\n",
        "33": "is dimensionality reduction as a\n",
        "36": "motivating example let's say that we've\n",
        "38": "collected a data set with many many many\n",
        "40": "features and I plotted just two of them\n",
        "43": "here and let's say that unknown to us\n",
        "45": "two of the features were actually the\n",
        "47": "length of something in centimeters and a\n",
        "50": "different feature of x2 is the length of\n",
        "52": "the same thing in inches so this gives\n",
        "55": "us a highly redundant representation and\n",
        "57": "maybe instead of having two separate\n",
        "59": "features x1 and x2 both of which\n",
        "61": "basically measure the length maybe what\n",
        "65": "we want to do is reduce the data to one\n",
        "67": "dimensional and just have one number\n",
        "70": "measuring this length in case this\n",
        "72": "examples he has been contrived this this\n",
        "74": "centimeter an interest example is\n",
        "76": "actually not that unrealistic and not\n",
        "78": "that different from things that I see\n",
        "81": "happening in industry if you have\n",
        "83": "hundreds of thousands of features is\n",
        "85": "often just easy to lose track of exactly\n",
        "88": "what features you have and sometimes you\n",
        "89": "may have a few different engineering\n",
        "91": "teams maybe one engineering team gives\n",
        "92": "you two hundred features a second\n",
        "94": "engineer team gives you another three\n",
        "95": "hundred features then the third\n",
        "97": "engineering team gives you five of your\n",
        "99": "features so you have a thousand features\n",
        "100": "all together and it actually becomes\n",
        "102": "hard to keep track of you know exactly\n",
        "104": "which features you got from which team\n",
        "106": "and is actually not that hard to have\n",
        "107": "highly redundant features like these and\n",
        "111": "so if the length and centimeters was\n",
        "113": "rounded off to the nearest centimeter\n",
        "115": "and length in inches was rounded off to\n",
        "117": "the nearest inch then that's why these\n",
        "119": "examples don't lie perfectly on the\n",
        "120": "examples don't lie perfectly on the\n",
        "122": "you know roundoff error to the nearest\n",
        "124": "centimeter or DeNiro's inch and if we\n",
        "127": "can reduce the data to one dimensions\n",
        "130": "into the two dimensions that um reduces\n",
        "132": "the redundancy for a different example\n",
        "134": "again maybe one that seems slightly less\n",
        "137": "contrived for many years I've been\n",
        "139": "working with autonomous helicopter\n",
        "141": "pilots orders I've been working with\n",
        "145": "pilots that fly helicopters and so if\n",
        "149": "you were to measure if you were to do a\n",
        "150": "survey or do a test of these different\n",
        "153": "pilots you might have one feature x1\n",
        "155": "which is maybe the skill of these\n",
        "159": "helicopter pilots and maybe x2 could be\n",
        "163": "the pilot enjoyment that is you know how\n",
        "165": "much they enjoy flying and maybe these\n",
        "167": "two features will be highly correlated\n",
        "169": "and what you really care about might be\n",
        "174": "this all over this direction a different\n",
        "176": "feature that really measures you know\n",
        "181": "pilot aptitude and I'm making up the\n",
        "183": "name aptitude of course but again if you\n",
        "184": "have highly correlated features maybe\n",
        "185": "have highly correlated features maybe\n",
        "187": "you really want to reduce the dimension\n",
        "189": "so let me say a little bit more about\n",
        "191": "what it really means to reduce the\n",
        "193": "dimension of the data from two\n",
        "195": "dimensions that is from 2d to\n",
        "198": "one-dimensional or to 1d let me color in\n",
        "200": "this example slightly different colors\n",
        "204": "and in this case by reducing the\n",
        "206": "dimension what I mean is that I would\n",
        "209": "like to find maybe this line this you\n",
        "211": "know direction on which most of the data\n",
        "213": "seems to lie and project all the data\n",
        "216": "onto that line religious group and by\n",
        "219": "doing so what I can do is just measure\n",
        "221": "the position of each of the examples on\n",
        "223": "that line and what I can do is come up\n",
        "228": "with a new feature z1 and to specify the\n",
        "229": "position on the line I need only one\n",
        "232": "number so z1 is a new feature that\n",
        "235": "specifies the location of each of those\n",
        "238": "points on this\n",
        "239": "and what this means is that whereas\n",
        "242": "previously if I had an example of x1\n",
        "246": "maybe this was my first example x1 so in\n",
        "249": "order to represent x1 originally x1 I\n",
        "251": "needed a 2 dimensional number for a 2\n",
        "254": "dimensional feature vector instead now I\n",
        "257": "can represent you\n",
        "262": "z1 I can use just z1 to represent my\n",
        "265": "first example and that's going to be a\n",
        "267": "real number and similarly x2\n",
        "270": "you know if x2 is my second example\n",
        "274": "there then previously whereas this was\n",
        "276": "this required two numbers to represent\n",
        "280": "if I instead compute the projection of\n",
        "285": "that back cross onto the line and now I\n",
        "289": "need only one real number which is z2 to\n",
        "292": "represent the location of this point z2\n",
        "296": "on the line and so on through my M\n",
        "299": "examples so just to summarize if we\n",
        "302": "allow ourselves to approximate the\n",
        "305": "original data set by projecting all of\n",
        "307": "my original examples onto this green\n",
        "311": "line over here then I need only one\n",
        "313": "number I need only one real number to\n",
        "315": "specify the position of a point on the\n",
        "319": "line and so what I can do is therefore\n",
        "321": "use just one number to represent the\n",
        "324": "location of each of my training examples\n",
        "326": "after they've been projected onto that\n",
        "328": "green line so this is an approximation\n",
        "330": "to the original training set because I\n",
        "332": "have projected all my training examples\n",
        "335": "onto a line but now I need to spend and\n",
        "337": "now I need to keep around only one\n",
        "341": "number for each of my examples and so\n",
        "344": "this halves the memory requirement or\n",
        "345": "the displace requirement what\n",
        "348": "you four enough how to steal my data and\n",
        "350": "perhaps more interesting Li more\n",
        "353": "importantly what we'll see later in the\n",
        "356": "later video as well is that this will\n",
        "358": "allow us to make our learning algorithms\n",
        "360": "run more quickly as well and that is\n",
        "362": "actually perhaps even the more\n",
        "363": "interesting application of this data\n",
        "366": "compression rather than reducing the\n",
        "368": "memory or disk space requirement for\n",
        "371": "storing the data on the previous slide\n",
        "373": "we showed an example of reducing data\n",
        "376": "from 2d to 1d on this line I'm going to\n",
        "378": "show another example of reducing data\n",
        "380": "from three-dimensional 3d to 2\n",
        "381": "from three-dimensional 3d to 2\n",
        "383": "dimensional 2d by the way in a more\n",
        "385": "typical example of dimensionality\n",
        "387": "reduction we might have a thousand\n",
        "389": "dimensional data or a thousand D data\n",
        "392": "that we might want to reduce to let's\n",
        "393": "say a hundred dimensional\n",
        "396": "rgd but because of the limitations of\n",
        "398": "what I can plot on the slide we're going\n",
        "401": "to use examples of you know 3d to 2d or\n",
        "404": "2d to 1d so let's say I have a data set\n",
        "406": "like that shown here and so I would have\n",
        "409": "a set of examples X I which are points\n",
        "412": "in these three dimensional examples I\n",
        "413": "know it might be a little bit hard to\n",
        "415": "see this on the slide but I'll show a 3d\n",
        "419": "point cloud in a little bit and might be\n",
        "421": "hard to see here a bit so maybe all of\n",
        "423": "this data may be lies roughly on the\n",
        "428": "plane like so and so what we can do with\n",
        "430": "dimensionality reduction is take a hold\n",
        "432": "of all of this data and project the data\n",
        "435": "down onto a two-dimensional plane so\n",
        "437": "here what I've done is I've taken all\n",
        "439": "the data and I projected all of the data\n",
        "442": "so that it all lies on the plane now\n",
        "444": "finally in order to specify the location\n",
        "447": "of a point within a plane we need two\n",
        "449": "numbers right we need to maybe specify\n",
        "451": "the location of a point along this axis\n",
        "454": "and then also specified as location\n",
        "456": "that accident so we need two numbers\n",
        "460": "maybe call Z 1 and Z 2 to specify the\n",
        "462": "location of a point within the plane and\n",
        "465": "so what that means is that we can now\n",
        "468": "represent each example each training\n",
        "471": "example using two numbers so I've drawn\n",
        "475": "here Z 1 and Z 2 so our data can present\n",
        "477": "it can be represented using vector Z\n",
        "482": "which are in R 2 and these subscripts Z\n",
        "484": "substitute 1z subscript 2 what I just\n",
        "486": "mean by that is that my vectors here are\n",
        "488": "Z you know on two dimensional vectors Z\n",
        "492": "1 Z 2 and so if I have some particular\n",
        "494": "examples Zi well that's the\n",
        "501": "2-dimensional vector zi1 zi2 and on the\n",
        "503": "previous slide when I was reducing data\n",
        "505": "to one dimensional data then I had only\n",
        "508": "z1 right but that's what the z1\n",
        "510": "subscript on the previous line was but\n",
        "512": "here I have two dimensional data so I\n",
        "514": "have Z 1 and Z 2 has the two components\n",
        "517": "of the data now let me just make sure\n",
        "518": "of the data now let me just make sure\n",
        "519": "that these figures make sense so let me\n",
        "522": "just show these exams three figures\n",
        "525": "again but whoops VD plots now the\n",
        "527": "process we went through was that shown\n",
        "529": "in the left is the optional data set in\n",
        "530": "the middle of the data step project on\n",
        "533": "to 2d and on the right the 2d data set\n",
        "536": "when z1 and z2 s z axes let's look a\n",
        "538": "little bit further here's my original\n",
        "541": "data set my shell on the left and so it\n",
        "542": "has started off with a 3d point cloud\n",
        "545": "like zones with the axes are labeled x1\n",
        "548": "x2 x3 and so this is a 3d point cloud\n",
        "550": "but most of the data may be roughly lies\n",
        "552": "on you know not too far from some 2d\n",
        "555": "plane so what we can do is take this\n",
        "558": "data and here's my middle finger but\n",
        "560": "projected onto 2d so I've projected this\n",
        "562": "data so that all of it now lies on this\n",
        "565": "2d surface and you can see all all the\n",
        "568": "lies on lane to projected everything\n",
        "571": "onto a plane and so what this means is\n",
        "574": "that now I need only two numbers Z 1 and\n",
        "576": "Z 2 to represent the location of a point\n",
        "581": "on the plane and so that's the process\n",
        "583": "that we can go through to reduce our L\n",
        "585": "data from three dimensional to two\n",
        "587": "dimensional\n",
        "590": "so that's dimensionality reduction and\n",
        "593": "how we can use it to compress our data\n",
        "596": "and as we'll see later this will allow\n",
        "598": "us to make some of our learning\n",
        "599": "algorithms run much later as well but\n"
    },
    "_lrHXJRukMw": {
        "0": " \n",
        "2": "in this video we'll talk about matrix\n",
        "4": "matrix multiplication or how to multiply\n",
        "7": "two matrices together when we talk about\n",
        "9": "the method in linear regression for how\n",
        "11": "to solve for the parameters theta 0 and\n",
        "13": "theta 1 all in one shot so that without\n",
        "15": "needing an iterative algorithm like\n",
        "17": "gradient descent when we talk about that\n",
        "19": "algorithm it turns out that matrix\n",
        "21": "matrix multiplication is one of the key\n",
        "25": "steps that you need to know so let's as\n",
        "29": "usual start of an example let's say I\n",
        "31": "have two matrices and I want to multiply\n",
        "34": "them together let me again just work\n",
        "35": "through this example and then I'll tell\n",
        "38": "you in a little bit what happens so the\n",
        "40": "first thing I'm gonna do is I'm going to\n",
        "44": "pull out the first column of this matrix\n",
        "48": "on the right and I'm going to take this\n",
        "49": "matrix on the left and multiply it by\n",
        "53": "you know a vector that's just this first\n",
        "56": "column okay and it turns out if I do\n",
        "60": "that I'm going to get the vector 11:9 so\n",
        "62": "this is the same your matrix vector\n",
        "64": "multiplication as you saw in the last\n",
        "67": "video I work this out in advance I know\n",
        "70": "it's 11 9 and then the second thing I'm\n",
        "73": "going to do is I'm going to pull out the\n",
        "75": "second column this matrix on the right\n",
        "79": "and I'm then going to you know take this\n",
        "80": "matrix on the left right so I think that\n",
        "84": "matrix and multiply it by that second\n",
        "85": "column on the right so again this is a\n",
        "88": "matrix vector multiplication step which\n",
        "90": "is so from the previous video and it\n",
        "91": "turns out that if you multiply this\n",
        "92": "turns out that if you multiply this\n",
        "95": "matrix in this vector you get 10:14 and\n",
        "97": "by the way if you want to practice your\n",
        "100": "matrix vector multiplication feel free\n",
        "101": "to pause the video and check this\n",
        "104": "product yourself then I'm just going to\n",
        "106": "take these two results and put them\n",
        "108": "together and that would be my answer so\n",
        "110": "turns out the outcome of this product is\n",
        "111": "turns out the outcome of this product is\n",
        "113": "gonna be a two by two matrix and the way\n",
        "115": "I'm going to throw in this matrix is\n",
        "118": "just by taking you know my elements 11 9\n",
        "122": "and plugging them here and taking 10:14\n",
        "124": "and plugging them into\n",
        "128": "the second column okay so that was the\n",
        "130": "mechanics of how to multiply a matrix by\n",
        "133": "another matrix you basically look at the\n",
        "135": "second Matrix one column at a time and\n",
        "137": "you assemble the answers and again we'll\n",
        "139": "step through this much more carefully in\n",
        "143": "but just one point out also this first\n",
        "146": "example is a 2x3 matrix multiplying that\n",
        "150": "by a three by two matrix and the outcome\n",
        "153": "of this product that turns out to be a\n",
        "157": "two by two matrix then again we'll see\n",
        "160": "in a second why this was the case all\n",
        "161": "right that was the mechanics of the\n",
        "162": "calculation\n",
        "163": "unless let's actually look at the\n",
        "165": "details and look at them what exactly\n",
        "168": "happened here are the details I have a\n",
        "171": "matrix a and one two I want to multiply\n",
        "173": "that with a matrix B and the result will\n",
        "176": "be some new matrix C and it turns out\n",
        "178": "you can only multiply together matrices\n",
        "181": "whose dimensions match so a is an M by n\n",
        "184": "matrix or M rows and columns and we'll\n",
        "187": "multiply that with an N by M matrix so\n",
        "189": "it turns out this end here must match\n",
        "192": "this n here so the number of columns in\n",
        "194": "the first matrix must equal to the\n",
        "196": "number of rows in the second Matrix and\n",
        "199": "the result of this product will be a M\n",
        "205": "by o matrix like the matrix C here and\n",
        "207": "in the previous video everything we did\n",
        "210": "correspond to the special case of o\n",
        "212": "being equal to one okay that was those\n",
        "215": "two the case of B being a vector but now\n",
        "218": "we're gonna view of the case of values\n",
        "221": "of a larger than one so here's how you\n",
        "224": "multiply together the two matrices in\n",
        "227": "order to get what I'm going to do is I'm\n",
        "231": "going to take the first column of B and\n",
        "234": "treat that as a vector and multiply the\n",
        "237": "matrix a or the first column of B and\n",
        "240": "the result of that will be a M by 1\n",
        "244": "vector and I'm gonna put that over here\n",
        "248": "then I'm going to take the second column\n",
        "254": "of B right so this is another n by 1\n",
        "256": "vector so this column here this is right\n",
        "258": "n by one was an in dimensional vector\n",
        "260": "going to multiply this matrix with this\n",
        "264": "n by 1 vector the result will be a M\n",
        "266": "dimensional vector which will be put\n",
        "270": "there and so on okay and so you know and\n",
        "272": "then I'm going to take the third column\n",
        "275": "multiplied by this matrix I get a\n",
        "278": "dimensional vector and so on until you\n",
        "281": "get to the last column times the matrix\n",
        "283": "times the last column gives you the last\n",
        "288": "column of C just to say that again\n",
        "292": "the if-- column of the matrix C is\n",
        "295": "obtained by taking the matrix a and\n",
        "298": "multiplying the matrix a with the I've\n",
        "301": "column of the matrix B for the values of\n",
        "305": "I equals 1 2 up through oh ok so this is\n",
        "308": "just a summary of what we did up there\n",
        "311": "in order to compute a matrix C let's\n",
        "313": "look at just one more example let's say\n",
        "315": "I want to multiply together these two\n",
        "318": "matrices so what I'm going to do is\n",
        "323": "first pull out the first column of my\n",
        "325": "second matrix that those images beyond\n",
        "328": "those my matrix B on the previous slide\n",
        "331": "and I therefore have this matrix times a\n",
        "334": "vector and so let's do this calculation\n",
        "336": "quickly this is going to be equal to\n",
        "341": "right 1 3 times 0 3 so that gives 1\n",
        "346": "times 0 plus 3 times 3 and the second\n",
        "350": "element only to 5 times 0 3 so\n",
        "355": "definitely 2 times 0 plus 5 times 3 and\n",
        "360": "that is 915 ok\n",
        "362": "I hope actually let me write that in\n",
        "365": "green so this is 9\n",
        "371": "Dean and Nick's I'm going to pull out\n",
        "375": "the second column of this and do the\n",
        "377": "corresponding calculation so there's\n",
        "380": "that this matrix times this vector 1 2\n",
        "383": "let's also do this quickly so this 1\n",
        "388": "times 1 plus 3 times 2 so just that was\n",
        "392": "that row and let's do the other one so\n",
        "397": "let's see that gives me what 2 times 1\n",
        "403": "plus x 2 and so that is going to be\n",
        "407": "equal to let's see 1 times 1 plus 3\n",
        "411": "times 1 is 4 and 2 times 1 plus 5 times\n",
        "416": "2 is as well so now I have these two and\n",
        "420": "so my outcome so the product of these\n",
        "423": "two matrices is going to be this goes\n",
        "432": "here and this goes here so I get 9 15\n",
        "438": "and 4 12 and you you may notice also\n",
        "440": "that the result of multiplying a 2 by 2\n",
        "443": "matrix with another 2 by 2 matrix the\n",
        "445": "resulting dimension is going to be that\n",
        "447": "first two times that second two so the\n",
        "454": " \n",
        "456": "finally let me show you one more neat\n",
        "459": "trick that you can do with matrix matrix\n",
        "462": "multiplication let's say as before that\n",
        "466": "we have four houses whose prices we want\n",
        "469": "to predict only now we have three\n",
        "472": "competing hypotheses shown here on the\n",
        "476": "right so if you want to apply all three\n",
        "478": "competing hypotheses to all four of your\n",
        "480": "houses it turns out you can do that very\n",
        "482": "efficiently using a matrix matrix\n",
        "486": "multiplication so here on the left is my\n",
        "488": "usual matrix same as from the last video\n",
        "491": "where you know these values on my\n",
        "494": "housing prices and I'll put once here on\n",
        "496": "the left as well and what I'm going to\n",
        "497": "do is\n",
        "500": "strucked another matrix where here these\n",
        "503": "the first column is this minus 40 and\n",
        "508": "0.25 and the second column is this 200\n",
        "514": "open one and so on okay and it turns out\n",
        "517": "that if you multiply these two matrices\n",
        "521": "what you find is that this first column\n",
        "524": "you know open Jordan and blue well how\n",
        "526": "do you get this first column write a\n",
        "529": "procedure for matrix matrix\n",
        "531": "multiplication is the way you get this\n",
        "533": "first column as you tip this matrix and\n",
        "535": "you multiply it by this first column and\n",
        "538": "we saw in the previous video that this\n",
        "541": "is exactly the predicted housing prices\n",
        "546": "of the first hypothesis rate of this\n",
        "550": "first hypothesis year and how about the\n",
        "551": "second column well how do you set you\n",
        "553": "can second column the way you get the\n",
        "555": "second column is well you take this\n",
        "557": "matrix and you multiply it by this\n",
        "560": "second column and so the second column\n",
        "564": "turns out to be the predictions of the\n",
        "568": "second hypothesis of the second\n",
        "571": "hypothesis up there and similarly for\n",
        "575": "the third column and so I didn't step\n",
        "577": "through all the details but hopefully in\n",
        "579": "just a few feet uh you know pause the\n",
        "581": "video and check the math yourself and\n",
        "583": "check that what I just claimed really is\n",
        "585": "true but it turns out that by\n",
        "587": "constructing these two matrices what you\n",
        "590": "can therefore do is very quickly apply\n",
        "592": "all three hypotheses to all four whole\n",
        "596": "sizes to get you know all 12 predicted\n",
        "598": "prices output by your three hypotheses\n",
        "601": "on your four houses so with just one\n",
        "602": "on your four houses so with just one\n",
        "604": "matrix multiplication step you managed\n",
        "607": "to make 12 predictions and even better\n",
        "609": "it turns out that in order to do that\n",
        "611": "matrix multiplication there are lots of\n",
        "613": "good linear algebra libraries in order\n",
        "616": "to do this multiplication step for you\n",
        "618": "and no matter so pretty much any\n",
        "620": "reasonable programming language that you\n",
        "621": "might be used\n",
        "623": "you know certainly all the top 10 most\n",
        "626": "popular program Angeles will have great\n",
        "628": "linear algebra libraries and there'll be\n",
        "629": "good linear algebra libraries that are\n",
        "632": "highly optimized in order to do that\n",
        "634": "matrix matrix multiplication very\n",
        "637": "efficiently including taking taking\n",
        "639": "advantage of any sort of parallel\n",
        "642": "computation that your computer may be\n",
        "644": "capable of whether your computer has you\n",
        "646": "know multiple cores or some multiple\n",
        "649": "processors or so within the processor\n",
        "651": "sometimes as this parallelism as well\n",
        "653": "called sandy parallelism they compute\n",
        "656": "and take care of and you should be the\n",
        "658": "and there are very good free libraries\n",
        "660": "that you can use to do this matrix\n",
        "662": "matrix multiplication very efficiently\n",
        "665": "so that you can very efficiently you\n",
        "667": "know make lots of predictions of lots of\n"
    },
    "bQI5uDxrFfA": {
        "0": " \n",
        "1": "in this video I'm going to define\n",
        "3": "whether it's probably the most common\n",
        "5": "type of machine learning problem which\n",
        "7": "is supervised learning I'll define\n",
        "8": "supervised learning more formally later\n",
        "9": "supervised learning more formally later\n",
        "11": "but it's probably best to explain I'll\n",
        "14": "start with an example of what it is and\n",
        "16": "we'll do the formal definition later\n",
        "18": "let's say you want to predict housing\n",
        "21": "prices a while back a student collected\n",
        "24": "data sets from the city of Portland\n",
        "27": "Oregon and let's say you plot the data\n",
        "29": "set and it looks like this here on the\n",
        "31": "horizontal axis the size of different\n",
        "33": "houses and square feet and on the\n",
        "36": "vertical axis the practice of different\n",
        "40": "houses in thousands of dollars so given\n",
        "42": "this data let's say you have a friend\n",
        "46": "who owns a house that is saying 750\n",
        "49": "square feet and they're hoping to sell\n",
        "50": "the house and they want to know how much\n",
        "53": "they can get for the house so how can\n",
        "55": "the learning algorithm help you one\n",
        "56": "thing a learning algorithm might be able\n",
        "58": "to do is put a straight line through the\n",
        "60": "date arrow so the fit a straight line to\n",
        "64": "the data and based on that it looks like\n",
        "68": "maybe their holes can be so full maybe\n",
        "71": "about 150 thousand dollars but maybe\n",
        "73": "this isn't the only learning algorithm\n",
        "74": "you can use and it might be a better one\n",
        "75": "you can use and it might be a better one\n",
        "77": "for example instead of fitting a\n",
        "79": "straight line to the data we might\n",
        "80": "decide that it's better to fill a\n",
        "82": "quadratic function or a second-order\n",
        "83": "polynomial\n",
        "86": "to this data and if you do that to make\n",
        "89": "a prediction here then it looks like\n",
        "92": "well maybe they can sell the house well\n",
        "95": "closer to $200,000 one of the things\n",
        "97": "we'll talk about later is how to choose\n",
        "100": "and how to decide do you want to fit a\n",
        "102": "straight line to the data or do you want\n",
        "104": "to fit a quadratic function the data and\n",
        "106": "there's no fair picking whichever one\n",
        "107": "gives your friend the better house to\n",
        "110": "sell but each of these would be a fine\n",
        "113": "example of a learning algorithm so this\n",
        "116": "is an example of a supervised learning\n",
        "119": "algorithm and the term supervised\n",
        "122": "learning refers to the fact that we gave\n",
        "124": "the algorithm a data set in which the\n",
        "128": "right answers were given that is we gave\n",
        "130": "it a data set of houses in which for\n",
        "131": "every exam\n",
        "134": "all in this data set we told it what is\n",
        "136": "the right price what was the actual\n",
        "140": "price that that holds so for and the TAS\n",
        "142": "of the algorithm was to just produce\n",
        "145": "more of these right answers such as for\n",
        "147": "this new house you know that your friend\n",
        "150": "may be trying to sell to define a bit\n",
        "152": "more terminology this is also called a\n",
        "155": "regression problem and by regression\n",
        "157": "problem I mean we're trying to predict a\n",
        "160": "continuous value output namely the price\n",
        "162": "so technically against prices can be\n",
        "164": "rounded off to the nearest cent\n",
        "166": "so maybe prices are actually discrete\n",
        "168": "value but usually we think of the price\n",
        "170": "of a house as a roll number was a scalar\n",
        "172": "value as a continuous value number and\n",
        "175": "the term regression refers to the fact\n",
        "176": "that we're trying to predict this sort\n",
        "178": "of a continuous value to attribute\n",
        "180": "here's another supervised learning\n",
        "182": "examples some friends and I were\n",
        "184": "actually working on this earlier let's\n",
        "186": "say you want to look at medical records\n",
        "188": "and try to predictive a breast cancer as\n",
        "190": "malignant or benign if someone discovers\n",
        "193": "a breast tumor a lump in their breast a\n",
        "196": "malignant tumor is a tumor that is\n",
        "199": "harmful and dangerous and a benign tumor\n",
        "202": "is a tumor this is harmless so obviously\n",
        "204": "people care a lot about this\n",
        "206": "let's see collect the data set and\n",
        "208": "suppose you're in your data set you have\n",
        "212": "on your horizontal axis the size of the\n",
        "214": "tumor and on the vertical axis I'm going\n",
        "217": "to plot 1 or 0 yes or no whether or not\n",
        "220": "these are examples of tumors we've seen\n",
        "223": "before are malignant which is 1 or 0 of\n",
        "227": "not malignant or benign so let's say\n",
        "228": "your data set looks like this where we\n",
        "230": "saw a tumor of this size that turned out\n",
        "233": "to be benign one of this size one of\n",
        "237": "this size and so on and sadly we also\n",
        "239": "saw a few malignant tumors so one of\n",
        "242": "that size one of that size one of that\n",
        "247": "size so on so in this example I have 5\n",
        "251": "examples of benign tumors shown down\n",
        "253": "here and 5 examples of malignant tumors\n",
        "257": "shown with a vertical axis value of 1\n",
        "259": "and let's name a friend who tragically\n",
        "262": "has a breast tumor and let's say her\n",
        "264": "breast tumor size\n",
        "268": "maybe somewhere around this value the\n",
        "269": "machine learning question is can you\n",
        "271": "estimate what is the probability what's\n",
        "273": "the chance that the tumor is malignant\n",
        "276": "versus benign to introduce a bit more\n",
        "279": "terminology this is an example of a\n",
        "281": "classification problem\n",
        "284": "the term classification refers to the\n",
        "286": "fact that here we're trying to predict a\n",
        "289": "discrete value output zero or one\n",
        "292": "malignant or benign and it turns out\n",
        "295": "that in classification problem sometimes\n",
        "298": "you can have more than two values for\n",
        "300": "the two possible values for the output\n",
        "303": "as a complete example maybe there are\n",
        "306": "three types of breast cancers and so you\n",
        "307": "may try to predict the discrete value\n",
        "308": "may try to predict the discrete value\n",
        "310": "output 0 1 2 or 3\n",
        "313": "where 0 may mean benign benign tumor so\n",
        "317": "no cancer and one may mean a type 1\n",
        "319": "cancer I give three types of cancer\n",
        "321": "whatever type one means and two may mean\n",
        "323": "the second type of cancer and 3 may mean\n",
        "326": "a third type of cancer but this would\n",
        "328": "also be a classification problem because\n",
        "331": "this other discrete value set of outputs\n",
        "333": "corresponding to no cancer or cancer\n",
        "336": "type 1 or cancer type 2 or type 3 in\n",
        "338": "classification problems there is another\n",
        "341": "way to plot this data let me show you\n",
        "343": "what I mean I'm going to use a slightly\n",
        "345": "different set of symbols to plot this\n",
        "348": "data so if tumor science is going to be\n",
        "350": "the attribute that I'm going to use to\n",
        "353": "predict malignancy or benign Ness I can\n",
        "354": "also draw my data like this I'm going to\n",
        "356": "use different symbols to denote my\n",
        "359": "benign and malignant or my a negative\n",
        "361": "and positive examples so instead of\n",
        "363": "drawing crosses I'm now going to draw\n",
        "368": "holes for the benign tumors like so and\n",
        "373": "I'm going to keep using X's to denote my\n",
        "377": "malignant tumors ok I hope this make it\n",
        "379": "make sense all I did was I took you know\n",
        "382": "these my data set on top and I just\n",
        "387": "mapped it down to this real line like so\n",
        "390": "and started to use different symbol\n",
        "393": "circles and crosses to denote malignant\n",
        "395": "versus benign examples\n",
        "398": "now in this example we use only one\n",
        "401": "feature or one attribute namely the\n",
        "403": "tumor size in order to predict whether\n",
        "407": "tumor is malignant or benign in other\n",
        "408": "machine learning problems we may have\n",
        "410": "more than one feature or more than one\n",
        "413": "attribute here's an example let's say\n",
        "415": "that instead of just knowing the tumor\n",
        "416": "size we know about the age of the\n",
        "419": "patient and the tumor size in that case\n",
        "421": "maybe your data set would look like this\n",
        "426": "where I may have a set of patients with\n",
        "428": "those ages in that tumor size and look\n",
        "432": "like this and different set of patients\n",
        "436": "that look a little different whose\n",
        "439": "tumors turn out to be malignant as\n",
        "444": "denoted by the crosses so let's say you\n",
        "447": "have a friend who tragically has a tumor\n",
        "451": "and maybe their tumor size and age falls\n",
        "455": "around there so given the data set like\n",
        "457": "this what the learning algorithm may do\n",
        "460": "is for the straight line to the data to\n",
        "463": "try to separate out the malignant tumors\n",
        "465": "from the benign ones and so the learning\n",
        "468": "algorithm may decide to for the straight\n",
        "470": "line like that to separate out the two\n",
        "473": "classes of tumors and you know with this\n",
        "475": "hopefully we can decide that your\n",
        "478": "friend's tumor is more likely so if it\n",
        "480": "was over there then hopefully in the\n",
        "481": "learning algorithm will say that your\n",
        "484": "friend's tumor falls on this benign side\n",
        "486": "and is therefore more likely to be\n",
        "489": "benign than malignant in this example we\n",
        "491": "had two features namely the age of the\n",
        "494": "patient and the size of the tumor in\n",
        "497": "other machine learning problems we will\n",
        "499": "often have more features and my friends\n",
        "500": "that work on this problem they actually\n",
        "502": "use other features like these which is\n",
        "505": "clump thickness clump thickness of the\n",
        "507": "breast tumor uniformity of cell size of\n",
        "509": "the tumor uniformity of cell shape of\n",
        "512": "the tumor and so on and other features\n",
        "514": "as well and it turns out one of the\n",
        "516": "interests most interesting learning\n",
        "517": "algorithms that we'll see in this false\n",
        "519": "as a learning algorithm they can deal\n",
        "522": "with not just two or three or five\n",
        "524": "features but an infinite\n",
        "528": "features on this slide I've listed a\n",
        "530": "total of five different features right -\n",
        "532": "on the axes and three more up here but\n",
        "534": "it turns out that for some learning\n",
        "536": "problems what you really want is not to\n",
        "538": "use like three or five features but\n",
        "539": "instead you want to use an infinite\n",
        "541": "number of features an infinite number of\n",
        "543": "attributes so that your learning\n",
        "545": "algorithm has lots of attributes or\n",
        "547": "features or cues with which to make\n",
        "550": "those predictions so how do you deal\n",
        "552": "with an infinite number of features and\n",
        "554": "so how do you even store an infinite\n",
        "556": "number of things on a computer and your\n",
        "558": "computer can run on the memory well it\n",
        "560": "turns out that when we talk about an\n",
        "561": "algorithm called the support vector\n",
        "562": "machine there will be a neat\n",
        "565": "mathematical trick that will allow a\n",
        "567": "computer to deal with an infinite number\n",
        "570": "of features imagine that I didn't just\n",
        "571": "write down you know two features here\n",
        "573": "and three features on the right but\n",
        "575": "imagine that I wrote down an infinitely\n",
        "577": "long list I just kept writing more and\n",
        "578": "more and more feature that's like an\n",
        "581": "infinitely long list of features turns\n",
        "582": "out we'll be able to come up with an\n",
        "585": "algorithm that can deal with that so\n",
        "588": "just to recap in this class we'll talk\n",
        "590": "about supervised learning and the idea\n",
        "593": "is that in supervised learning in every\n",
        "595": "example in our dataset we are told what\n",
        "598": "is the correct answer that we would have\n",
        "600": "quite like the algorithms are predicted\n",
        "602": "on that example such as the price of the\n",
        "605": "house or whether a tumor is malignant or\n",
        "607": "benign we also talked about the\n",
        "609": "regression problem and by regression\n",
        "611": "that means that our goal is to predict a\n",
        "614": "continuous valued output and we talked\n",
        "616": "about the classification problem where\n",
        "618": "the goal is to predict a discrete value\n",
        "622": "output just a quick wrap-up question\n",
        "625": "suppose you're running a company and you\n",
        "626": "want to develop learning algorithms to\n",
        "629": "address each of two problems in the\n",
        "632": "first problem you have a large inventory\n",
        "635": "of identical items so imagine that you\n",
        "637": "have thousands of copies of some\n",
        "640": "identical item to sell and you want to\n",
        "642": "predict how many of these items you sell\n",
        "645": "over the next three months in the second\n",
        "647": "problem problem to you you light you you\n",
        "650": "have lots of users and you want to write\n",
        "653": "software to examine each individual of\n",
        "655": "your customers accounts so each one of\n",
        "656": "your customers account\n",
        "659": "and for each account decide whether or\n",
        "661": "not the account has been hacked or\n",
        "663": "compromised so for each of these\n",
        "665": "problems should they be treated as a\n",
        "667": "classification problem or as a\n",
        "670": "regression problem when the video pauses\n",
        "672": "please use your mouse to select\n",
        "675": "whichever of these four options on the\n",
        "681": "left you think is the correct answer so\n",
        "683": "hopefully you got that this is the\n",
        "686": "answer for problem 1 I would treat this\n",
        "688": "as a regression problem because if I\n",
        "690": "have you know thousands of items well I\n",
        "692": "would probably just treat this as a real\n",
        "696": "value as a continuous value and treat\n",
        "698": "therefore the number of items I sell as\n",
        "702": "a continuous value and for the second\n",
        "703": "problem I would treat that as a\n",
        "706": "classification problem because I might\n",
        "709": "say set the value I want to predict to\n",
        "712": "be 0 to denote the account has not been\n",
        "716": "hacked and set the value 1 to denote an\n",
        "719": "account that has been hacked into so\n",
        "721": "just like your breast cancers right 0 or\n",
        "723": "benign 1 is malignant so I might set\n",
        "725": "this be 0 or 1 depending whether it's\n",
        "727": "been hacked and have an algorithm try to\n",
        "731": "predict each one of these two discrete\n",
        "733": "values and because that's a small number\n",
        "735": "of discrete values I would therefore\n",
        "738": "treat it as a classification problem so\n",
        "741": "that's it for supervised learning and in\n",
        "743": "the next video I will talk about\n",
        "745": "unsupervised learning which is the other\n"
    },
    "c7GhnL2N--I": {
        "0": " \n",
        "2": "matrix multiplication is really useful\n",
        "4": "since you can pack a lot of computation\n",
        "6": "into just one matrix multiplication\n",
        "9": "operation but you should be careful of\n",
        "11": "how you use them in this video\n",
        "13": "I want to tell you about a few\n",
        "18": "properties of matrix multiplication when\n",
        "20": "working with just real numbers or when\n",
        "24": "working with scalars multiplication is\n",
        "27": "commutative and what I mean by that is\n",
        "31": "if you take 3 times 5 that is equal to 5\n",
        "33": "times 3 and the ordering of this\n",
        "35": "multiplication doesn't matter and this\n",
        "39": "is called the commutative property of\n",
        "42": "multiplication of real numbers it turns\n",
        "44": "out this property they can you know\n",
        "46": "reverse the order in which you multiply\n",
        "48": "things this is not true for matrix\n",
        "52": "multiplication so concretely if a and B\n",
        "54": "are matrices then in general a times B\n",
        "57": "is not equal to B times a so just be\n",
        "58": "careful that there's not okay to\n",
        "61": "arbitrarily reverse the order in which\n",
        "63": "you multiply matrices so we say that\n",
        "65": "matrix multiplication is not commutative\n",
        "68": "it's a fancy way of saying it as a\n",
        "70": "concrete example here are two matrices\n",
        "73": "this matrix one 100 times Oh - oh and if\n",
        "75": "you multiply these two matrices you get\n",
        "77": "this result on the right now let's swap\n",
        "78": "this result on the right now let's swap\n",
        "80": "around the order of these two matrices\n",
        "81": "so I'm going to you have taken these two\n",
        "84": "matrices and just reverse them it turns\n",
        "86": "out if you multiply these two matrices\n",
        "89": "you get the second answer on the right\n",
        "91": "and you know well clearly where these\n",
        "95": "these two matrices are not equal to each\n",
        "99": "other so in fact in general if you have\n",
        "103": "a matrix operation like a times B if a\n",
        "109": "is an M by n matrix and B is an N by M\n",
        "113": "matrix just as an example then it turns\n",
        "118": "out that the matrix a times B right is\n",
        "123": "going to be an M by M matrix whereas the\n",
        "128": "matrix B times a is going to be an\n",
        "130": "n-by-n matrix so the dimensions don't\n",
        "133": "even match right so if a times B and B\n",
        "135": "times a may not even be the same\n",
        "137": "dimension in the example on the Left I\n",
        "139": "have all two-by-two matrices so the\n",
        "140": "dimensions were the same but you know in\n",
        "143": "general reversing the order of the\n",
        "146": "matrices can even change the dimension\n",
        "150": "of the outcome so matrix multiplication\n",
        "154": "is not commutative here's the next\n",
        "156": "property I want to talk about so when\n",
        "158": "talking about real numbers or scalars\n",
        "163": "let's say I have 3 times 5 times 2 I can\n",
        "167": "either multiply 5 times 2 first and I\n",
        "171": "can compute this as 3 times 10 or I can\n",
        "174": "multiply 3 times 5 first and I can\n",
        "177": "compute this as you know 15 times 2 and\n",
        "179": "both of these give you the same answer\n",
        "181": "right each both of these is equal to 30\n",
        "183": "so so if it doesn't matter whether I\n",
        "187": "multiply five times two first or whether\n",
        "191": "I multiply 3 times 5 first because sort\n",
        "195": "of well 3 times 5 times 2 is equal to 3\n",
        "200": "times 5 times 2 and this is called the\n",
        "205": "associative property of row number\n",
        "208": "multiplication it turns out that matrix\n",
        "210": "multiplication is associative\n",
        "212": "so concretely let's say I have a product\n",
        "215": "of three matrices a times B times C then\n",
        "220": "I can compute this either as a times B\n",
        "223": "times C or I can compute this as a times\n",
        "227": "B times C and these will actually give\n",
        "228": "me the same answer I'm not going to\n",
        "230": "prove this but you just take my word for\n",
        "232": "it I guess so just be clear what I mean\n",
        "234": "by these two cases let's look at the\n",
        "236": "first one right this first case what I\n",
        "238": "mean by that is if you actually want to\n",
        "240": "compute a times B times C what you can\n",
        "243": "do is you can first compute B times C so\n",
        "245": "that D equals B times C then compute a\n",
        "247": "times D and so this is you're really\n",
        "252": "computing a times B times C or for this\n",
        "256": "second case you can compute this as you\n",
        "257": "can set e\n",
        "260": "a times B then compete a time C and this\n",
        "263": "is then the same as you know a times B\n",
        "267": "times C and it turns out that both of\n",
        "270": "these both of these options will give\n",
        "272": "you this guaranteed to give you the same\n",
        "275": "answer and so we say that matrix\n",
        "277": "multiplication does enjoy the\n",
        "281": "associative property okay and don't\n",
        "283": "worry about the terminology associative\n",
        "284": "and commutative that's what it's called\n",
        "286": "but I'm not really gonna use a certain\n",
        "288": "ology later in this class so don't worry\n",
        "291": "about memorizing those terms finally I\n",
        "294": "want to tell you about the identity\n",
        "296": "matrix which is a special matrix so\n",
        "299": "let's again make the analogy to what we\n",
        "301": "know of roll numbers so in when dealing\n",
        "303": "with real numbers or of scalar numbers\n",
        "306": "the number one is you can think of it as\n",
        "310": "the identity of multiplication and what\n",
        "313": "I mean by that is that for any number Z\n",
        "316": "you know the number one times Z is equal\n",
        "321": "to Z times 1 and that's just equal to\n",
        "324": "the number Z right for any for any row\n",
        "326": "number Z so one is the identity\n",
        "328": "operation and sort of satisfies this\n",
        "331": "equation so it turns out that that's in\n",
        "334": "the space of matrices that an identity\n",
        "337": "matrix as well and is usually denoted I\n",
        "340": "or sometimes we write to this I of n by\n",
        "342": "n we want to make explicit the dimension\n",
        "345": "so I subscript n by n this team and by n\n",
        "348": "identity matrix and so there's a\n",
        "350": "different identity matrix for each\n",
        "352": "dimension N and here are a few examples\n",
        "356": "here's the 2 by 2 identity matrix here's\n",
        "357": "the 3 by 3 identity matrix here's the 4\n",
        "360": "by 4 identity matrix so the identity\n",
        "363": "matrix has the property that it has ones\n",
        "369": "along the diagonals right and so on and\n",
        "374": "0 everywhere else and so by the way the\n",
        "376": "one by one identity matrix you know it's\n",
        "378": "just a number one this is one by one\n",
        "379": "matrix was just 1 in it so it's not very\n",
        "381": "interesting identity matrix and\n",
        "384": "informally when I or others are being\n",
        "386": "sloppy very often\n",
        "388": "right the identity matrices the final\n",
        "390": "tation right draw square brackets just\n",
        "394": "write 1 1 1 1 and then well maybe\n",
        "396": "somewhat sloppily write a bunch of zeros\n",
        "400": "there and these zeros on the business\n",
        "402": "you know this big zero and this big zero\n",
        "404": "that's meant to denote that this matrix\n",
        "406": "is 0 everywhere except for the diagonal\n",
        "408": "so this is just how you know my sloppy\n",
        "410": "then you write the identity matrix and\n",
        "413": "it turns out that the identity matrix s\n",
        "416": "is property that for any matrix a a\n",
        "419": "times identity equals I times a equals a\n",
        "422": "so that's a lot like this equation that\n",
        "425": "we had up here right so that you know\n",
        "428": "one x equals E times 1 equals ESL so I\n",
        "432": "times 8 equals a times I equals a just\n",
        "433": "make sure we have the dimensions right\n",
        "437": "so if a is an M by n matrix then this\n",
        "440": "identity matrix here that's an N by n\n",
        "442": "identity matrix\n",
        "446": "and if is n by n then this identity\n",
        "448": "matrix right for matrix multiplication\n",
        "451": "make sense that has been M by M matrix\n",
        "456": "because this M has a match up that M and\n",
        "458": "in either case the outcome of this\n",
        "461": "process as you get back to matrix a\n",
        "466": "which is M by n so whenever we write the\n",
        "468": "identity matrix I you know very often\n",
        "471": "the dimension right will be implicit\n",
        "473": "from the context so these two eyes\n",
        "474": "they're actually different dimension\n",
        "476": "matrices one may be n by n the other is\n",
        "479": "n by M but when we want to make the\n",
        "482": "dimension of the matrix explicit then\n",
        "484": "sometimes the right to this I subscript\n",
        "486": "n by M kind of like we had up here there\n",
        "488": "very often the dimension will be\n",
        "493": "implicit finally doesn't point out that\n",
        "497": "earlier i said that a times B is not in\n",
        "502": "general equal to B times a right then\n",
        "504": "you know that for most matrices a and B\n",
        "506": "this is not true but when B is the\n",
        "509": "identity matrix this does hold true that\n",
        "511": "a time's the identity matrix the Xindi\n",
        "514": "equal to identity times\n",
        "515": "it's just that you know this is not true\n",
        "520": "for other matrices B in general so\n",
        "522": "that's it for the properties of matrix\n",
        "524": "multiplication and the special matrices\n",
        "526": "like the identity matrix I want to tell\n",
        "529": "you about in the next and final video on\n",
        "532": "our linear algebra review I'm going to\n",
        "535": "quickly tell you about a couple of\n",
        "538": "special matrix operations and after that\n",
        "539": "you need everything you need to know\n"
    },
    "cObOAIImeVQ": {
        "0": " \n",
        "2": "so it's taking us a lot of videos to get\n",
        "4": "through the neural network learning\n",
        "6": "algorithm in this video what I'd like to\n",
        "9": "do is try to put all the pieces together\n",
        "12": "to give a overall summary or a bigger\n",
        "14": "picture view of how all the pieces fit\n",
        "17": "together and of the overall process for\n",
        "18": "how to implement a neural network\n",
        "22": "learning now when training a neural\n",
        "24": "network the first thing you need to do\n",
        "27": "is pick some network architecture and by\n",
        "29": "architecture I just mean connectivity\n",
        "31": "pattern between the neurons so you know\n",
        "33": "we might choose between say a neural\n",
        "36": "network with three input units and five\n",
        "38": "hidden units and four output units\n",
        "41": "versus one with three five hidden five\n",
        "44": "million for output and here at three\n",
        "47": "five five five units in each of three\n",
        "50": "hidden layers and four output units and\n",
        "52": "some of these choices of how many hidden\n",
        "54": "units in each layer and how many hidden\n",
        "55": "layers\n",
        "58": "those are architecture choices so how do\n",
        "61": "you make these choices well first the\n",
        "63": "number of input units well that's pretty\n",
        "65": "well defined and once you've decided on\n",
        "68": "a fixed set of features X the number of\n",
        "69": "input units would just be you know the\n",
        "72": "dimension of your few features X Y will\n",
        "74": "be determined by that and if you're\n",
        "75": "be determined by that and if you're\n",
        "77": "doing multi-class classification the\n",
        "78": "number of output units will be\n",
        "80": "determined by the number of classes in\n",
        "83": "your classification problem and just a\n",
        "85": "reminder if you have a multi-class\n",
        "88": "classification where Y takes on say\n",
        "91": "values between 1 and 10 so that you have\n",
        "95": "10 possible classes then remember to\n",
        "99": "rewrite your purpose Y as these source\n",
        "101": "effectors right instead of class 1 you\n",
        "104": "recode it as a vector like that\n",
        "107": "second class you mean coldness effect\n",
        "110": "like that so if one of your examples is\n",
        "113": "takes on the fifth class here y equals 5\n",
        "115": "then what you're showing to your neural\n",
        "117": "network is not actually a value of y\n",
        "120": "equals 5 instead here at the upper layer\n",
        "122": "which would have a 10 output units you\n",
        "125": "will instead feed to the vector which\n",
        "128": "you know we're done with a 1 in the\n",
        "130": "fifth position and then a bunch of zeros\n",
        "132": "down there so the choice of number of\n",
        "134": "input units and number of output units\n",
        "136": "may be somewhat reasonably is\n",
        "139": "straightforward and as for the number of\n",
        "141": "hidden units and the number of hidden\n",
        "144": "layers a reasonable default is to use a\n",
        "148": "single hidden layer so this type of\n",
        "150": "neural network shown on the left with\n",
        "153": "just one hidden layer is probably the\n",
        "156": "most common or if you use more than one\n",
        "159": "hidden layer again a reasonable default\n",
        "160": "will be to have the same number of\n",
        "162": "hidden units in every single layer so\n",
        "163": "hidden units in every single layer so\n",
        "166": "here we have two hidden layers and each\n",
        "168": "of these hidden layers has the same\n",
        "170": "number five of hidden units and here we\n",
        "173": "have you know three hidden layers and\n",
        "175": "each of them has the same number of\n",
        "177": "there's five hidden units but rather\n",
        "178": "there's five hidden units but rather\n",
        "179": "than doing these this this sort of\n",
        "181": "network architecture on the left would\n",
        "184": "be a perfectly reasonable default and as\n",
        "186": "for the number of hidden units usually\n",
        "188": "the more hidden units the better is just\n",
        "190": "that if you have a lot of hidden units\n",
        "192": "it can become more computationally\n",
        "194": "expensive but very often having more\n",
        "197": "hidden units is a good thing and usually\n",
        "199": "the number of hidden units in each layer\n",
        "202": "will be may be comparable to the\n",
        "204": "dimension of X comparable to the number\n",
        "205": "of features or it could be anywhere from\n",
        "209": "same number of hidden units as simple\n",
        "211": "features to maybe twice of that three or\n",
        "213": "four times of that so\n",
        "214": "having a number of hidden units as\n",
        "216": "culpable you know several times are\n",
        "217": "somewhat bigger than the number of\n",
        "219": "different features is often a useful\n",
        "223": "thing to do so hopefully this gives you\n",
        "225": "one reasonable set of default choices\n",
        "228": "when they are architecture and if you\n",
        "230": "follow these guidelines you probably get\n",
        "232": "something that works well but it's in a\n",
        "234": "link to a set of videos where our talks\n",
        "236": "specifically about advice for how to\n",
        "238": "apply learning algorithms I'll actually\n",
        "241": "say a lot more about how to choose in\n",
        "243": "your network architecture actually have\n",
        "244": "quite a lot more to say later about how\n",
        "247": "to make good choices for the number of\n",
        "249": "hidden units number of hidden areas and\n",
        "253": "next here's what we need to implement in\n",
        "255": "order to train in your network there are\n",
        "257": "actually six steps that I have I have\n",
        "259": "reformed the slide and two more steps on\n",
        "262": "the next slide first step is to set up\n",
        "263": "the neural network and to randomly\n",
        "266": "initialize the values of the weights and\n",
        "267": "we usually initialize the weights to\n",
        "271": "values to small values near zero then we\n",
        "275": "implement for propagation so that we can\n",
        "277": "input any inflects in the neural network\n",
        "279": "and compute H of X which is this output\n",
        "284": "vector of the Y values we then also\n",
        "286": "intimate code to compute this cost\n",
        "291": "function J of theta and next we\n",
        "293": "implement that proper back or the back\n",
        "295": "propagation algorithm to compute these\n",
        "298": "partial derivatives terms a partial\n",
        "300": "derivatives of j of theta with respect\n",
        "303": "to the parameters concretely to\n",
        "305": "implement back prop usually we will do\n",
        "308": "that with a full loop over the training\n",
        "310": "examples some of you may have heard of\n",
        "313": "advances of a frankly very advanced\n",
        "314": "vectorization methods where you don't\n",
        "317": "have a for loop over the M training\n",
        "319": "examples but the first time we're\n",
        "320": "implementing back route there should\n",
        "322": "almost certainly be for loop in your\n",
        "325": "code where you're iterating over these\n",
        "331": " \n",
        "333": "and then in the second iteration of the\n",
        "336": "for loop you do forward propagation and\n",
        "337": "back propagation on the second example\n",
        "340": "and so on until you get through the\n",
        "343": "finer example so there should be a for\n",
        "345": "loop in your implementation of back prop\n",
        "346": "at least the first time you're\n",
        "347": "at least the first time you're\n",
        "349": "implementing it and then the thankfully\n",
        "350": "somewhat complicated ways to do this\n",
        "352": "will work with all with all they follow\n",
        "354": "but I definitely do not recommend trying\n",
        "356": "to do that much more complicated version\n",
        "358": "the first time you try to make for a\n",
        "360": "back problem so concretely you will have\n",
        "362": "a for loop over my M training examples\n",
        "365": "and inside the for loop we're going to\n",
        "367": "perform for a profit back problem using\n",
        "369": "just this one example and what that\n",
        "371": "means is that we're going to take X I\n",
        "374": "and feedback to my infant layer before\n",
        "377": "for profit perform back prop and that\n",
        "379": "would give me all of these activations\n",
        "381": "and all of these Delta terms for all of\n",
        "384": "my all of the layers of all my units\n",
        "387": "ative you and that's worth then is still\n",
        "390": "inside this for loop then with your some\n",
        "392": "curly braces just to show the scope of\n",
        "394": "the for loop this is an octave code of\n",
        "396": "course but there's more.they C++ Java\n",
        "398": "code and the for loop they come to solve\n",
        "400": "this we're going to compute those Delta\n",
        "403": "terms which here's the formula that we\n",
        "408": "gave earlier plus Delta L plus 1 times\n",
        "411": "they transform into some of the code and\n",
        "414": "then finally outside the for-loop you\n",
        "415": "know having computed these Delta terms\n",
        "418": "these accumulation terms you would then\n",
        "420": "have some other code and then that will\n",
        "422": "allow us to compute these partial\n",
        "425": "derivative terms right end of these\n",
        "427": "partial derivative terms have to take\n",
        "428": "into account the regularization term\n",
        "431": "long term as well and so that those\n",
        "433": "formulas were given in an earlier video\n",
        "437": "so having done that you now hopefully\n",
        "439": "have code to confuse these partial\n",
        "442": "derivative terms next in step 5\n",
        "445": "what I do is then use gradient checking\n",
        "447": "to compare these partial derivative\n",
        "449": "terms that were computed so if compare\n",
        "451": "the variations computed using back\n",
        "454": "propagation versus the partial\n",
        "456": "derivatives computer and using the\n",
        "458": "numerical estimates that is using the\n",
        "460": "numerical estimate of the derivatives\n",
        "461": "that do gradient checking to make sure\n",
        "463": "that both of these gives you very\n",
        "466": "similar values having done gradient\n",
        "468": "checking does now reassures us that our\n",
        "470": "implementation of backpropagation is\n",
        "472": "correct and is then very important that\n",
        "474": "we disable gradient checking because the\n",
        "475": "gradient checking code is\n",
        "480": "computationally very slow and finally we\n",
        "483": "then use a optimization algorithm such\n",
        "485": "as gradient descent or one of the\n",
        "487": "advanced optimization that there's such\n",
        "490": "as LVADs Kunshan gradient as embodied\n",
        "492": "into effect menu and CEO or other\n",
        "494": "optimization methods we use these\n",
        "497": "together with back propagation so back\n",
        "499": "propagation is the thing that computes\n",
        "502": "these partial derivatives for us and so\n",
        "503": "we know how to compute the cost function\n",
        "504": "we know how to compute the partial\n",
        "507": "derivatives using back propagation so we\n",
        "508": "can use one of these optimization\n",
        "511": "methods to try to minimize J of theta as\n",
        "513": "a function of\n",
        "515": "pronounces theta and by the way for\n",
        "518": "neural networks this cost function J of\n",
        "521": "theta is non convex or as not convex and\n",
        "526": "so it can theoretically be susceptible\n",
        "528": "to local minima and in fact algorithms\n",
        "530": "like gradient descent and the advanced\n",
        "533": "optimization methods can in theory get\n",
        "536": "stuck in local Optima but it turns out\n",
        "538": "that in practice this is not usually a\n",
        "540": "huge problem and even though we can't\n",
        "542": "guarantee that these algorithms will\n",
        "544": "find the global Optima usually\n",
        "545": "algorithms like gradient descent will do\n",
        "548": "a very good job minimizing this cost\n",
        "550": "function J of theta and get to a very\n",
        "552": "good local minimum even if it doesn't\n",
        "555": "get to the global optimum finally\n",
        "558": "gradient descent for a new network might\n",
        "560": "all seem a little bit magical so let me\n",
        "563": "just show one more figure to try to get\n",
        "564": "better intuition about what gradient\n",
        "566": "descent for a neural network is doing\n",
        "568": "this was actually similar to the figure\n",
        "570": "that I was using earlier to explain\n",
        "573": "gradient descent so we have some cost\n",
        "575": "function and we have a number of\n",
        "577": "parameters our neural network right here\n",
        "578": "I've just written down two of the\n",
        "581": "parameter values in reality of course in\n",
        "582": "the neural network we can have lots of\n",
        "585": "parameters so these theta 1 theta 2 all\n",
        "587": "of these are matrices right so we can\n",
        "589": "have very high dimensional parameters\n",
        "591": "but because of the limitations of the\n",
        "593": "source of possibly draw I'm pretending\n",
        "595": "that we have only two parameters in this\n",
        "597": "neural network although obviously we\n",
        "599": "have a lot more in practice now this\n",
        "602": "cost function J of theta measures how\n",
        "604": "well the neural network fits the\n",
        "607": "training data so if you take a point\n",
        "610": "like this one down here that's a point\n",
        "613": "where J of theta is pretty low and so\n",
        "616": "this corresponds to a setting of the\n",
        "617": "parameters there's a setting of the\n",
        "620": "parameters theta where you know for most\n",
        "624": "of the training examples the output\n",
        "627": "hypothesis that may be pretty close to\n",
        "629": "why I and if this is true then that's\n",
        "631": "what causes my cost function to be\n",
        "633": "pretty low whereas in contrast you were\n",
        "636": "to take a value like that a point like\n",
        "638": "that corresponds to where for many\n",
        "640": "training examples the output of my\n",
        "643": "neural network is far from the actual\n",
        "645": "value why I there was observed in the\n",
        "647": "training set so points like this on the\n",
        "651": "right correspond to where the hypothesis\n",
        "653": "where the neural network is outputting\n",
        "655": "values on the training set but far from\n",
        "657": "Y is not fitting the training set well\n",
        "659": "where as points like this with long\n",
        "661": "values of the cost function corresponds\n",
        "665": "to where J of theta is low and therefore\n",
        "667": "corresponds to where the neural network\n",
        "669": "happens to be fitting my training set\n",
        "671": "well because I mean this this is what's\n",
        "673": "needed to be true in order for J of\n",
        "675": "theta to be small so what gradient\n",
        "678": "descent does is so start some from some\n",
        "680": "random initial point like that out over\n",
        "683": "there and it will repeatedly go downhill\n",
        "685": "and so what back propagation is doing\n",
        "688": "this is computing the direction of the\n",
        "690": "gradient then what gradient descent is\n",
        "691": "doing this is taking rule stairs\n",
        "694": "downhill until hopefully it gets to in\n",
        "697": "this case the pretty good local optimum\n",
        "699": "so when you implement back propagation\n",
        "702": "and use gradient descent or one of the\n",
        "704": "advanced optimization that is this\n",
        "706": "picture sort of explains what the\n",
        "708": "algorithm is doing is trying to find a\n",
        "711": "value of the parameters where the open\n",
        "713": "values of the neural network closely\n",
        "716": "matches the values of the Y's observed\n",
        "718": "in your training\n",
        "720": "so hopefully this gives you a better\n",
        "723": "sense of how the many different pieces\n",
        "726": "of neural network learning fit together\n",
        "729": "in case even after this video in case\n",
        "730": "you still feel like there are like a lot\n",
        "732": "of different pieces and it's not\n",
        "734": "entirely clear what some of them do how\n",
        "737": "all of these pieces come together that's\n",
        "739": "actually okay neural network learning\n",
        "741": "and back propagation is a complicated\n",
        "744": "algorithm and even though I've seen the\n",
        "746": "math behind back propagation many years\n",
        "748": "not used back propagation I think very\n",
        "750": "successful in many years even today I\n",
        "752": "still feel like I don't always have a\n",
        "755": "grasp of exactly what back propagation\n",
        "757": "is doing sometimes and of what the\n",
        "759": "optimization process looks like of\n",
        "762": "minimizing J of theta and this is much\n",
        "764": "harder algorithm to feel like I have a\n",
        "766": "much less good handle on you know\n",
        "768": "exactly what this is doing compared to\n",
        "770": "say linear regression or logistic\n",
        "772": "regression which were mathematically and\n",
        "774": "conceptually much simpler and much\n",
        "776": "cleaner algorithms but so in case you\n",
        "778": "feel the same way you know that's\n",
        "781": "actually perfectly okay but so if you do\n",
        "783": "implement back-propagation hopefully\n",
        "785": "what you find is that this is one of the\n",
        "787": "most powerful learning algorithms and if\n",
        "789": "you implement this algorithm and for the\n",
        "790": "back propagation and implement one of\n",
        "793": "these optimization methods you find that\n",
        "795": "back propagation will be able to fit\n",
        "797": "very complex powerful nonlinear\n",
        "800": "functions to your data and this is one\n",
        "801": "of the most\n"
    },
    "cnCzY5M3txk": {
        "0": " \n",
        "1": "in the last video we talked about\n",
        "4": "dimensionality reduction for the purpose\n",
        "6": "of compressing the data in this video\n",
        "8": "I'd like to tell you about a second\n",
        "9": "application of dimensionality reduction\n",
        "13": "and that is to visualize the data for a\n",
        "15": "lot of machine learning applications it\n",
        "17": "really helps us to develop effective\n",
        "19": "learning algorithms if we can understand\n",
        "21": "our data better if we have some way of\n",
        "24": "visualizing the data better and so the\n",
        "26": "mention ality reduction offers us often\n",
        "29": "another useful tool to do so let's sort\n",
        "31": "of an example let's say we've collected\n",
        "34": "a large data set of many statistics and\n",
        "35": "facts about different countries around\n",
        "37": "the world so maybe the first feature x1\n",
        "39": "this is the country's GDP or the gross\n",
        "43": "domestic product x2 is the per capita\n",
        "46": "meaning the per person GDP x3 human\n",
        "48": "development exponent index life\n",
        "51": "expectancy x5 expectancy then we may\n",
        "54": "have a huge data set like this with you\n",
        "57": "know maybe on 50 features for every\n",
        "59": "country and we have a huge set of\n",
        "62": "countries so is that something that we\n",
        "64": "can do to try to understand our data\n",
        "67": "better so I've given us a huge table of\n",
        "69": "numbers you know can we compete how do\n",
        "71": "you visualize this data if you have 50\n",
        "74": "features is very difficult to plot 50\n",
        "77": "dimensional data so what is a good way\n",
        "81": "to examine this data using\n",
        "83": "dimensionality reduction what we can do\n",
        "86": "is instead of having each country\n",
        "89": "represented by this feature vector X I\n",
        "93": "which is 50 dimensional so instead of\n",
        "95": "say having a country like you know\n",
        "97": "Canada instead of having 50 numbers to\n",
        "99": "represent 10 the features of Canada\n",
        "101": "let's say we can come up with a\n",
        "104": "different feature representation that is\n",
        "110": "these D vectors that is in R 2 if that's\n",
        "111": "the case if we can have just a pair of\n",
        "114": "numbers Z 1 and Z 2 that somehow\n",
        "116": "summarizes my 50 numbers maybe what we\n",
        "119": "can do is then plot these countries in R\n",
        "122": "2 and use that to try to understand\n",
        "124": "the space and different easels of\n",
        "126": "features of different countries a little\n",
        "129": "bit better and so here what we're going\n",
        "133": "to what you can do is reduce the data\n",
        "138": "from 50 D from 50 dimensions to 2 D so\n",
        "140": "you can plot this as a two-dimensional\n",
        "143": "plot and when you do that it turns out\n",
        "144": "that if you look at the output of the\n",
        "146": "dimensionality reduction algorithms it\n",
        "149": "usually doesn't ascribe a particular\n",
        "152": "meaning to diesel new feature Z 1 and Z\n",
        "154": "2 and it's often up to us to figure out\n",
        "155": "you know roughly what these features\n",
        "158": "means but if you plot what these\n",
        "159": "features here's what you might find\n",
        "162": "so here every country is represented by\n",
        "166": "a point Zi which is an r2 and so each of\n",
        "168": "those dots in this figure represents a\n",
        "171": "country and so here's z1 and here's the\n",
        "175": "z2 and highlight a couple of these so we\n",
        "176": "might find for example that the\n",
        "180": "horizontal axis the z1 axis corresponds\n",
        "183": "roughly you know to the overall country\n",
        "187": "size or the overall economic activity of\n",
        "190": "a countries of the overall GDP overall\n",
        "194": "economic size of a country whereas the\n",
        "197": "vertical axis in our data mike respond\n",
        "200": "to the per person\n",
        "203": "you know GDP or the per person\n",
        "205": "well-being or the purpose and economic\n",
        "209": " \n",
        "212": "and you might find that given these 50\n",
        "214": "features you know these are really the\n",
        "217": "two main dimensions of variation and so\n",
        "219": "out here you might have a country like\n",
        "223": "the USA which has a relatively large GDP\n",
        "226": "as a it was a very large GDP and a\n",
        "228": "relatively high per person GDP as well\n",
        "231": "whereas here you might have a country\n",
        "234": "like Singapore which you know actually\n",
        "236": "has a very high per person GDP as well\n",
        "238": "but because Singapore is a much smaller\n",
        "241": "country the overall you know economy\n",
        "244": "size of Singapore this is much smaller\n",
        "247": "than the US and over here\n",
        "250": "you know you would have countries where\n",
        "253": "individuals are so unfortunately someone\n",
        "254": "less well-off in addition to life\n",
        "255": "expectancy\n",
        "257": "let's healthcare that's good less\n",
        "259": "economic activity as well as smaller\n",
        "262": "countries whereas on a point like this\n",
        "265": "will correspond to a country that has a\n",
        "266": "will correspond to a country that has a\n",
        "267": "fair it has a substantial amount of\n",
        "270": "economic activity but where individuals\n",
        "272": "you know tend to be someone less well\n",
        "274": "but so you might find that the D axis Z\n",
        "277": "1 and Z 2 can help you to most\n",
        "278": "succinctly capture really what are the\n",
        "280": "two main dimensions of variations\n",
        "284": "amongst different countries such as the\n",
        "286": "overall economic activity of the country\n",
        "289": "protected by the size of the country's\n",
        "292": "overall economy as well as the per\n",
        "295": "person individual well-being measured by\n",
        "296": "your purpose in GDP per person\n",
        "300": "healthcare and things like that\n",
        "303": "so that's how you can use dimensionality\n",
        "306": "reduction in order to reduce data from\n",
        "308": "50 inventions or whatever down to two\n",
        "310": "men dimensions or maybe down to three\n",
        "312": "dimensions so that you can plot it and\n",
        "315": "understand your data better in the next\n",
        "317": "video we'll start to develop a specific\n",
        "319": "algorithm called PCA or principal\n",
        "321": "components analysis which will allow us\n",
        "324": "to do this and also do the earlier\n",
        "325": "application I talked about of\n"
    },
    "dlEoLfA4MSQ": {
        "0": " \n",
        "1": "in the previous video we talked about\n",
        "4": "how to use back propagation to compute\n",
        "6": "the derivatives of your cost function in\n",
        "8": "this video I want to quickly tell you\n",
        "10": "about one implementational detail of\n",
        "14": "unrolling your parameters from matrices\n",
        "16": "into vectors which we'll need in order\n",
        "17": "to use the advanced optimization\n",
        "21": "routines concretely let's say you've\n",
        "23": "implemented a cost function that takes\n",
        "24": "implemented a cost function that takes\n",
        "26": "as input your parameters theta and\n",
        "28": "returns the cost function and returns\n",
        "31": "derivatives then you can post this to an\n",
        "33": "advanced optimization algorithm by\n",
        "35": "fminunc and fminunc isn't the only one\n",
        "36": "by the way there are there are also\n",
        "39": "other advanced optimization algorithms\n",
        "42": "but what all of them do is take as input\n",
        "44": "the point of the cost function and some\n",
        "48": "initial value of theta and both and this\n",
        "51": "these routines assume that theta and the\n",
        "54": "initial value of theta that these are\n",
        "57": "parameter vectors may be RN or RN plus 1\n",
        "60": "but that these are vectors and it also\n",
        "63": "seems that your cost function will\n",
        "65": "return as a second return value this\n",
        "68": "gradient which is also like RM our RN\n",
        "71": "plus 1 so also a vector this worked fine\n",
        "73": "when we were using logistic regression\n",
        "75": "but now that we're using a neural\n",
        "77": "network our parameters are no longer\n",
        "79": "vectors but instead they're these\n",
        "81": "matrices where for a folder in your\n",
        "84": "network we would have parameter matrices\n",
        "87": "theta 1 theta 2 theta 3 that we might\n",
        "89": "represent in octave as these matrices\n",
        "92": "theta 1 theta 2 theta 3 and similarly\n",
        "94": "these gradient terms that were expected\n",
        "96": "to return well in the previous video we\n",
        "99": "show how to compute these gradient\n",
        "102": "matrices which was capital D 1 capital D\n",
        "104": "2 in faculty 3 which we might represent\n",
        "108": "in octave as matrices D 1 D 2 D 3 in\n",
        "110": "this video I want to quickly tell you\n",
        "111": "about the idea of how to take these\n",
        "114": "matrices and unroll them into vectors so\n",
        "116": "that they end up being in a format\n",
        "119": "suitable for passing into as theta here\n",
        "122": "all for getting out for a gradient there\n",
        "125": "concretely let's say we have a neural\n",
        "128": "network with one input layer with\n",
        "130": "units hidden there with ten units and\n",
        "132": "one output there with just one unit so\n",
        "135": "s1 is the number of units in layer 1 s2\n",
        "138": "is number of units in layer 2 and s 3 is\n",
        "140": "the number of layers is number of units\n",
        "143": "in layer 3 in this case the dimension of\n",
        "146": "your matrices theta and D are going to\n",
        "149": "be given by these expressions now for\n",
        "151": "example theta one is going to be a 10 by\n",
        "155": "11 matrix and so on so in octave if you\n",
        "158": "want to convert between these matrices\n",
        "161": "and vectors we can do is take your theta\n",
        "164": "1 theta 2 theta 3 and write this piece\n",
        "166": "of code and this will take all the\n",
        "168": "elements of your 3 theta matrices and\n",
        "171": "take all the elements of theta 1 all the\n",
        "173": "elements of theta 2 all the elements of\n",
        "176": "theta 3 and unroll them and put all the\n",
        "178": "elements into a big long vector which is\n",
        "181": "thetavec and similarly the second\n",
        "183": "command would take all of your D\n",
        "186": "matrices and unroll them into a big long\n",
        "189": "vector and call it D vector and finally\n",
        "191": "if you want to go back from the vector\n",
        "193": "representations to the matrix\n",
        "195": "representations what you do to get back\n",
        "198": "data one say is take van Tate thetavec\n",
        "201": "and pull out the first 110 elements so\n",
        "204": "theta one has 110 elements because\n",
        "207": "there's a 10 by 11 matrix so that pulls\n",
        "208": "out the first hundred and 10 elements\n",
        "211": "and then you can use the reshape command\n",
        "213": "to reshape this back into theta 1 and\n",
        "216": "similarly to get back beta 2 you pull\n",
        "218": "the next hundred and ten elements and\n",
        "221": "reshape it and for theta 3 you pull out\n",
        "222": "the final 11\n",
        "228": " \n",
        "230": "here's a quick octave demo of that\n",
        "233": "process so for this example let's set\n",
        "238": "theta 1 equal to be a 1 of 10 by 11 this\n",
        "240": "is a matrix of all ones and just to make\n",
        "242": "this easier see you let's set that to be\n",
        "247": "two times ones 10 by 11 and let's set\n",
        "253": "theta 3 equals 3 times ones of 1 by 11\n",
        "256": "so this is three separate matrices theta\n",
        "258": "1 theta 2 theta 2 me if I want to put\n",
        "259": "all of these into the vector I've set\n",
        "265": "theta Veck equals theta 1 semicolon\n",
        "272": "theta 2 theta 3 right there's a colon in\n",
        "277": "the middle and like so and now theta\n",
        "280": "that is going to be a very long vector\n",
        "283": "there's 231 elements if I if I display\n",
        "287": "it I find that this is very long vector\n",
        "289": "of all the elements in the first matrix\n",
        "291": "all the others second matrix then all\n",
        "293": "the elements of the third matrix and if\n",
        "296": "I want to get back my original matrices\n",
        "301": "I can do Lee shape theta deck let's pull\n",
        "304": "out the first 110 elements and reshape\n",
        "307": "that into 10 by 11 matrix and just gives\n",
        "310": "me back beta 1 and if I then pull out\n",
        "313": "the next 110 elements so that's indices\n",
        "318": "101 to 220 I get back all of my 2s and\n",
        "322": "if I go from 2 to 1 up to the lost\n",
        "325": "elements just element 2 3 1 and reciate\n",
        "330": " \n",
        "332": "to make this process really concrete\n",
        "336": "here's how we use the unrolling idea to\n",
        "338": "implement our learning algorithm let's\n",
        "340": "say that you have some initial value of\n",
        "342": "the parameters theta 1 theta 2 theta 3\n",
        "345": "what we're going to do is take these and\n",
        "348": "unroll them into a long vector we're\n",
        "351": "going to call initial theta to pass into\n",
        "354": "fminunc as this initial setting of the\n",
        "356": "parameters theta the other thing we need\n",
        "358": "to do is to implement the cost function\n",
        "361": "here's my implementation of the cost\n",
        "364": "function the cost function is going to\n",
        "366": "give us input theta vector which is\n",
        "368": "going to be all my parameter vector is\n",
        "370": "that in the form that's been unrolled\n",
        "372": "into a vector so the first thing I'm\n",
        "374": "going to do is I'm going to use data\n",
        "376": "back and I'm going to use the reshape\n",
        "378": "function to pull elements from thetavec\n",
        "381": "and use reshape to get back my original\n",
        "383": "parameter matrices theta 1 theta 2 theta\n",
        "386": "3 so these are going to be matrices that\n",
        "388": "I get so that gives me a more convenient\n",
        "391": "form I wish to use these matrices so\n",
        "393": "they can run forward propagation and\n",
        "395": "back propagation to compute my\n",
        "397": "derivatives and to compute my cost\n",
        "400": "function J of theta and finally I can\n",
        "403": "then take my derivatives and unroll them\n",
        "405": "so keeping the elements in the same\n",
        "407": "ordering as I did want to unroll my\n",
        "409": "Thetas then I'm going to unroll d1 d2 d3\n",
        "413": "to get gradient back which is now what's\n",
        "416": "my cost function and return to return a\n",
        "418": "vector of these\n",
        "421": "so hopefully you now have a good sense\n",
        "423": "of how to convert back and forth between\n",
        "425": "their matrix representation of the\n",
        "427": "parameters versus the vector\n",
        "429": "representation of the parameters the\n",
        "431": "advantage of the matrix representation\n",
        "433": "is that when your parameters are stored\n",
        "435": "as matrices is more convenient when\n",
        "437": "you're doing forward propagation and\n",
        "440": "back propagation and is easier when your\n",
        "442": "parameters are stored as matrices to\n",
        "444": "take advantage of the sort of vectorized\n",
        "447": "implementations whereas in contrast the\n",
        "449": "advantage of the vector representation\n",
        "452": "we have like thetavec o defect is that\n",
        "454": "when you're using the advanced\n",
        "456": "optimization algorithms those algorithms\n",
        "458": "tend to assume that you have all of your\n",
        "460": "parameters unrolled into a big long\n",
        "463": "vector and so with what we just went\n",
        "464": "through hopefully you can now quickly\n"
    },
    "dnCzy_XKGbA": {
        "0": " \n",
        "2": "in this video I'd like to talk about a\n",
        "4": "new large scale machine learning setting\n",
        "6": "called the online learning setting the\n",
        "8": "online learning setting allows us to\n",
        "10": "model problems where we have a\n",
        "12": "continuous flood or continuous stream of\n",
        "15": "data coming in and we would like an\n",
        "19": "algorithm to learn from that today many\n",
        "21": "of the largest websites or many of the\n",
        "23": "largest website companies use different\n",
        "26": "versions of online learning algorithms\n",
        "28": "to learn from the flood of users that\n",
        "30": "keep on coming to back to the web site\n",
        "32": "specifically if you have a continuous\n",
        "35": "stream of data generated by a continuous\n",
        "37": "stream of users coming to your website\n",
        "39": "what you can do is sometimes use an\n",
        "42": "online learning algorithm to learn user\n",
        "44": "preferences from the stream of data and\n",
        "46": "use that to optimize some of the\n",
        "52": "decisions on your website suppose you\n",
        "54": "run a shipping service so you know users\n",
        "56": "come and ask you to help ship their\n",
        "58": "packages from location a to location B\n",
        "61": "and suppose you've run a website where\n",
        "63": "users repeat to become and they tell you\n",
        "66": "where they want to send the package from\n",
        "67": "and where they want to send it to so the\n",
        "70": "origin and destination and your website\n",
        "72": "offers to ship the package for some\n",
        "73": "asking pricing as well share your\n",
        "76": "package for $50 I'll ship it with $20\n",
        "79": "and based on the price that you office\n",
        "81": "in the users the users sometimes choose\n",
        "83": "to use a shipping service that's a\n",
        "85": "positive example and sometimes they go\n",
        "89": "away and they do not choose to purchase\n",
        "93": "your shipping service so let's say that\n",
        "95": "we want a learning algorithm to help us\n",
        "98": "to optimize one of the asking price that\n",
        "100": "we want to offer to our users and\n",
        "101": "we want to offer to our users and\n",
        "103": "specifically let's say we come up with\n",
        "104": "some set of features they capture\n",
        "106": "properties are the users so we know\n",
        "108": "anything about the demographics they\n",
        "110": "capture and you know the origin and\n",
        "111": "destination of the package where they\n",
        "114": "want to ship the package and what is the\n",
        "116": "price that we offer to them for shipping\n",
        "118": "the package and what we want to do is\n",
        "121": "learn what is the probability that they\n",
        "124": "will elect to ship the package you using\n",
        "126": "our shipping service given these\n",
        "127": "features\n",
        "129": "and again just as a reminder these\n",
        "132": "features X also captures the price that\n",
        "135": "were asking for and so if we could\n",
        "137": "estimate the chance that they'll agree\n",
        "139": "to I use our service for any given price\n",
        "141": "then we can you know try to pick a price\n",
        "143": "so that they have a pretty high\n",
        "145": "probability of choosing our website\n",
        "148": "while simultaneously hopefully offering\n",
        "150": "us a fair return offering us a fair\n",
        "153": "profit for shipping their package so we\n",
        "156": "can learn this probably of y equals 1\n",
        "157": "given any price and given the other\n",
        "159": "features we can really use this to\n",
        "162": "choose appropriate prices as new users\n",
        "165": "come to us so in order to model the\n",
        "167": "probability of y equals 1 what we can do\n",
        "170": "is use logistic regression or a new a\n",
        "171": "is use logistic regression or a new a\n",
        "172": "network or some other album like that\n",
        "173": "for there starts with logistic\n",
        "178": "regression now if you have a website\n",
        "182": "that just runs continuously here's what\n",
        "184": "an online learning outlet will do I'm\n",
        "187": "gonna repeat forever this just means\n",
        "189": "that our website is going to you know\n",
        "192": "keep on staying up and what happens on\n",
        "195": "the website is occasionally a user will\n",
        "198": "come and for the user that comes we'll\n",
        "204": "get some XY pair right corresponding to\n",
        "207": "a customer or to a user on the website\n",
        "210": "so the features X are you know the\n",
        "212": "origin and destination specified by this\n",
        "213": "user and the price that we happen to\n",
        "216": "offer to them this time rel and Y is\n",
        "218": "either 1 or 0 depending on whether or\n",
        "221": "not they chose to use our shipping\n",
        "224": "service now once we get this XY pair\n",
        "226": "what an online learning algorithm does\n",
        "230": "is then update the parameters theta\n",
        "236": "using just this example X comma Y and in\n",
        "238": "particular we will update my parameters\n",
        "242": "theta as theta J gets updated as theta J\n",
        "246": "minus the learning rate alpha times my\n",
        "251": "usual Radian descent rule for logistic\n",
        "254": "regression so we do this for J equals 0\n",
        "257": "up to n and that's my close curly brace\n",
        "260": "so for other learning algorithms instead\n",
        "262": "of writing XY right I was writing things\n",
        "266": "that X I comma Y I but in this online\n",
        "267": "learning setting we're actually\n",
        "269": "discarding the notion of there being a\n",
        "271": "fixed training set instead we have an\n",
        "274": "algorithm and what happens as we get an\n",
        "276": "example and then we learn using that\n",
        "279": "example like so and then we throw that\n",
        "281": "example away we discount that example\n",
        "284": "and we never use it again and so that's\n",
        "286": "why we just look at one example at a\n",
        "288": "and we learn from that example we\n",
        "290": "discard it which is why you know we're\n",
        "292": "also doing away with this notion of\n",
        "294": "there being this sort of fixed training\n",
        "298": "set index by eye and if you really want\n",
        "301": "run a major website what you really have\n",
        "304": "a continuous stream of users coming then\n",
        "306": "this sort of online learning algorithm\n",
        "307": "is actually a pretty reasonable\n",
        "308": "is actually a pretty reasonable\n",
        "311": "algorithm because of data is essentially\n",
        "314": "free if you have so much data that data\n",
        "316": "is essentially unlimited then there's\n",
        "318": "really maybe no need to look at a\n",
        "320": "training example more than once of\n",
        "322": "course if you had only a small number of\n",
        "325": "users then rather than using an online\n",
        "327": "learning algorithm itthis you might be\n",
        "329": "better off saving away all your data and\n",
        "331": "the fixed training set and then running\n",
        "332": "some algorithm over that training set\n",
        "334": "but if you really have a continuous\n",
        "336": "stream of data then an online learning\n",
        "339": "algorithm can be very effective I should\n",
        "341": "mention also that one interesting effect\n",
        "342": "of this sort of online learning\n",
        "346": "algorithm is that it can adapt to\n",
        "351": "changing user preferences and in\n",
        "354": "particular if over time because of\n",
        "358": "changes in the economy maybe users start\n",
        "360": "to become more price sensitive and\n",
        "361": "willing to pay you less willing to pay\n",
        "363": "high prices or do come less price\n",
        "365": "sensitive and they're willing to pay\n",
        "367": "higher prices or if different things\n",
        "369": "become more important to users if you\n",
        "371": "started new types of users coming to\n",
        "373": "your website this sort of online\n",
        "376": "learning algorithm can also adapt to\n",
        "378": "changing user preferences and kind of\n",
        "379": "keep track of what your changing\n",
        "382": "population of users may be willing to\n",
        "385": "and it does that because if your pool\n",
        "388": "have uses changes then these updates to\n",
        "390": "your parameters data will just slowly\n",
        "392": "adapt your parameters to whatever your\n",
        "395": "latest pool of users move what here's\n",
        "397": "another example of the sort of\n",
        "399": "application to which you might apply\n",
        "401": "online learning this is an application\n",
        "403": "in product search in which we want to\n",
        "406": "apply learning algorithm to learn to\n",
        "408": "give good search listings to a user\n",
        "410": "let's say you run an online store that\n",
        "413": "sells phones themselves mobile phones\n",
        "415": "will come cells cell phones and you have\n",
        "417": "a user interface where a user can come\n",
        "418": "to your website and type in the query\n",
        "421": "like your Android phone 1080p camera so\n",
        "423": "1080p is a type of a specification for a\n",
        "425": "video camera that you might have enough\n",
        "426": "in the phone and the cell phone a mobile\n",
        "430": "phone suppose suppose we have a hundred\n",
        "432": "phones in our sport and because of the\n",
        "434": "way our website is laid out when a user\n",
        "436": "types in the query for the search query\n",
        "439": "we would like to find a choice of 10\n",
        "441": "different phones to show or to offer to\n",
        "444": "the user what we'd like to do is have a\n",
        "446": "learning algorithm help us to figure out\n",
        "448": "what are the 10 phones out of the 100 we\n",
        "451": "should return to a user in response to a\n",
        "453": "user search query you like the 1 year\n",
        "455": "here's how we can go about the problem\n",
        "459": "for each phone and given a specific user\n",
        "463": "query we can construct a feature vector\n",
        "465": "X so the feature vector X might capture\n",
        "467": "different properties of the phone it\n",
        "469": "might capture things like how similar\n",
        "471": "they use the search queries to the phone\n",
        "473": "so we capture things like how many words\n",
        "475": "and they use a search query match the\n",
        "477": "name of the phone how many words they\n",
        "478": "use the search query match the\n",
        "480": "description of the phone and so on so\n",
        "482": "the features X capture properties in the\n",
        "484": "film and it captures things about how\n",
        "487": "similar or how well the phone matches\n",
        "488": "the user query along different\n",
        "489": "the user query along different\n",
        "491": "dimensions what we'd like to do is\n",
        "494": "estimate the probability that the user\n",
        "496": "will click on the link for a specific\n",
        "498": "phone because we want to show the user\n",
        "500": "of phones that they're likely to want to\n",
        "502": "buy want to show the user phones that\n",
        "504": "they have a high probability of clicking\n",
        "507": "on in the web browser so I'm going to\n",
        "510": "define y equals 1 if a user clicks on a\n",
        "512": "link for a phone and y equals 0 other\n",
        "514": "and what I would like to do is learn the\n",
        "516": "probability that user will click on a\n",
        "517": "probability that user will click on a\n",
        "518": "specific phone given you know the\n",
        "520": "features X which capture properties the\n",
        "522": "phone and how well the query matches the\n",
        "525": "phone to give this problem a name in the\n",
        "528": "language of people they run websites\n",
        "529": "like this this the problem of learning\n",
        "532": "this is actually called the problem of\n",
        "534": "learning the predicted click-through\n",
        "536": "rates or predicted CTR just means\n",
        "538": "learning the probability that the user\n",
        "540": "will click on a specific link that you\n",
        "543": "offer them so CPR is an abbreviation for\n",
        "546": "click-through rate and if you can\n",
        "547": "estimate the predicted click-through\n",
        "550": "rate for any particular phone what we\n",
        "552": "can do is use this to show the user the\n",
        "554": "10 phones that the most likely to click\n",
        "556": "on because although the hundred phones\n",
        "560": "we can compute this for each of the\n",
        "561": "hundred phones and just select the 10\n",
        "563": "phones that you know the users most\n",
        "565": "likely to click on and this would be a\n",
        "567": "pretty reasonable way to decide what 10\n",
        "570": "results are showed in the user just be\n",
        "572": "clear and suppose that every time a user\n",
        "574": "does a search we return 10 results what\n",
        "577": "that will do is it will actually give us\n",
        "581": "10 XY pairs so this actually gives us 10\n",
        "583": "training examples every time you user\n",
        "584": "that comes that website because because\n",
        "586": "for the 10 phones that we chose to show\n",
        "588": "the user for each of those 10 phones we\n",
        "590": "get a feature vector X and for each of\n",
        "593": "those 10 films we show the user we will\n",
        "595": "also get a value from Y we will also\n",
        "597": "observe the value of Y depending on\n",
        "600": "whether or not they clicked on that URL\n",
        "603": "and so one way to run the website like\n",
        "605": "this would be to continuously show the\n",
        "607": "user you know your 10 best guesses for\n",
        "609": "what are the phones they might like and\n",
        "611": "so each time we'd use it comes you would\n",
        "614": "get 10 examples 10 XY pairs and then use\n",
        "616": "an online learning algorithm to update\n",
        "619": "the parameters using essentially 10\n",
        "621": "steps of gradient descent on these 10\n",
        "623": "examples and then you can throw the data\n",
        "625": "away and if you really have a continuous\n",
        "627": "stream of users coming to your website\n",
        "629": "this would be a pretty reasonable way to\n",
        "632": "learn parameters for your algorithms so\n",
        "634": "as to show the 10 phones to users that\n",
        "636": "are maybe most promising under their\n",
        "637": "most like\n",
        "639": "so this is a product search problem we\n",
        "641": "are learning to rank phones are learning\n",
        "643": "to search for Phil's example so quickly\n",
        "646": "mention a few others one is if you have\n",
        "647": "a website you're trying to decide you\n",
        "649": "know what special offers to show the\n",
        "653": "user it is very similar to phones or if\n",
        "655": "you have a website and you show\n",
        "657": "different users different news articles\n",
        "659": "so if you're a news aggregator website\n",
        "661": "then you can again use a similar system\n",
        "664": "to select to show to the user you know\n",
        "666": "what are the news articles that they are\n",
        "667": "most likely to be interested in what are\n",
        "669": "the news articles that the most likely\n",
        "671": "click on closely related to special\n",
        "673": "offers will be positive recommendations\n",
        "675": "and in fact if you have a collaborative\n",
        "678": "filtering system you can even imagine\n",
        "680": "the collaborative filtering system\n",
        "682": "giving you additional features to feed\n",
        "684": "into a logistic regression classifier to\n",
        "685": "into a logistic regression classifier to\n",
        "687": "try to predict the click-through rates\n",
        "688": "for different products that you might\n",
        "690": "recommend to a user of course I should\n",
        "691": "recommend to a user of course I should\n",
        "692": "say that any of these problems could\n",
        "696": "also have been formulated as a standard\n",
        "697": "machine learning problem where you have\n",
        "699": "a fixed training set maybe you can run\n",
        "701": "your website for a few days and then\n",
        "703": "save away a training set a fixed\n",
        "704": "training set and run and learning opera\n",
        "708": "on that but these are the actual source\n",
        "709": "of problems where you do see large\n",
        "712": "companies get so much data that there's\n",
        "714": "really maybe no need to save away a\n",
        "716": "fixed training set but instead you can\n",
        "718": "use a but online learning algorithm so\n",
        "720": "just learn continuously from the data\n",
        "722": "that users you are generating on your\n",
        "724": "website\n",
        "727": "so that was the online learning setting\n",
        "729": "and as we saw the algorithm that we\n",
        "731": "applied to this is really very similar\n",
        "732": "to the stochastic gradient descent\n",
        "734": "algorithm only instead of scanning\n",
        "736": "through a fixed training set we're\n",
        "738": "instead getting one example from the\n",
        "740": "users learning from that example in\n",
        "743": "discarding it and moving on and if you\n",
        "745": "have a continuous stream of data for\n",
        "747": "some application this sort of algorithm\n",
        "749": "may be well worth considering for your\n",
        "750": "may be well worth considering for your\n",
        "752": "application and of course one of the\n",
        "754": "advantage of online learning is also\n",
        "756": "that if you have a changing pool of\n",
        "758": "users or if the things are trying to\n",
        "761": "predict are slowly changing user taste\n",
        "763": "slowly changing the online learning\n",
        "765": "algorithm can slowly adapt your learned\n",
        "768": "hypothesis to whatever the latest sets\n"
    },
    "fDQkUN9yw44": {
        "0": " \n",
        "1": "if you run the learning algorithm and it\n",
        "3": "doesn't do as well as you are hoping\n",
        "6": "almost all the time it will be because\n",
        "8": "you have either a high bias problem or a\n",
        "10": "high variance problem in other words\n",
        "12": "either an under fitting problem or an\n",
        "15": "overfitting problem and in this case is\n",
        "16": "very important to figure out which of\n",
        "18": "these two problems is bias or variance\n",
        "20": "or a bit of both that you actually have\n",
        "22": "because knowing which of these two\n",
        "24": "things is happening will give a very\n",
        "26": "strong indicator for what are the useful\n",
        "28": "and promising ways to try to improve\n",
        "31": "your algorithm in this video I'd like to\n",
        "33": "delve more deeply into this bias and\n",
        "35": "variance issue and understand them\n",
        "36": "better\n",
        "38": "that's was figure out how to look at a\n",
        "40": "learning algorithm and evaluate or\n",
        "42": "diagnose whether we might have a bias\n",
        "44": "problem or a variance problem since this\n",
        "46": "would be critical to figuring out how to\n",
        "48": "improve the performance of a learning\n",
        "52": "algorithm that you may implement so\n",
        "53": "you've already seen this figure a few\n",
        "55": "times where if you fit two simple\n",
        "58": "hypothesis like this straight line that\n",
        "60": "under fits the data if you fit a to\n",
        "63": "complex hypothesis then that might fit\n",
        "65": "the training set perfectly but overfit\n",
        "68": "the data and this may be hypotheses of\n",
        "71": "some intermediate level of complexities\n",
        "74": "of some may be degree two polynomials or\n",
        "76": "not too low and not too high degree\n",
        "78": "that's like just right and gives you the\n",
        "80": "best generalization error all of these\n",
        "83": "options now that we're armed with the\n",
        "85": "notion of trained training and\n",
        "87": "validation and test sets we can\n",
        "89": "understand the concepts of bias and\n",
        "91": "variance a little bit better concretely\n",
        "94": "let's let our training error and\n",
        "96": "cross-validation ever be defined as in\n",
        "99": "the previous videos just say the squared\n",
        "101": "error the average squared error as\n",
        "103": "measured on the training sets or as\n",
        "106": "measured on the cross-validation set now\n",
        "108": "let's plot the following figure on the\n",
        "110": "horizontal axis I'm going to plot the\n",
        "113": "degree of polynomial so as I go to the\n",
        "115": "right I'm going to I'm going to be\n",
        "116": "fitting higher and higher order\n",
        "119": "polynomials so we're on the left of this\n",
        "122": "figure where maybe D equals 1 we're\n",
        "124": "going to be fitting very simple\n",
        "126": "functions whereas way on the right of\n",
        "127": "this\n",
        "130": "this maybe D equals four relatively O's\n",
        "133": "mean or maybe even larger numbers I'm\n",
        "135": "going to be fitting very complex high\n",
        "137": "order polynomials that might fit the\n",
        "139": "training set with much more complex\n",
        "145": " \n",
        "148": "whereas we here on the right of the\n",
        "151": "horizontal axis have much larger values\n",
        "153": "of diesel a much higher degree\n",
        "155": "polynomial and so here that's going to\n",
        "158": "correspond to fitting much more complex\n",
        "161": "functions to your training set let's\n",
        "163": "look at the training error and a cross\n",
        "165": "validation error and plot them on this\n",
        "167": "finger let's start with the training\n",
        "170": "error as we increase the degree of the\n",
        "172": "polynomial we're going to be able to fit\n",
        "174": "our training set better and better and\n",
        "177": "so if d equals one whenever relatively\n",
        "179": "high training error if we have a very\n",
        "181": "high degree polynomial our training\n",
        "183": "error is going to really low maybe even\n",
        "184": "zero because they'll fit the training\n",
        "186": "set really well and so as we increase\n",
        "188": "the degree of polynomial we find\n",
        "190": "typically to the training error\n",
        "197": "decreases right J subscript train of\n",
        "198": "data there because our training error\n",
        "202": "tends to decrease with the degree of the\n",
        "204": "polynomial that we fit to the data\n",
        "206": "next let's look at the cross-validation\n",
        "209": "error all for that matter if we look at\n",
        "212": "the test set error we'll get a pretty\n",
        "214": "similar result as a if we were to plot\n",
        "217": "the cross-validation error so we know\n",
        "221": "that if d equals 1 we're fitting a very\n",
        "223": "simple function and so we may be under\n",
        "225": "fitting the training set and so we're\n",
        "227": "going to be a very high cross-validation\n",
        "230": "error if we fit you know an intermediate\n",
        "232": "degree polynomial this we have the D\n",
        "234": "equals 2 in our example in the previous\n",
        "236": "slide we're going to have a much lower\n",
        "238": "cross validation error because we're\n",
        "240": "just fitting finding a much better fit\n",
        "243": "to the data and conversely if D were too\n",
        "247": "high so the talk on sale value of 4 then\n",
        "249": "we're again overfitting and so we end up\n",
        "250": "with a high value for cross-validation\n",
        "254": "error so if you were to very dismay and\n",
        "256": "plot the curve you might end up with a\n",
        "257": "curve like that\n",
        "261": "where that's JCV of\n",
        "263": "negating the view plot J test of theta\n",
        "267": "you get something very similar and so\n",
        "270": "the solar thought also helps us to\n",
        "272": "better understand the notions of bias\n",
        "275": "and variance concretely if you have a\n",
        "278": "learning algorithm that's not performing\n",
        "281": "as well as you wanted it to how can you\n",
        "283": "figure out if your learning algorithm is\n",
        "285": "suffering or quickly\n",
        "287": "suppose you've applied a learning\n",
        "289": "algorithm and it's not as performing as\n",
        "292": "well as you are hoping so so if your\n",
        "294": "cross-validation sets error or your test\n",
        "296": "set error is high how can we figure out\n",
        "298": "if the learning algorithm is suffering\n",
        "300": "from high bias or of the suffering from\n",
        "303": "high variance so the setting of the\n",
        "305": "cross-validation error being high\n",
        "308": "corresponds to either this regime or\n",
        "312": "this regime so this regime on the Left\n",
        "315": "corresponds to a high bias problem that\n",
        "318": "is if you are fitting a overly low order\n",
        "320": "polynomial such as a D equals one when\n",
        "322": "we really needed a higher order\n",
        "325": "polynomial to fit the data whereas in\n",
        "327": "contrast this regime corresponds to a\n",
        "331": "high variance problem that is a D the\n",
        "333": "degree of polynomial was too large for\n",
        "336": "the data set that we have and this\n",
        "338": "figure gives us a clue for how to\n",
        "340": "distinguish between these two cases\n",
        "345": "concretely for the high bias case that\n",
        "347": "is the case of underfitting\n",
        "350": "what we find is that both the\n",
        "352": "cross-validation error and the training\n",
        "355": "error are going to be high so if your\n",
        "357": "algorithm is suffering from a bias\n",
        "363": "problem the training set error will be\n",
        "367": "high and you might find that the\n",
        "371": "cross-validation error were also behind\n",
        "375": "it might be a closed maybe just slightly\n",
        "376": "higher than the training our\n",
        "378": "and so if you see this combination\n",
        "380": "that's a sign that during the outbreak\n",
        "385": "may be suffering from high bias in\n",
        "386": "contrast if your algorithm is suffering\n",
        "390": "from high variance then if you look here\n",
        "394": "well notice that J train that is the\n",
        "399": "training error is going to be low that\n",
        "401": "is your fitting the training set very\n",
        "407": " \n",
        "409": "assuming that this is a say the squared\n",
        "411": "error what you're trying to minimize say\n",
        "414": "whereas in contrast your error on the\n",
        "416": "cross-validation set or your cost\n",
        "418": "function like cross-validation set will\n",
        "421": "be much bigger than your training set\n",
        "424": "error this double greater-than sign here\n",
        "427": "it means much bigger than is much\n",
        "429": "greater than C denoted by day to plate\n",
        "432": "so there's a double greater-than sign\n",
        "434": "that's the math symbol for much greater\n",
        "437": "than denoted by two greater than signs\n",
        "441": "and so if you see this combination then\n",
        "446": "what you find then and so if you see\n",
        "449": "this combination of values then that\n",
        "451": "might give you that's a clue that your\n",
        "453": "learning algorithm may be suffering from\n",
        "455": "high variance and might be overfitting\n",
        "458": "and the key that distinguishes these two\n",
        "460": "cases is if you have a high bias problem\n",
        "462": "your training set error will also be\n",
        "463": "your training set error will also be\n",
        "465": "high your hypothesis is just not fitting\n",
        "467": "the training set well and if you have a\n",
        "469": "high variance problem your training set\n",
        "472": "error will usually be low that is much\n",
        "475": " \n",
        "477": "so hopefully that gives you somewhat\n",
        "479": "better understanding of the two problems\n",
        "481": "of buyers and variants I still have a\n",
        "483": "lot more to say about bias and variance\n",
        "485": "in the next few videos but what we'll\n",
        "488": "see later is that by diagnosing whether\n",
        "489": "a learning algorithm may be suffering\n",
        "492": "from high bias or high variance I'll\n",
        "494": "show you even more details and how to do\n",
        "496": "that in later videos but we'll see that\n",
        "498": "by figuring out whether a learning\n",
        "499": "algorithm may be suffering from high\n",
        "501": "bias or high variance or a combination\n",
        "503": "of both that that would give us much\n",
        "504": "better guidance for what might be\n",
        "506": "promising things to try in order to\n",
        "508": "improve the performance of a learning\n"
    },
    "g2YBWQnqOpw": {
        "0": " \n",
        "1": "in the last video we talked about the\n",
        "4": "Gaussian distribution in this video\n",
        "6": "let's apply that to develop an anomaly\n",
        "11": "detection algorithm let's say that we\n",
        "13": "have an unlabeled training set of M\n",
        "16": "examples and each of these examples is\n",
        "18": "going to be a feature in RN so your\n",
        "21": "training set could be an feature vectors\n",
        "23": "from the loss M aircraft engines the\n",
        "26": "manufactured or it could be features\n",
        "29": "from M users or something else the way\n",
        "31": "we're going to address anomaly detection\n",
        "33": "this would be great to model P effects\n",
        "35": "from this data set so try to figure out\n",
        "38": "what are high probability features and\n",
        "40": "what are the probability types of\n",
        "45": "features so X is a vector and what we're\n",
        "47": "going to do is model P of X as\n",
        "50": "probability of x1 that is of the first\n",
        "53": "component of x times the probability of\n",
        "56": "x2 so probably of the second feature\n",
        "59": "times the probability of third feature\n",
        "62": "and so on up to the probability of the\n",
        "65": "final feature X and then leaving space\n",
        "68": "here because I found something a minute\n",
        "71": "so how do we model each of these terms P\n",
        "74": "of X 1 P of X 2 and so on what we're\n",
        "75": "going to use what we're going to do is\n",
        "78": "assume that the feature X 1 is\n",
        "81": "distributed according to a Gaussian\n",
        "85": "distribution with some mean we write as\n",
        "86": "mu 1 and some variance which I'm going\n",
        "90": "to write as Sigma Squared's 1 and so P\n",
        "92": "of X 1 is going to be a Gaussian\n",
        "94": "probability distribution with mean mu1\n",
        "98": "and variance Sigma Squared's 1 and\n",
        "101": "similarly I've been assumed that X 2 is\n",
        "103": "distributed Gaussian that's what does\n",
        "105": "the Moto that stands for the mean is\n",
        "107": "distributed this disputed Gaussian with\n",
        "111": "mean mu 2 and Sigma squared - so it's\n",
        "113": "distributed according to a different\n",
        "115": "Gaussian which has a different set of\n",
        "118": "parameters mu 2 Sigma squared 2 and\n",
        "124": "similarly you know X 3 is yet another\n",
        "127": "Gaussian and so this can have a\n",
        "129": "different mean and the different\n",
        "131": "standard deviation then the other\n",
        "136": "features and so on up to xn\n",
        "139": "and so that's my model just as a side\n",
        "141": "comment for those of you that are\n",
        "142": "experts in statistics it turns out that\n",
        "144": "this equation that I just wrote up\n",
        "146": "actually corresponds to an independence\n",
        "148": "assumption on the values of the features\n",
        "151": "x1 through xn but in practice it turns\n",
        "153": "out that the Avrum are going to describe\n",
        "155": "it works just fine whether or not these\n",
        "157": "features are anywhere close to\n",
        "158": "independence and even if the\n",
        "160": "independence assumption doesn't have it\n",
        "163": "true this airman was just fine but in\n",
        "164": "case you don't know those terms that\n",
        "166": "just use you know independence of some\n",
        "168": "students and so on don't worry about it\n",
        "171": "the uu be able to understand and\n",
        "173": "implement this algorithm just fine and\n",
        "174": "that comment was really meant only for\n",
        "179": "the experts in statistics and finally in\n",
        "182": "all the Raptors up let me take this\n",
        "184": "expression and writes a little bit more\n",
        "187": "compactly so I'm going to write this is\n",
        "191": "a product from J equals 1 through n of P\n",
        "197": "of XJ parameters by mu J comma Sigma\n",
        "203": "squared J so this funny symbol here\n",
        "205": "especially a capital Greek alphabet PI\n",
        "207": "that funny symbol there corresponds to\n",
        "209": "taking the product of a set of values\n",
        "212": "and so you know you're familiar with the\n",
        "214": "summation notation so that write sum\n",
        "218": "from I equals 1 through n you know if I\n",
        "222": "this means 1 plus 2 Plus 3 plus daughter\n",
        "225": "thought up to n whereas this this funny\n",
        "228": "symbol here this product symbol read the\n",
        "231": "product from I equals 1 through n of I\n",
        "233": "then this means that it's just a\n",
        "234": "summation except that we're now\n",
        "238": "multiplying so this becomes 1 x 2 x 3 x\n",
        "240": "dot dot dot of\n",
        "243": "and so using this product notation this\n",
        "245": "product from J equals 1 through n of\n",
        "247": "this expression that's just a more\n",
        "250": "compact really shorter way of writing\n",
        "253": "out this huge product of all of these\n",
        "256": "terms out there since we're taking these\n",
        "258": "P of XJ given mu J comma Sigma squared J\n",
        "260": "terms that were multiplying them\n",
        "263": "together and by the way the problem of\n",
        "266": "estimating this distribution P of X s\n",
        "268": "sometimes called the probably the\n",
        "270": "problem of density estimation so has the\n",
        "273": "title of the slide\n",
        "275": "so putting everything together here's\n",
        "278": "our anomaly detection algorithm the\n",
        "281": "first step is to choose speeches or come\n",
        "283": "up with features X I that we think might\n",
        "286": "be indicative of anomalous example so\n",
        "288": "what I mean by that is Charlie come over\n",
        "291": "features so that when there's an unusual\n",
        "293": "user in your system that would be doing\n",
        "295": "fraudulent things or when for the\n",
        "296": "aircraft engine example well you know\n",
        "297": "there's something funny is something\n",
        "298": "there's something funny is something\n",
        "299": "strange about one of the aircraft\n",
        "302": "engines choose features anxiety that you\n",
        "304": "think might take on unusually large\n",
        "307": "values or unusually small values for\n",
        "310": "what an anomalous example might look\n",
        "312": "like but more generally you know just\n",
        "313": "try to choose features that describe\n",
        "316": "what you think that describe general\n",
        "318": "properties of the things that you're\n",
        "320": "collecting data on Nick's given a\n",
        "325": "training set of M unlabeled examples x1\n",
        "329": "through XM we then fit the parameters mu\n",
        "331": "1 through mu N and Sigma squared one is\n",
        "333": "from Sigma square n and so these were\n",
        "336": "the formulas similar to the formulas we\n",
        "337": "had in the previous video that we're\n",
        "339": "going to use to estimate each of these\n",
        "342": "parameters and just to give some\n",
        "346": "interpretation mu J that's my average\n",
        "349": "value of the J feature right so mu J\n",
        "352": "goes in this term P of XJ which is\n",
        "355": "parameterized by mu J and Sigma square J\n",
        "358": "and so this says you know for the MU J\n",
        "361": "just take the mean over my training set\n",
        "364": "of the values of the J feature and just\n",
        "366": "I mentioned that you do this you you\n",
        "369": "compute these formulas for J equals 1\n",
        "371": "through n so use these formulas to\n",
        "375": "estimate mu 1 to MU 2 and so on up to\n",
        "378": "you N and similarly for Sigma squared\n",
        "380": "and it's also possible to come up with\n",
        "382": "vectorized versions of these so if you\n",
        "386": "think of mu as a vector so mu is a\n",
        "389": "Tara this new one new to down team UN\n",
        "394": "then a vectorized version of that said\n",
        "396": "the parameters can be written like so\n",
        "399": "some for my equals one to end thanks I\n",
        "401": "and so this formula that I just rolled\n",
        "404": "out estimates you know where these X I\n",
        "407": "are the feature vectors this SVM you for\n",
        "409": "all the values of n simultaneously and\n",
        "410": "there's also possible to come up with a\n",
        "413": "vectorized formula for estimating Sigma\n",
        "417": "squared J finally when you given a new\n",
        "419": "example so when you have a new aircraft\n",
        "420": "engine then you want to know is this\n",
        "422": "aircraft engine anomalous what we need\n",
        "424": "to do is then compute P of X what's the\n",
        "427": "probability of this new example so P of\n",
        "430": "X is equal to this product and what you\n",
        "432": "implement what you compute is this\n",
        "436": "formula and we're over here you know\n",
        "439": "this thing here is just the formula for\n",
        "440": "the Gaussian probability and so you\n",
        "443": "compute this thing and finally if this\n",
        "445": "probability is very small you didn't\n",
        "447": "flag this thing as an anomaly here's an\n",
        "450": "example of an application of this method\n",
        "452": "that thing we have this data set plotted\n",
        "457": "on the upper left of this slide if you\n",
        "458": "look at this one let's look at the\n",
        "460": "features of x1 if you if you look at\n",
        "461": "this data set it looks like on average\n",
        "464": "the features x1 has a mean of about 5\n",
        "468": "and standard deviation if you look at\n",
        "470": "just the X 1 values of this data set it\n",
        "472": "has a standard deviation that may be 2\n",
        "476": "so that's Sigma 1 and looks like you\n",
        "479": "notice x2 the values of the features as\n",
        "481": "measured on the vertical axis and looks\n",
        "482": "like that has an average value of about\n",
        "485": "3 and the standard deviation of 1 so you\n",
        "487": "think to say this end and if you\n",
        "489": "estimate you know me1 me2 Sigma 1 Sigma\n",
        "492": "2 this is what you can gain I'm writing\n",
        "493": "Sigma here I'm thinking of standard\n",
        "495": "deviations but the formula on the\n",
        "497": "previous slide actually gave estimates\n",
        "498": "for the squares of these\n",
        "500": "mr. Cygnus great one in signal salinity\n",
        "503": "so just be careful whether using Sigma 1\n",
        "505": "Sigma 2 or Sigma square one or the Sigma\n",
        "507": "square 2 so Sigma 2 great one of course\n",
        "509": "would be equal to 4 for example there's\n",
        "513": "a square of 2 and in pictures what P of\n",
        "516": "x1 parametrized by mu 1 is Sigma square\n",
        "518": "one in P of x2 parametrized by mu 2 and\n",
        "520": "Sigma square - that would look like\n",
        "522": "these two distributions over here and\n",
        "525": "turns out that if you were to plot your\n",
        "526": "turns out that if you were to plot your\n",
        "528": "P of X right which is the product of\n",
        "530": "these two things you can actually get a\n",
        "532": "surface part that looks like this so\n",
        "535": "this is a part of P of X where the\n",
        "538": "height above this where the height of\n",
        "540": "the surface at a particular point so I'm\n",
        "543": "given a particular you know X 1 X 2\n",
        "546": "values of the fix 2 if X y equals 2x\n",
        "548": "equals 2 that's this point and the\n",
        "551": "height of this 3d surface here that's P\n",
        "554": "of X so P of X that is the height of\n",
        "557": "this plot so this is literally just P of\n",
        "560": "X 1 term rise by nu 1 Sigma squared 1\n",
        "565": "times P of X 2 parametrized by mu 2\n",
        "569": "Sigma square 2 now so this is how we fit\n",
        "572": "the parameters to this data let's say we\n",
        "574": "have a couple new examples maybe have a\n",
        "577": "new example there it's just an anomaly\n",
        "579": "or not so maybe I have a different\n",
        "581": "example you know maybe have a second\n",
        "583": "example over there so is that an anomaly\n",
        "587": "or not the way we do that is we would\n",
        "589": "set some value for epsilon let's say\n",
        "591": "I've chosen epsilon equals 0.02 we'll\n",
        "592": "see what was in the state I'll see later\n",
        "595": "how we choose epsilon but let's take\n",
        "597": "this first example then we call this\n",
        "598": "example x1\n",
        "601": "test and let me call the second example\n",
        "604": "you know x2\n",
        "607": "test what we do is B then compute P of\n",
        "609": "x1 test so we use this formula to\n",
        "612": "compute it and this looks like a pretty\n",
        "614": "large value in particular this is\n",
        "618": "greater than was greater than equal to\n",
        "619": "Epsilon and so there's a pretty high\n",
        "621": "probability at least bigger than epsilon\n",
        "624": "so we'll say that x1 test is not an\n",
        "627": "anomaly whereas if you compute P of x2\n",
        "629": "test well that's just a much smaller\n",
        "631": "value so this is less than epsilon\n",
        "634": "instead we'll say that that is indeed an\n",
        "636": "anomaly but this is much smaller than\n",
        "639": "that epsilon that we chose and in fact I\n",
        "641": "didn't prove it here you know what this\n",
        "642": "is really saying is that if you look at\n",
        "646": "this 3d surface plot is saying that all\n",
        "649": "the values of x1 and x2 that have a high\n",
        "652": "height above the surface corresponds to\n",
        "655": "a non-anomalous of an OK or normal\n",
        "657": "example whereas all the points far out\n",
        "659": "here you know all the points out here\n",
        "661": "all those points have very low\n",
        "663": "probability and so we're gonna flag\n",
        "664": "probability and so we're gonna flag\n",
        "665": "those points as anomalous and so it's\n",
        "667": "going to define some region that maybe\n",
        "669": "looks like this so that everything\n",
        "674": "outside of this affects as anomalous\n",
        "677": "whereas the things inside this lips I\n",
        "680": "just drew if it considers okay or not\n",
        "681": "but normal that's not at all this\n",
        "685": "example so this example x2 test you know\n",
        "687": "lies outside that region so it has very\n",
        "689": "small probability and so we considered\n",
        "692": "an anomalous example in this video we\n",
        "694": "talked about how to estimate P of X the\n",
        "696": "probability of X for the purpose of\n",
        "698": "developing an anomaly detection\n",
        "701": "algorithm and in this video we also step\n",
        "704": "through an entire process of giving and\n",
        "706": "instead you know fitting the parameters\n",
        "709": "doing practice mission to get mu and\n",
        "711": "Sigma parameters and then taking new\n",
        "713": "examples and designing of the new\n",
        "715": "examples our anomalous or not in the\n",
        "717": "next few videos we'll delve deeper into\n",
        "720": "this algorithm and talk a bit more about\n"
    },
    "gAKQOZ5zIWg": {
        "0": " \n",
        "1": "in this video I want to tell you about\n",
        "4": "how to use neural networks to do\n",
        "6": "multi-class classification where we may\n",
        "8": "have more than one category that we're\n",
        "11": "trying to distinguish in it in the last\n",
        "14": "part of the last video where we had the\n",
        "16": "hammer ten digit recognition problem\n",
        "17": "that was actually a multi-class\n",
        "19": "classification problem because there\n",
        "21": "were ten possible categories for\n",
        "23": "recognizing the digits from 0 through 9\n",
        "26": "and so if you want to fill you in on the\n",
        "29": "details on how to do that\n",
        "32": "the way we do multi-class classification\n",
        "35": "in a neural network is essentially an\n",
        "37": "extension of the one versus all method\n",
        "40": "so let's say that we have a computer\n",
        "43": "vision example where instead of just\n",
        "45": "trying to recognize cause as in the\n",
        "47": "original example that started off with\n",
        "49": "but let's say that we're trying to\n",
        "52": "recognize you know four categories of\n",
        "53": "objects so given an image we want to\n",
        "55": "decide if it's a pedestrian a car a\n",
        "58": "motorcycle or a truck if that's the case\n",
        "61": "what we would do is we would build a\n",
        "64": "neural network with four output units so\n",
        "66": "that our neural network now outputs a\n",
        "70": "vector of four numbers so the output now\n",
        "72": "is actually going to be a vector of four\n",
        "74": "numbers and what we're going to try to\n",
        "77": "do is get the first output unit to\n",
        "80": "classify is the image of pedestrian yes\n",
        "83": "or no the second unit to classify is the\n",
        "85": "emission car yes or no this unit to\n",
        "89": "classify is the image a motorcycle yes\n",
        "91": "or no and just to classify is the image\n",
        "95": "a truck yes or no and thus when the\n",
        "97": "image is of a pedestrian we would\n",
        "100": "ideally want two networks up in 100 row\n",
        "102": "when there's a car we wanted to output\n",
        "105": "Oh 100 ones a motorcycle we get it too\n",
        "108": "or rather we want it to output over 100\n",
        "112": "and so on so this is just like the one\n",
        "114": "versus all method that we talked about\n",
        "117": "when we were describing logistic\n",
        "120": "regression and here we have essentially\n",
        "122": "for logistic regression classifier each\n",
        "124": "of which is trying to reckon\n",
        "127": "is one of the four horses that we want\n",
        "130": "to distinguish among so rearranging\n",
        "132": "slide a bit here's our neural network\n",
        "135": "with four upper units and those are one\n",
        "137": "one H of X to be when we have the\n",
        "140": "different images and the way we're going\n",
        "142": "to represent the training set in these\n",
        "144": "settings is as follows so when we have a\n",
        "146": "training set with different images of\n",
        "149": "pedestrians cars motorcycles and trucks\n",
        "151": "what we're going to do in this example\n",
        "154": "is that whereas previously we had\n",
        "157": "written out the labels as Y being an\n",
        "162": "integer from 1 2 3 or 4 instead of\n",
        "165": "representing Y this way we're going to\n",
        "168": "instead represent Y as as follows namely\n",
        "175": "Y I will be either a 100 100 or 100 1\n",
        "178": "depending on what the corresponding\n",
        "180": "image X items and so one training\n",
        "183": "example will be one pair X I comma Y\n",
        "186": "where X I is an image with you know one\n",
        "188": "of the four objects and why I will be\n",
        "191": "one of these vectors and hopefully we\n",
        "194": "can find a way to get our network to\n",
        "197": "output some value so that H of X is\n",
        "202": "approximately Y and both H of X and y i\n",
        "205": "both of these are going to be in our\n",
        "207": "example for dimensional vectors when we\n",
        "211": "have four classes\n",
        "213": "so that's how you get a neural network\n",
        "216": "to do multi France classification this\n",
        "218": "wraps up our discussion on how to\n",
        "220": "represent neural network status on a\n",
        "223": "hypothesis representation in the next\n",
        "225": "set of videos let's start to talk about\n",
        "227": "how to take a training set and how to\n",
        "229": "automatically learn the parameters of\n"
    },
    "gPegoVYp64w": {
        "0": " \n",
        "1": "in this video I'd like to start talking\n",
        "4": "about how to multiply together two\n",
        "6": "matrices we'll start with a special case\n",
        "8": "of that of matrix vector multiplication\n",
        "11": "of multiplying a matrix together with a\n",
        "13": "vector let's start with an example\n",
        "16": "here's a matrix and here's a vector and\n",
        "19": "let's say we want to multiply together\n",
        "23": "this matrix with this vector what's the\n",
        "25": "result let me just work through this\n",
        "27": "example and then we can step back and\n",
        "29": "look at just what the steps were it\n",
        "31": "turns out the result of this\n",
        "33": "multiplication process is going to be\n",
        "35": "itself a vector and I'm going to work\n",
        "37": "with this first the later we'll come\n",
        "38": "back and see you know just what I did\n",
        "41": "here to get the first element of this\n",
        "43": "vector I'm going to take these two\n",
        "48": "numbers and multiply them with the first\n",
        "50": "row of the matrix and add up the\n",
        "53": "corresponding numbers and I take one\n",
        "57": "multiplied by one and take three and\n",
        "62": "multiply by five and that's why there's\n",
        "64": "one plus 15 so that gives me 16 and I\n",
        "70": "write 16 here then for the second row\n",
        "73": "second element I'm going to take the\n",
        "75": "second row and multiply by this vector\n",
        "81": "so I have 4 times 1 plus 0 times 5 which\n",
        "85": "is equal to 4 so I get a 4 there and\n",
        "89": "finally for the last one I have 2 1\n",
        "95": "times 1 5 7 2 by 1 plus 1 by 5 which is\n",
        "101": "equal to 7 and so I get a 7 over there\n",
        "106": "ok and it turns out that the results of\n",
        "110": "multiplying a that's a 3 by 2 matrix by\n",
        "113": "a 2 by 1 matrix there's also just a two\n",
        "116": "dimensional vector the result of this is\n",
        "123": "going to be a 3 by 1 matrix so that's\n",
        "126": "why I get a 3 by 1 matrix\n",
        "130": "in other words a 3x1 matrix is just a\n",
        "133": "three-dimensional vector so I realized\n",
        "135": "that I did that pretty quickly and\n",
        "137": "you're pretty not sure you can repeat\n",
        "139": "this process yourself but let's look in\n",
        "141": "more detail at was just at what just\n",
        "142": "happened and what this process of\n",
        "144": "multiplying a matrix by a vector looks\n",
        "147": "like here are the details of how to\n",
        "150": "multiply a matrix by a vector let's say\n",
        "152": "I have a matrix a and I want to multiply\n",
        "156": "that by a vector X the result is going\n",
        "159": "to be you know some vector Y so the\n",
        "162": "matrix a here is an M by n dimensional\n",
        "165": "matrix so M rows and n columns and I'm\n",
        "167": "going to multiply that by a n by 1\n",
        "168": "matrix so in other words line n\n",
        "171": "dimensional vector it turns out this n\n",
        "174": "here has to match this n here in other\n",
        "176": "words the number of columns in this\n",
        "179": "matrix so the number of columns you have\n",
        "183": "to be n columns number of columns here\n",
        "185": "has to match the number of rows here\n",
        "187": "where has to match the dimension of this\n",
        "190": "vector and the result of this product is\n",
        "196": "going to be an M dimensional vector Y ok\n",
        "199": "so in other words the number of rows\n",
        "204": "here M is going to be equal to you know\n",
        "208": "the number of rows in this matrix a so\n",
        "210": "how do you actually compute this vector\n",
        "212": "Y well it turns out to compute this\n",
        "215": "vector Y the process is to get Y I\n",
        "219": "multiply a row with the elements of the\n",
        "221": "vector X and add them up so here's what\n",
        "222": "vector X and add them up so here's what\n",
        "226": "I mean in order to get the first element\n",
        "229": "of Y you that first number whether that\n",
        "231": "turns out to me we're going to take the\n",
        "235": "first row of the matrix a and multiply\n",
        "238": "them one at a time with the elements of\n",
        "241": "this vector X so we'll take this first\n",
        "243": "number multiplied by this verse number\n",
        "246": "then take the second number multiplied\n",
        "248": "by the second number take this third\n",
        "249": "number whatever\n",
        "251": "multiply the third number and so on\n",
        "253": "until we get to the end and I'm going to\n",
        "255": "add up the results of these products and\n",
        "257": "the result of hang that up is going to\n",
        "260": "give us this first element of Y then\n",
        "262": "when we want to get the second element\n",
        "265": "of Y they say this element the way we do\n",
        "268": "that is we take the second row of a and\n",
        "270": "we repeat the whole thing is we take the\n",
        "272": "second row of a and multiply\n",
        "275": "element-wise with the elements of X and\n",
        "277": "add up the results of the products and\n",
        "279": "that would give me the second element of\n",
        "281": "Y and keep going to get and then I'm\n",
        "283": "going to take the third row of a\n",
        "286": "multiply element wise you know with the\n",
        "288": "vector X some of the results and then I\n",
        "290": "get the third element and so on until I\n",
        "295": "get down to the last row so okay\n",
        "298": "so that's the procedure let's do one\n",
        "303": "more example here's the example so let's\n",
        "305": "look at the dimensions right here this\n",
        "310": "is a three by four dimensional matrix\n",
        "313": "this is a four dimensional vector or a 4\n",
        "316": "by 1 matrix and so the result of this\n",
        "318": "the result of this product is going to\n",
        "320": "be a three dimensional vector I'm going\n",
        "324": "to write a vector with room for three\n",
        "329": "elements let's do the let's carry out\n",
        "331": "the products so for the first element\n",
        "334": "I'm going to take these phone numbers\n",
        "337": "and multiply them with the vector X so I\n",
        "342": "have 1 times 1 plus 2 times 3 plus 1\n",
        "348": "times 2 plus 5 times 1 which is equal to\n",
        "354": "s1 plus 6 plus 2 plus 6 which gives me\n",
        "359": "14 and then for the second element I'm\n",
        "361": "going to take this row now and multiply\n",
        "366": "it with this vector as I start with 0\n",
        "371": "one plus three right so 0 times 1 plus 3\n",
        "379": "times 3 plus 0 times 2 plus 4 times 1\n",
        "380": "times 3 plus 0 times 2 plus 4 times 1\n",
        "383": "which is equal to let's see that's 9\n",
        "387": "plus 4 just 13 and finally for the last\n",
        "389": "element I'm going to take this last row\n",
        "395": "so I have a minus 1 times 1 yup - 2 or\n",
        "399": "radians plus negative 2 I guess times 3\n",
        "403": "plus 0 times 2 plus 0 times 1 and so\n",
        "405": "that's going to be minus 1 minus 6 is\n",
        "408": "negative 7 so that's negative 7\n",
        "412": "ok so my final answer is this vector 14\n",
        "415": "just write to that without the colors 14\n",
        "423": "13 negative 7 and as promised the result\n",
        "428": "here is a three by one matrix so that's\n",
        "431": "how you multiply a matrix and a vector I\n",
        "433": "know that a lot just happened on the\n",
        "434": "slide so if you're not quite sure where\n",
        "436": "all these numbers when you know feel\n",
        "438": "free to pause the video and so take a\n",
        "440": "slow careful look at this big\n",
        "442": "calculation that we just did try to make\n",
        "443": "sure that you understand the steps of\n",
        "445": "what just happened to get us these\n",
        "449": "numbers 14 13 7\n",
        "452": "finally let me show you a neat trick\n",
        "455": "let's say we have a set of four houses\n",
        "458": "so four houses with four sizes like\n",
        "460": "these and let's say I have a hypothesis\n",
        "461": "for predicting what is the price of a\n",
        "465": "house and let's say I want to compute\n",
        "467": "you know H of X for each of my four\n",
        "470": "houses here it turns out there's a neat\n",
        "472": "way of posing this they're applying this\n",
        "475": "hypothesis that all of my houses at the\n",
        "477": "same time it's the teens turns out\n",
        "479": "there's a neat way to poses as a matrix\n",
        "483": "vector multiplication so here's how I'm\n",
        "484": "going to do it I'm going to construct\n",
        "487": "the matrix as follows my matrix is going\n",
        "492": "to be 1 1 1 1 times and I want to write\n",
        "495": "down you know the sizes of my four\n",
        "500": "houses here and I'm going to construct a\n",
        "504": "vector as well and my vector is going to\n",
        "509": "be this vector of two elements that's\n",
        "512": "here these are minus 40 and 0.25 that's\n",
        "515": "these two coefficients theta 0 and theta\n",
        "517": "1 and what I'm going to do is take this\n",
        "519": "matrix and that vector and multiply them\n",
        "521": "together at times is this multiplication\n",
        "526": "symbol so what I get well this is a 4 by\n",
        "532": "2 matrix this is a 2 by 1 matrix so the\n",
        "536": "outcome is going to be a 4 by 1 vector\n",
        "544": "right so let me oh so this is going to\n",
        "548": "be a 4 by 1 matrix is the outcome or 4/1\n",
        "549": "really a 4 dimensional vector\n",
        "550": "really a 4 dimensional vector\n",
        "552": "so let me write in one of my 4 elements\n",
        "556": "we have 4 real numbers here now it turns\n",
        "559": "out the first element of this result\n",
        "561": "right the way I'm going to get that is\n",
        "563": "I'm going to take this and multiply it\n",
        "566": "by this vector and so this is going to\n",
        "571": "be minus 40 times 1\n",
        "578": "plus 0.25 times 2 1 0 4 by the way on\n",
        "581": "the earlier slides I was writing 1 times\n",
        "584": "minus 40 and 2 104 times 0.25 right but\n",
        "585": "the order doesn't matter\n",
        "588": "minus 40 times 1 that's the same as 1\n",
        "590": "times minus 40 and this first element of\n",
        "596": "course is H applied to to 104 so it's\n",
        "598": "really the predicted price of my first\n",
        "602": "house well how about the second element\n",
        "605": "hope you can see where I'm going to get\n",
        "608": "the second element right I'm going to\n",
        "610": "take this and multiply it by my vector\n",
        "613": "and so that's going to be minus 40 times\n",
        "619": "1 plus 0.25 times 1 for 1 6 and so this\n",
        "626": "is going to be H of 1 4 1 6 right and so\n",
        "630": "on for the 3rd and DM and so on for the\n",
        "635": "3rd and the 4th elements of this 4 by 1\n",
        "637": "vector and just be clear right this\n",
        "639": "thing here this thing here did I just\n",
        "641": "drew the green box around that's a real\n",
        "643": "number ok that's a single real number\n",
        "646": "and this thing here did I drew the\n",
        "648": "magenta box around the purple magenta\n",
        "650": "color box around that's a real number\n",
        "653": "right and so this thing on the right is\n",
        "654": "this thing on the right overall this is\n",
        "657": "a 4 by 1 dimensional matrix with a 4\n",
        "660": "dimensional vector and the neat thing\n",
        "662": "about this is that when you're actually\n",
        "664": "implementing this in software so when\n",
        "666": "you have four houses and when you want\n",
        "669": "to use your hypothesis to predict the\n",
        "671": "prices within the price why of all of\n",
        "673": "these four houses what this means is\n",
        "674": "that you know you can you can write this\n",
        "676": "in one line of code when we talk about\n",
        "678": "octave and programming languages later\n",
        "681": "you can actually you actually write this\n",
        "684": "in one line of code prediction equals my\n",
        "692": "you know data matrix times um parameters\n",
        "697": "right where data matrix is this thing\n",
        "699": "here and parameters existing here and\n",
        "703": "this x is a matrix vector multiplication\n",
        "705": "and if you just do this then this\n",
        "708": "variable prediction sorry for my bad\n",
        "709": "handwriting but we just you know\n",
        "711": "implement this one line of code\n",
        "713": "assuming you have an appropriate library\n",
        "715": "to do a major exact modification if you\n",
        "718": "just do this then prediction becomes\n",
        "720": "this all by one dimensional vector on\n",
        "722": "the right that just gives you all the\n",
        "726": "predicted prices and the alternative you\n",
        "728": "know to doing this as a matrix vector\n",
        "730": "multiplication would be to write\n",
        "731": "something like you know for I equals 1\n",
        "732": "something like you know for I equals 1\n",
        "734": "to 4 right and you have say a thousand\n",
        "737": "houses it before I equals 1 mm whatever\n",
        "739": "open you have to write a prediction you\n",
        "743": "know of I equals and then do a bunch\n",
        "746": "more work over there and it turns out\n",
        "747": "that when you have a large number of\n",
        "749": "houses if you were trying to predict the\n",
        "751": "prices of not just for that maybe of a\n",
        "754": "thousand houses then it turns out that\n",
        "756": "when you implement this in the computer\n",
        "759": "implementing it like this in any of\n",
        "761": "various languages this is not true only\n",
        "763": "for all to it but for super sauce Java\n",
        "765": "Python other high level other languages\n",
        "767": "as well it turns out that by writing\n",
        "770": "code in this style on the left it allows\n",
        "773": "you to not only simplify the code\n",
        "775": "because now you're just writing one line\n",
        "777": "of code rather than the formula bunch of\n",
        "779": "things inside but for subtle reasons\n",
        "781": "that we'll see later it turns out to be\n",
        "784": "much more computationally efficient to\n",
        "786": "make predictions on all of the prices of\n",
        "788": "all of your houses doing it the way on\n",
        "791": "the left then the way on the right then\n",
        "792": "if you were to run your own for loop and\n",
        "795": "I'll say more about this later when we\n",
        "798": "talk about vectorization but so by post\n",
        "799": "a prediction this way you get not only a\n",
        "801": "simple piece of code but a more\n",
        "805": "efficient one so that's it for matrix\n",
        "807": "vector multiplication and we'll make\n",
        "809": "good use of these sorts of operations as\n",
        "811": "we develop the linear regression and\n",
        "813": "other models further but in the next\n",
        "815": "video we're going to take this and\n",
        "818": "generalizes to the case of matrix matrix\n"
    },
    "giIXNoiqO_U": {
        "0": " \n",
        "2": "in this next set of videos I'd like to\n",
        "4": "tell you about recommender systems there\n",
        "6": "are two reasons I had two motivations\n",
        "8": "but why I wanted to talk about\n",
        "10": "recommender systems the first is just\n",
        "12": "that there's an important application of\n",
        "14": "machine learning over the last few years\n",
        "16": "occasionally I visit different\n",
        "18": "technology companies here in Silicon\n",
        "20": "Valley and I often talk to people\n",
        "21": "working on machine learning applications\n",
        "23": "there and so the most people what are\n",
        "25": "your most important applications of\n",
        "27": "machine learning or one of the machine\n",
        "28": "learning applications that you would\n",
        "30": "most like to get an improvement in the\n",
        "32": "performance of and one of the most\n",
        "34": "frequent answer I heard was that there\n",
        "36": "are many groups up in Silicon Valley now\n",
        "38": "trying to build better recommender\n",
        "41": "systems so if you think about what of\n",
        "44": "the website of the Amazon or what\n",
        "46": "Netflix or what's eBay or what's up\n",
        "49": "iTunes genius made by Apple this there\n",
        "51": "are many websites or systems that try to\n",
        "53": "recommend new promised views so Amazon\n",
        "55": "recommends the books to you Netflix try\n",
        "57": "to recommend movies through and so on\n",
        "59": "and these sorts of recommender systems\n",
        "60": "and these sorts of recommender systems\n",
        "61": "that look at what books you may have\n",
        "63": "purchased in the past or what movies you\n",
        "65": "rated in the past but these sorts of\n",
        "67": "systems that are responsible for today a\n",
        "70": "substantial fraction of Amazon's revenue\n",
        "72": "and for computing Netflix the\n",
        "74": "recommendations that they make to the\n",
        "76": "users is also responsible for\n",
        "78": "substantial fraction of the movies\n",
        "80": "watched by the users and so an\n",
        "82": "improvement in the performance of a\n",
        "84": "recommender system can have a\n",
        "87": "substantial and immediate impact on the\n",
        "90": "bottom line of many of these companies o\n",
        "92": "recommender systems is kind of a funny\n",
        "95": "problem within academic machine learning\n",
        "96": "so that you could go to an academic\n",
        "99": "elearning conference the problem of\n",
        "100": "recommender systems actually receives\n",
        "102": "you know relatively little attention or\n",
        "104": "releases of a smallish fraction of what\n",
        "107": "goes on within academia but if you look\n",
        "109": "at what's happening many technology\n",
        "111": "companies the ability to build these\n",
        "113": "systems seems to be a high priority for\n",
        "115": "many companies and that's one of the\n",
        "116": "reasons why I want to talk about it in\n",
        "119": "response the second reason that I want\n",
        "121": "to talk about recommender systems is\n",
        "124": "that as we approach the last few sets of\n",
        "126": "videos of this class I wanted to talk\n",
        "128": "about a few of the big ideas in machine\n",
        "129": "learning\n",
        "131": "share view also the big ideas in machine\n",
        "134": "learning and we've already seen in this\n",
        "136": "class that features are important for\n",
        "138": "machine learning the features you choose\n",
        "141": "will have a big effect on the\n",
        "142": "performance of your learning algorithm\n",
        "144": "so this is Big Idea machine learning\n",
        "146": "which is that for some problems\n",
        "148": "maybe not all problems with some\n",
        "151": "problems there are algorithms that can\n",
        "154": "try to automatically learn a good set of\n",
        "156": "features for you so rather than trying\n",
        "157": "to hand design your hand code the\n",
        "159": "features which is mostly what them do so\n",
        "161": "far there are a few settings where you\n",
        "162": "might be able to have an algorithm just\n",
        "163": "might be able to have an algorithm just\n",
        "165": "learn what features and use and the\n",
        "166": "recommender systems is just one example\n",
        "168": "that sort of setting there are many\n",
        "170": "others but in going through recommender\n",
        "173": "systems will be able to go a little bit\n",
        "174": "into this idea of learning the features\n",
        "175": "into this idea of learning the features\n",
        "177": "and you'll be able to see at least one\n",
        "179": "example of this I think big idea in\n",
        "181": "machine learning as well so without\n",
        "184": "further ado let's get started and talk\n",
        "185": "about the recommender system problem\n",
        "189": "formulation as my running example I'm\n",
        "192": "going to use the modem problem of\n",
        "194": "predicting movie ratings so here's a\n",
        "197": "problem imagine that you're a website or\n",
        "200": "a company that sells or rent some movies\n",
        "201": "or what have you and so you know Amazon\n",
        "204": "and Netflix and I think iTunes are all\n",
        "207": "examples of company I do our examples of\n",
        "210": "companies that do this and let's say you\n",
        "212": "let your users rate different movies\n",
        "215": "using a one to five star rating so using\n",
        "218": "rate you know something one two three\n",
        "221": "four or five stars in order to make this\n",
        "223": "example just a little bit nicer I'm\n",
        "226": "going to allow the 0 to 5 stars as well\n",
        "228": "because that just makes some of the math\n",
        "230": "come out nicer although most of most of\n",
        "231": "these websites use it one to five stars\n",
        "236": "view so here I have five movies no love\n",
        "238": "the last romance forever a few puppies\n",
        "241": "of love nonstop consciousness sources\n",
        "244": "the karate and we have four users which\n",
        "246": "calling your Alice Bob Carol and Dave\n",
        "248": "with the initials ap C and D call them\n",
        "251": "users one two three and four so let's\n",
        "253": "say Alice really likes love at last\n",
        "255": "embrace that five stars really likes\n",
        "258": "romance forever grace at five stars she\n",
        "259": "did not watch two puppies a lot but did\n",
        "261": "not rate it so we don't\n",
        "264": "arranging for that and Alice really did\n",
        "266": "not like non-stop car chases all sources\n",
        "270": "karate and different user Bob use the to\n",
        "273": "maybe raise the different sets maybe he\n",
        "275": "likes love it laws did not to watch\n",
        "281": "romance forever just as 18400 and maybe\n",
        "284": "a third user raise the zero did not\n",
        "287": "watch that 1:05 founder and you know\n",
        "290": "let's just learn some of the numbers\n",
        "294": "okay and so just to introduce a bit of\n",
        "296": "notation this notation be using\n",
        "298": "throughout I'm going to use nu to denote\n",
        "300": "the number of users so in this example\n",
        "304": "and you will be equal to four so the new\n",
        "307": "subscript stands for users and nm I'm\n",
        "309": "going to use to denote the number of\n",
        "311": "movies so here I have five movies so in\n",
        "313": "M equals five and you know for this\n",
        "314": "M equals five and you know for this\n",
        "317": "example um I have for this example have\n",
        "320": "loosely three maybe romantic romantic\n",
        "324": "comedy movies and three two action\n",
        "326": "movies and you know if you look at a\n",
        "328": "small example it looks like Alice and\n",
        "331": "Bob are giving high ratings to these\n",
        "334": "romantic comedies or movies about love\n",
        "336": "and giving very low ratings about the\n",
        "339": "action movies and for Karen Davis the\n",
        "340": "opposite right Carol or Dave uses three\n",
        "342": "info really like the action movies and\n",
        "345": "during high ratings but don't like the\n",
        "349": "romance and love type movies as much\n",
        "352": "specifically in the recommender system\n",
        "354": "problem we are given the following data\n",
        "356": "or data comprises following we have\n",
        "360": "these values are IJ and RR j is 1 if\n",
        "362": "user J has rated movie I so I use this\n",
        "364": "rate only some of the movies and so you\n",
        "366": "know has we don't have ratings with\n",
        "370": "those movies and whenever R IJ is equal\n",
        "372": "to 1 whenever user J has rated movie I\n",
        "374": "we also get the num\n",
        "376": "get this number of yr J which is the\n",
        "379": "rating given by user J to movie R and so\n",
        "382": "y IJ will be a number you know from 0 up\n",
        "385": "to five depending on the star rating 0\n",
        "388": "to 5 stars that the user gave that but\n",
        "389": "the Ruby\n",
        "392": "so the recommender system problem is\n",
        "395": "given this data set that has given these\n",
        "398": "them are our GS and the y I J's to look\n",
        "400": "through your data and look at all the\n",
        "403": "movie ratings not missing and to try to\n",
        "405": "predict what these values of the\n",
        "408": "question mark should be in this specific\n",
        "409": "example I have a very small number of\n",
        "411": "movies in a very small number of users\n",
        "412": "and so most users have rates in most\n",
        "415": "movies but in the realistic setting your\n",
        "417": "users with each of your users may have\n",
        "418": "grade to only a minuscule fraction of\n",
        "420": "your movies but look in this data you\n",
        "422": "have Alice and Bob both like the\n",
        "424": "romantic movies maybe we think that\n",
        "426": "Alice would have given this a 5 maybe\n",
        "428": "with involved with a given dessert 4.5\n",
        "430": "or some high value whereas we think\n",
        "432": "maybe caramel David and during these\n",
        "435": "very low ratings and Dave well if Dave\n",
        "437": "really likes action movies maybe he\n",
        "440": "would have given Sorensen courante for\n",
        "442": "rating or mediafire rating okay and so\n",
        "444": "the job in developing a recommender\n",
        "448": "system is to come up with a learning\n",
        "450": "algorithm that can automatically fill in\n",
        "453": "these missing values for us so that we\n",
        "455": "can look at say the movies that the user\n",
        "458": "has not yet watched and recommend new\n",
        "460": "movies to that use it to watch you try\n",
        "461": "to predict what else might be\n",
        "464": "interesting to a user\n",
        "467": "so that's the formalism of the\n",
        "469": "recommender system problem in the next\n",
        "471": "video we'll start to develop a learning\n"
    },
    "hCOIMkcsm_g": {
        "0": " \n",
        "2": "by now you've seen a range of different\n",
        "4": "learning algorithms we've been\n",
        "6": "supervised learning the performance of\n",
        "8": "many supervised learning algorithms will\n",
        "10": "be pretty similar and what matters less\n",
        "12": "will often be whether you use learning\n",
        "15": "out from a or learning R&amp;B but what\n",
        "16": "matters more will often be things like\n",
        "18": "the amount of data you train these\n",
        "20": "algorithms on that's always your skill\n",
        "22": "in applying these algorithms things like\n",
        "24": "your choice of the features that you\n",
        "26": "design to give these learning algorithms\n",
        "28": "and how you choose the regularization\n",
        "30": "parameter and things like that but\n",
        "34": "there's one more algorithm that is very\n",
        "36": "powerful and is very widely used both\n",
        "39": "within industry and in academia and\n",
        "41": "that's called the support vector machine\n",
        "44": "and compared to both logistic regression\n",
        "47": "and neural networks the support vector\n",
        "50": "machine or the SVM sometimes gives a\n",
        "52": "cleaner and sometimes more powerful way\n",
        "54": "of learning complex nonlinear functions\n",
        "57": "and so like to take the next videos to\n",
        "60": "talk about that later in this course I\n",
        "63": "will do a quick survey of a range of\n",
        "64": "different supervised learning algorithms\n",
        "67": "just as a very briefly describe them but\n",
        "69": "the support vector machine or giving us\n",
        "71": "popularity in home-home for this this\n",
        "73": "will be the last of these supervised\n",
        "75": "learning algorithms that I'll spend a\n",
        "77": "significant amount of time on in this\n",
        "80": "course as with our development of other\n",
        "82": "learning algorithms we're going to start\n",
        "83": "by talking about the optimization\n",
        "86": "objective so let's get started on this\n",
        "90": "algorithm in order to describe the\n",
        "92": "support vector machine I'm actually\n",
        "94": "going to start with logistic regression\n",
        "97": "and show how we can modify it a bit and\n",
        "99": "get what is essentially the support\n",
        "101": "vector machine so in logistic regression\n",
        "104": "we have our familiar form of the\n",
        "106": "hypothesis there and the sigmoid\n",
        "109": "activation function shown on the right\n",
        "112": "and in order to explain some of the map\n",
        "114": "I'm going to use Z to denote theta\n",
        "118": "transpose X here now let's think about\n",
        "120": "what we would like logistic regression\n",
        "123": "to do if we have an example with y\n",
        "125": "equals 1 and by this side I mean an\n",
        "127": "example in either the training set or\n",
        "128": "the test set you know or the\n",
        "131": "cross-validation set but if Y is equal\n",
        "133": "to 1 then we're sort of hoping that H of\n",
        "135": "X will be close to 1 so right we're\n",
        "137": "hoping they'll correctly classify that\n",
        "140": "example and what having HR x close to 1\n",
        "142": "what that means is that theta transpose\n",
        "145": "X must be much larger than 0 so this is\n",
        "146": "greater than greater than sign that\n",
        "150": "means much much greater than 0 and\n",
        "153": "that's because it is Z that is theta\n",
        "155": "transpose X is when Z is much bigger\n",
        "157": "than 0 is far to the right of this\n",
        "159": "figure that you know the output of\n",
        "164": "logistic regression becomes close to 1\n",
        "166": "conversely if we have an example where Y\n",
        "169": "is equal to 0 then what we're hoping for\n",
        "170": "is that the hypothesis will output a\n",
        "173": "value close to 0 and that corresponds to\n",
        "176": "theta transpose X or Z be much less than\n",
        "178": "0 because that corresponds to the\n",
        "181": "hypothesis outputting a value close to 0\n",
        "184": "if you look at the cost function of\n",
        "187": "logistic regression what you find is\n",
        "190": "that each example X comma Y contributes\n",
        "193": "a term like this to the overall cost\n",
        "195": "function right so for the overall cost\n",
        "198": "function we usually we will also have a\n",
        "201": "sum over all the training examples and 1\n",
        "203": "over m term but this expression here\n",
        "204": "over m term but this expression here\n",
        "206": "that's the term that a single training\n",
        "209": "example contributes to the overall\n",
        "210": "objective function for logistic\n",
        "215": "regression now if I take the definition\n",
        "217": "for the form of my hypothesis and\n",
        "220": "plugged it in over here then what I get\n",
        "221": "is that each training example\n",
        "225": "contributes this term right ignoring the\n",
        "227": "1 over M but it contributes that term to\n",
        "229": "my overall cost function for logistic\n",
        "233": "regression now let's consider the two\n",
        "236": "cases of when Y is equal to 1 and when y\n",
        "238": "is equal to 0\n",
        "239": "in the first case let's suppose that y\n",
        "243": "is equal to one in that case only this\n",
        "246": "first term in the objective matters\n",
        "248": "because this 1 minus y term would be\n",
        "254": "equal to 0 if Y is equal to 1 so when y\n",
        "256": "is equal to 1 when in our example X\n",
        "259": "comma Y when y is equal to 1 what we get\n",
        "262": "is this term minus log of 1 over 1 plus\n",
        "264": "the e to the negative Z where as similar\n",
        "266": "to last slide I'm using Z to denote\n",
        "270": "theta transpose X and of course in the\n",
        "272": "cost we actually have this minus y but\n",
        "274": "we just said if Y is equal to 1 so\n",
        "276": "that's equal to 1 I just simplified it\n",
        "278": "away in the expression that I have\n",
        "283": "written down here and if we plug this\n",
        "285": "function as a function of Z what you\n",
        "288": "find is that you get this curve shown on\n",
        "291": "the lower left of the slide and thus we\n",
        "294": "also see that when Z is equal to launch\n",
        "297": "that is when theta transpose X is large\n",
        "299": "that corresponds to a value of Z that\n",
        "302": "gives us a fairly small value a very\n",
        "305": "fairly small contribution to the cost\n",
        "307": "function and this kind of explains why\n",
        "310": "when logistic regression sees a positive\n",
        "313": "example with y equals 1 it tries to set\n",
        "315": "theta transpose X to be very large\n",
        "318": "because that corresponds to this term in\n",
        "321": "the cost function being small now to\n",
        "323": "build a support vector machine here's\n",
        "324": "what we're going to do we're going to\n",
        "326": "take this cost function this minus log 1\n",
        "328": "over 1 plus Z is a negative Z and modify\n",
        "332": "it a little bit let me take let me take\n",
        "336": "this point 1 over here and let me draw\n",
        "338": "the cost function I'm going to use the\n",
        "339": "new cost function is going to be flat\n",
        "342": "from here on out and then I'm going to\n",
        "346": "draw something that grows as a straight\n",
        "350": "line similar to logistic regression that\n",
        "352": "this is going to be a straight line\n",
        "355": "distortion so the curve that I just drew\n",
        "357": "in magenta the curves I just drew perp\n",
        "360": "or magenta so it was a pretty close\n",
        "363": "approximation to the cost function used\n",
        "365": "by logistic regression except that is\n",
        "367": "now made up of two line segments this is\n",
        "370": "flat portion on the right and then\n",
        "373": "is a straight line portion on the left\n",
        "376": "and don't worry too much about the slope\n",
        "378": "of the straight line portion it doesn't\n",
        "382": "matter that much but that's the new cost\n",
        "383": "function we're going to use but when y\n",
        "385": "is equal to one and you can imagine it\n",
        "386": "should do something pretty similar to\n",
        "389": "logistic regression but turns out that\n",
        "391": "this will give the support vector\n",
        "393": "machine computational advantages and\n",
        "396": "give us later on an easier optimization\n",
        "399": "problem that that would be easier for\n",
        "401": "software to solve we just talked about\n",
        "403": "the case of y equals one the other case\n",
        "406": "is if y is equal to zero in that case if\n",
        "409": "you look at the cost then only the\n",
        "411": "second term will apply because the first\n",
        "414": "term goes away where Y is equal to zero\n",
        "415": "then nearly over zero here so you're\n",
        "417": "left only with the second term in the\n",
        "420": "expression above and so the cost of an\n",
        "422": "example or the contribution of the cost\n",
        "424": "function is going to be given by this\n",
        "427": "term over here and if you plot that as a\n",
        "429": "function of Z so I have here Z on the\n",
        "431": "horizontal axis you end up with this\n",
        "434": "curve and for the support vector machine\n",
        "436": "once again we're going to replace this\n",
        "439": "blue line with something similar and in\n",
        "441": "particular I'm going to replace it with\n",
        "443": "a new cost this flat out here there's\n",
        "445": "zero out here and that then grows as a\n",
        "449": "straight line like so so let me give\n",
        "453": "these two functions names this function\n",
        "455": "on the left I'm going to call cost\n",
        "459": "subscript 1 of Z and this function on\n",
        "460": "the right I'm going to call cost\n",
        "464": "subscript 0 of Z and the subscript just\n",
        "467": "refers to the cost corresponding to an y\n",
        "469": "is equal to 1 versus and y is equal to 0\n",
        "471": "armed with these definitions we're now\n",
        "473": "ready to build a support vector machine\n",
        "476": "here's the cost function J of theta that\n",
        "478": "we have for logistic regression in case\n",
        "480": "this equation looks a bit unfamiliar is\n",
        "483": "because previously we had a minus sign\n",
        "485": "outside but here what I did was I\n",
        "488": "instead move the minus signs inside this\n",
        "489": "expression so just it makes it look a\n",
        "491": "little bit different\n",
        "494": "for the support vector machine what\n",
        "496": "we're going to do is essentially take\n",
        "501": "this and replace this with cost one of Z\n",
        "504": "that is cost one of theta transpose X\n",
        "507": "and I'm going to take this and replace\n",
        "511": "it with cost zero of Z that is of cause\n",
        "515": "zero of theta transpose X where where\n",
        "517": "the cost one function is what we had on\n",
        "519": "the previous slide that looks like this\n",
        "522": "and the cost zero function again what we\n",
        "524": "had on the previous slide that looks\n",
        "528": "like this so what we have for the\n",
        "529": "support vector machine is a minimization\n",
        "535": "problem of 1 over m sum over my training\n",
        "540": "examples of y:i x plus 1 theta transpose\n",
        "547": "X I plus 1 minus y I times 0 of theta\n",
        "552": "transpose X I and then plus my usual\n",
        "564": "regularization parameter like so now by\n",
        "565": "convention for the support vector\n",
        "568": "machine we actually write things\n",
        "569": "slightly differently or we parameterize\n",
        "572": "this just very slightly differently\n",
        "575": "first we're going to get rid of the 1\n",
        "578": "over m terms and this just distance just\n",
        "579": "happens to be a slightly different\n",
        "581": "convention that people use for support\n",
        "584": "vector machines compared to for logistic\n",
        "586": "regression but here's what I mean you're\n",
        "587": "what I'm going to do is I'm just going\n",
        "590": "to get rid of these 1 over m terms and\n",
        "592": "this should give me the same optimal\n",
        "594": "value for theta right because 1 over m\n",
        "597": "is just a constant so you know whether I\n",
        "599": "solve this minimization problem with 1\n",
        "601": "over m in front or not I should end up\n",
        "603": "with the same optimal value for theta\n",
        "606": "here's what I mean to give you a\n",
        "608": "concrete example suppose I had a\n",
        "610": "minimization problem that you minimize\n",
        "614": "over a real number U of U minus 5\n",
        "619": "squared plus 1 right well the minimum of\n",
        "621": "this happens happen to know the minimum\n",
        "623": "of this is U equals 5 now if I were to\n",
        "625": "take this objective function and\n",
        "629": "multiply it by 10 so here my\n",
        "632": "minimization problem is min over U of 10\n",
        "637": "u minus 5 squared plus 10 well the value\n",
        "639": "of U that minimizes this is still u\n",
        "642": "equals 5 right so multiplying something\n",
        "644": "that you're minimizing over by some\n",
        "646": "constant 10 in this case it does not\n",
        "650": "change the value of U that gives us that\n",
        "652": "that minimizes this function so in the\n",
        "655": "same way what I've done by crossing out\n",
        "658": "this M is on doing is multiplying my\n",
        "660": "objective function by some constant M\n",
        "662": "and it doesn't change the value of theta\n",
        "666": "that achieves the minimum the second bit\n",
        "668": "of notational change which is just again\n",
        "670": "the most standard convention when using\n",
        "672": "SVM's instead of logistic regression\n",
        "674": "there's a following so for logistic\n",
        "677": "regression we had two terms two the\n",
        "680": "objective function the first is this\n",
        "682": "term which is the cost that comes from\n",
        "684": "the training set and the second is this\n",
        "686": "term which the regularization term and\n",
        "690": "what we had was we had a control the\n",
        "692": "trade-off between these by saying you\n",
        "695": "know we wanted to minimize a plus and\n",
        "697": "then my regularization parameter lambda\n",
        "703": "and then times some of the term B where\n",
        "705": "I guess I'm using your a to denote this\n",
        "707": "first term and I'm using B to denote\n",
        "709": "that second term maybe without the\n",
        "713": "lambda and instead of parametrizing this\n",
        "718": "is a plus lambda B we could and so what\n",
        "720": "we did was by setting different values\n",
        "722": "for this regularization parameter lambda\n",
        "724": "we could trade off the relative way\n",
        "726": "between how much we want to fit the\n",
        "728": "training set well that is minimizing a\n",
        "731": "versus how much we care about keeping\n",
        "733": "the values of the parameters small so\n",
        "736": "that would be the parameters B for the\n",
        "737": "support vector machine just by\n",
        "739": "convention we're going to use a\n",
        "741": "different parameter so instead of using\n",
        "743": "lambda here to control the relative\n",
        "745": "weighting between you know the first and\n",
        "747": "second terms once they are going to use\n",
        "748": "a different parameter which by\n",
        "749": "a different parameter which by\n",
        "751": "convention is called C\n",
        "753": "and when is that going to minimize C\n",
        "759": "times a plus B so for logistic\n",
        "760": "times a plus B so for logistic\n",
        "762": "regression if we set a very large value\n",
        "764": "of lambda that means you know give be a\n",
        "767": "very high weight here is that if we set\n",
        "770": "C to be very small value that that\n",
        "772": "corresponds to giving be much larger\n",
        "775": "weight than C than eight so this is just\n",
        "777": "a different way of controlling the trade\n",
        "778": "off or just a different way of\n",
        "781": "parameterizing how much we care about\n",
        "783": "optimizing the first term versus how\n",
        "784": "much we care about optimizing the second\n",
        "786": "term and if you want you can think of\n",
        "789": "this as the parameter C playing a role\n",
        "794": "similar to 1 over lambda and it's not\n",
        "797": "that these two equations or these two\n",
        "799": "expressions will be equal x equals 1\n",
        "801": "over lambda that's not the case my\n",
        "803": "rather is that if C is equal to 1 over\n",
        "805": "lambda then these two optimization\n",
        "807": "objectives should give you the same\n",
        "809": "value the same optimal value for theta\n",
        "812": "so just filling that in and when I cross\n",
        "814": "out lambda here and write in the\n",
        "818": "constant C there so that gives us our\n",
        "821": "overall optimization objective function\n",
        "823": "for the support vector machine and when\n",
        "826": "you minimize that function then what you\n",
        "831": " \n",
        "834": "finally unlike logistic regression the\n",
        "836": "support vector machine doesn't output\n",
        "838": "the probability instead what we have is\n",
        "840": "we have this cost function which we\n",
        "842": "minimize to get the parameters theta and\n",
        "845": "what a support vector machine does is it\n",
        "848": "just makes a prediction of y be equal 1\n",
        "851": "or 0 directly so the hypothesis will\n",
        "855": "predict 1 if theta transpose X is\n",
        "858": "greater than or equal to 0 and a predict\n",
        "861": "0 otherwise and so having learned the\n",
        "864": "parameters theta this is the form of the\n",
        "866": "hypothesis for the support vector\n",
        "868": "machine so that was a mathematical\n",
        "870": "definition of what the support vector\n",
        "872": "machine does in the next few videos\n",
        "874": "let's try to get better intuition about\n",
        "877": "what this optimization objective leads\n",
        "878": "to it whether the source of the\n",
        "881": "hypothesis SVM will learn and also talk\n",
        "884": "about how to modify this just a little\n"
    },
    "hDmNF9JG3lo": {
        "0": " \n",
        "2": "in the clustering problem we are given\n",
        "4": "an unlabeled data set\n",
        "6": "and we would like to have an algorithm\n",
        "8": "automatically group the data into\n",
        "10": "coherent subsets or into coherent\n",
        "14": "clusters for us the k-means algorithm is\n",
        "16": "by far the most popular by far the most\n",
        "19": "widely used clustering algorithm and in\n",
        "21": "this video I'd like to tell you what the\n",
        "25": "k-means algorithm is and how it works\n",
        "27": "the convenient clustering algorithm is\n",
        "30": "best illustrated in pictures let's say I\n",
        "32": "want to take an unlabeled data set like\n",
        "34": "the one shown here and I want to group\n",
        "38": "the data into two clusters if I run the\n",
        "40": "k-means clustering algorithm here's what\n",
        "42": "I'm going to do the first step is to\n",
        "45": "randomly initialize two points called\n",
        "47": "the cluster centroids so these two\n",
        "49": "crosses here these are called the\n",
        "54": "cluster centroids and I have two of them\n",
        "57": "because I want to group my data into two\n",
        "60": "clusters k-means is an iterative\n",
        "63": "algorithm and it does two things first\n",
        "66": "is a cluster assignment step and second\n",
        "68": "is a roof centroid step so let me tell\n",
        "71": "you what those things mean the first of\n",
        "73": "the two steps in the inner loop of\n",
        "75": "k-means is this cluster assignment step\n",
        "77": "what that means is that it's going to go\n",
        "79": "through each of the examples each of\n",
        "81": "these green dots shown here and\n",
        "84": "depending on whether it is closer to the\n",
        "86": "red cluster centroid or the blue cluster\n",
        "88": "centroid it is going to assign each of\n",
        "90": "the data points to one of the two\n",
        "92": "cluster centroids specifically what I\n",
        "93": "cluster centroids specifically what I\n",
        "94": "mean by that is going to go through your\n",
        "97": "data set and color each of the points\n",
        "100": "either red or blue depending on whether\n",
        "101": "it's closer to the red cluster centroid\n",
        "103": "or the blue cluster centroid and I've\n",
        "107": "done that in this diagram here so that\n",
        "109": "was the cluster assignment step the\n",
        "112": "other part of k-means the inner loop of\n",
        "114": "k-means is the move centroid step and\n",
        "116": "what we're going to do is going to take\n",
        "117": "the two closest central is that is the\n",
        "119": "Red Cross and the Blue Cross and we'll\n",
        "122": "move them to the average of the points\n",
        "125": "colored the same color so what I'm going\n",
        "127": "to do is look at all the red points and\n",
        "130": "compute the average was really the mean\n",
        "131": "the location of all the red points and\n",
        "133": "we're going to move the red cluster\n",
        "135": "centroid there and the same thing for\n",
        "136": "the blue cluster Center look at all the\n",
        "138": "blue dots and compute their mean and\n",
        "139": "then move the blue cluster centroid\n",
        "141": "there so let me do that now we'll move\n",
        "144": "the cluster centroids as follows and\n",
        "147": "I've now moved them to that new means\n",
        "150": "into the red one move like that and the\n",
        "152": "do one move like that and a great one\n",
        "155": "move like that and then we go back to\n",
        "156": "another cluster assignment step so we're\n",
        "158": "again going to look at all of my\n",
        "160": "unlabeled examples and depending on\n",
        "162": "whether it's closer to the red or the\n",
        "164": "blue cluster centroid I'm going to color\n",
        "166": "them either red or blue so I'm going to\n",
        "168": "assign each point to one of the two\n",
        "170": "cluster centroids let me do that now and\n",
        "172": "so the colors of some of the points just\n",
        "174": "changed and then I'm going to do another\n",
        "176": "move centroid step so I'm going to\n",
        "177": "compute the average of all the blue\n",
        "179": "points compute the average of all the\n",
        "181": "red points and move my cluster centroids\n",
        "186": "like this and so let's do that again let\n",
        "187": "me do one more cluster assignment step\n",
        "190": "so color each point red or blue based on\n",
        "193": "what is closer to and then do another\n",
        "200": "roof centroid step and we're done and in\n",
        "202": "fact if you keep running additional\n",
        "204": "iterations of k-means from here the\n",
        "207": "k-means the cluster centroids will not\n",
        "208": "change any further and the colors of the\n",
        "210": "points will not change any further and\n",
        "213": "so you know this is the at this point\n",
        "215": "k-means has converged and it's done a\n",
        "218": "pretty good job finding the two clusters\n",
        "220": "in this data let's write out the k-means\n",
        "222": "algorithm more formally the k-means\n",
        "225": "algorithm takes two inputs one is a\n",
        "227": "parameter K which is the number of\n",
        "229": "clusters you want to find in the data I\n",
        "231": "like to say how we might go about trying\n",
        "234": "to choose K but for now let's just say\n",
        "236": "that we've decided we want a certain\n",
        "237": "number of clusters and we're going to\n",
        "238": "number of clusters and we're going to\n",
        "239": "tell the algorithm how many clusters we\n",
        "240": "think they're on the days\n",
        "243": "and then k-means also takes as input\n",
        "245": "this sort of unlabeled training set of\n",
        "247": "just the exits and because this is\n",
        "248": "unsupervised learning we don't have the\n",
        "251": "labels wine and for unsupervised\n",
        "254": "learning or for K means I'm going to use\n",
        "257": "the convention that X I is an RN\n",
        "258": "dimensional vector and that's why my\n",
        "260": "training examples are now n-dimensional\n",
        "263": "rather than n plus 1 dimensional vectors\n",
        "266": "this is what the k-means algorithm does\n",
        "269": "the first step is that it randomly\n",
        "272": "initializes K cluster centroids which\n",
        "276": "we'll call nu 1 nu 2 up to MU K and so\n",
        "278": "in the earlier diagram the cluster\n",
        "281": "centroids corresponding to the location\n",
        "283": "of the Red Cross and the location of the\n",
        "286": "Blue Cross so there we have to cluster\n",
        "287": "centroids so maybe the Red Cross was new\n",
        "290": "one and the Blue Cross was u2 and more\n",
        "292": "generally we would have k cluster\n",
        "295": "centroids rather than just two then the\n",
        "297": "inner loop of K means less the following\n",
        "299": "we're going to repeatedly do the\n",
        "302": "following first for each of my training\n",
        "305": "examples I'm going to set this variable\n",
        "306": "CI\n",
        "309": "DVD index 1 through K of the cluster\n",
        "312": "centroid process to XR so this was my\n",
        "317": "cluster assignment step where we took\n",
        "319": "each of my examples and colored it\n",
        "322": "either red or blue depending on which\n",
        "324": "cluster centroid it was closest to think\n",
        "326": "so CI is going to be a number from 1 to\n",
        "328": "K that tells us you know is it closer to\n",
        "331": "the red cross or as a closer to the Blue\n",
        "333": "Cross and another way of writing this is\n",
        "338": "on I'm going to for each to compute C\n",
        "340": "I'm going to take my life example X I\n",
        "343": "and I'm going to measure its distance to\n",
        "346": "each of my cluster centroids this mu and\n",
        "349": "then a lowercase K right so capital K is\n",
        "351": "the total number of centralising I'm\n",
        "354": "going to use lowercase K here to index\n",
        "356": "into the different centroids but so CI\n",
        "359": "is going to I'm going to minimize over\n",
        "362": "my values of K and find the value of K\n",
        "364": "that minimizes the distance between X I\n",
        "366": "the centroid and then you know the value\n",
        "369": "that minimizes the value of K that\n",
        "371": "minimizes this that's what's to get set\n",
        "375": "into CI so here's another way of writing\n",
        "378": "out what C is they've a right the norm\n",
        "379": "out what C is they've a right the norm\n",
        "383": "between X I minus mu K then this is the\n",
        "385": "distance between my I for training\n",
        "388": "example X I and the cluster centroid mu\n",
        "391": "subscript K this is this here that's a\n",
        "394": "lowercase K so uppercase K is going to\n",
        "396": "be used to denote the total number of\n",
        "399": "cluster centroids and this lowercase K\n",
        "401": "you know the lowercase K is number\n",
        "403": "between 1 and capital K just using\n",
        "405": "lowercase K to index into my different\n",
        "409": "cluster centroids Nexis lowercase K so\n",
        "410": "on that's the distance between an\n",
        "412": "example in the cluster centroid and so\n",
        "414": "what I'm going to do is find the value\n",
        "417": "of K of lowercase K that minimizes this\n",
        "421": "and so the value of K that minimizes\n",
        "423": "this you know that's what I'm going to\n",
        "427": "set as CI and by convention here I've\n",
        "429": "written the distance between X on the\n",
        "431": "cluster centroid by convention people\n",
        "432": "actually tend to write this as the\n",
        "434": "squared distance so we think of CI as\n",
        "436": "picking the cluster centroid with the\n",
        "439": "smallest squared distance to my training\n",
        "441": "example X I but of course minimizing\n",
        "443": "squared distance and minimizing distance\n",
        "445": "that should give you the same value of\n",
        "446": "CI but we usually put in the square\n",
        "447": "CI but we usually put in the square\n",
        "449": "there just as a as a convention that\n",
        "451": "people use for k-means so that was the\n",
        "454": "cluster assignment step the other in the\n",
        "457": "loop of k-means does the move centroid\n",
        "459": "step\n",
        "462": "and what that does is for each of my\n",
        "464": "cluster centroids so if a lowercase K\n",
        "466": "equals one through K it sets new K\n",
        "467": "equals to the average of the points\n",
        "469": "assigned to cluster so as a concluding\n",
        "472": "example let's say that so one of my\n",
        "473": "cluster centroid says the cluster\n",
        "475": "centroid two has training examples you\n",
        "482": "have one five six and ten assigned to it\n",
        "485": "and what this means is really this means\n",
        "491": "that C 1 equals 2 C 5 equals 2 C 6\n",
        "495": "equals 2 and similarly while C 10 equals\n",
        "497": "2 right if we got that from the cluster\n",
        "499": "assignment step then that means that\n",
        "502": "examples 1 5 6 and 10 were assigned to\n",
        "504": "cluster centrality then in this move\n",
        "506": "central is that what I'm going to do is\n",
        "508": "just compute the average of these four\n",
        "515": "things so X 1 plus X 5 plus X 6 plus X\n",
        "518": "10 and then I'm going to average them so\n",
        "520": "here I have four points assigned to this\n",
        "522": "cluster centroid so just take one\n",
        "526": "quarter of that and now mu2 is going to\n",
        "529": "be an n-dimensional vector because each\n",
        "532": "of these examples X 1 X 5 X 6 X 10 each\n",
        "534": "of them were an n-dimensional vector and\n",
        "536": "when I add up these things and you\n",
        "538": "divide by 4 because I have four points\n",
        "541": "of signs of the centroid I end up with\n",
        "544": "my move centroid step for my cluster\n",
        "546": "centroid mewtwo and this has the effect\n",
        "549": "of moving you two to be average of those\n",
        "550": "four points listed here one thing to\n",
        "553": "have sometimes malice ISM well here we\n",
        "555": "said let's get new K be the average of\n",
        "557": "the points assigned to that cluster but\n",
        "559": "what if there's a cluster or what if\n",
        "561": "there's a cluster centroid with no\n",
        "563": "points with zero points assigned to it\n",
        "565": "in that case the more common thing to do\n",
        "567": "is to just eliminate that cluster\n",
        "568": "centroid and if you do that you end up\n",
        "572": "with K minus 1 clusters instead of K\n",
        "573": "clusters\n",
        "576": "oh but sometimes if you really need K\n",
        "578": "clusters then the other thing you can do\n",
        "579": "if you have a cluster centroid with no\n",
        "581": "points assigned to it is you can just\n",
        "583": "randomly reinitialize that cluster\n",
        "585": "centroid but it is a more common to just\n",
        "586": "centroid but it is a more common to just\n",
        "588": "eliminate the cluster if somewhere\n",
        "590": "during canyons it ends up with no points\n",
        "591": "it assigned to that cluster centroid and\n",
        "593": "that can't happen although in practice\n",
        "597": "it happens not so that's the k-means\n",
        "598": "algorithm\n",
        "600": "before wrapping up this video I just\n",
        "602": "want to tell you about one other common\n",
        "605": "application of k-means - and that's -\n",
        "607": "the problems with non well separated\n",
        "609": "clusters here's what I mean so far we've\n",
        "611": "been picturing k-means and applying it\n",
        "613": "to data sets like that shown here where\n",
        "615": "we have pretty where we have three\n",
        "617": "pretty well separated clusters and we'd\n",
        "619": "like an algorithm to find maybe the\n",
        "621": "three clusters for us but it turns out\n",
        "623": "that very often k-means is also applied\n",
        "625": "to data sets that look like this where\n",
        "628": "they may not be you know several very\n",
        "630": "well separated clusters here's an\n",
        "633": "example application on to t-shirt sizing\n",
        "635": "so let's say you're a t-shirt\n",
        "637": "manufacturer and what you've done is\n",
        "639": "you've gone to the population that you\n",
        "641": "want to sell t-shirts to and you've\n",
        "643": "collected a number of examples of the\n",
        "645": "height and weight of these people in\n",
        "647": "your population so then I guess height\n",
        "648": "and weight tend to be positively\n",
        "651": "correlated and so maybe you end up with\n",
        "652": "a data set like this you know with a\n",
        "655": "sample set of examples of different\n",
        "656": "people's height and weight let's say you\n",
        "658": "want to size your t-shirts let's say I\n",
        "660": "want to you know design and sell\n",
        "663": "t-shirts of three sizes small medium and\n",
        "666": "large so how big should I make my small\n",
        "667": "and how big should I make my medium and\n",
        "669": "how bitumen should I make my large\n",
        "671": "t-shirts one way to do this would be to\n",
        "673": "run the k-means clustering algorithm on\n",
        "674": "this data set that I have shown on the\n",
        "677": "right and maybe what k-means will do is\n",
        "680": "group all of these points into one\n",
        "683": "cluster and group all of these points\n",
        "685": "into a second cluster and group all of\n",
        "688": "those points into a third cluster so\n",
        "690": "even though the data you know beforehand\n",
        "692": "it does seem like we have three well\n",
        "694": "separated clusters k-means we're kind of\n",
        "696": "separate the kind of separate out the\n",
        "698": "data into multiple clusters for you and\n",
        "700": "what you can do is then look at this\n",
        "704": "first population of people and look at\n",
        "705": "them and you know\n",
        "707": "at the height and weight and try to\n",
        "709": "design a small t-shirt so that kind of\n",
        "711": "fits this first population of people\n",
        "714": "well and then design a medium t-shirt\n",
        "716": "and design a large t-shirt and this is\n",
        "719": "in fact kind of an example of market\n",
        "721": "segmentation where you're using k-means\n",
        "724": "to separate your market into three\n",
        "726": "different segments so you can design a\n",
        "728": "product separately that is the small\n",
        "730": "medium and large teachers that tries to\n",
        "731": "medium and large teachers that tries to\n",
        "733": "suit the needs of each of your three\n",
        "736": "separate popular subpopulations well so\n",
        "738": "that's the k-means algorithm and by now\n",
        "740": "you should know how to implement the\n",
        "742": "k-means algorithm and kind of get it to\n",
        "744": "work for some problems but in the next\n",
        "746": "few videos what I want to do is really\n",
        "748": "get more deeply into the nuts and bolts\n",
        "750": "of k-means and talk a bit about how to\n"
    },
    "iPNN805konI": {
        "0": " \n",
        "2": "in the last video we gave a mathematical\n",
        "4": "definition of how to represent for how\n",
        "6": "to compute the hypothesis used by a\n",
        "9": "neural network in this video I'd like to\n",
        "11": "show you how to actually carry out that\n",
        "14": "computation efficiently and that is show\n",
        "17": "you a vectorized implementation and\n",
        "19": "second and more importantly I want to\n",
        "21": "start giving you intuition about why\n",
        "23": "these neural network representations\n",
        "25": "might be a good idea and how they can\n",
        "27": "help us to learn complex nonlinear\n",
        "30": "hypotheses consider this neural network\n",
        "32": "previously we said that the sequence of\n",
        "34": "steps we need in order to compute the\n",
        "36": "output of a hypothesis is these\n",
        "38": "equations given on the left where we\n",
        "41": "compute the activation values of the\n",
        "44": "three hidden units and then we use those\n",
        "45": "to compute the final output of the\n",
        "48": "hypothesis H of X now I'm going to\n",
        "51": "define a few extra terms so this term\n",
        "54": "that I'm underlining here I'm going to\n",
        "57": "define that to be Z superscript 2\n",
        "61": "subscript 1 so that we have that a 2 1\n",
        "66": "which is this term is equal to G of Z 2\n",
        "70": "1 and by the way these superscript 2 you\n",
        "72": "know what that means is that the Z 2 and\n",
        "75": "this a 2 as well the superscript 2 in\n",
        "77": "parentheses means that these are values\n",
        "79": "associated with layer 2 that is what the\n",
        "82": "hidden layer in the neural network now\n",
        "86": "on this term here I'm going to similarly\n",
        "92": "define as Z 2 2 so and finally this last\n",
        "94": "term here the underlining then we define\n",
        "98": "that as Z 2 3 so that similarly we have\n",
        "103": "a 2 3 equals G of Z 2\n",
        "107": "so these are Z values are just a linear\n",
        "110": "combination weighted linear combination\n",
        "114": "of the input values x0 x1 x2 x3 that go\n",
        "118": "into a particular neuron now if you look\n",
        "122": "at this block of numbers you may notice\n",
        "124": "that that block of numbers corresponds\n",
        "128": "suspiciously similar to the matrix\n",
        "130": "vector operation matrix vector\n",
        "133": "multiplication of x1 times the vector X\n",
        "136": "using this observation we're going to be\n",
        "139": "able to vectorize this computation of\n",
        "142": "the neural network concretely let's\n",
        "145": "define the feature vector X as usual to\n",
        "148": "be the vector of X 0 X 1 X 2 X 3 where X\n",
        "151": "0 as usual is always equal to 1 and\n",
        "154": "let's define Z 2 to be the vector of\n",
        "158": "these Z values of Z 2 1 Z 2 2 Z 2 3\n",
        "161": "notice that there z 2 this is a three\n",
        "165": "dimensional vector we can now vectorize\n",
        "168": "the computation of a21 a.22 a23 as\n",
        "170": "follows we can just write this in two\n",
        "173": "steps we can compute Z 2 as theta 1\n",
        "176": "times X and that would give us this\n",
        "181": "vector Z 2 and then a 2 is G of Z 2 and\n",
        "184": "just be clear and Z 2 here this is a 3\n",
        "188": "dimensional vector and a 2 is also a 3\n",
        "190": "dimensional vector and thus this\n",
        "193": "activation G this applies the sigmoid\n",
        "196": "function element-wise to each of Z 2 s\n",
        "199": "about elements and by the way to make\n",
        "201": "our notation a little more consistent\n",
        "203": "with what we'll do later in this input\n",
        "206": "layer we have the inputs X but we can\n",
        "208": "also think of this as the activations of\n",
        "211": "the first layer so if I define a 1 to be\n",
        "214": "equal to X so the a 1 is a vector I can\n",
        "216": "now take this X here and replace this\n",
        "220": "with Z 2 equals theta 1 times a 1 just\n",
        "221": "by defining a\n",
        "225": "to be activations in my input layer now\n",
        "227": "with what I've written so far I have now\n",
        "231": "gotten myself the values for a1 a2 a3\n",
        "234": "and really I should put the superscripts\n",
        "237": "there as well but I need one more value\n",
        "241": "which is I also want this a 0 2 and that\n",
        "244": "corresponds to a bias unit in the hidden\n",
        "246": "layer that goes to the upper layer of\n",
        "248": "course there was advise you in here to\n",
        "250": "them you know I just didn't draw under\n",
        "251": "in here\n",
        "253": "but to take care of this extra bias unit\n",
        "256": "what we're going to do is add an extra\n",
        "260": "a0 superscript 2 that is equal to 1 and\n",
        "262": "after taking the step we now have that\n",
        "265": "a2 is going to be a 4 dimensional\n",
        "267": "feature vector because we just added\n",
        "269": "this extra you know a 0 which is equal\n",
        "272": "to 1 corresponding to the bias unit in\n",
        "277": "the hidden layer and finally to compute\n",
        "278": "the actual value output of our\n",
        "281": "hypotheses we then simply need to\n",
        "287": "compute Z 3 so Z 3 is equal to this term\n",
        "288": "here that I'm just underlining this\n",
        "295": "inner term here is Z 3 and Z 3 is theta\n",
        "298": "2 times a 2 and finally my hypothesis\n",
        "301": "outputs H of X which is a 3 that's a\n",
        "306": "deactivation of my one and only unit in\n",
        "307": "the upper layer so that's just real\n",
        "310": "number you can read in this a 3 or a 3 1\n",
        "314": "and that's G of Z 3 this process of\n",
        "317": "computing H of X is also called\n",
        "319": "forward propagation and is called that\n",
        "321": "because we spawn off with the\n",
        "323": "activations of the input units and then\n",
        "326": "we sort of forward propagate that to the\n",
        "328": "hidden layer and computing activations\n",
        "330": "of hidden layer and then we sort of\n",
        "333": "forward propagate that and compute the\n",
        "335": "activations of the output layer but this\n",
        "338": "process of computing the activations\n",
        "340": "the input and hidden then the upper\n",
        "342": "layer is also called for propagation and\n",
        "345": "what we just did was we worked out a\n",
        "347": "vectorized implementation of this\n",
        "349": "procedure so if you implement it using\n",
        "351": "these equations and we have on the right\n",
        "353": "this will give you an efficient way a\n",
        "355": "relatively efficient way of computing H\n",
        "356": "relatively efficient way of computing H\n",
        "360": "of X this forward propagation view also\n",
        "363": "helps us understand what neural networks\n",
        "364": "might be doing and why they might help\n",
        "367": "us to learn interesting nonlinear\n",
        "368": "hypotheses\n",
        "370": "consider the following in your network\n",
        "372": "and let's say I cover up the left part\n",
        "375": "of this picture for now if you look at\n",
        "377": "what's left in this picture this looks a\n",
        "379": "lot like logistic regression where what\n",
        "381": "we're doing is we're using that node\n",
        "383": "that's just a logistic regression unit\n",
        "386": "and we're using that to make a\n",
        "389": "prediction in H of X and concretely what\n",
        "391": "the hypothesis is outputting is H of X\n",
        "394": "is going to be equal to G which is my\n",
        "397": "sigmoid activation function times theta\n",
        "406": "0 times a0 is equal 1 plus theta 1 plus\n",
        "414": "theta 2 times a2 plus theta 3 times a3\n",
        "417": "where the values a 1 a 2 a 3 are those\n",
        "421": "given by these 3 in the units now to be\n",
        "423": "actually curve consistent with our\n",
        "425": "earlier notation actually we need to\n",
        "428": "throw in the superscript twos here\n",
        "430": "I guess need to phone the superscript\n",
        "434": "twos everywhere and I also have these\n",
        "435": "indices 1 there\n",
        "438": "because I have only one of the unit but\n",
        "440": "if you focus on the blue parts of the\n",
        "442": "notation this is here this looks awfully\n",
        "444": "like the standard logistic regression\n",
        "446": "model except I now have a capital theta\n",
        "450": "into the lowercase data but um and what\n",
        "452": "this is doing is it's just logistic\n",
        "455": "regression but where the features get\n",
        "457": "into logistic regression are these\n",
        "461": "values computed by the hidden layer just\n",
        "462": "to say that again\n",
        "464": "what does neural network is doing is is\n",
        "467": "just like logistic regression except\n",
        "469": "that rather than using the original\n",
        "473": "features x1 x2 x3 is using these new\n",
        "477": "features a1 a2 a3 the gain put put\n",
        "480": "superscript there to be consistent with\n",
        "483": "notation and the cool thing about this\n",
        "487": "is that the features a1 a2 a3 they\n",
        "489": "themselves are learned as functions of\n",
        "492": "the input concretely the function\n",
        "495": "mapping from layer 1 to layer 2 that is\n",
        "497": "determined by some other set of\n",
        "500": "parameters theta 1 so as as if the\n",
        "502": "neural network instead of being\n",
        "505": "constrained to feed the features x1 x2\n",
        "507": "x3 2 interest regression it gets to\n",
        "511": "learn its own features a1 a2 a3 to feed\n",
        "515": "into logistic regression and as you can\n",
        "517": "imagine depending on what parameters it\n",
        "519": "chooses for theta1 you can learn some\n",
        "521": "pretty interesting and complex features\n",
        "524": "and therefore you can end up with a\n",
        "527": "better hypothesis then if you were\n",
        "528": "constrained to use the raw features x1\n",
        "532": "x2 x3 or if you were constrained to say\n",
        "534": "choose the polynomial term 0 X 1 X 2 X 2\n",
        "537": "X 3 and so on but instead this algorithm\n",
        "539": "has the flexibility to try to learn\n",
        "542": "whatever features at once using these a\n",
        "545": "1 a 2 a 3 in order to feed into this\n",
        "548": "loss unit in that that's essentially a\n",
        "549": "logistic regression\n",
        "554": "I realize this example is described in a\n",
        "556": "somewhat high level and so I'm not sure\n",
        "558": "if this intuition of the neural network\n",
        "560": "you know having more complex features\n",
        "563": "will quite make sense yet but if it\n",
        "565": "doesn't yet in the next two videos I'm\n",
        "567": "going to go through a specific example\n",
        "570": "of how a neural network can use this\n",
        "571": "hidden layer to compute more complex\n",
        "574": "features to feed into this final output\n",
        "576": "layer and how that can learn more\n",
        "579": "complex hypotheses so in case what I'm\n",
        "580": "saying here doesn't quite make sense\n",
        "582": "stick with me for the next two videos\n",
        "584": "and hopefully after working through\n",
        "586": "those examples this explanation will\n",
        "589": "make a little bit more sense but just to\n",
        "590": "point out you can have neural networks\n",
        "593": "with other types of diagrams as well and\n",
        "595": "the way that neural networks are\n",
        "598": "connected that's called the architecture\n",
        "600": "so the term architecture refers to how\n",
        "602": "the different neurons are connected to\n",
        "605": "each other this is a an example of a\n",
        "606": "different neural network architecture\n",
        "609": "and once again you may be able to get\n",
        "612": "this intuition of how the second layer\n",
        "615": "here we have three hidden units that are\n",
        "617": "computing some complex function maybe of\n",
        "619": "the input layer and then the third layer\n",
        "621": "can take the second layer of features\n",
        "623": "and compute even more complex features\n",
        "625": "in layer 3 so that by the time you get\n",
        "628": "to the upper layer therefore you can\n",
        "629": "have fun you know even more complex\n",
        "632": "features of what you are able to compute\n",
        "634": "in layer 3 and so get very interesting\n",
        "637": "nonlinear hypotheses by the way in a\n",
        "639": "network like this layer 1 this is called\n",
        "642": "an input layer layer 4 is still upper\n",
        "644": "layer and this network has two hidden\n",
        "648": "layers so anything does not a input\n",
        "650": "layer or an upper layer is called\n",
        "652": "anything there\n",
        "655": "so hopefully from this video you've got\n",
        "657": "to the sense of how the feed-forward\n",
        "659": "propagation step in the neural network\n",
        "661": "works where you start from the\n",
        "663": "activations of the input layer and for\n",
        "665": "propagate that to the first in there\n",
        "667": "then the second hidden there and then\n",
        "669": "finally the output layer and you also\n",
        "671": "saw how we can vectorize that\n",
        "675": "computation in the NICS I realized that\n",
        "677": "some of the intuitions in this video of\n",
        "680": "how the other certain layers are\n",
        "682": "computing complex features of the early\n",
        "683": "layers I realize some of that intuition\n",
        "686": "may be still slightly abstract and kind\n",
        "688": "of at a high level and so what I'd like\n",
        "690": "to do in the next two videos is work\n",
        "692": "through a detailed example of how a\n",
        "695": "neural network can be used to compute\n",
        "698": "nonlinear functions of the input and\n",
        "700": "hopefully now give you a good sense of\n",
        "702": "the sorts of complex nonlinear\n",
        "704": "hypotheses we can get out of neural\n"
    },
    "jAA2g9ItoAc": {
        "0": " \n",
        "1": "in this video we'll talk about the\n",
        "3": "second major type of machine learning\n",
        "5": "problem called unsupervised learning in\n",
        "7": "the last video we talked about\n",
        "10": "supervised learning back then we count\n",
        "12": "data sets that look like this where each\n",
        "16": "example was labeled either as a positive\n",
        "18": "or a negative example whether it was a\n",
        "21": "benign or a malignant tumor so for each\n",
        "23": "example in supervised learning we were\n",
        "25": "told explicitly what is the so-called\n",
        "27": "right answer whether it's benign or\n",
        "30": "malignant in unsupervised learning we're\n",
        "32": "given data that looks different in green\n",
        "34": "data that looks like this that doesn't\n",
        "36": "have any labels or that of all has the\n",
        "39": "same label or really no labels so we're\n",
        "41": "given the data set and we're not told\n",
        "43": "what to do a bit and we're not told what\n",
        "46": "each data point is instead we just told\n",
        "48": "here is a data set can you find some\n",
        "51": "structure in the data given this data\n",
        "52": "set an unsupervised learning algorithm\n",
        "55": "might decide that the data lives in two\n",
        "57": "different clusters so there's one\n",
        "60": "cluster and there's a different cluster\n",
        "62": "and the unsupervised learning algorithm\n",
        "64": "may break this data into these two\n",
        "67": "separate clusters so this is called a\n",
        "69": "clustering algorithm and this turns out\n",
        "72": "to be used in many places one example\n",
        "74": "where clustering is used is in Google\n",
        "75": "where clustering is used is in Google\n",
        "76": "News and if you're not seeing this\n",
        "78": "before you can actually go to this URL\n",
        "81": "news.google.com to take a look what\n",
        "83": "Google News does is every day it goes\n",
        "85": "and looks at tens of thousands or\n",
        "86": "hundreds of thousands of new stories on\n",
        "89": "the web and it groups them into cohesive\n",
        "92": "news stories for example let's look here\n",
        "97": "the URLs here link to different news\n",
        "101": "stories about the BP oil well story so\n",
        "103": "let's say I click on one of these URLs\n",
        "105": "and I click on one of these URLs what\n",
        "107": "I'll get to is a web page like this\n",
        "109": "here's a Wall Street Journal article\n",
        "112": "about you know the BP oil wells still\n",
        "115": "stories of BP cause Macondo which is the\n",
        "118": "name of the un-- scope and if you click\n",
        "121": "on a different URL from that group then\n",
        "122": "you might get the different story\n",
        "125": "she's a CNN story about again the BP oil\n",
        "128": "spill and if you click on get a third\n",
        "129": "spill and if you click on get a third\n",
        "130": "link that you might get\n",
        "133": "a different story here's the UK Guardian\n",
        "137": "story about the BP oil spill so a Google\n",
        "138": "News is done is looked at tens of\n",
        "140": "thousands of new stories and\n",
        "143": "automatically cluster them together so\n",
        "145": "the new stories that are all about the\n",
        "148": "same topic get to display together it\n",
        "150": "turns out that clustering algorithms and\n",
        "152": "unsupervised learning algorithms are\n",
        "154": "used in many other problems as well\n",
        "157": "here's one on understanding genomics\n",
        "160": "here's an example of DNA microarray data\n",
        "162": "the idea is you have a group of\n",
        "164": "different individuals and for each of\n",
        "165": "different individuals and for each of\n",
        "167": "them you measure how much they do or do\n",
        "169": "not have a certain gene technically you\n",
        "171": "measure how much of certain genes are\n",
        "175": "expressed so these colors red green gray\n",
        "177": "and so on they show the degree to which\n",
        "179": "different individuals do or do not have\n",
        "183": "a specific gene and what you can do is\n",
        "185": "then run a clustering algorithm to group\n",
        "187": "individuals into different categories\n",
        "190": "onto different types of people so this\n",
        "192": "is unsupervised learning because we're\n",
        "193": "not telling the algorithm in advance\n",
        "195": "that you know these are Taiwan people\n",
        "197": "those are type 2 persons those are type\n",
        "200": "3 persons and so on and instead what\n",
        "202": "we're saying is you know here's a bunch\n",
        "204": "of data I don't know what's in this data\n",
        "206": "I don't know who's in what type I didn't\n",
        "207": "know what the different types of people\n",
        "209": "are but can you automatically find\n",
        "211": "structure in the data from you\n",
        "212": "automatically cluster the individuals\n",
        "213": "automatically cluster the individuals\n",
        "214": "into these types that I don't know in\n",
        "216": "advance because we're not giving them\n",
        "220": "the algorithm the right answer for the\n",
        "222": "examples in my data set this is\n",
        "224": "unsupervised learning unsupervised\n",
        "226": "learning or clustering is used for a\n",
        "228": "bunch of other applications is used to\n",
        "231": "organize large computer clusters I had\n",
        "233": "some friends looking at large data\n",
        "234": "centers that there's large computer\n",
        "236": "clusters and trying to figure out which\n",
        "239": "machines tend to work together and if\n",
        "241": "you can put those machines together you\n",
        "243": "can make your data center work more\n",
        "246": "efficiently your second application on\n",
        "248": "social network analysis so given\n",
        "250": "knowledge about which friends you email\n",
        "252": "the most or given your Facebook friends\n",
        "253": "or your Google+ Circles\n",
        "256": "can we automatically identify which are\n",
        "259": "cohesive groups of friends so what are\n",
        "260": "groups of people that all know each\n",
        "261": "other\n",
        "265": "market segmentation many companies have\n",
        "267": "huge databases of customer information\n",
        "269": "so can you look at this customer data\n",
        "271": "set and automatically discover market\n",
        "273": "segments and automatically group your\n",
        "276": "customers into different market segments\n",
        "279": "so that you can automatically and more\n",
        "282": "efficiently sell or market to your\n",
        "284": "different market segments together again\n",
        "286": "this is unsupervised learning because we\n",
        "288": "have all this customer data but we don't\n",
        "289": "know in advance what are the market\n",
        "292": "segments and for the customers in our\n",
        "293": "data set you know we don't know in\n",
        "295": "advance who is in microsecond one who is\n",
        "298": "in market segments two and so on but we\n",
        "299": "have to let the algorithm discover all\n",
        "302": "this just from the data finally it turns\n",
        "304": "out that unsupervised learning is also\n",
        "307": "used for surprisingly astronomical data\n",
        "310": "analysis and these clustering algorithms\n",
        "312": "give surprisingly interesting or useful\n",
        "315": "theories of how galaxies are formed all\n",
        "318": "these are examples of clustering which\n",
        "320": "is just one type of unsupervised\n",
        "322": "learning let me tell you about another\n",
        "324": "one I'm going to tell you about the\n",
        "327": "cocktail party problem so we've been the\n",
        "328": "cocktail parties before right where you\n",
        "330": "can imagine there's a party room for\n",
        "332": "people all sitting around all talk at\n",
        "334": "the same time and there are all these\n",
        "336": "overlapping voices because everyone's\n",
        "338": "talked in the same time and it's almost\n",
        "339": "hard to hear the person in front of you\n",
        "343": "so maybe have a cocktail party with two\n",
        "344": "so maybe have a cocktail party with two\n",
        "347": "people two people talking the same time\n",
        "350": "and someone small cocktail party and\n",
        "352": "we're going to put two microphones in\n",
        "355": "the room so there are microphones and\n",
        "357": "because these microphones are at two\n",
        "359": "different distances from the speaker's\n",
        "362": "each microphone records a different\n",
        "365": "combination of these two speakers voices\n",
        "367": "maybe speaker one is a little louder in\n",
        "370": "microphone one and maybe speaker two is\n",
        "372": "a little bit louder in microphone two\n",
        "374": "because you know the two microphones are\n",
        "376": "at different positions relative to the 2\n",
        "380": "speakers but each microphone recalls an\n",
        "382": "overlapping combination of both speakers\n",
        "383": "voices\n",
        "386": "so here's an here's an actual recording\n",
        "389": "two speakers recorded by a researcher\n",
        "391": "let me play for you the first one the\n",
        "394": "first microphone sounds like one two\n",
        "402": "three one my head alright maybe not the\n",
        "404": "most interesting cocktail party is two\n",
        "406": "people counting from one to ten in two\n",
        "408": "languages that you know there you go\n",
        "410": "what you just heard was the first\n",
        "412": "microphone recording here's the second\n",
        "422": "recording you see the names so we can do\n",
        "424": "is take these two microphone recordings\n",
        "426": "and give them to an unsupervised\n",
        "427": "learning algorithm called the cocktail\n",
        "430": "party algorithm and tell the algorithm\n",
        "433": "find structure in this data for me and\n",
        "435": "what the album will do is listen to\n",
        "438": "these audio recordings and say you know\n",
        "440": "it sounds like the two audio recordings\n",
        "442": "that being added together or there being\n",
        "444": "some together to produce these\n",
        "446": "recordings that we had moreover what the\n",
        "448": "cocktail party algorithm will do is\n",
        "450": "separate out these two audio sources\n",
        "453": "that were being added or being summed\n",
        "455": "together to form our recordings and in\n",
        "457": "fact here's the first output of the\n",
        "461": "cocktail party algorithm one two three\n",
        "467": "oh five six seven eight nine ten so\n",
        "470": "separated out the English voice in one\n",
        "472": "of us in one of the recordings and\n",
        "476": "here's the second output uno dos tres\n",
        "480": "cuatro Cinco seis fifty all true really\n",
        "484": "it is not too bad to give you one more\n",
        "486": "example here's another recording of\n",
        "488": "another similar situation here's the\n",
        "496": " \n",
        "498": "okay so the poor guys gone home from the\n",
        "500": "cocktail party and he's now studying the\n",
        "503": "route by himself talking to his video he\n",
        "512": " \n",
        "514": "when you give these two microphone\n",
        "516": "recording to the same algorithm what it\n",
        "518": "does is again say you know it sounds\n",
        "519": "does is again say you know it sounds\n",
        "521": "like there are two audio sources and\n",
        "524": "moreover the album says here is the\n",
        "525": "first of the audio sources\n",
        "530": "I found one two three four five six\n",
        "535": "seven eight nine ten so that wasn't\n",
        "537": "perfect it got the boys but they'll also\n",
        "540": "go up into the music in there then\n",
        "541": "here's the second output of the\n",
        "548": " \n",
        "551": "II not too bad in that second output and\n",
        "553": "managed to get rid of the voice entirely\n",
        "556": "and just Co cleaned up the music and got\n",
        "558": "rid of the counting from one to ten so\n",
        "560": "you might look at an unsupervised\n",
        "563": "learning algorithm like this and ask how\n",
        "565": "complicated is it to implement this it\n",
        "567": "seems like in order to you know build\n",
        "569": "this application it seems like to do\n",
        "571": "audio processing you need to write a ton\n",
        "573": "of code or maybe link into like a bunch\n",
        "575": "of C++ or Java libraries to process\n",
        "577": "audio it seems like an really\n",
        "579": "complicated program to do this audio\n",
        "581": "separating out audio and so on\n",
        "584": "it turns out the algorithm to do what\n",
        "587": "you just heard that can be done with one\n",
        "591": "line of code shown right here it did\n",
        "593": "take me searches a long time to come up\n",
        "595": "with this line of code so I'm not saying\n",
        "597": "this is an easy problem but it turns out\n",
        "598": "that we use the right programming\n",
        "601": "environment many learning algorithms can\n",
        "604": "be really short programs so this is also\n",
        "606": "why in this class we're going to use the\n",
        "609": "octave programming environment octave is\n",
        "611": "free open source software and using a\n",
        "614": "tool like octave or MATLAB many learning\n",
        "616": "algorithms become just a few lines of\n",
        "618": "code to implement later in this class\n",
        "620": "also teach you a little bit about how to\n",
        "622": "use octave and you'll be implementing\n",
        "624": "some of these albums in octave or if you\n",
        "627": "have MATLAB you can use that - turns out\n",
        "629": "that Silicon Valley for a lot of machine\n",
        "631": "learning algorithms what we do is first\n",
        "633": "prototype our software in octave because\n",
        "635": "software that octave makes it incredibly\n",
        "637": "fast implement these learning algorithms\n",
        "639": "here are each of these functions like\n",
        "642": "for example the SVD function that sends\n",
        "644": "the singular value decomposition but\n",
        "646": "that turns out to be a linear algebra\n",
        "648": "routine that is just built into octave\n",
        "651": "if you were trying to do this a C++ or\n",
        "653": "Java this would be many many lines of\n",
        "655": "code linking complex C++ or Java\n",
        "657": "libraries so you can implement this\n",
        "659": "stuff in C++ or Java or Python there's\n",
        "661": "just much more complicated to use to do\n",
        "664": "some of those languages what I've seen\n",
        "666": "after having taught machine learning for\n",
        "668": "almost about a decade now is that you\n",
        "671": "learn much faster if you use octave as\n",
        "674": "your programming environment and if you\n",
        "677": "use octave as your learning tool and as\n",
        "679": "your prototyping tool I'll let you learn\n",
        "681": "and prototype learning\n",
        "683": "rooms much more quickly and in fact what\n",
        "685": "many people will do in the large Silicon\n",
        "687": "Valley companies is in fact use an\n",
        "689": "algorithm like octave to first prototype\n",
        "692": "the learning algorithm and only after\n",
        "693": "you've gotten to where and then you\n",
        "697": "migrate it to C++ or Java or whatever it\n",
        "698": "turns out that by doing things this way\n",
        "700": "you can often get your algorithm to work\n",
        "702": "much faster than if you were starting\n",
        "705": "out in C++ so I know that this instruct\n",
        "708": "as an instructor I get to say trust me\n",
        "710": "on this one only a finite number of\n",
        "712": "times but for those of you who have\n",
        "714": "never used these octave type programming\n",
        "715": "environments before I'm going to answer\n",
        "718": "there trust me on this one and say that\n",
        "721": "you you will I think your time when you\n",
        "722": "develop a time is one of the most\n",
        "725": "valuable resources and having seen lots\n",
        "727": "of people do this I think you as a\n",
        "729": "machine learning researcher or machine\n",
        "731": "learning developer will be much more\n",
        "733": "productive if you learn to stuff in\n",
        "735": "prototype to stuff in octave and in some\n",
        "740": "of the language finally to wrap up this\n",
        "742": "video have one quick review question for\n",
        "745": "you we talked about unsupervised\n",
        "747": "learning which is the learning setting\n",
        "748": "where you give the algorithm a ton of\n",
        "751": "data and just ask it to find structure\n",
        "753": "in the data for us of the following four\n",
        "756": "examples which ones which of these four\n",
        "758": "do you think would be an unsupervised\n",
        "760": "learning out algorithm as opposed to a\n",
        "763": "supervised learning problem for each of\n",
        "765": "the four check boxes on the Left check\n",
        "768": "the ones or which you think of\n",
        "770": "unsupervised learning algorithm would be\n",
        "772": "appropriate and then click the button on\n",
        "774": "the lower right to check your answer so\n",
        "777": "when the video pauses please answer the\n",
        "782": "question on the slide so hopefully\n",
        "784": "remember the spam filter problem if you\n",
        "787": "have labeled data you know with of spam\n",
        "789": "and non-spam email we treat this as a\n",
        "792": "supervised learning problem the new\n",
        "795": "story example that's exactly the Google\n",
        "796": "News example that we saw in this video\n",
        "798": "we saw how you can use a clustering\n",
        "800": "algorithm to cluster news articles\n",
        "802": "together so that's unsurprising learning\n",
        "805": "the market segmentation example I talked\n",
        "807": "a little bit earlier do that as an\n",
        "810": "unsupervised learning problem because\n",
        "811": "I'm just going to give my\n",
        "813": "from data and ask you to discover market\n",
        "816": "segments automatically and the final\n",
        "818": "example diabetes well that's actually\n",
        "821": "just like our breast cancer example from\n",
        "823": "only instead of you know good and bad\n",
        "826": "cancer tumors or benign and malignant\n",
        "829": "tumors we instead have have diabetes or\n",
        "831": "not and so we will use that as a\n",
        "833": "supervised we will solve that as a\n",
        "835": "supervised learning problem just like we\n",
        "838": "did for the breast tumor data so that's\n",
        "841": "it for unsupervised learning and in the\n",
        "843": "next video we'll delve more into\n",
        "846": "specific learning algorithms and start\n",
        "848": "to talk about just how these algorithms\n",
        "850": "work and how we can how you can go about\n"
    },
    "k1JGvqr56Yk": {
        "0": " \n",
        "1": "in the last video I talked about how\n",
        "3": "when faced with a machine learning\n",
        "5": "problem there are often lots of\n",
        "7": "different ideas for how to improve the\n",
        "9": "algorithm in this video let's talk about\n",
        "12": "the concepts of error analysis which\n",
        "14": "will hopefully give you a way to more\n",
        "15": "systematically make some of these\n",
        "19": "decisions if you are starting work on a\n",
        "21": "machine learning problem or building a\n",
        "23": "machine learning application is often\n",
        "26": "considered very good practice to start\n",
        "28": "not by building a very complicated\n",
        "30": "system with lots of complex features and\n",
        "33": "so on but to instead start by building a\n",
        "35": "very simple algorithm that you can\n",
        "38": "implement quickly and when I start of a\n",
        "40": "learning problem what I usually do is\n",
        "42": "spend at most one day literally at most\n",
        "45": "24 hours to try to get something really\n",
        "47": "quick and dirty frankly not at all\n",
        "49": "sophisticated system but get something\n",
        "51": "really quick and dirty running and\n",
        "54": "implement it and then test it on my\n",
        "56": "cross validation data once you've done\n",
        "58": "that you can then plot learning curves\n",
        "61": "which this is what we talked about in\n",
        "63": "the previous set of videos but plot\n",
        "66": "learning curves of the training and test\n",
        "68": "errors to try to figure out if your\n",
        "70": "learning algorithm may be suffering from\n",
        "72": "high bias or high variance or something\n",
        "75": "else and use that to try to decide if\n",
        "77": "having more data more features and so on\n",
        "79": "are likely to help and the reason that\n",
        "82": "this is a good approach is often when\n",
        "83": "you're just starting out on the learning\n",
        "85": "problem there's really no way to tell in\n",
        "87": "advance whether you need more complex\n",
        "89": "because or whether you need more more\n",
        "91": "data or something else and it's just\n",
        "93": "very hard to tell in advance that is in\n",
        "96": "the absence of evidence in the absence\n",
        "99": "of seeing a learning curve it's just\n",
        "101": "incredibly difficult to figure out where\n",
        "103": "you should be spending your time and as\n",
        "106": "often by implementing even a very very\n",
        "108": "quick and dirty implementation and by\n",
        "110": "plotting learning curves that that helps\n",
        "113": "you make these decisions so if you like\n",
        "114": "you can think of this as a way of\n",
        "116": "avoiding what's sometimes called\n",
        "119": "premature optimization in\n",
        "121": "programming and this is idea that just\n",
        "124": "says that we should let evidence guide\n",
        "126": "our decisions on where to spend our time\n",
        "129": "rather than use gut feeling which is\n",
        "130": "often wrong\n",
        "132": "in addition to plotting learning curves\n",
        "134": "one other thing that is often very\n",
        "136": "useful to do is what's called error\n",
        "139": "analysis and what I mean by that is that\n",
        "141": "when building say a spam classifier I\n",
        "145": "will often look at my cross validation\n",
        "149": "set and manually look at the emails that\n",
        "151": "my algorithm is making Arizona so look\n",
        "153": "at the spam emails and non-spam emails\n",
        "156": "that the algorithm is misclassifying and\n",
        "159": "see if you can spot - any systematic\n",
        "161": "patterns in what type of examples of\n",
        "163": "this misclassifying and often by doing\n",
        "165": "that this is the process that will\n",
        "169": "inspire you to design new features or\n",
        "171": "they'll tell you whether the current\n",
        "173": "things will current shortcomings of the\n",
        "175": "system and give you the inspiration you\n",
        "177": "need to come up with improvements to it\n",
        "180": "concretely here's a specific example\n",
        "182": "let's say you've built a spam classifier\n",
        "188": "and you have 500 examples in your\n",
        "190": "cross-validation set and let's say in\n",
        "192": "this example that the algorithm has a\n",
        "193": "very high error rate and in this\n",
        "196": "classifies 100 of these trials\n",
        "199": "validation examples so what I do is\n",
        "201": "manually examine these hundred errors\n",
        "204": "and manually categorize them based on\n",
        "207": "things like what type of email it is and\n",
        "208": "what cues or what features you think\n",
        "210": "might have helped the algorithm classify\n",
        "214": "them correctly so specifically by what\n",
        "216": "type of email it is you know if I look\n",
        "218": "through these hundred errors I might\n",
        "221": "find that maybe the most common types of\n",
        "223": "spam emails in this class files or maybe\n",
        "226": "emails on farmer owns pharmacies that\n",
        "228": "they see emails trying to sell drugs\n",
        "231": "maybe emails that are trying to sell\n",
        "234": "replicas of fake watches fake\n",
        "237": "those things maybe have some emails\n",
        "240": "trying to spew passwords these are also\n",
        "242": "called phishing emails because that's\n",
        "244": "another big category of emails and maybe\n",
        "248": "other categories so in terms of classify\n",
        "250": "what type of email is I would actually\n",
        "252": "go through and count up you know 100\n",
        "255": "emails maybe I find the top of the\n",
        "258": "mislabel emails are farming emails and\n",
        "260": "maybe four of them emails trying to sell\n",
        "262": "replicas they sell fake watches or\n",
        "265": "something and maybe I find that fifty\n",
        "268": "three of them are these on what's called\n",
        "269": "phishing emails basically emails trying\n",
        "270": "phishing emails basically emails trying\n",
        "271": "to persuade you to give them give them\n",
        "273": "your password and 31 emails are other\n",
        "276": "types of emails and this by counting up\n",
        "278": "the number of emails in these different\n",
        "280": "categories that you might discover for\n",
        "282": "example that the algorithm is doing\n",
        "285": "Reaper ticularly poorly on emails trying\n",
        "287": "to steal passwords and that might\n",
        "289": "suggest that it might be worth your\n",
        "291": "effort to look more carefully at that\n",
        "294": "type of email and see if you can come up\n",
        "296": "with better features to categorize them\n",
        "299": "correctly and also what I might do is\n",
        "301": "look at what cues or what features\n",
        "303": "additional features might have helped\n",
        "305": "the algorithm classify the emails so\n",
        "306": "the algorithm classify the emails so\n",
        "307": "let's say that some of our hypotheses\n",
        "310": "about things or features that might help\n",
        "313": "us cause my emails better are trying to\n",
        "316": "detect deliberate misspellings versus\n",
        "319": "unusual email routing versus unusual you\n",
        "321": "know Stanley punctuation such as if\n",
        "323": "people use a lot of exclamation marks\n",
        "325": "and once again I would manually go\n",
        "327": "through and let's say I find five cases\n",
        "331": "of this and 16 of this and 32 of this\n",
        "333": "and a bunch of other types of emails as\n",
        "337": "well and if this is what you get on your\n",
        "339": "cross validation so then it really tells\n",
        "341": "you that you know maybe deliberate\n",
        "342": "spellings is a sufficiently rare\n",
        "344": "phenomenon that maybe it's not worth a\n",
        "347": "lot of time trying to write write\n",
        "350": "algorithms to detect that but if you\n",
        "352": "find a lot of spammers are using you\n",
        "354": "know unusual punctuation then maybe\n",
        "356": "that's a strong sign that it might\n",
        "358": "actually be worth your while to spend\n",
        "360": "the time to develop develop more\n",
        "361": "sophisticated features based on the\n",
        "362": "punch\n",
        "366": "so the sort of error analysis which is\n",
        "369": "really the process of manually examining\n",
        "370": "the mistakes that the algorithm makes\n",
        "373": "can often help guide you to the most\n",
        "376": "fruitful avenues to pursue and this also\n",
        "378": "explains why I often recommend\n",
        "379": "implementing a quick and dirty\n",
        "382": "implementation of an algorithm what we\n",
        "384": "really want to do is figure out what are\n",
        "386": "the most difficult examples for an\n",
        "389": "algorithm to classify and very often for\n",
        "391": "different algorithms with different\n",
        "393": "learning algorithms they'll often find\n",
        "395": "you know similar categories of examples\n",
        "398": "difficult and by having a quick and\n",
        "399": "dirty implementation that's often a\n",
        "402": "quick way to let you identify some\n",
        "404": "errors and quickly identify what are the\n",
        "407": "hot examples so that you can focus your\n",
        "410": "efforts on those mostly when developing\n",
        "413": "learning algorithms one other useful tip\n",
        "416": "is to make sure that you have a way you\n",
        "419": "have a numerical evaluation of your\n",
        "422": "learning algorithm and what I mean by\n",
        "424": "that is that if you're developing a\n",
        "427": "learning algorithm is often incredibly\n",
        "429": "helpful if you have a way of evaluating\n",
        "431": "your learning algorithm that just gives\n",
        "433": "you back a single real number\n",
        "435": "maybe accuracy maybe error but the\n",
        "437": "single real number that tells you how\n",
        "440": "well your learning RM is doing I'll talk\n",
        "442": "more about this specific concept in\n",
        "444": "later videos but here's a specific\n",
        "447": "example let's say we're trying to decide\n",
        "448": "whether or not we should treat words\n",
        "450": "like discount discounts discounted\n",
        "452": "discounting as the same word so you know\n",
        "455": "maybe one way to do that is to just look\n",
        "458": "at the first few characters in the world\n",
        "460": "that you if you just look at the first\n",
        "463": "few characters of a word then you you\n",
        "466": "figure out that maybe all of these words\n",
        "470": "are roughly have similar meanings in\n",
        "472": "natural language processing the way that\n",
        "474": "this is done is actually using a type of\n",
        "477": "software called stemming software if you\n",
        "479": "ever want to do this yourself search on\n",
        "482": "a website engine for the porter stemmer\n",
        "484": "you know one reasonable piece of\n",
        "486": "software for doing this sort of stemming\n",
        "488": "which will let you treat all of these\n",
        "489": "words discount discounts and so on as\n",
        "495": "the same word but using a dubbing\n",
        "497": "software that basically looks at the\n",
        "500": "first few alphabets of a word more or\n",
        "502": "less it can help but it can hurt and it\n",
        "504": "can hurt because for example the\n",
        "507": "software made mistake the worst universe\n",
        "510": "and university as being the same thing\n",
        "513": "because you know these two words start\n",
        "514": "off with very similar card with the same\n",
        "519": "alphabet so if you're trying to decide\n",
        "521": "whether or not to use stemming software\n",
        "525": "for a spam classifier it's not always\n",
        "527": "easy to tell and in particular error\n",
        "530": "analysis may not actually be helpful to\n",
        "533": "for deciding if this sort of stemming\n",
        "536": "idea is a good idea instead the best way\n",
        "538": "to figure out if using stemming software\n",
        "541": "is going to help your classifier is if\n",
        "543": "you have a way to very quickly just try\n",
        "547": "it and see if it works\n",
        "551": "and in order to do this having a way to\n",
        "553": "numerically evaluate your algorithm is\n",
        "557": "going to be very helpful concretely the\n",
        "559": "maybe the most natural thing to do is to\n",
        "561": "look at the cross-validation error of\n",
        "563": "the algorithms performance with and\n",
        "565": "without stemming so if you run your\n",
        "567": "algorithm without stemming and you end\n",
        "569": "up with let's say 5 percent\n",
        "572": "classification error and you rerun it\n",
        "574": "and you end up with that's a three\n",
        "576": "percent classification error then this\n",
        "578": "decrease in error very quickly allows\n",
        "581": "you to decide that you know looks like\n",
        "583": "using stemming is a good idea for this\n",
        "585": "particular problem there's a very\n",
        "587": "natural single real number evaluation\n",
        "590": "metric maybe the cross validation error\n",
        "593": "will see later examples we're coming up\n",
        "594": "with this sort of single real number\n",
        "597": "evaluation metric male it'll need a\n",
        "599": "little bit more work but as we'll see\n",
        "602": "the later video doing so but also then\n",
        "604": "let you make these decisions much more\n",
        "606": "quickly of say whether or not to use\n",
        "609": "stemming and just as one more quick\n",
        "611": "example let's say that you're also\n",
        "613": "trying to decide whether or not to\n",
        "615": "distinguish between upper versus lower\n",
        "617": "case so you know is the word mom with\n",
        "618": "case so you know is the word mom with\n",
        "621": "upper case M versus lower case M should\n",
        "623": "that be treated as the same word or as\n",
        "624": "different words should these be treated\n",
        "626": "as the same feature or as different\n",
        "628": "features and so once again because we\n",
        "631": "have a way to evaluate our algorithm if\n",
        "633": "you try this out you know if I stop\n",
        "635": "distinguishing upper and lower case\n",
        "638": "maybe I end up with 3.2 percent error\n",
        "641": "and I find that therefore this is a dis\n",
        "643": "that's worse then you know if I use only\n",
        "645": "stemming and so this lets me very\n",
        "648": "quickly decide to go ahead and to\n",
        "650": "distinguish will do not distinguish\n",
        "652": "between upper and lower case so when\n",
        "654": "you're developing a learning algorithm\n",
        "656": "very often you'll be trying out lots of\n",
        "659": "new ideas and lots of new versions of\n",
        "661": "your learning algorithm if every time\n",
        "664": "you try out a new idea if you end up\n",
        "666": "manually examining a bunch of examples\n",
        "668": "to see if it got better or worse you\n",
        "670": "know that's going to make it really hard\n",
        "671": "to make decision\n",
        "672": "is on do you stemming or not do you\n",
        "674": "distinguish upper and lower case or not\n",
        "676": "but by having a single real number\n",
        "679": "evaluation metric you can then just look\n",
        "681": "and see oh did he ever go up a little\n",
        "684": "down and you can use that to much more\n",
        "686": "rapidly try out new ideas and almost\n",
        "690": "right away tell if your new idea has\n",
        "692": "improved or worse in the performance of\n",
        "695": "the learning algorithm and this will let\n",
        "698": "you often make much faster progress so\n",
        "700": "the recommended strongly recommended way\n",
        "702": "to do error analysis is on the\n",
        "704": "cross-validation set rather than the\n",
        "707": "test set but you know there are people\n",
        "709": "that will do this on the test set even\n",
        "711": "though that's definitely less math math\n",
        "713": "the appropriate so they're less\n",
        "715": "recommended waiting thing to do than to\n",
        "717": "do error analysis on your\n",
        "719": "cross-validation set so to wrap up this\n",
        "722": "video when starting on the new machine\n",
        "724": "learning problem what I almost always\n",
        "727": "recommend is to implement a quick and\n",
        "728": "dirty implementation of your learning\n",
        "731": "algorithm and I've almost never seen\n",
        "734": "anyone spend too little time on this\n",
        "737": "quick and dirty implementation of pretty\n",
        "740": "much all pretty much only ever seen\n",
        "743": "people spend much too much time building\n",
        "745": "their first you know supposedly quick\n",
        "747": "and dirty implementation so really don't\n",
        "750": "worry about it being too quick or don't\n",
        "752": "worry about it being too dirty but\n",
        "754": "really implement something as quickly as\n",
        "756": "you can and once you have the initial\n",
        "758": "implementation this has been a powerful\n",
        "760": "tool for deciding where to spend your\n",
        "762": "time links because first you can look at\n",
        "764": "the errors in base and do the sort of\n",
        "766": "error analysis to see what in mistakes\n",
        "768": "and makes and you start to inspire\n",
        "770": "further development and second assuming\n",
        "772": "your quick and dirty implementation\n",
        "774": "incorporated a single real number\n",
        "776": "evaluation metric this can then be a\n",
        "778": "vehicle for you to try out different\n",
        "781": "ideas and quickly see if the different\n",
        "783": "ideas you're trying out to improve\n",
        "784": "performance of your algorithm and\n",
        "786": "therefore let you may be much more\n",
        "788": "quickly make decisions about what things\n",
        "791": "mm or what things to incorporate into\n"
    },
    "kHwlB_j7Hkc": {
        "0": " \n",
        "1": "our first learning algorithm will be\n",
        "3": "linear regression in this video you see\n",
        "5": "what the model looks like and more\n",
        "7": "importantly you also see what the\n",
        "8": "overall process of supervised learning\n",
        "13": "you'll flip let's use a motivating\n",
        "15": "example of predicting housing prices\n",
        "18": "we're going to use a data set of housing\n",
        "21": "prices from the city of Portland Oregon\n",
        "24": "and you're going to plot my data set of\n",
        "27": "a number of houses there were different\n",
        "29": "sizes that were so for a range of\n",
        "32": "different prices let's say that given\n",
        "33": "this data set you have a friend that's\n",
        "36": "trying to sell a house and let's see\n",
        "39": "your friend's house is of size 1,250\n",
        "41": "square feet and you want to tell them\n",
        "43": "how much they might be able to sell the\n",
        "46": "house for well one thing you could do is\n",
        "49": "fit to a model maybe put a straight line\n",
        "52": "to this data then write something like\n",
        "54": "that and based on that maybe you could\n",
        "57": "tell your friend that looks likely to\n",
        "60": "maybe sell the whole square around to\n",
        "64": "$18,000 so this is an example of a\n",
        "67": "supervised learning algorithm and it's\n",
        "69": "supervised learning because we're given\n",
        "72": "the quote right answer for each of our\n",
        "75": "examples namely were told what was the\n",
        "77": "actual house what was the actual price\n",
        "79": "that each of the houses in our data set\n",
        "82": "was so for and moreover does an example\n",
        "84": "of a regression problem where the term\n",
        "86": "regression refers to the fact that we're\n",
        "88": "predicting a real-valued output maybe\n",
        "90": "the price and then just remind you the\n",
        "92": "other type the other most common type of\n",
        "94": "supervised learning problem is called\n",
        "96": "the classification problem where we\n",
        "101": "predict discrete values outputs such as\n",
        "104": "if we are looking at cancer tumors and\n",
        "107": "trying to decide if a tumor is malignant\n",
        "110": "or benign so there's a 0 1 value\n",
        "113": "distrito more formally in supervised\n",
        "115": "learning we have a data set and this\n",
        "118": "data set is called a training set so for\n",
        "120": "a housing price an example we have a\n",
        "122": "training set of different housing prices\n",
        "125": "and our job is to learn from this data\n",
        "127": "how to predict the prices of the houses\n",
        "130": "let's define some notation that we're\n",
        "131": "using throughout this course I'm going\n",
        "133": "to define quite a lot of symbols\n",
        "135": "is okay if you don't remember all the\n",
        "136": "symbols right now but as the course\n",
        "138": "progresses so be useful with a\n",
        "141": "convenient notation so I'm going to use\n",
        "143": "lowercase M throughout this course to\n",
        "145": "denote the number of training examples\n",
        "148": "so in this data set if I have you know\n",
        "151": "let's say 47 rows in this table then I\n",
        "153": "have 47 training examples and M equals\n",
        "157": "47 let me use lowercase X to denote the\n",
        "159": "input variables often also called the\n",
        "162": "features so though the X's here will be\n",
        "165": "on input features and I'm going to use Y\n",
        "167": "to denote my output variables or the\n",
        "169": "target variable region to predict its\n",
        "172": "Alexis second column here a little bit\n",
        "176": "more notation I'm going to use X comma Y\n",
        "184": "to denote a single training example so a\n",
        "186": "single row in this table corresponds to\n",
        "189": "a single training example and to refer\n",
        "192": "to a specific training example I'm going\n",
        "199": "to use this notation X I comma Y I and\n",
        "203": "going to use this to refer to the\n",
        "210": "training example so this superscript I\n",
        "214": "over here this is not exponentiation\n",
        "217": "right this X I Y I the superscript I in\n",
        "220": "parenthesis that's just an index into my\n",
        "222": "training set and it refers to the I row\n",
        "225": "in this table okay so this is not except\n",
        "229": "our of iy ^ I instead X I Y I just\n",
        "233": "refers to the I fro of this table so for\n",
        "235": "example X 1\n",
        "239": "you know refers to the input value for\n",
        "241": "the first training example so that's 21\n",
        "246": "0 4 represents X to the first row X 2\n",
        "251": "would be equal to 14 16 right the second\n",
        "256": "x and y 1 will be equal to 460 with\n",
        "258": "that's the first the Y value for my\n",
        "261": "first training example that's what that\n",
        "263": "one refers to\n",
        "266": "so as I mentioned occasionally I'll ask\n",
        "269": "you a question to let you check your own\n",
        "271": "understanding and a few seconds in this\n",
        "273": "video a multiple-choice question will\n",
        "275": "pop up in the video when it does please\n",
        "277": "use your mouse to select what you think\n",
        "278": "is the right answer\n",
        "281": "we're defined by the training set is and\n",
        "283": "so here's how a supervised learning\n",
        "286": "works we solve them of a training set\n",
        "287": "like our training set of housing prices\n",
        "290": "and we feed that to our learning\n",
        "292": "algorithm is the job of a learning\n",
        "294": "algorithm to then output a function\n",
        "297": "which by convention is usually denoted\n",
        "300": "lowercase H and H stands for hypothesis\n",
        "304": "and what the job of the hypothesis is is\n",
        "307": "is a function that takes as input the\n",
        "310": "size of a house like maybe the size of a\n",
        "311": "new house that your friend is trying to\n",
        "314": "sell it takes in a value of X and it\n",
        "321": "tries to output the estimated value of y\n",
        "325": "for their corresponding house so H is a\n",
        "333": "function that map's from X's to YS um\n",
        "335": "people often ask me you know why is this\n",
        "338": "function called a hypothesis some of you\n",
        "340": "may know the meaning of the term\n",
        "341": "hypothesis from the dictionary or from\n",
        "343": "signs or whatever it turns out that the\n",
        "346": "machine learning this is a name that was\n",
        "347": "used in the early days of machine\n",
        "349": "learning and this kind of stuff because\n",
        "351": "maybe not a great name for this sort of\n",
        "353": "function for mapping from sizes of\n",
        "356": "houses to the predictions but you know I\n",
        "359": "think the term hypothesis maybe isn't\n",
        "360": "the best possible name for this but is\n",
        "362": "what this is the standard terminology\n",
        "364": "that people using here you know so don't\n",
        "366": "worry do it don't worry too much about\n",
        "369": "why people call it that when designing a\n",
        "371": "learning algorithm the next thing we\n",
        "373": "need to decide is how do we represent\n",
        "377": "this hypothesis H for this in the next\n",
        "379": "few videos I'm going to choose our\n",
        "381": "initial choice for representing the\n",
        "383": "hypothesis will be the following going\n",
        "385": "to represent H as follows and with the\n",
        "387": "right of this subscript theta of x\n",
        "395": "equals theta 0 plus theta 1 of X and\n",
        "397": "as a shorthand sometimes instead of\n",
        "398": "as a shorthand sometimes instead of\n",
        "400": "writing you know H subscript theta of X\n",
        "402": "sometimes it's a shorthand I'll just\n",
        "404": "write this is H of X but more often our\n",
        "407": "rector the subscript theta over there\n",
        "409": "and plotting to some pictures all this\n",
        "410": "and plotting to some pictures all this\n",
        "413": "means is that we are going to you know\n",
        "417": "predict that Y is a linear function of X\n",
        "418": "predict that Y is a linear function of X\n",
        "421": "right so there's a data set and what\n",
        "423": "this function is doing is just\n",
        "427": "predicting that Y is some straight line\n",
        "431": "function of X s of x equals 3 0 plus\n",
        "437": "theta 1 X ok and why a linear function\n",
        "440": "well sometimes we'll want to fit more\n",
        "442": "complicated perhaps nonlinear functions\n",
        "444": "as well but since this linear case is\n",
        "446": "the simple building block we'll start\n",
        "448": "with this example first so fitting\n",
        "450": "linear functions and we'll build on this\n",
        "452": "to eventually have more complex models\n",
        "455": "in more complex learning algorithms let\n",
        "457": "me also give this particular model a\n",
        "460": "name small though is called linear\n",
        "462": "regression or district example is a\n",
        "464": "actually linear regression with one\n",
        "468": "variable will be variable being X some\n",
        "469": "connecting housing prices functions in\n",
        "472": "one variable X and another name for this\n",
        "474": "model is you need the area linear\n",
        "477": "regression and you need area is just you\n",
        "481": "know a fancy way of saying one variable\n",
        "485": "so that's linear regression in the next\n",
        "487": "video we'll start to talk about just how\n"
    },
    "kvtLGRthW4U": {
        "0": " \n",
        "2": "seen over and over that one of the most\n",
        "4": "reliable ways to get a high-performance\n",
        "6": "machine learning system is to take a low\n",
        "9": "biased learning algorithm and to train\n",
        "11": "it on a massive training set but where\n",
        "13": "do you get so much training data from\n",
        "14": "turns out then the machine learning\n",
        "16": "there's a fascinating idea called\n",
        "19": "artificial data synthesis this doesn't\n",
        "21": "apply to every single problem and to\n",
        "23": "apply to a specific problem often take\n",
        "25": "some thoughts and innovation and insight\n",
        "28": "but if this idea applies to your machine\n",
        "30": "learning problem it can sometimes be an\n",
        "33": "easy way to get a huge training sense to\n",
        "35": "give to your learning algorithm the idea\n",
        "37": "of artificial data synthesis comprises\n",
        "40": "the two main variations the first is if\n",
        "43": "we're essentially creating data from\n",
        "44": "poke holes is creating new data from\n",
        "47": "strategy and the second is if we already\n",
        "50": "have a small label training set and with\n",
        "52": "somehow amplify that training set or use\n",
        "54": "a small training set to turn that into a\n",
        "56": "larger training set and in this video\n",
        "60": "we'll go over both of those ideas to\n",
        "63": "talk about the artificial data synthesis\n",
        "65": "out here let's use the character\n",
        "67": "recognition a portion of the photo OCR\n",
        "69": "pipeline we want to take as input an\n",
        "72": "image and recognize my character again\n",
        "75": "if we go out and collect a large label\n",
        "77": "dataset here's what the dataset would\n",
        "79": "look like for this patron example I've\n",
        "82": "chosen the square aspect ratio so we're\n",
        "84": "taking square image patches and the goal\n",
        "87": "is to take an image patch and recognize\n",
        "89": "the character in the middle of that\n",
        "91": "image patch and for the sake of\n",
        "93": "simplicity I'm going to treat these\n",
        "96": "images as grayscale images rather than\n",
        "98": "color images it turns out that using\n",
        "100": "color doesn't seem to help that much for\n",
        "101": "this particular problem\n",
        "103": "so given this image patch we would like\n",
        "105": "to recognize the density given this\n",
        "107": "image manager than to recognize\n",
        "111": "yes given that image patch we'd like to\n",
        "113": "recognize that that's an eye and so on\n",
        "116": "so all of these are examples of real\n",
        "118": "images how can we come up with a much\n",
        "120": "larger training set modern computers\n",
        "124": "often have a huge font library and if\n",
        "126": "you use a word processing software\n",
        "128": "depending on what word processor you use\n",
        "130": "it might have all of these files and\n",
        "132": "many many more already stored inside and\n",
        "133": "in fact if you go to different websites\n",
        "137": "there are again huge free font libraries\n",
        "138": "on the internet where you can download\n",
        "140": "many many different types of fonts\n",
        "142": "hundreds of apps thousands of different\n",
        "145": "forms so if you want more training\n",
        "148": "examples one thing you can do is just\n",
        "150": "take characters from different fonts and\n",
        "153": "paste these characters against different\n",
        "155": "brands and backgrounds so you might take\n",
        "157": "you know this alphabet C and paste that\n",
        "160": "Steve against a random background and if\n",
        "162": "you do that you now have a training\n",
        "165": "example of an image of the character C\n",
        "168": "so after some amount of work you know\n",
        "170": "this and it is a little bit of work to\n",
        "172": "synthesize realistic looking data but\n",
        "174": "after some amount of work you can get a\n",
        "177": "synthetic training set like that every\n",
        "179": "image shown on the right was actually a\n",
        "182": "synthesized image where you take a font\n",
        "184": "from maybe a random font downloaded off\n",
        "187": "the web and you paste an image of a one\n",
        "188": "character or a few characters from that\n",
        "191": "font against a sort of random background\n",
        "193": "image and then apply you know maybe\n",
        "195": "little blurring operators living little\n",
        "198": "affine distortions FM meaning just a\n",
        "201": "sharing and scaling and little rotation\n",
        "203": "operations now you do that you get a\n",
        "205": "synthetic training set like the one\n",
        "209": "shown here and this is work it is it\n",
        "212": "takes for to work in order to make the\n",
        "214": "synthetic data look realistic and if you\n",
        "216": "do a sloppy job in terms of how you\n",
        "218": "create the synthetic data then it\n",
        "220": "well but if you look at these you know\n",
        "222": "the synthetic data actually looks\n",
        "224": "remarkably similar to the raw data and\n",
        "227": "so by using synthetic data you have\n",
        "229": "essentially an unlimited supply of\n",
        "231": "training examples for artificial data\n",
        "234": "synthesis and so if you use this sort of\n",
        "236": "synthetic data you have essentially an\n",
        "240": "unlimited supply of label data to train\n",
        "242": "a supervised learning algorithm for the\n",
        "245": "character recognition problem so this is\n",
        "248": "an example of artificial data synthesis\n",
        "250": "where you are basically creating new\n",
        "252": "data from scratch you're just generating\n",
        "254": "brand new images from scratch the other\n",
        "256": "main approach to artificial data\n",
        "259": "synthesis is where you take examples\n",
        "261": "that you currently have so we take a\n",
        "263": "roll example maybe from row image and\n",
        "266": "you create additional data source to\n",
        "268": "amplify your training set so here's an\n",
        "270": "image of a character from a real image\n",
        "273": "nor the synthesized image and I've\n",
        "274": "overlaid this with the grid lines here\n",
        "275": "just for the purpose of illustration\n",
        "277": "shouldn't actually have these grid lines\n",
        "279": "of course but what you can do is then\n",
        "281": "take this alphabet a take this image and\n",
        "284": "introduce artificial warping of\n",
        "286": "artificial distortions into the image so\n",
        "288": "that you can take the image a and turn\n",
        "291": "that into 16 new examples so in this way\n",
        "293": "you can take a maybe a small label\n",
        "295": "training set and amplify your training\n",
        "297": "set to sort of suddenly get a lot more\n",
        "302": "examples all of it again in order to do\n",
        "303": "this for a particular application it\n",
        "305": "does take thoughts and the dusting\n",
        "306": "insight to figure out what are\n",
        "309": "reasonable sets of distortions or\n",
        "310": "whether reasonable ways that amplifier\n",
        "312": "to multiply your training set and for\n",
        "315": "the specific example of character\n",
        "316": "recognition\n",
        "318": "introducing these warping seems like a\n",
        "320": "natural choice but for a different\n",
        "322": "machine learning application there may\n",
        "323": "be different distortions that might make\n",
        "326": "no sense let me just show one example of\n",
        "328": "an ax totally different domain of speech\n",
        "331": "recognition so speech recognition let's\n",
        "334": "say you have audio clips and you want to\n",
        "336": "learn from the audio clip to recognize\n",
        "338": "what was what were the words spoken in\n",
        "340": "that clip so let's say you have one\n",
        "342": "label of training example so let's say\n",
        "344": "you have one label training example of\n",
        "346": "someone saying a few specific words so\n",
        "347": "let's\n",
        "353": "that will do cliff here 0 1 2 3 4 I all\n",
        "355": "right so some counting from 0 to 5 and\n",
        "358": "so you know you want to try to apply a\n",
        "360": "learning algorithm to try to recognize\n",
        "363": "the word said in that so how can we\n",
        "365": "amplify the data set well what do we do\n",
        "367": "is introduce additional audio\n",
        "370": "distortions into the data set so here\n",
        "373": "I'm going to add background sounds to\n",
        "374": "simulate the bad cellphone connection\n",
        "376": "when you hear beeping sounds that's\n",
        "378": "actually a part of your audio track\n",
        "379": "there's nothing wrong with speakers but\n",
        "385": "let me play this now 0 1 2 3 4 5 right\n",
        "387": "so you can listen to that sort of audio\n",
        "389": "clip and recognize the sounds that seems\n",
        "391": "like another useful training is out of\n",
        "392": "the house here's another example\n",
        "399": "noisy back down 0 1 2 3 you know cars\n",
        "400": "driving past people walk in the\n"
    },
    "l4lSUAcvHFs": {
        "0": " \n",
        "1": "in the previous video we talked about\n",
        "4": "stochastic gradient descent and how that\n",
        "6": "can be much faster than batch gradient\n",
        "8": "descent in this video let's talk about\n",
        "11": "another variation on these ideas called\n",
        "13": "mini batch gradient descent that can\n",
        "15": "work sometimes even if it faster than\n",
        "19": "stochastic gradient descent to summarize\n",
        "21": "the algorithms we've talked about so far\n",
        "23": "in batch gradient descent we will use\n",
        "27": "all em in each generation whereas in\n",
        "29": "stochastic gradient descent we will use\n",
        "32": "a single example in each iteration what\n",
        "34": "mini batch gradient descent does is\n",
        "37": "somewhere in between specifically what\n",
        "38": "this algorithm we're going to use be\n",
        "42": "examples in each iteration where B is a\n",
        "47": "parameter how the mini batch size so the\n",
        "50": "idea is that this is somewhat in between\n",
        "51": "batch gradient descent it's too costly\n",
        "54": "during descent and it's just like Baxter\n",
        "55": "in this end except that we're going to\n",
        "58": "use a much smaller batch size a typical\n",
        "61": "choice for the value of B might be B\n",
        "64": "equals 10 let's say and typical range\n",
        "66": "really might be anywhere from B equals 2\n",
        "68": "up to B equals 100 so that would be a\n",
        "72": "pretty typical range of values for the\n",
        "75": "mini batch size and the idea is that\n",
        "78": "rather than using one example at a time\n",
        "79": "or M examples I'm going to use B\n",
        "82": "examples at a time so let me just write\n",
        "84": "this out informally we're going to get\n",
        "88": "let's say B for this example let's say B\n",
        "90": "equals 10 Samia you know the next 10\n",
        "94": "examples from my training set so that\n",
        "98": "may be some set of examples X I Y I if\n",
        "100": "it's 10 examples then the indexing would\n",
        "101": "it's 10 examples then the indexing would\n",
        "104": "be up to X I plus 9\n",
        "108": "why i +99 so that's ten examples all\n",
        "111": "together and then we'll perform\n",
        "114": "essentially a gradient descent update\n",
        "119": "using these ten examples so that's the\n",
        "121": "learning rate times 1/10 times the sum\n",
        "129": "over k equals I through I plus nine of H\n",
        "136": "subscript theta of XK minus y k times x\n",
        "143": "k J and so in this expression we're\n",
        "146": "summing the gradient terms over my 10\n",
        "149": "examples so there's a number 10 that's\n",
        "151": "you know my mini batch size and just I\n",
        "154": "plus 9 again the 9 comes from the choice\n",
        "156": "of the parameter beat and then after\n",
        "161": "this we will then increase you know I by\n",
        "164": "10 to go on to the next 10 examples and\n",
        "167": "then keep living like this so just to\n",
        "169": "write out the entire algorithm in into\n",
        "173": "info in order to simplify the indexing\n",
        "175": "for this for 1 over the writer I'm going\n",
        "177": "to see we have a mini batch size of 10\n",
        "179": "and a training set size of a thousand\n",
        "182": "what we're going to do is have this sort\n",
        "184": "of 4 degree or for I equals 1 11:21 so\n",
        "186": "stepping in steps of 10 because we look\n",
        "189": "at 10 examples of the time and then we\n",
        "191": "perform this sort of a gradient descent\n",
        "194": "update using 10 examples at a time so\n",
        "197": "this 10 and this I plus 9\n",
        "200": "those are consequence having chosen my\n",
        "202": "mini batch size to be 10 and you know\n",
        "205": "this Ultimo's folder this ends at\n",
        "209": "9:1 here because if I have a thousand\n",
        "211": "training examples then I need a hundred\n",
        "213": "steps of size ten in order to get\n",
        "215": "through my entire training set so this\n",
        "218": "is mini-batch gradient descent compared\n",
        "221": "to batch gradient descent this also\n",
        "222": "allows us to make progress much faster\n",
        "226": "so we have again our running example of\n",
        "229": "US census data of 300 million training\n",
        "231": "examples then what we're saying is after\n",
        "233": "looking at just the first ten examples\n",
        "235": "we can start to make progress in\n",
        "238": "improving the parameters data so we\n",
        "240": "don't need to scan through the entire\n",
        "241": "training set we just need to look at the\n",
        "243": "first ten examples and this will start\n",
        "245": "letting us make progress and then we can\n",
        "246": "look at the second ten examples then\n",
        "248": "multiply the parameters a little bit\n",
        "251": "again and so on so that's why mini-batch\n",
        "253": "gradient descent can be faster than\n",
        "254": "batch gradient descent namely you can\n",
        "257": "start making progress in modifying the\n",
        "259": "parameters after looking at just ten\n",
        "261": "examples rather than leaving to wait\n",
        "262": "till your scan through every single\n",
        "264": "training example all 300 million of them\n",
        "267": "so how about mini batch gradient descent\n",
        "269": "versus stochastic gradient descent so\n",
        "272": "why do we want to look at the examples\n",
        "274": "at the time rather than look at just a\n",
        "277": "single example at a time as it's the\n",
        "278": "cost of unison\n",
        "282": "the answer is in vectorization in\n",
        "284": "particular mini batch gradient descent\n",
        "287": "is likely to outperforms the Casagrande\n",
        "289": "descent only if you have a good vector\n",
        "292": "rise implementation in that case the sum\n",
        "296": "over ten examples can be performed in a\n",
        "300": "more vectorized way which will allow you\n",
        "304": "to partially paralyzed your computation\n",
        "306": "over the 10 examples so in other words\n",
        "308": "by using appropriate vectorization to\n",
        "310": "compute the derivative terms you can\n",
        "312": "sometimes partially use the good\n",
        "314": "numerical linear algebra libraries to\n",
        "317": "parallelize your gradient computations\n",
        "320": "over the B examples whereas if you were\n",
        "321": "looking at just a single example the\n",
        "323": "time with stochastic great descent then\n",
        "325": "you know looking at just one example the\n",
        "326": "time there\n",
        "328": "much to paralyse over Lisa's left to\n",
        "330": "paralyse over one disadvantage of\n",
        "332": "mini-batch gradient descent is that\n",
        "335": "there's now this extra parameter V the\n",
        "336": "mini batch size which you may have the\n",
        "338": "federal event which may therefore take\n",
        "341": "time but if you have a good vectorized\n",
        "343": "implementation this can sometimes run\n",
        "345": "even faster than stochastic gradient\n",
        "346": "descent\n",
        "349": "so that was mini-batch gradient descent\n",
        "351": "which is an algorithm that in some sense\n",
        "353": "does something the somewhere in between\n",
        "355": "what's the constant gradient descent us\n",
        "357": "and what batch gradient descent us and\n",
        "360": "if you choose a reason without your B\n",
        "362": "I'll usually use B equals 10 but you\n",
        "365": "know other values anywhere from stay to\n",
        "368": "to 100 would be reasonably common so if\n",
        "369": "you choose a good value of B and then\n",
        "371": "you use a good vectorized implementation\n",
        "373": "sometimes it can be faster than both\n",
        "375": "stochastic gradient descent and faster\n"
    },
    "lCipWj-Cets": {
        "0": " \n",
        "2": "in the last video we talked about the\n",
        "4": "process of evaluating an anomaly\n",
        "6": "detection algorithm and then we started\n",
        "8": "to use some label data with examples\n",
        "10": "that we knew were either anomalous or\n",
        "13": "not anomalous with y equals 1 or y\n",
        "15": "equals 0 and so the question then arises\n",
        "18": "of if we have this label data we have\n",
        "20": "some examples that know to be anomalies\n",
        "22": "and something you know not to be about\n",
        "23": "nominees why don't we just use a\n",
        "24": "nominees why don't we just use a\n",
        "26": "supervised learning algorithm so when we\n",
        "28": "just use logistic regression or a neural\n",
        "30": "network to try to learn directly from\n",
        "33": "our label data to predict whether y\n",
        "35": "equals 1 and y equals 0 in this video or\n",
        "37": "trying to share of you some of the\n",
        "39": "thinking and some guidelines for when\n",
        "41": "you should probably use an anomaly\n",
        "42": "detection algorithm and when it might be\n",
        "44": "more fruitful to consider using a\n",
        "47": "supervised learning algorithm this slide\n",
        "49": "shows what are the settings under which\n",
        "52": "you should maybe use anomaly detection\n",
        "54": "versus when supervised learning might be\n",
        "57": "more fruitful if you have a problem with\n",
        "59": "a very small number of positive examples\n",
        "62": "and remember the examples of y equals 1\n",
        "66": "are the anomalous examples then you\n",
        "67": "might consider using an anomaly\n",
        "71": "detection algorithm so having 0 to 20\n",
        "73": "maybe up to 50 positive examples might\n",
        "75": "be pretty typical and usually would have\n",
        "78": "such a small positive set of positive\n",
        "80": "examples we're going to save the\n",
        "81": "positive examples just for the\n",
        "83": "cross-validation set in the test sets\n",
        "87": "and in contrast in a typical normal\n",
        "89": "anomaly detection setting we will often\n",
        "91": "have a relatively large number of\n",
        "94": "negative examples of the normal examples\n",
        "97": "of normal aircraft engines and and we\n",
        "98": "of normal aircraft engines and and we\n",
        "99": "can then use this very large number of\n",
        "102": "negative examples with which to fit the\n",
        "105": "model P of X and so this is idea that in\n",
        "108": "many anomaly detection applications you\n",
        "110": "have very few positive examples and lots\n",
        "112": "of negative examples and when we're\n",
        "116": "doing the process of estimating P of X\n",
        "117": "the fitting all those Gaussian\n",
        "119": "parameters you know we need only\n",
        "120": "negative examples to do that so if you\n",
        "122": "have a long- AIDS\n",
        "126": "in still fit pfx pretty well in contrast\n",
        "128": "for supervised learning more typically\n",
        "130": "we would have a reasonably large number\n",
        "132": "of both positive and negative examples\n",
        "136": "and so this is one way to look at your\n",
        "138": "problem in deciding if you should use an\n",
        "140": "anomaly detection algorithm or\n",
        "142": "supervised learning operable here's\n",
        "143": "another way that people often think\n",
        "145": "about anomaly detection algorithm\n",
        "147": "so for anomaly detection applications\n",
        "149": "often there are many different types of\n",
        "151": "anomalies so think about aircraft\n",
        "152": "engines you know there are so many\n",
        "154": "different ways where your crab engines\n",
        "155": "are going wrong right there's so many\n",
        "156": "things that could go wrong that could\n",
        "159": "break in an aircraft engine and so if\n",
        "161": "that's the case and if you have a pretty\n",
        "163": "small positive instead of positive\n",
        "165": "examples then it can be hard for an\n",
        "167": "algorithm difficult for an algorithm to\n",
        "169": "learn from your small set of positive\n",
        "171": "examples what the anomalies look like\n",
        "173": "and in particular you know future\n",
        "175": "anomalies may look nothing like the ones\n",
        "177": "you've seen so far so maybe in your set\n",
        "179": "of positive examples maybe you've seen\n",
        "181": "five or ten or twenty different ways\n",
        "183": "that an aircraft engine could go wrong\n",
        "186": "but maybe tomorrow you need to detect a\n",
        "189": "totally new set a totally new type of\n",
        "192": "anomaly a totally new way for an\n",
        "194": "aircraft engine to be broken that you've\n",
        "196": "just never seen before and if that's the\n",
        "199": "case then it might be more promising to\n",
        "201": "just model the negative examples with\n",
        "204": "this sort of a Gaussian model P of X\n",
        "206": "rather than try too hard to model the\n",
        "208": "positive examples because you know\n",
        "209": "tomorrow's anomaly may be nothing like\n",
        "212": "the ones you've seen so far in contrast\n",
        "216": "in in some other problems you have\n",
        "217": "enough positive examples for an\n",
        "219": "algorithm to get a sense of what the\n",
        "221": "positive examples are like and in\n",
        "223": "particular if you think that future\n",
        "225": "positive examples are likely to be\n",
        "227": "similar to ones in the training set then\n",
        "229": "in that setting you know it might be\n",
        "231": "more reasonable to have a supervised\n",
        "232": "learning out\n",
        "233": "they're not said of all the positive\n",
        "235": "examples looks to them you know the\n",
        "237": "negative examples and uses that to try\n",
        "239": "to distinguish between positives and\n",
        "242": "negatives so hopefully this gives you a\n",
        "244": "sense of you know if you have a specific\n",
        "247": "problem should you think about using an\n",
        "249": "anomaly detection algorithm or a\n",
        "251": "supervised learning algorithm and the\n",
        "253": "key difference really is that in the\n",
        "254": "null means tension often that we have\n",
        "256": "such a small number of positive examples\n",
        "259": "that is not possible for a learning\n",
        "261": "algorithm to learn that much from the\n",
        "263": "positive examples and so what we do\n",
        "265": "instead is take a large set of negative\n",
        "267": "examples and have it just learn a lot\n",
        "270": "the P of X from just a negative examples\n",
        "272": "of the normal aircraft engine say and we\n",
        "274": "reserved the small number of positive\n",
        "277": "examples for evaluating our algorithm to\n",
        "279": "use an either the cross-validation set\n",
        "282": "or the test set and just a side comment\n",
        "284": "about this many different types of\n",
        "286": "anomalies you know in some earlier\n",
        "288": "videos we talked about the email spam\n",
        "291": "example so in those examples there are\n",
        "293": "actually many different types of spam\n",
        "294": "email right there spam email is trying\n",
        "295": "email right there spam email is trying\n",
        "297": "to sell you things spam email trying to\n",
        "298": "steal your passwords\n",
        "300": "it's called phishing emails and many\n",
        "302": "different types of spam emails but for\n",
        "304": "the spam problem we usually have enough\n",
        "307": "examples of spam email to see you know\n",
        "309": "most of these different types of spam\n",
        "311": "email because we have a large set of\n",
        "313": "examples of spam and that's why we\n",
        "315": "usually think of spam as a supervised\n",
        "317": "learning setting even though only are\n",
        "318": "there many different types of sm\u00c3\u00a5land\n",
        "323": "and so if we look at some applications\n",
        "326": "of anomaly detection versus supervised\n",
        "328": "learning we'll find that you know in for\n",
        "330": "detection if you have many different\n",
        "333": "types of ways for people to try to come\n",
        "334": "in frauds and a relatively small\n",
        "336": "training set a small number of\n",
        "338": "fraudulent users on your website do not\n",
        "340": "use a anomaly detection algorithm I\n",
        "344": "should say if you have if you're a very\n",
        "346": "major you know online retailer\n",
        "348": "if you actually have had a lot of people\n",
        "349": "or trying to commit fraud on your\n",
        "351": "website so if you actually have a lot of\n",
        "354": "examples but y equals one then you know\n",
        "356": "sometimes fraud detection could actually\n",
        "357": "shift over to the supervised learning\n",
        "360": "column but if you haven't had if you\n",
        "362": "haven't you know if you haven't seen\n",
        "364": "that many examples of users doing\n",
        "366": "strange things on your website then more\n",
        "369": "frequently for detection this actually\n",
        "370": "treated as an anomaly detection\n",
        "372": "algorithm rather than well supervised\n",
        "374": "learning hopefully other examples you\n",
        "376": "talk to a manufacturing already\n",
        "378": "hopefully you've seen more normal\n",
        "380": "examples and not that many anomalies but\n",
        "382": "if again for some manufacturing\n",
        "384": "processes you know if if you're\n",
        "385": "manufacturing very large volumes and\n",
        "388": "they've seen a lot of bad examples maybe\n",
        "390": "manufacturing could ship to the\n",
        "392": "supervised learning column as well but\n",
        "394": "if you haven't seen that many bad\n",
        "397": "examples of your old products do not do\n",
        "399": "this anomaly detection monitoring\n",
        "401": "machine is a datacenter making some of\n",
        "403": "the custom it's similar similar sorts of\n",
        "406": "arguments apply whereas email spam\n",
        "408": "classification weather prediction and\n",
        "410": "you know classifying cancers if you have\n",
        "412": "equal numbers of positive and negative\n",
        "414": "example also if you have many examples\n",
        "416": "of your positive and your negative\n",
        "418": "examples then we would tend to treat all\n",
        "422": "of these as supervised learning problems\n",
        "425": "so hopefully that gives you a sense of\n",
        "427": "one of the properties of a learning\n",
        "430": "problem that would cause you to treat it\n",
        "432": "as an anomaly detection problem versus a\n",
        "435": "supervised learning problem and for many\n",
        "436": "of the problems that are faced by me\n",
        "438": "know various technology companies and so\n",
        "440": "on we actually are in these settings\n",
        "443": "where we have very few or sometimes zero\n",
        "445": "positive training examples maybe they\n",
        "447": "just there's so many different types of\n",
        "448": "anomalies we've never seen them before\n",
        "450": "and for those sorts of problems\n",
        "453": "very often the algorithm that is used is\n"
    },
    "lbR5br5yvrY": {
        "0": " \n",
        "2": "in this video I'd like to talk about one\n",
        "4": "last detail of k-means clustering which\n",
        "6": "is how to choose the number of clusters\n",
        "9": "or how to choose the value of the\n",
        "12": "parameter capital k to be honest there\n",
        "15": "actually isn't a great way of answering\n",
        "17": "this or of doing this automatically and\n",
        "19": "by far the most common way of choosing\n",
        "21": "the number of clusters is still choosing\n",
        "24": "it manually by the visualizations or by\n",
        "25": "looking at the output of the clustering\n",
        "28": "algorithm or something else but I do get\n",
        "29": "asked this question quite a lot of how\n",
        "31": "do you choose the number of clusters and\n",
        "33": "so just want to tell you you know what\n",
        "35": "is what are people's current thinking on\n",
        "37": "it although the most common thing is\n",
        "38": "actually to choose the number of\n",
        "44": "clusters by hand a large part of why it\n",
        "46": "might not always be easy to choose the\n",
        "48": "number of clusters is that is often\n",
        "51": "genuinely ambiguous how many clusters\n",
        "53": "there are in the data looking at this\n",
        "56": "data set some of you may see 4 clusters\n",
        "59": "and that would suggest using K equals 4\n",
        "62": "or some of you may see 2 clusters and\n",
        "65": "that will suggest K equals 2 and yet\n",
        "69": "others may see 3 clusters and so looking\n",
        "70": "at the data set like that is the the\n",
        "72": "true number of clusters it actually\n",
        "74": "seems genuinely ambiguous to me and I\n",
        "77": "don't think there is right one right\n",
        "79": "answer and this is part of unsupervised\n",
        "81": "learning we aren't given labels and so\n",
        "83": "there isn't always a clear-cut answer\n",
        "84": "there isn't always a clear-cut answer\n",
        "86": "and this is one of the things that makes\n",
        "88": "it more difficult to say have an\n",
        "90": "automatic algorithm for choosing how\n",
        "92": "many clusters to have when people talk\n",
        "94": "about ways of choosing the number of\n",
        "96": "clusters one method that people\n",
        "97": "sometimes talk about is something called\n",
        "99": "the elbow method let me just tell you a\n",
        "102": "little bit about that and then mention\n",
        "103": "some of these advantages but also\n",
        "106": "shortcomings so the elbow method what\n",
        "109": "we're going to do is vary K which is the\n",
        "110": "total number of clusters so we're going\n",
        "113": "to run k-means with one cluster that\n",
        "114": "means really everything gets grouped\n",
        "116": "into a single cluster and compute the\n",
        "118": "cost function compute the distortion J\n",
        "120": "and that here\n",
        "121": "and then we're going to run k-means with\n",
        "123": "two clusters\n",
        "124": "maybe with multiple random\n",
        "127": "initializations maybe not but then you\n",
        "128": "know with two classes we should get\n",
        "130": "hopefully a smaller distortion and so\n",
        "132": "plot that there and then run k-means\n",
        "134": "with three clusters hopefully you can\n",
        "136": "leave a smaller for the distortion and\n",
        "138": "plot that there and run k-means four\n",
        "140": "four five and so on so we end up with a\n",
        "144": "curve showing how the distortion you\n",
        "146": "know goes down as we increase the number\n",
        "148": "of clusters and so we get a curve that\n",
        "151": "maybe looks like this and if you look at\n",
        "153": "this curve what the elbow method does is\n",
        "156": "says well let's look at this plot looks\n",
        "160": "like there's a clear elbow there right\n",
        "162": "this is a movie by analogy to the human\n",
        "164": "arm where you know if you imagine that\n",
        "169": "you reach out your arm then this is your\n",
        "171": "shoulder joint this is your elbow joint\n",
        "172": "and against your hand is at the end of\n",
        "174": "here and so this is the elbow method\n",
        "176": "then you find this sort of pattern where\n",
        "178": "the distortion goes down rapidly from\n",
        "181": "one to two and two to three and then you\n",
        "183": "reach an elbow at three and then the\n",
        "185": "distortion goes down very slowly after\n",
        "186": "that then it looks like you know what\n",
        "190": "maybe using three clusters is the right\n",
        "192": "number of clusters because that's the\n",
        "195": "elbow of this curve right that goes down\n",
        "197": "distortion goes on rapidly until tables\n",
        "199": "three then it goes now very slowly after\n",
        "202": "that so let's pick K equals three\n",
        "205": "if you apply the elbow method and if you\n",
        "207": "get a plot that actually looks like this\n",
        "209": "then that's pretty good and this would\n",
        "211": "be a reasonable way of choosing the\n",
        "214": "number of clusters it turns out the\n",
        "216": "elbow method isn't used that often and\n",
        "218": "one reason is that if you actually use\n",
        "220": "this one with clustering problem turns\n",
        "223": "out that fairly often you know you end\n",
        "224": "up of a curve that looks much more\n",
        "227": "ambiguous so maybe something like this\n",
        "229": "and if you look at this I don't know\n",
        "231": "maybe there's no clear elf over it looks\n",
        "233": "like Distortion continuously goes down\n",
        "235": "maybe three is a good number\n",
        "237": "maybe force a good number maybe five is\n",
        "239": "also not bad and so if you actually do\n",
        "241": "this in practice you know if your plot\n",
        "242": "looks like the one on the left and\n",
        "244": "that's great gives you a clear answer\n",
        "247": "but just as often you end up with a plot\n",
        "248": "that looks like the one on the right and\n",
        "252": "it's not clear what the where the\n",
        "254": "location of the elbow is it makes it\n",
        "256": "harder to choose a number of clusters\n",
        "257": "using this method\n",
        "259": "so maybe the quick summary of the elbow\n",
        "261": "method is that it's worth a shot\n",
        "263": "that I wouldn't necessarily you know\n",
        "265": "have a very high expectation that\n",
        "267": "they're working for any particular\n",
        "269": "problem\n",
        "271": "finally here's one other way of how\n",
        "273": "thinking about how you choose the value\n",
        "276": "of K very often people are running\n",
        "278": "k-means in order to get clusters for\n",
        "280": "some later purpose or for some sort of\n",
        "282": "downstream purpose maybe you want to use\n",
        "284": "k-means in order to do market\n",
        "286": "segmentation like in the t-shirt sizing\n",
        "288": "example that we talked about maybe you\n",
        "291": "want came in to organize a computer\n",
        "292": "cluster better or maybe a learning\n",
        "294": "cluster for some different purpose and\n",
        "297": "so if that later downstream purpose such\n",
        "299": "as market segmentation if that gives you\n",
        "302": "an evaluation metric then often a better\n",
        "304": "way to determine the number of classes\n",
        "307": "is to see how well different numbers of\n",
        "309": "clusters serve that later downstream\n",
        "313": "purpose let me step through a specific\n",
        "315": "example let me go through the t-shirt\n",
        "317": "sizing example again and I'm trying to\n",
        "320": "decide do I want three t-shirt sizes so\n",
        "322": "I choose K equals three then I might\n",
        "326": "have small medium and large t-shirts or\n",
        "328": "maybe I want to choose K equals five and\n",
        "331": "then I might have your extra small small\n",
        "334": "medium large and extra-large t-shirt\n",
        "336": "sizes so kind of like three t-shirt\n",
        "339": "sizes or four or five t-shirt sizes we\n",
        "341": "can also have four teacher sizes but I'm\n",
        "343": "just showing three and five you know\n",
        "347": "just to simplify this slide for now so\n",
        "349": "ever run k-means with k equals three\n",
        "353": "maybe I end up with that's my small and\n",
        "357": "that's my medium and that's my large\n",
        "362": "whereas I run k-means with five clusters\n",
        "365": "maybe I end up with those are my extra\n",
        "370": "small teachers these are my small these\n",
        "374": "online medium these are my large and\n",
        "379": "these are my extra-large and the nice\n",
        "381": "thing about this example is that this\n",
        "383": "then maybe gives us another way to\n",
        "386": "choose whether we want three or four or\n",
        "389": "five clusters and in particular what you\n",
        "391": "can do is you know think about this from\n",
        "393": "the perspective of the t-shirt business\n",
        "396": "and ask well if I have five segments\n",
        "399": "then how well can I how well my t-shirts\n",
        "401": "fit my customers and so how many\n",
        "403": "t-shirts going to sell how happy with my\n",
        "405": "customers be you know what really makes\n",
        "407": "sense from from the prospective t-shirt\n",
        "409": "business in terms of whether I want to\n",
        "412": "have more t-shirt sizes so that my\n",
        "415": "fits my customers better or do I want to\n",
        "418": "have fewer t-shirt sizes so that on I\n",
        "421": "make fewer sizes of t-shirts and I can\n",
        "422": "sell them to the customers more cheaply\n",
        "425": "and so as a t-shirt selling business\n",
        "427": "that might give you a way to decide\n",
        "429": "between three clusters versus five\n",
        "432": "clusters so that gives you an example of\n",
        "436": "how a later downstream purpose like the\n",
        "437": "problem of deciding what t-shirts the\n",
        "439": "manufacturer how that can give you an\n",
        "441": "evaluation metric for choosing the\n",
        "443": "number of clusters for those of you that\n",
        "446": "are doing the programming exercises if\n",
        "447": "you look at this week's program exercise\n",
        "450": "associative k-means that's an example\n",
        "451": "there of using k-means for image\n",
        "454": "compression and so if you were trying to\n",
        "455": "choose how many clusters to use for that\n",
        "456": "problem\n",
        "459": "you could also again use the evaluation\n",
        "461": "think of image compression to choose the\n",
        "463": "number of clusters k so how good do you\n",
        "465": "want to image the look versus how much\n",
        "467": "do you want to compress the file size of\n",
        "469": "the images and you know if you do the\n",
        "471": "programs of size whether just said will\n",
        "474": "make more sense at that time so just\n",
        "477": "summarize there for the most part the\n",
        "480": "number of clusters K is still chosen by\n",
        "482": "hand by human inputs or human in science\n",
        "485": "one way to try to do so is to use the\n",
        "486": "elbow method but I wouldn't always\n",
        "488": "expect that to work well but I think the\n",
        "490": "better way to think about how to choose\n",
        "492": "the number of clusters is also for what\n",
        "495": "purpose are you running k-means and then\n",
        "497": "to think what is the number of clusters\n",
        "499": "k that serves that whatever later\n",
        "501": "purpose that you're actually running\n"
    },
    "lrAe6457ri4": {
        "0": " \n",
        "2": "in the next few videos we'll talk about\n",
        "4": "large-scale machine learning that is\n",
        "7": "algorithms for dealing with big data\n",
        "10": "sets if you look back at the recent five\n",
        "12": "or ten-year history of machine learning\n",
        "14": "one of the reasons that learning\n",
        "15": "algorithms were of so much better now\n",
        "18": "than even say five years ago it's just\n",
        "20": "the sheer amount of data that we have\n",
        "22": "now and that we can train our algorithms\n",
        "24": "on in these next few videos we talk\n",
        "26": "about algorithms but dealing with when\n",
        "33": "we have such massive data sets so why do\n",
        "35": "we want to use such large data sets\n",
        "37": "we've already seen that one of the best\n",
        "39": "ways to get a high performance machine\n",
        "42": "learning system is if you take a low\n",
        "44": "bias learning algorithm and train that\n",
        "48": "on a lot of data and so one early\n",
        "49": "example they were already seeing was\n",
        "52": "this example of classifying between\n",
        "54": "confusable words so for breakfast I ate\n",
        "58": "two T wo eggs and we saw in this example\n",
        "62": "these sorts of results where you know so\n",
        "64": "long as you feed the algorithm a lot of\n",
        "67": "data it seems to do very well and so\n",
        "68": "this results ideas that has led to the\n",
        "71": "saying in machine learning that often is\n",
        "72": "not who has the best hamburger wins is\n",
        "75": "who has the most data so we want to\n",
        "77": "learn from large data sets at least when\n",
        "80": "we can get such large data sets but\n",
        "82": "learning with large data sets comes with\n",
        "85": "his own unique problems specifically\n",
        "88": "computational problems let's say your\n",
        "92": "training set size is M equals a hundred\n",
        "94": "million and this is actually pretty\n",
        "97": "realistic for many modern data sets if\n",
        "99": "you look at the US census data set if\n",
        "101": "there are you know 300 million people in\n",
        "103": "the US you can use to get hundreds of\n",
        "105": "millions of records if you look at the\n",
        "107": "amount of traffic that popular websites\n",
        "109": "get you easily get training sets are\n",
        "111": "much larger than hundreds of millions of\n",
        "114": "examples and let's say you want to train\n",
        "116": "a linear regression model or maybe a\n",
        "118": "logistic regression model in which case\n",
        "122": "this is the gradient descent rule and if\n",
        "124": "you look at what you need to do to\n",
        "126": "compute the gradient which is this term\n",
        "129": "over here then when M is a hundred\n",
        "131": "million you need to carry out a\n",
        "133": "summation over a hundred million terms\n",
        "136": "in order to compute this derivative term\n",
        "138": "and to perform a single step of gradient\n",
        "141": "descent because of the computational\n",
        "144": "expense of summing over a hundred\n",
        "146": "million entries in order to compute just\n",
        "149": "one step of gradient descent in the next\n",
        "151": "few videos we'll talk about techniques\n",
        "153": "for either replacing this algorithm or\n",
        "155": "something else or to find more efficient\n",
        "158": "ways to compute this derivative by the\n",
        "160": "end of the sequence of videos on\n",
        "162": "large-scale machine learning you know\n",
        "165": "how to fit models linear regression\n",
        "166": "logistic Russian neural networks and so\n",
        "169": "on even two data sets with say a hundred\n",
        "172": "million examples of course before we put\n",
        "174": "in the effort into training a model with\n",
        "176": "100 million examples we should also ask\n",
        "179": "ourselves well why not use just a\n",
        "182": "thousand examples maybe we can randomly\n",
        "185": "pick a subset of a thousand examples out\n",
        "187": "of a hundred million examples and train\n",
        "189": "our algorithm on just a thousand\n",
        "192": "examples so before investing the effort\n",
        "194": "into actually developing the software\n",
        "196": "needed to train these massive models\n",
        "198": "there's often good to sanity check if\n",
        "200": "training on just a thousand examples\n",
        "204": " \n",
        "206": "the way it's a sanity check of using a\n",
        "209": "much smaller training set might do just\n",
        "211": "as well that is if using a much smaller\n",
        "214": "N equals 1000 size training set might do\n",
        "216": "just as well it is the usual method of\n",
        "218": "plotting the learning curves so if you\n",
        "221": "were to plot the learning curves and if\n",
        "226": "your training objective where to look\n",
        "229": "like this that's J train theta and it's\n",
        "234": "your cross-validation set objective JCV\n",
        "236": "of theta would look like this then you\n",
        "238": "know it looks like this does like a high\n",
        "240": "variance learning algorithm and we would\n",
        "242": "be more confident that adding extra\n",
        "244": "training examples would improve\n",
        "247": "performance whereas in contrast if you\n",
        "251": "were to plot the learning curves if your\n",
        "254": "training objective where to look like\n",
        "257": "this and if your cross-validation\n",
        "260": "objective was to look like that then\n",
        "262": "this looks like the classical high bias\n",
        "264": "learning algorithm and in the latter\n",
        "267": "case you know if you were to plot this\n",
        "270": "up to say M equals 1000 and so that's a\n",
        "273": "equals 500 to M equals 1,000 then it\n",
        "276": "seems unlikely that increasing m to a\n",
        "279": "hundred million would do much better and\n",
        "281": "then you'd be just fine sticking to any\n",
        "283": "calls a thousand rather than investing a\n",
        "285": "lot of effort to figure out how to scale\n",
        "287": "at the algorithm of course if you were\n",
        "290": "in the situation shown by the figure on\n",
        "291": "the right then one natural thing to do\n",
        "294": "would be to add extra features or add\n",
        "296": "extra hidden units to a neural network\n",
        "299": "goal and so on so then you end up with a\n",
        "301": "similar situation and closer to down on\n",
        "303": "the Left where maybe this is up to M\n",
        "305": "equals a thousand and this then gives\n",
        "307": "you more confidence that trying to\n",
        "309": "infrastructure or change the algorithm\n",
        "311": "to use much more than a thousand\n",
        "313": "examples that that might actually be a\n",
        "315": "good use of your time so a large scale\n",
        "317": "machine learning we'd like to come up\n",
        "319": "with computationally reasonable weights\n",
        "321": "or computationally efficient ways to\n",
        "324": "deal with very big data sets in the next\n",
        "326": "few videos we'll see two main ideas the\n",
        "328": "first is called stochastic gradient\n",
        "331": "and the second is called MapReduce but\n",
        "334": "dealing with very big datasets and after\n",
        "335": "you've learned about these methods\n",
        "337": "hopefully that will allow you to scale\n",
        "339": "up your learning algorithms to big data\n",
        "341": "and allow you to get much better\n",
        "344": "performance on many different\n"
    },
    "m3U1_Zv4_Ik": {
        "0": " \n",
        "1": "neural networks are a pretty old\n",
        "4": "algorithm that was originally motivated\n",
        "6": "by the goal of having machines that can\n",
        "9": "mimic the brain now in this class of\n",
        "10": "course on teaching neural networks to\n",
        "12": "you because they work really well for\n",
        "14": "different machine learning problems and\n",
        "16": "not certainly not just because they're\n",
        "18": "biologically motivated but in this video\n",
        "20": "I'd like to give you some of the\n",
        "23": "background on neural networks so that we\n",
        "25": "can get a sense of what we can expect\n",
        "27": "them to do both in the sense of applying\n",
        "29": "them to modern-day machine learning\n",
        "31": "problems as well as for those of you\n",
        "33": "that might be interested in the maybe\n",
        "35": "the big AI dream of someday building\n",
        "39": "truly intelligent machines also how\n",
        "41": "neural networks might pertain to that\n",
        "45": "the origins of neural networks was as\n",
        "47": "algorithms that try to mimic the brain\n",
        "49": "and there was the sense that if we want\n",
        "51": "to know learning systems well why not\n",
        "54": "mimic perhaps the most amazing learning\n",
        "56": "machine we know about which is perhaps\n",
        "58": "the brain neural networks came to be\n",
        "61": "very widely used throughout the 1980s\n",
        "64": "and 1990s and for various reasons as\n",
        "67": "popularity diminished in the late 90s\n",
        "70": "but more recently neural networks have\n",
        "73": "had a major recent resurgence one of the\n",
        "75": "reasons for this resurgence is that\n",
        "78": "neural networks are computationally\n",
        "80": "somewhat more expensive algorithm and\n",
        "82": "so's only you know maybe someone more\n",
        "86": "recently that computers became fast\n",
        "88": "enough to really run large-scale neural\n",
        "90": "networks and because of that as well as\n",
        "92": "a few other technical reasons which\n",
        "93": "we'll talk about later\n",
        "96": "modern neural networks today are these\n",
        "98": "state-of-the-art technique for many\n",
        "100": "applications so when you think about\n",
        "103": "mimicking the brain well what the human\n",
        "104": "brain does so many amazing things right\n",
        "105": "brain does so many amazing things right\n",
        "107": "the brain can learn to see process\n",
        "109": "images learn to hear learn to process or\n",
        "111": "sense of touch you can you know learn to\n",
        "114": "do math and to do calculus and the brain\n",
        "116": "does so many different amazing things it\n",
        "117": "seems like if you want to mimic the\n",
        "119": "brain it seems like you have to write\n",
        "121": "lots of different pieces of software to\n",
        "123": "mimic all of these different fascinating\n",
        "125": "amazing things that the brain does but\n",
        "127": "there's this fascinating hypothesis that\n",
        "129": "the way the brain does all of these\n",
        "131": "different things is not with like a\n",
        "133": "thousand different programs but instead\n",
        "135": "the brain does the way the brain does it\n",
        "138": "is with just a single learning algorithm\n",
        "141": "this is just a hypothesis but let me\n",
        "143": "share with you some of the evidence for\n",
        "146": "this this part of the brain that little\n",
        "148": "red part of the brain is your auditory\n",
        "150": "cortex and the way your understanding my\n",
        "153": "voice now is that um your ear is taking\n",
        "155": "the sound signal and routing the sound\n",
        "157": "signal to your auditory cortex and\n",
        "159": "that's what's allowing you to understand\n",
        "162": "my words neuroscientists have done the\n",
        "164": "following fascinating experiment where\n",
        "167": "you cut the wire from the ears to the\n",
        "170": "auditory cortex and you rewire in this\n",
        "172": "case an animal's brain so that the\n",
        "174": "signal from the eyes from the optic\n",
        "176": "nerve eventually gets relative to the\n",
        "179": "auditory cortex if you do this it turns\n",
        "181": "out the auditory cortex will learn to\n",
        "184": "see and this is a in every single sense\n",
        "186": "of the word see as we know it so if you\n",
        "188": "do this to the animals the animals can\n",
        "191": "perform visual discrimination talks and\n",
        "193": "that is they can look at images and make\n",
        "195": "appropriate decisions based on the\n",
        "197": "images and they're doing it with that\n",
        "200": "piece of brain tissue here's another\n",
        "202": "example that red piece of brain tissue\n",
        "205": "is your somatosensory cortex that's how\n",
        "207": "you process your sense of touch if you\n",
        "210": "do a similar rewiring process then the\n",
        "212": "somatosensory cortex will learn see\n",
        "215": "because of this and other similar\n",
        "217": "experiments these are called\n",
        "219": "neural rewiring experiments there's this\n",
        "222": "sense that if the same piece of physical\n",
        "225": "brain tissue can process sight or sounds\n",
        "227": "or touch then maybe there's a one\n",
        "229": "learning algorithm that can process\n",
        "232": "sight or sound or touch and instead of\n",
        "234": "needing to implement a thousand\n",
        "235": "different programs or a thousand\n",
        "237": "different algorithms to do the you know\n",
        "238": "thousand wonderful things that the brain\n",
        "241": "does maybe what we need to do is figure\n",
        "244": "out some approximation or\n",
        "245": "to whatever the brains learning\n",
        "248": "algorithm is and implement that and let\n",
        "250": "the brain learn by itself how to process\n",
        "253": "these different types of data to\n",
        "255": "surprisingly large extent it seems as\n",
        "257": "though we can plug in almost any sensor\n",
        "259": "to almost any part of the brain and\n",
        "262": "select within reason the brain will\n",
        "266": "learn to do of it here are a few more\n",
        "269": "examples on the upper left is an example\n",
        "272": "of learning to see with your tongue the\n",
        "274": "way it works is on this is actually a\n",
        "276": "system called brain port undergoing you\n",
        "278": "know FDA trials now to help line people\n",
        "281": "see by the way it works is you strap a\n",
        "284": "grayscale camera to your forehead facing\n",
        "286": "forward taking that that takes a low\n",
        "288": "resolution grayscale image of what's in\n",
        "291": "front of you and you then run a wire to\n",
        "293": "an array of electrodes that you place on\n",
        "296": "your tongue so that each pixel gets\n",
        "298": "mapped to a pixel to a location on your\n",
        "300": "tongue where maybe a high voltage\n",
        "303": "response to a dark pixel and a low\n",
        "306": "voltage corresponds to a bright pixel\n",
        "309": "and even as adults today with this solar\n",
        "311": "system you and I will be able to learn\n",
        "313": "to see you know intensive minutes\n",
        "315": "without tunnels here's a second example\n",
        "319": "of a human echolocation of humans sonar\n",
        "321": "so there are two ways you can do this\n",
        "324": "you can either snap your fingers or\n",
        "328": "click your tongue oh can do very well\n",
        "331": "and but they're blind people today that\n",
        "333": "are actually being trained in schools to\n",
        "335": "do this and learn to interpret the\n",
        "337": "pattern of sounds bouncing off your\n",
        "341": "environment as sonar so if if after you\n",
        "342": "search on YouTube you're actually videos\n",
        "344": "that this amazing kid who tragically\n",
        "347": "because of cancer has his eyeballs\n",
        "348": "removed so this is a kid with no\n",
        "351": "eyeballs but by snapping his fingers he\n",
        "353": "can walk around and never hit anything\n",
        "356": "he can ride a skateboard she can shoot a\n",
        "359": "basketball - hoop and she's a kid with\n",
        "362": "no eyeballs third example is the haptic\n",
        "367": "belt where if you have a strap around\n",
        "368": "your waist\n",
        "370": "ring up buzzers and always have the\n",
        "371": "northmost one but\n",
        "372": "northmost one but\n",
        "374": "you can give a human a directions and\n",
        "377": "similar to maybe how birds can you sense\n",
        "379": "that what the sense where North is and\n",
        "382": "some bizarre example but if you plug it\n",
        "384": "third eye into the frog the frog will\n",
        "387": "learn to use that eye as well so it's\n",
        "390": "pretty amazing to what extent is as if\n",
        "392": "you can plug in almost any sensor to the\n",
        "395": "brain and the brains learning algorithm\n",
        "397": "will just figure out how to learn from\n",
        "399": "dead laser and view of that data and\n",
        "402": "there's a sense that if we can figure\n",
        "404": "out what the brains learning algorithm\n",
        "406": "is and you're implemented or implement\n",
        "408": "some approximation to that algorithm on\n",
        "410": "a computer maybe that would be a bad\n",
        "412": "shot it you're making real progress\n",
        "414": "towards the AI the artificial\n",
        "416": "intelligence dream of someday building\n",
        "419": "through the intelligent machines now of\n",
        "421": "course I'm not teaching neural networks\n",
        "423": "you know just because they might give us\n",
        "425": "a window into this far off AI dream even\n",
        "427": "though I'm personally that's one of the\n",
        "429": "things that I personally work on in my\n",
        "431": "research life but the main reason I'm\n",
        "433": "teaching neural networks in this class\n",
        "434": "is because it's actually a very\n",
        "437": "effective study art technique for modern\n",
        "440": "day machine learning applications so in\n",
        "441": "the next few videos we'll start diving\n",
        "443": "into the technical details of neural\n",
        "446": "networks so that you can apply them to\n",
        "448": "modern day machine learning applications\n",
        "449": "and get them to work well one on\n",
        "452": "problems but for me you know one of the\n",
        "455": "reasons they excite me is that maybe\n",
        "458": "they give us this this window into what\n",
        "461": "we might do if we're also thinking of\n",
        "464": "what algorithms might someday be able to\n"
    },
    "mOmkv5SI9hU": {
        "0": " \n",
        "1": "in the previous video we talked about\n",
        "4": "the backpropagation algorithm to a lot\n",
        "6": "of people seen for the first time the\n",
        "8": "first impression is often that while\n",
        "10": "this is a really complicated algorithm\n",
        "12": "and there are all these different steps\n",
        "14": "and I'm not quite sure how they fit\n",
        "15": "together and is like kind of a black box\n",
        "18": "with all these complicated steps in case\n",
        "19": "that's how you're feeling about back\n",
        "22": "propagation that's actually okay um back\n",
        "25": "propagation maybe unfortunately is a\n",
        "27": "less mathematically clean or less\n",
        "29": "mathematically simple algorithm compared\n",
        "31": "to linear regression or logistic\n",
        "33": "regression and I've actually used\n",
        "35": "backward back propagation you know\n",
        "37": "pretty successfully for many years and\n",
        "39": "even today I stole them sometimes feel\n",
        "41": "like I have a very good sense of just\n",
        "43": "what's it's doing most of intuition\n",
        "45": "about what back propagation is doing if\n",
        "48": "for those of you that are doing the\n",
        "50": "programming exercises that will and\n",
        "52": "these mechanically step you through the\n",
        "54": "different steps of how to implement back\n",
        "56": "prop so you'll be able to get in to work\n",
        "59": "for yourself and what I want to do in\n",
        "61": "this video is look a little bit more at\n",
        "63": "the mechanical steps of back propagation\n",
        "65": "and try to give you a little more\n",
        "66": "and try to give you a little more\n",
        "68": "intuition about what the mechanical\n",
        "70": "steps of back prop is doing to hopefully\n",
        "71": "convince you that you know it's at least\n",
        "74": "a reasonable algorithm in case even\n",
        "76": "after this video in case back\n",
        "79": "propagations though seems very black box\n",
        "81": "and kind of like you know too many\n",
        "83": "complicated steps a little bit magical\n",
        "86": "to you that's actually okay and even\n",
        "88": "though you know I've used back prop for\n",
        "90": "many years sometimes this is a difficult\n",
        "93": "algorithm to understand but hopefully\n",
        "96": "this video will help a little bit in\n",
        "98": "order to better understand back\n",
        "100": "propagation let's take another closer\n",
        "102": "look at what forward propagation is\n",
        "104": "doing here's the neural network with two\n",
        "107": "input units that is not counting the\n",
        "109": "bias unit and two hidden units in this\n",
        "112": "this layer and two hidden units in the\n",
        "114": "next layer and then finally one output\n",
        "117": "unit and again these counts two to two\n",
        "120": "are not counting these bias units\n",
        "123": "in order to illustrate forward\n",
        "125": "propagation I'm going to draw this\n",
        "128": "network a little bit differently and in\n",
        "130": "particular I'm going to draw this neural\n",
        "132": "network with the nose drawn has diese\n",
        "134": "very fact ellipsis so that I can write\n",
        "135": "text in them\n",
        "137": "when performing forward propagation we\n",
        "139": "might have some particular example say\n",
        "143": "some example X I comma Y I and it will\n",
        "145": "be this X I then we feed into the to the\n",
        "149": "input layer so this may be x i1 and x i2\n",
        "152": "are the values we set the input layer 2\n",
        "154": "and when we forward propagate it to the\n",
        "156": "first hidden layer here what we do is\n",
        "163": "compute Z 2 1 and Z 2 2 so these are the\n",
        "165": "weighted sum of inputs of the input\n",
        "168": "units and then we apply the sigmoid of\n",
        "172": "the logistic function and the sigmoid\n",
        "174": "activation function applied to the Z\n",
        "177": "value gives us these activation values\n",
        "180": "so that gives us a 2 1 and a 2 2 and\n",
        "182": "then we forward propagate again to get\n",
        "187": "you know here Z 3 1 apply the sigmoid of\n",
        "189": "the logistic function the activation\n",
        "191": "function to that to get a 3 1 and\n",
        "197": "similarly like so until we get Z 4 1\n",
        "199": "apply the activation function this gives\n",
        "202": "us a 4 1 which is the final output value\n",
        "205": "of the neural network let's erase this\n",
        "207": "arrow to give myself some more space and\n",
        "210": "if you look at what this computation\n",
        "214": "really is doing focusing on this hidden\n",
        "216": "unit let's say we have that this weight\n",
        "219": "shown in magenta there is my weight\n",
        "223": "theta 2 1 0 the indexing is that\n",
        "227": "important and this way here which it\n",
        "229": "gives on highlighting in red that is\n",
        "232": "Theta 2 1 1\n",
        "234": "and this wait here which I'm drawing in\n",
        "240": "green in the cyan is theta212 so the way\n",
        "244": "we compute this value Z 3 1 is Z 3 1 is\n",
        "249": "as equal to this magenta weight times\n",
        "255": "this value so that's a theta 2 1 0 times\n",
        "259": "1 and then Plus this red weight times\n",
        "264": "this value so that's a theta 2 1 1 times\n",
        "269": "a 2 1 and finally this cyan weight times\n",
        "274": "this value which is therefore plus theta\n",
        "280": "2 1 2 times a 2 1 and so that's forward\n",
        "284": "propagation and it turns out that as\n",
        "285": "we'll see later in this video what back\n",
        "288": "propagation is doing is doing a process\n",
        "290": "very similar to this except that instead\n",
        "293": "of the computations flowing from the\n",
        "295": "left to the right of this network the\n",
        "297": "computations instead flow from the right\n",
        "300": "to the left of the network and using a\n",
        "302": "very similar computation as this and\n",
        "305": "I'll say in two slides exactly what I\n",
        "307": "mean by that to better understand what\n",
        "309": "back propagation is doing let's look at\n",
        "311": "the cost function here's the cost\n",
        "313": "function that we had for when we have\n",
        "315": "only one output unit if we have more\n",
        "317": "than more than one output unit we just\n",
        "319": "have a summation you know over the\n",
        "321": "output unit index by K there but they\n",
        "323": "have only one output unit then this is\n",
        "326": "the cost function and we do forward\n",
        "329": "propagation and back propagation on one\n",
        "331": "example at a time so let's just focus on\n",
        "335": "a single example X I Y I and focus in\n",
        "337": "the case of having one output unit so\n",
        "340": "why I here is just a real number and\n",
        "343": "let's ignore regularization so lambda\n",
        "345": "equals 0 and this final term there right\n",
        "348": "goes away now if you look inside this\n",
        "351": "summation you find that the cost term\n",
        "354": "associated with the i'f training example\n",
        "356": "that is the cost associated with\n",
        "360": "training example X iyi that's going to\n",
        "361": "be given by this expression then the\n",
        "364": "cost sort of training example I is\n",
        "367": "written as follows and what this cost\n",
        "369": "function does is it plays a role similar\n",
        "372": "to the squared error so rather than\n",
        "373": "looking at this complicated expression\n",
        "375": "if you want you can think of Kosovo\n",
        "378": "being approximately you know the squared\n",
        "379": "difference between what the neural\n",
        "381": "network outputs versus what is the\n",
        "383": "actual value just as in logistic\n",
        "385": "regression we actually prefer to use the\n",
        "387": "slightly more complicated cost function\n",
        "389": "using the log but for the purpose of\n",
        "391": "feel free to think of the cost function\n",
        "393": "as being this sort of squared error cost\n",
        "396": "function and so this cost by measures\n",
        "398": "how well is the network doing on\n",
        "401": "correctly predicting example I how close\n",
        "403": "is the output to the actual observed\n",
        "406": "label lie on now let's look at what back\n",
        "409": "propagation is doing what useful\n",
        "412": "intuition is that back propagation is\n",
        "414": "computing these Delta superscript L\n",
        "417": "subscript J terms and we can think of\n",
        "420": "these as the quote error of the\n",
        "423": "activation value that we got for unit J\n",
        "427": "in the layer and elf layer more formally\n",
        "430": "for that this is maybe only for those of\n",
        "432": "you that are familiar with calculus more\n",
        "435": "formally what the Delta terms actually\n",
        "437": "are is this that the partial derivative\n",
        "440": "with respect to z LJ that is the\n",
        "441": "weighted sum of inputs that were\n",
        "443": "computing these Z terms partial\n",
        "445": "derivative respect to these things of\n",
        "448": "the cost function so concretely the cost\n",
        "450": "function is a function that the label Y\n",
        "453": "and the value this H of x up about\n",
        "455": "neural network and if we could go inside\n",
        "458": "the neural network and just change those\n",
        "461": "z LJ values a little bit then that will\n",
        "464": "affect these values that the neural\n",
        "466": "network is outputting and so that will\n",
        "467": "end up changing the\n",
        "469": "cost function and again really this is\n",
        "471": "only for those theater exploiting\n",
        "474": "calculus if you're familiar with if\n",
        "475": "you're comfortable with partial\n",
        "478": "derivatives what these Delta terms are\n",
        "479": "is they turn out to be the partial\n",
        "482": "derivative of the cost function with\n",
        "483": "respect to these intermediate terms that\n",
        "486": "were confusing and so they're a measure\n",
        "489": "of how much would we like to change the\n",
        "491": "neural networks weights in order to\n",
        "494": "affect these intermediate values of the\n",
        "497": "computation so as to affect the final\n",
        "499": "output of the neural network H of X and\n",
        "501": "therefore affect the overall cost in\n",
        "503": "case this lost positive does the partial\n",
        "505": "derivative intuition in case that didn't\n",
        "507": "make sense don't worry about it the rest\n",
        "510": "of this we can do without without really\n",
        "512": "talking about partial derivatives but\n",
        "513": "let's look in more detail about what\n",
        "516": "back propagation is doing for the output\n",
        "518": "layer it first sets this Delta term\n",
        "525": "we'll say Delta for one as Y i if we're\n",
        "526": "doing a forward propagation and back\n",
        "529": "propagation on this training example I\n",
        "533": "it says as why I minus a for one so\n",
        "534": "that's really the error is the\n",
        "536": "difference between the actual value of y\n",
        "538": "- what was the dahle predicted instead\n",
        "541": "we're going to compute Delta for one\n",
        "545": "like so next we're going to do propagate\n",
        "547": "these values backwards I explained this\n",
        "549": "in a second and end up computing the\n",
        "551": "Delta terms of the previous layer when\n",
        "556": "they end up with Delta 3 1 Delta 3 2 and\n",
        "558": "then we propagate this further backward\n",
        "563": "and end up computing Delta 2 1 and\n",
        "568": "- - now the backpropagation calculation\n",
        "570": "is a lot like running the forward\n",
        "572": "propagation algorithm but doing it\n",
        "574": "backwards so here's what I mean let's\n",
        "576": "look at how we end up with this value of\n",
        "581": "Delta 2 2 so we have Delta 2 2 and\n",
        "583": "similar to forward propagation let me\n",
        "585": "label a couple of the weight so this\n",
        "587": "weight which I'm going to run cyan let's\n",
        "593": "say that weight is Theta 2 of 1/2 and\n",
        "596": "this way down here let me highlight this\n",
        "598": "in red that's going to be let's say\n",
        "604": "theta 2 of 2 - so if we look at how\n",
        "608": "Delta 2 2 is computed how's compute for\n",
        "610": "this node it turns out that what we're\n",
        "611": "going to do is when I take this value\n",
        "614": "and multiply by this weight and add it\n",
        "618": "to this value multiplied by that weight\n",
        "621": "so it's really a way to some of the\n",
        "624": "neuro of DS our Delta values weighted by\n",
        "626": "the corresponding edge strength so\n",
        "628": "concretely let me fill this in this is\n",
        "631": "our Delta 2 2 is going to be equal to\n",
        "634": "theta - 1 2 which is that magenta weight\n",
        "639": "times Delta 3 1 + and then the thing I\n",
        "644": "had in red that's a theta 2 - 2 times\n",
        "648": "Delta 3 - so it's really literally this\n",
        "651": "red weight times this value plus this\n",
        "653": "magenta weight times this value and\n",
        "655": "that's how we wind up with that value of\n",
        "658": "Delta and just as another example let's\n",
        "660": "look at this value how do we get that\n",
        "664": "value well is a similar process if this\n",
        "666": "weight which are I'm going to highlight\n",
        "669": "in green if this way it is equal to say\n",
        "675": "Delta 3 1 2 then we have that Delta 3 2\n",
        "678": "is going to be equal to that green\n",
        "680": "weight theta 3\n",
        "686": "two times Delta one and by the way so\n",
        "688": "far I've been writing the Delta values\n",
        "691": "only for the hidden units and not but\n",
        "693": "not that excluding the biased units\n",
        "695": "depending on how you define the back\n",
        "697": "propagation algorithm or depending on\n",
        "699": "how you implement it you know you may\n",
        "701": "end up implementing something to compute\n",
        "703": "Delta values for these bias units as\n",
        "706": "well the bias unit is always output the\n",
        "708": "value of +1 and they are just what they\n",
        "709": "are and there's no way for us to change\n",
        "712": "the value and so depending on your\n",
        "714": "implementation of back prop the way I\n",
        "715": "usually implemented I do end up\n",
        "717": "computing these Delta values but we just\n",
        "719": "discard them and we don't use them\n",
        "722": "because they don't end up being part of\n",
        "724": "the calculation needed to compute the\n",
        "727": "derivative so hopefully that gives you a\n",
        "729": "little better intuition about what back\n",
        "731": "propagation is doing\n",
        "734": "in case of all of this still seems for\n",
        "736": "magical install of black box in a later\n",
        "738": "video India putting it together a video\n",
        "740": "I'll try to give a little bit more\n",
        "742": "intuition about what backpropagation is\n",
        "744": "doing but unfortunately this is a you\n",
        "746": "know difficult algorithm to try to\n",
        "748": "visualize and understand what is really\n",
        "751": "doing but fortunately you know up and I\n",
        "753": "guess many people have been using very\n",
        "755": "successfully for many years and if you\n",
        "758": "implement the algorithm you have a very\n",
        "760": "effective learning algorithm even though\n",
        "762": "the inner workings of exactly how it\n"
    },
    "mTyT-oHoivA": {
        "0": " \n",
        "2": "in this video I'd like to start adapting\n",
        "4": "support vector machines in order to\n",
        "6": "develop complex nonlinear classifiers\n",
        "9": "the main technique for doing that is\n",
        "12": "something called kernels let's see what\n",
        "15": "these kernels are and how to use them if\n",
        "17": "you have a training set that looks like\n",
        "20": "and you want to find a nonlinear\n",
        "22": "decision boundary to distinguish the\n",
        "24": "positive negative examples maybe a\n",
        "27": "boundary that looks like that one way to\n",
        "29": "do so is to come up with a set of\n",
        "32": "complex polynomial features right so a\n",
        "34": "set of features that looks like this so\n",
        "37": "that you end up with a hypothesis X that\n",
        "41": "predicts 1 if you know that what theta 0\n",
        "44": "plus theta 1 X 1 plus dot dot dot\n",
        "46": "all those polynomial features is greater\n",
        "49": "than 0 and predict 0 otherwise\n",
        "53": "and another way of writing this to\n",
        "56": "introduce a little bit of new notation\n",
        "58": "that I'll use later is that we can think\n",
        "61": "about hypotheses as computing a decision\n",
        "63": "boundary using this so theta 0 plus\n",
        "69": "theta 1 f1 plus theta 2 f2 plus theta 3\n",
        "73": "f3 plus and so on where I'm going to use\n",
        "76": "this new notation F 1 F 2 F 3 and so on\n",
        "79": "to denote these new sort of features I'm\n",
        "82": "computing so F 1 is just equal to X 1 F\n",
        "88": "2 is equal to X 2 F 3 is equal to this\n",
        "93": "thing here so X 1 X 2 F 4 is equal to\n",
        "96": "x1 squared at 5 is equal to x2 squared\n",
        "99": "and so on and we seen previously that\n",
        "101": "coming up with these high order\n",
        "103": "polynomials is one way to come up with\n",
        "106": "lots more features but the question is\n",
        "109": "is there a different choice of features\n",
        "110": "or is a better choice of features than\n",
        "113": "these high order polynomials because you\n",
        "115": "know it's not clear that these higher\n",
        "116": "order polynomials is what we want and\n",
        "118": "when we talked about computer vision\n",
        "121": "talked about when the input is an image\n",
        "123": "with lots of pixels we also saw how\n",
        "126": "using high order polynomials becomes\n",
        "128": "very computationally expensive because a\n",
        "130": "lot of these higher order polynomial\n",
        "132": "terms so it's very different or a better\n",
        "134": "choice of the features that we can use\n",
        "138": "to plug into this sort of hypothesis\n",
        "140": "flip so here's one idea for how to\n",
        "145": "define a new features f1 f2 f3 in on\n",
        "147": "this slide I'm going to define only\n",
        "149": "three new features but for real problems\n",
        "151": "we're going to define much larger number\n",
        "153": "but here's what I'm going to do in this\n",
        "156": "space of features x1 x2 and I'm going to\n",
        "158": "leave x0 all of this the Interceptor x0\n",
        "161": "but then this space x1 x2 I'm going to\n",
        "163": "just you know manually pick a few points\n",
        "166": "so we'll call this point l1 I'm going to\n",
        "169": "pick a different point let's call that\n",
        "173": "l2 and let's pick a third one we call\n",
        "175": "this one l3\n",
        "176": "and for now let's just say that I'm\n",
        "178": "going to choose these 3 points of\n",
        "180": "manually and we're going to call these\n",
        "182": "three points land now so that map one\n",
        "184": "two three what I'm going to do is define\n",
        "187": "my new features as follows given an\n",
        "191": "example X let me define my first feature\n",
        "198": "f1 to be some measure of the similarity\n",
        "202": "the t my training example X and my first\n",
        "207": "landmark and the specific formula I'm\n",
        "210": "going to use to measure similarity is\n",
        "213": "going to be this is e to the minus the\n",
        "218": "length of X minus l1 squared divided by\n",
        "221": "2 Sigma squared so depending on whether\n",
        "222": "or not you watched your previous\n",
        "225": "optional video this this notation\n",
        "228": "you know this notation right this is the\n",
        "231": "length of the vector W and so this thing\n",
        "236": "here this X minus l1 this is actually\n",
        "238": "just the Euclidean distance\n",
        "240": "well squared is the Euclidean distance\n",
        "243": "between the point X and the landmark l1\n",
        "247": "say more about this later but that's my\n",
        "249": "first feature and my second feature f2\n",
        "253": "is going to be you know similarity\n",
        "255": "function that measures how similar X is\n",
        "258": "to L 2 and the gain is going to be\n",
        "263": "defined as the following function so\n",
        "266": "it's e to the minus of the square of the\n",
        "269": "Euclidean distance between X and the\n",
        "270": "second landmark that's why that\n",
        "272": "numerator is and then divided by 2 6\n",
        "275": "squared and similarly f3 is you know\n",
        "281": "similarity well between X and L 3 which\n",
        "286": "is equal to again similar formula and\n",
        "289": "what this similarity function is the\n",
        "291": "mathematical term for this is that this\n",
        "295": "is going to be a kernel function and the\n",
        "297": "specific kernel that I'm using here this\n",
        "300": "is actually called a Gaussian kernel and\n",
        "302": "so this formula this particular choice\n",
        "304": "of similarity function is called a\n",
        "306": "Gaussian kernel but the way the\n",
        "308": "terminology goes is that you know in the\n",
        "310": "abstract these different similarity\n",
        "311": "functions are called kernels and we can\n",
        "313": "have different similarity functions and\n",
        "315": "the specific example I'm giving here is\n",
        "317": "called the Gaussian kernel we'll see\n",
        "319": "other examples of other kernels but for\n",
        "321": "now just think of these as similarity\n",
        "323": "functions and so instead of writing\n",
        "326": "similarity between X and L sometimes we\n",
        "329": "also write this as a kernel denoted you\n",
        "332": "know lowercase K between X and one of my\n",
        "333": "lab mouse\n",
        "336": "all right so let's see what these\n",
        "339": "kernels actually do and why why these\n",
        "341": "sorts of similarity functions why these\n",
        "344": "e to the salon expressions might make\n",
        "348": "sense so let's take my first landmark my\n",
        "349": "landmark l1 which is one of those points\n",
        "353": "like I chose on my figures now so the\n",
        "355": "similarity of the kernel between X and\n",
        "357": "l1 is given by this expression just to\n",
        "359": "make sure you know we're on the same\n",
        "361": "page about what the numerator term is\n",
        "363": "the numerator can also be written as a\n",
        "366": "sum from J equals 1 to N of sort of the\n",
        "368": "distance that is the component-wise\n",
        "371": "distance between the vector X and the\n",
        "373": "vector L and again for this for the\n",
        "375": "purpose of these slides on the ignoring\n",
        "378": "x0 so just ignoring the intercept term\n",
        "380": "x0 which is always equal to 1\n",
        "383": "so you know this is how you compute the\n",
        "385": "kernel or the similarity between X and a\n",
        "388": "landmark so let's see what this function\n",
        "391": "does suppose X is close to one of the\n",
        "395": "landmarks then this Euclidean distance\n",
        "397": "formula in the numerator will be close\n",
        "400": "to zero right that that is a this term\n",
        "402": "here the distance or the squared\n",
        "403": "distance between X and O be close to\n",
        "408": "zero and so f1 this simple feature will\n",
        "412": "be approximately e to the minus zero in\n",
        "414": "the numerator squared over 2 Sigma\n",
        "417": "squared so then e to the 0 e to the\n",
        "420": "minus 0 e to the 0 is in equals to 1\n",
        "423": "right and I put the approximation symbol\n",
        "425": "here because you know the distance may\n",
        "427": "not be exactly 0 but if X is close to\n",
        "429": "landmark this this term will be close to\n",
        "433": "0 and so f1 will be close to 1\n",
        "437": "conversely if X is far from l1 then this\n",
        "440": "first feature x1 f1 will be e to the\n",
        "444": "minus of some large number squared\n",
        "447": "divided by 2 Sigma squared and e to the\n",
        "450": "minus of a large number is going to be\n",
        "452": "close to 0\n",
        "455": "so what these features do is they\n",
        "457": "measure how similar X is from one of\n",
        "459": "your land miles and the feature F is\n",
        "462": "going to be close to one wonder when X\n",
        "464": "is close your landmark and it's going to\n",
        "466": "be zero or close to zero when X is far\n",
        "469": "from your landlord and each of these\n",
        "471": "landmarks on the previous slide I drew\n",
        "476": "three landmarks l1 l2 l3 each of these\n",
        "481": "landmarks defines a new feature f1 f2\n",
        "484": "ff3 that is given a training example X\n",
        "487": "we can now compute three new features f1\n",
        "491": "f2 f3 um given you know the three\n",
        "493": "landmarks that I wrote out just now but\n",
        "495": "first let's look at this exponentiation\n",
        "497": "function let's look at the similarity\n",
        "499": "function and plotted in figures and just\n",
        "501": "you know understand better what this\n",
        "504": "really looks like for this example let's\n",
        "506": "say I have two features x1 and x2 and\n",
        "509": "let's say my first landmark l1 is at a\n",
        "514": "location 3 5 so and so let's say I set\n",
        "516": "Sigma squared equals 1/4 now if I plot\n",
        "517": "Sigma squared equals 1/4 now if I plot\n",
        "519": "what this feature looks like what I get\n",
        "522": "is this figure so the vertical axis the\n",
        "525": "height of the surface is the value of f1\n",
        "528": "and down here on the horizontal axis are\n",
        "530": "if I have some training example and\n",
        "533": "that's x1 and x2 you know given a\n",
        "535": "certain training example of the training\n",
        "537": "example here for a certain value of x1\n",
        "540": "x2 or the height above the surface shows\n",
        "543": "the corresponding value of f1 and down\n",
        "545": "below is the same figure shown using a\n",
        "549": "contour plot with x1 on horizontal axis\n",
        "552": "x2 and horizontal axis and so this this\n",
        "554": "figure on the bottom is just a contour\n",
        "557": "plot of this 3d surface you notice that\n",
        "564": "when X is equal to 3/5 exactly then here\n",
        "567": "f1 takes on the value 1 because that's\n",
        "568": "at this maximum\n",
        "570": "and X moves up and as X moves away as X\n",
        "574": "goes further away then this feature\n",
        "577": "takes on values that are close to zero\n",
        "580": "and so this is really a feature f1\n",
        "583": "measures you know how close X is to the\n",
        "587": "first landmark and it varies between 0 1\n",
        "589": "depending on how close X is to the first\n",
        "593": "landmark l1 now the other thing I want\n",
        "595": "to do on this slide is show the effects\n",
        "598": "of varying this parameter Sigma squared\n",
        "601": "so Sigma squared is a parameter of the\n",
        "603": "Gaussian kernel and as you vary it you\n",
        "605": "get slightly different effects let's set\n",
        "607": "Sigma squared to be equal to 0.5 and see\n",
        "610": "what we get reset Sigma squared to 0.5\n",
        "611": "what you find is that the kernel looks\n",
        "613": "similar except that the width of this\n",
        "615": "bump becomes narrower the contours\n",
        "617": "shrink a bit too so if Sigma squared\n",
        "620": "equals 0.5 then as you start from you\n",
        "621": "equals 0.5 then as you start from you\n",
        "623": "know x equals 3/5 and s you move away\n",
        "628": "then the feature f1 Falls to 0 much more\n",
        "632": "rapidly and conversely if you were to\n",
        "634": "increase Sigma squared so here I've set\n",
        "637": "Sigma square equals 3 in that case as I\n",
        "640": "move away from you know L so this point\n",
        "642": "here is really L right as L 1 is at\n",
        "649": "location 3 5 shown up here and if Sigma\n",
        "650": "squared is large then as you move away\n",
        "655": "from the from l1 the feed the value of\n",
        "662": " \n",
        "665": "so given this definition of the features\n",
        "668": "let's see what sorts of hypotheses we\n",
        "670": "can learn given the training example X\n",
        "674": "we're going to compute these features f1\n",
        "679": "f2 f3 and our hypothesis is going to\n",
        "681": "predict 1 when theta 0 plus theta 1 f1\n",
        "683": "plus a 2 f2 and so on is greater than or\n",
        "686": "equal to 0 for this particular example\n",
        "688": "let's say that I've already run a\n",
        "689": "learning algorithm and let's say that\n",
        "692": "you know somehow I ended up with these\n",
        "694": "values of the parameters so theta 0\n",
        "697": "equals minus 0.5 theta 1 equals 1 theta\n",
        "702": "2 equals 1 and theta 3 equals 0 and what\n",
        "705": "I want to do is consider what happens if\n",
        "708": "we have a training example that takes\n",
        "713": "that has a location at this magenta dot\n",
        "714": "right where I just drew this dot over\n",
        "716": "here so let's say I have a training\n",
        "718": "example X what will my hypothesis with a\n",
        "724": "well if I look at this formula because\n",
        "727": "my training example X is close to l1 we\n",
        "731": "have that f1 is going to be close to 1\n",
        "733": "but because my training example X is far\n",
        "736": "from l2 and l3 I'll have that you know\n",
        "739": "f2 will be close to 0 and f3 will be\n",
        "740": "f2 will be close to 0 and f3 will be\n",
        "743": "close to 0 so if I look at that formula\n",
        "748": "I have theta 0 plus theta 1 times 1 plus\n",
        "751": "theta 2 times some value not exactly\n",
        "753": "zero but let's say close to zero then\n",
        "756": "plus theta 3 times something close to\n",
        "758": "zero and this is going to be equal to\n",
        "761": "you're plugging in these values now so\n",
        "765": "that gives minus 0.5 plus 1 times 1\n",
        "767": "which is 1 and so on which is equal to\n",
        "770": "0.5 mistreating equals 0 so at this\n",
        "773": "point we're going to predict\n",
        "776": "why equals 1 because that's greater than\n",
        "779": "equal to 0 now let's take a different\n",
        "782": "point now let's say I take a different\n",
        "784": "point I'm going to draw this one in a\n",
        "787": "different color in cyan say for a point\n",
        "789": "out there if that were my training\n",
        "792": "example X then if you make a similar\n",
        "796": "computation you find that f1 f2 f3 are\n",
        "799": "all going to be close to 0 and so you\n",
        "805": "have theta 0 plus theta 1 f1 plus so on\n",
        "808": "and this will be about equal to minus\n",
        "812": "0.5 because I guess theta 0 is minus 0.5\n",
        "815": "and in the f1 f2 f3 are 0 so this will\n",
        "818": "be minus 0.5 this is less than 0 and so\n",
        "820": "at this point out there we're going to\n",
        "825": "predict y equals 0 and if you do this\n",
        "827": "yourself for a range of different points\n",
        "828": "you should convince yourself that if you\n",
        "831": "have a training example that's close to\n",
        "834": "l2 say then at this point we'll also\n",
        "838": "predict y equals 1 and in fact what you\n",
        "839": "end up doing is you know if you look\n",
        "841": "around this decision about this space\n",
        "843": "what we'll find is that for points near\n",
        "846": "l1 and l2 we end up predicting positive\n",
        "848": "and for points far away from l1 and l2\n",
        "852": "that is the points far away from these\n",
        "854": "two landmarks we end up predicting that\n",
        "856": "the class is equal to zero and so what\n",
        "858": "we end up doing is that the decision\n",
        "862": "boundary of this hypothesis will end up\n",
        "864": "looking something like this where inside\n",
        "868": "this red district boundary we predict y\n",
        "871": "equals 1 and outside we predict\n",
        "875": "while your zero and so this is how with\n",
        "878": "this definition of the landmarks and of\n",
        "880": "a kernel function we can learn the\n",
        "881": "pretty complex nonlinear decision\n",
        "884": "boundary like what I just drew where we\n",
        "885": "predict positive when we're close to\n",
        "887": "either one of the two landmarks that we\n",
        "889": "predict negative when we're very far\n",
        "893": "away from any of the landmarks and so\n",
        "896": "this is a part of the idea of kernels of\n",
        "898": "and how we use them with a support\n",
        "900": "vector machine which is that we define\n",
        "902": "these extra features using landmarks and\n",
        "905": "similarity functions to learn more\n",
        "907": "complex nonlinear classifiers so\n",
        "910": "hopefully that gives you a sense of the\n",
        "912": "idea of kernels and how we can use it to\n",
        "914": "define new features for the support\n",
        "916": "vector machine but they're quite a\n",
        "917": "couple questions that we haven't\n",
        "919": "answered yet one is how do we get these\n",
        "920": "landmarks how do we choose these\n",
        "923": "landmarks and another is of what other\n",
        "925": "similarity functions if any can be used\n",
        "926": "other than the one we talked about\n",
        "929": "called Gaussian kernel in the next video\n",
        "930": "we'll give answers to these questions\n",
        "933": "and put everything together to show how\n",
        "935": "support vector machines with kernels can\n",
        "937": "be a powerful way to learn complex\n",
        "944": " \n"
    },
    "m_0lDnW-WD0": {
        "0": " \n",
        "2": "welcome to the final video of this\n",
        "4": "machine learning class we've been\n",
        "5": "through a lot of different videos\n",
        "8": "together in this video I like to just\n",
        "9": "quickly summarize the main topics of\n",
        "11": "this class and then say a few words at\n",
        "15": "the end and that will wrap up the class\n",
        "18": "so what are we done in this class we\n",
        "20": "spent a lot of time talking about\n",
        "21": "supervised learning algorithms like\n",
        "23": "linear regression logistic Russian\n",
        "26": "neural networks SVM's for problems where\n",
        "28": "you have labeled data and labeled\n",
        "33": "examples X I Y I and we also spent a lot\n",
        "35": "of time talking about unsupervised\n",
        "36": "learning algorithms like team is\n",
        "38": "clustering principal components analysis\n",
        "41": "for dimensionality reduction and anomaly\n",
        "43": "detection algorithms for when you have\n",
        "45": "only unlabeled data or X I\n",
        "47": "although anomaly detection can also use\n",
        "49": "some label data to evaluate the\n",
        "52": "algorithm we also spend some time\n",
        "54": "talking about special applications or\n",
        "56": "special topics like recommender systems\n",
        "58": "and large scale machine learning systems\n",
        "60": "including paralyzed like MapReduce\n",
        "62": "systems as well as some special\n",
        "65": "applications like sliding windows object\n",
        "67": "classification for computer vision and\n",
        "69": "finally we also spent a lot of time\n",
        "71": "talking about different aspects of sort\n",
        "74": "of advice on building a machine learning\n",
        "76": "system and this involved both trying to\n",
        "78": "understand what is it that makes a\n",
        "79": "machine learning algorithm work or not\n",
        "81": "works we talked about things like bias\n",
        "83": "variance and how regularization can help\n",
        "86": "with some variance problems and we also\n",
        "88": "spent a lot of time talking about this\n",
        "91": "question of how to decide what to work\n",
        "93": "on next or how to tire prioritize how\n",
        "95": "you spend your time when you're\n",
        "98": "developing a machine learning system so\n",
        "100": "we talked about evaluation of learning\n",
        "102": "algorithms evaluation metrics like you\n",
        "104": "know precision recall at one school as\n",
        "106": "well as practical aspects of evaluation\n",
        "109": "like their training profile addition and\n",
        "111": "test sets and we also spent a lot of\n",
        "113": "time talking about debugging learning\n",
        "115": "algorithms and making sure that you're\n",
        "116": "learning out\n",
        "117": "working so we talk about your\n",
        "120": "Diagnostics like learning hers and also\n",
        "122": "talked about some things like error\n",
        "124": "analysis and sealing analyses and so all\n",
        "126": "of these were different tools for\n",
        "128": "helping you to decide what to do next or\n",
        "130": "how to spend your valuable time when\n",
        "132": "you're developing a machine learning\n",
        "135": "system and so in addition to having the\n",
        "136": "tools of machine learning at your\n",
        "139": "disposal so knowing the tools of machine\n",
        "140": "learning like supervised learning and\n",
        "142": "supervised learning and so on I hope\n",
        "143": "supervised learning and so on I hope\n",
        "145": "that you now not only have the tools but\n",
        "148": "that you know how to apply these tools\n",
        "151": "really well to build powerful machine\n",
        "155": "learning systems so that's it those are\n",
        "157": "the topics of this class and if you've\n",
        "159": "worked all the way through this course\n",
        "161": "you should now consider yourself an\n",
        "164": "expert in machine learning as you know\n",
        "166": "machine learning is the technology is\n",
        "168": "having huge impact on science technology\n",
        "172": "industry and you now well qualify to use\n",
        "174": "these tools of machine learning to Train\n",
        "177": "effect I hope that many of you in this\n",
        "179": "class will find ways to use machine\n",
        "181": "learning to build cool systems and cool\n",
        "183": "applications and cool products and I\n",
        "186": "hope that you find ways to use machine\n",
        "188": "learning not only to make your life\n",
        "191": "better that maybe someday to use it to\n",
        "192": "make many other people's lives better as\n",
        "196": "well I also wanted to let you know that\n",
        "199": "this class has been great fun for me to\n",
        "202": "teach so thank you for that and before\n",
        "204": "wrapping up it was just one last thing I\n",
        "208": "want to say which is that was maybe not\n",
        "210": "so long ago that I was still with\n",
        "212": "students myself and even today you know\n",
        "213": "I still tried to take different\n",
        "215": "constants want to have time to try to\n",
        "218": "learn new things and so I know how\n",
        "220": "time-consuming it is to learn this stuff\n",
        "223": "I know that you're probably a busy\n",
        "225": "person with many men\n",
        "226": "other things going on in your life and\n",
        "229": "so the fact that you still found the\n",
        "231": "time and took the time to watch these\n",
        "233": "videos and you know I know that many of\n",
        "235": "these videos these videos just went on\n",
        "237": "for hours right and the fact that many\n",
        "240": "of you took the time to go through the\n",
        "242": "review questions and that many of you\n",
        "244": "took the time to even were through the\n",
        "247": "programming exercises and these long and\n",
        "249": "complicated programming exercises I\n",
        "254": "wanted to say thank you for that and I\n",
        "256": "know that many of you have worked hard\n",
        "259": "on this pause that many of you have put\n",
        "261": "a lot of time into this class that many\n",
        "263": "of you have put a lot of yourselves into\n",
        "267": "this class so I hope that you also got a\n",
        "270": "lot out of this class and I wanted to\n",
        "273": "say thank you very much for having been\n",
        "277": " \n"
    },
    "mh6rAYA0e7Q": {
        "0": " \n",
        "2": "in this video I'd like to talk about the\n",
        "4": "Gaussian distribution which is also\n",
        "7": "called the normal distribution in case\n",
        "10": "you're already intimately familiar with\n",
        "11": "the Gaussian distribution\n",
        "13": "there's probably okay to skip this video\n",
        "16": "but if you're not sure or if it's been a\n",
        "17": "while since you've worked with the\n",
        "19": "Gaussian distribution of the normal\n",
        "21": "distribution then please do watch this\n",
        "23": "video all the way to the end and in the\n",
        "25": "video after this we'll start applying\n",
        "27": "the Gaussian distribution to developing\n",
        "32": "an anomaly detection algorithm let's say\n",
        "35": "X is a real valued random variable so X\n",
        "38": "is a real number if the probability\n",
        "41": "distribution of X is Gaussian it with\n",
        "44": "mean mu and variance Sigma squared then\n",
        "47": "we'll write this as X the random\n",
        "50": "variable tell there that's this little\n",
        "52": "toe there at this rate this is\n",
        "60": "distributed as identity know the\n",
        "61": "Gaussian distribution with sometimes\n",
        "65": "going to write script n parentheses mu\n",
        "68": "comma Sigma square so this script n\n",
        "70": "stands for normal since Gaussian and\n",
        "72": "normal distribution they mean the same\n",
        "75": "pages of synonyms and a Gaussian\n",
        "77": "distribution is parameterised by two\n",
        "81": "parameters by a mean parameter which we\n",
        "83": "denote mu and a variance parameter which\n",
        "86": "we denote by a Sigma squared if we plot\n",
        "88": "the Gaussian distribution or Gaussian\n",
        "91": "probability density look like the\n",
        "94": "bell-shaped curve which you may have\n",
        "97": "seen before and so this bell-shaped\n",
        "99": "curve is parametrized by those\n",
        "101": "differences in mu and Sigma and the\n",
        "103": "location or the center of this\n",
        "107": "bell-shaped curve is the mean mu and the\n",
        "109": "width of this bell-shaped curve of\n",
        "113": "roughly that is the this parameter Sigma\n",
        "115": "is also called one standard\n",
        "118": "creation and so this specifies the\n",
        "120": "probability of X taking on different\n",
        "122": "values so X taking on values you know in\n",
        "124": "the middle here is pretty high since the\n",
        "125": "Gaussian density here is pretty high\n",
        "128": "whereas X taking on values further and\n",
        "131": "further away will be diminishing in\n",
        "132": "probability finally just for\n",
        "134": "completeness let me write out the\n",
        "137": "formula for the Gaussian distribution so\n",
        "139": "they're probably effects and I you\n",
        "140": "should all sometimes write this instead\n",
        "142": "of P of X I'm gonna write this as P of X\n",
        "145": "semicolon new comma Sigma squared and so\n",
        "147": "this knows that it probably affects it\n",
        "149": "is parameterize by the two parameters mu\n",
        "153": "and Sigma squared and the formula for\n",
        "155": "the Gaussian density is this 1 over root\n",
        "160": "2 Pi Sigma e to the negative X minus mu\n",
        "164": "squared over 2 Sigma squared so there's\n",
        "167": "no need to memorize this formula you\n",
        "169": "know this is just the formula for the\n",
        "171": "bell-shaped curve over here on the left\n",
        "173": "if there's no need to memorize it and if\n",
        "175": "you ever need to use this you can always\n",
        "177": "look this up and so that figure on the\n",
        "179": "left that is what you get if you take a\n",
        "181": "fixed value of mu and take a fixed value\n",
        "186": "of Sigma and you plot P of X so this\n",
        "189": "curve here this is really P of x plotted\n",
        "192": "as a function of X you know for a fixed\n",
        "195": "value of Nu and of Sigma Square and by\n",
        "196": "the way sometimes it's easier to think\n",
        "198": "in terms of Sigma squared that's called\n",
        "201": "the variance and sometimes is easier to\n",
        "203": "think in terms of Sigma so Sigma is\n",
        "208": "called the standard deviation and it so\n",
        "210": "that specifies the width of this\n",
        "213": "Gaussian probability density whereas the\n",
        "214": "square of Sigma of Sigma squared is\n",
        "217": "called the variance let's look at some\n",
        "219": "examples of what the Gaussian\n",
        "222": "distribution looks like if mu equals 0\n",
        "224": "Sigma equals 1 then we have a Gaussian\n",
        "227": "distribution that's centered around 0\n",
        "229": "because as mu and the width of this\n",
        "230": "Gaussian\n",
        "233": "that's one standard deviation is Sigma\n",
        "236": "over there let's look at some examples\n",
        "239": "of gaussians if mu is equal to zero\n",
        "242": "signals 1 then that corresponds to a\n",
        "244": "Gaussian distribution that is centered\n",
        "248": "at 0 since mu is 0 and the width of this\n",
        "253": "Gaussian is depend was controlled by\n",
        "256": "Sigma by that variance parameter signal\n",
        "261": "here's another example let's say mu is\n",
        "264": "equal to 0 and Sigma is equal to 1/2 so\n",
        "267": "the standard deviation is 1/2 and the\n",
        "269": "variance Sigma squared would therefore\n",
        "272": "be the square of 0.5 would be 0.25 and\n",
        "275": "in that case the Gaussian distribution\n",
        "277": "the Gaussian program density looks like\n",
        "280": "this is also centered at 0 but now the\n",
        "283": "width of this is much smaller because of\n",
        "285": "the smaller variance the the width of\n",
        "288": "this Gaussian density is roughly half as\n",
        "291": "wide but because this is a priority\n",
        "293": "distribution the area under the curve\n",
        "297": "that is the shaded area there that area\n",
        "298": "must integrate to 1 this is a problem\n",
        "300": "property of probability distributions\n",
        "303": "and so you know this is a much taller\n",
        "305": "Gaussian density because this half\n",
        "306": "that's why with half the standard\n",
        "309": "deviation but it's twice as tall another\n",
        "312": "example if Sigma is equal to 2 then you\n",
        "314": "get a much fatter or much wider Gaussian\n",
        "317": "density and so here the Sigma parameter\n",
        "320": "controls that this Gaussian density has\n",
        "323": "a wider width and once again the area\n",
        "325": "under the curve that is the shaded area\n",
        "326": "you know it always integrates to 1\n",
        "328": "there's a property of probability\n",
        "330": "distributions and because it's wider\n",
        "333": "it's also half as tall in order to still\n",
        "334": "integrate\n",
        "337": "and finally one last example would be if\n",
        "339": "we now change the mute params as well\n",
        "342": "then instead of being sent to the zero\n",
        "343": "we now have a Gaussian distribution\n",
        "348": "there Center at 3 because this shifts\n",
        "350": "over at least a Gaussian distribution\n",
        "353": "next let's talk about the parameter\n",
        "355": "estimation problem so what's the\n",
        "357": "parameter estimation problem let's say\n",
        "360": "we have a data set of M examples so X 1\n",
        "362": "through X M and let's say each of these\n",
        "364": "examples is a real number if you're in\n",
        "366": "the figure of plotted an example of a\n",
        "368": "data set so the horizontal axis is the x\n",
        "370": "axis and you know I have a range of\n",
        "372": "examples of X and I've just plotted them\n",
        "375": "on this figure here and the parameter\n",
        "377": "estimation problem is let's say I\n",
        "380": "suspect that these examples came from a\n",
        "381": "Gaussian distribution so let's say I\n",
        "383": "suspect that each of my examples Exxon\n",
        "386": "was distributed that's what this total\n",
        "388": "thing comes that means let's not suspect\n",
        "389": "that each of these examples was\n",
        "391": "distributed according to a normal\n",
        "392": "distribution or Gaussian distribution\n",
        "395": "with some parameter mu and some\n",
        "398": "parameter Sigma squared but I don't know\n",
        "400": "what the values of these parameters are\n",
        "402": "the problem what parameter estimation is\n",
        "404": "given my data set I want to try to\n",
        "406": "figure out I want to estimate what are\n",
        "410": "the values of MU and Sigma squared so if\n",
        "411": "you're given a dataset like this you\n",
        "414": "know it looks like maybe if I estimate\n",
        "416": "what Gaussian distribution the data came\n",
        "421": "from maybe that might be roughly the\n",
        "423": "Gaussian distribution it came from with\n",
        "426": "mu being the center of the distribution\n",
        "429": "and Sigma the standard deviation\n",
        "431": "controlling the width of this Gaussian\n",
        "433": "distribution it seems like a reasonable\n",
        "435": "fit to the data because you know it\n",
        "437": "looks like the data has a very high\n",
        "439": "probability of being in the central\n",
        "442": "region alone probably of being further\n",
        "443": "out even though probably of being\n",
        "445": "further out and so on so maybe this is a\n",
        "449": "reasonable estimate of MU and\n",
        "451": "of cygnus grid that is if it corresponds\n",
        "453": "to a Gaussian distribution that looks\n",
        "457": "like this so I'm going to use just write\n",
        "459": "out the formulas the standard formulas\n",
        "461": "for estimating the parameters mu and\n",
        "464": "Sigma script our estimate of the way\n",
        "467": "we're going to estimate mu is going to\n",
        "471": "be just the average of my example so mu\n",
        "473": "is the mean parameter just take my\n",
        "475": "training set take my M examples and\n",
        "477": "average them and that me or gives me the\n",
        "480": "center of this distribution\n",
        "482": "how about Sigma square well the variance\n",
        "484": "I'll just write out the standard formula\n",
        "487": "game I'm going to estimate as sum of 1\n",
        "492": "through m of X I minus mu squared and so\n",
        "493": "this mu here is actually the muted that\n",
        "496": "I compute over here using this formula\n",
        "498": "and what the variance is the one\n",
        "499": "interpretation of the variance is that\n",
        "501": "if you look at this term that's the\n",
        "504": "squared difference between the value I\n",
        "506": "thought in my example minus the mean\n",
        "508": "minus the center minus the mean of\n",
        "510": "distribution and so the variance I'm\n",
        "512": "gonna estimate as just the average of\n",
        "514": "the squared differences between my\n",
        "518": "examples of minus the mean and as a side\n",
        "520": "comment only for those of you that are\n",
        "522": "experts in statistics if you're an\n",
        "523": "experienced adjust things and if if\n",
        "525": "you've heard of maximum likelihood\n",
        "528": "estimation then these parameters these\n",
        "529": "estimates are actually the maximum\n",
        "532": "likelihood estimate of the parameters mu\n",
        "533": "and Sigma squared but if you haven't\n",
        "535": "heard of that before don't worry about\n",
        "536": "it all you need to know is that these\n",
        "539": "are the two standard formulas for how\n",
        "541": "you estimate for how you try to figure\n",
        "543": "out what about you and Sigma squared\n",
        "546": "given the data set finally one last side\n",
        "548": "comment again only for those of you that\n",
        "550": "have maybe taken a statistics class\n",
        "552": "before but if you take this cost before\n",
        "554": "some of you may have seen the formula\n",
        "556": "here where you know this is n minus 1\n",
        "558": "instead of M so so this first term\n",
        "561": "becomes 1 over n minus 1 so that 1 over\n",
        "564": "in machine learning people tend to use\n",
        "567": "this one over n formula but in practice\n",
        "569": "whether just 1 over M 1 over n minus 1\n",
        "571": "makes essentially no difference assuming\n",
        "572": "you know Emma's reasonably logical\n",
        "575": "reasonably large training set size so\n",
        "576": "just in case you sold you've seen this\n",
        "578": "other version before in either version\n",
        "581": "works just about equally well but\n",
        "583": "machine learning most people tend to use\n",
        "586": "1 over m in this formula and the two\n",
        "587": "versions have slightly different\n",
        "588": "theoretical properties like these\n",
        "590": "different mathematical properties for\n",
        "593": "the practice and it really makes a very\n",
        "595": "little difference event\n",
        "598": "so hopefully you now have a good sense\n",
        "600": "of what the Gaussian distribution looks\n",
        "602": "like as well as how to estimate the\n",
        "604": "parameters mu and Sigma squared of the\n",
        "605": "Gaussian distribution\n",
        "608": "if you given a training set that is if\n",
        "610": "you had given a set of data that you\n",
        "612": "suspect comes from a Gaussian\n",
        "614": "distribution with unknown parameters in\n",
        "617": "your Sigma squared in the next video\n",
        "618": "we'll start to take this and apply to\n"
    },
    "pkJjoro-b5c": {
        "0": " \n",
        "1": "in the previous video we talked about\n",
        "4": "the form of the hypothesis for linear\n",
        "6": "regression with multiple features with\n",
        "8": "multiple variables in this video let's\n",
        "10": "talk about how to fit the parameters of\n",
        "13": "that hypothesis in particular software\n",
        "15": "how to use gradient descent for linear\n",
        "20": "regression with multiple features to\n",
        "22": "quickly summarize our notation this is\n",
        "25": "our formal hypothesis in multivariate\n",
        "28": "linear regression where we adopted the\n",
        "32": "convention that x0 equals 1 all the\n",
        "35": "parameters of this model are theta 0\n",
        "38": "through theta n but instead of thinking\n",
        "40": "of this as n separate parameters which\n",
        "42": "is valid I'm instead going to think of\n",
        "46": "the parameters as theta where theta here\n",
        "52": "is a n plus 1 dimensional vector so I'm\n",
        "54": "just going to think of the parameter the\n",
        "57": "parameters of this model as itself being\n",
        "60": "a vector our cost function is J of theta\n",
        "63": "0 through theta n which is given by this\n",
        "65": "usual sum squared error term but again\n",
        "68": "instead instead of thinking of J as a\n",
        "71": "function of these n plus 1 numbers I'm\n",
        "74": "going to more commonly write J as just a\n",
        "76": "function of the parameter vector theta\n",
        "81": " \n",
        "84": "here's what gradient descent looks like\n",
        "87": "we're going to repeatedly update each\n",
        "89": "parameter theta J according to the theta\n",
        "91": "J minus alpha times this derivative term\n",
        "95": "and once again and just write this as J\n",
        "98": "of theta so theta J's update to that\n",
        "99": "state AJ minus the learning rate alpha\n",
        "102": "times the yo derivative a partial\n",
        "104": "derivative of the cost function with\n",
        "108": "respect to the parameter theta J let's\n",
        "110": "see what this looks like when we\n",
        "111": "implement gradient descent and in\n",
        "113": "particular let's go see what that\n",
        "115": "partial derivative term looks like\n",
        "118": "here's what we have a gradient descent\n",
        "120": "for the case of when we had N equals one\n",
        "123": "feature we had two separate update rules\n",
        "125": "for the parameters theta zero and theta\n",
        "126": "for the parameters theta zero and theta\n",
        "128": "one and hopefully these look familiar to\n",
        "132": "you and this term here was of course the\n",
        "134": "partial derivative of the cost function\n",
        "137": "with respect to the parameter theta zero\n",
        "140": "and similarly we had a different update\n",
        "142": "rule for the parameter theta one those\n",
        "144": "one little difference which is that when\n",
        "147": "we previously had only one feature we\n",
        "151": "would call that feature X I but now in\n",
        "153": "our new notation we would of course call\n",
        "156": "this X superscript I subscript one to\n",
        "157": "this X superscript I subscript one to\n",
        "159": "denote our one feature so that was four\n",
        "162": "we had only one feature let's look at\n",
        "164": "the new algorithm for we have more than\n",
        "165": "one feature when the number of features\n",
        "168": "n may be much larger than one we get\n",
        "171": "this update rule for gradient descent\n",
        "173": "and maybe for those of you that know\n",
        "176": "calculus if you take the definition of\n",
        "178": "the cost function and take the partial\n",
        "181": "derivative of the cost function J with\n",
        "184": "respect to the parameter theta J you\n",
        "186": "find that that partial derivative is\n",
        "189": "exactly that terms I've just drawn and\n",
        "191": "drawn the blue box around and if you\n",
        "194": "implement this you will get a working\n",
        "196": "implementation of gradient descent for\n",
        "199": "multivariate than aggression the last\n",
        "200": "thing I want to do on this slide is give\n",
        "203": "you a sense of why these some new and\n",
        "205": "old algorithms are you know sort of the\n",
        "208": "same thing or why they're both similar\n",
        "209": "algorithms why they both gradient\n",
        "211": "descent our\n",
        "213": "let's consider case where we have two\n",
        "215": "features or maybe more than two features\n",
        "218": "so we have three update rules for the\n",
        "221": "parameters theta0 theta1 theta2 and\n",
        "223": "maybe other values of theta as well if\n",
        "225": "you look at the update rule for theta 0\n",
        "230": "what you find is that this update rule\n",
        "234": "here is the same as the update rule that\n",
        "236": "we had previously for the case of N\n",
        "238": "equals 1 and the reason that they're\n",
        "241": "equivalent is of course because in a\n",
        "244": "notational convention we had this X 0\n",
        "247": "equals 1 Convention which is why these\n",
        "249": "two terms that I've drawn the magenta\n",
        "252": "boxes around are equivalent\n",
        "254": "similarly if you look at the update rule\n",
        "257": "for theta 1 you find that this term here\n",
        "260": "is equivalent to the term we previously\n",
        "263": "had of the equation the update where we\n",
        "265": "previously had for theta 1 where of\n",
        "267": "course we're just using this new\n",
        "272": "notation X subscript 1 to denote our new\n",
        "274": "notation for denoting the first feature\n",
        "276": "and now that we have more than one\n",
        "279": "feature we can have similar update rules\n",
        "281": "for for the other parameters like theta2\n",
        "285": "and so on there's a lot going on on the\n",
        "287": "slide so if so I definitely encourage\n",
        "290": "you if you need to to pause the video\n",
        "292": "and look at all the math on the slide\n",
        "293": "slowly to make sure you understand\n",
        "296": "everything that's going on here but um\n",
        "298": "if you implement the algorithm written\n",
        "300": "written up here then you will have a\n",
        "302": "working implementation of linear\n"
    },
    "ppFyPUx9RIU": {
        "0": " \n",
        "2": "in this video I'd like to show you a fun\n",
        "4": "and historically important example of\n",
        "7": "neural network learning of using a\n",
        "9": "neural network for autonomous driving\n",
        "12": "that is giving a car to learn to drive\n",
        "15": "yourself the video that I showed in a\n",
        "17": "minute was something that I've gotten\n",
        "19": "from Dean Pomerleau who's a colleague\n",
        "21": "who works out in Carnegie Mellon\n",
        "23": "University out on the east coast of the\n",
        "25": "United States and in part of the video\n",
        "28": "you see visualizations like this and I\n",
        "30": "want to tell you what the visualization\n",
        "31": "looks like before starting the video\n",
        "35": "down here on the lower left is the view\n",
        "37": "seen by the car of what's in front of it\n",
        "40": "and so here you kind of see a road as\n",
        "42": "maybe are going a bit to the left and\n",
        "48": "and up here on top this first horizontal\n",
        "51": "bar shows the direction selected by the\n",
        "54": "human driver and is the location of this\n",
        "57": "bright white white band that shows the\n",
        "59": "steering Direction selected by the human\n",
        "61": "driver where you know here for the Left\n",
        "64": "corresponds to steering hard left here\n",
        "66": "corresponds to steering harder right and\n",
        "69": "so this location which is a little bit\n",
        "71": "to the left a little bit left of center\n",
        "73": "means that the human driver at this\n",
        "74": "point was steering slightly to the left\n",
        "78": "and this second bar here corresponds to\n",
        "80": "the steering Direction selected by the\n",
        "82": "learning algorithm again the location of\n",
        "85": "this little white van means the neural\n",
        "87": "network was here selecting a steering\n",
        "89": "Direction that's like it's the left and\n",
        "90": "in fact the for the neural network\n",
        "93": "starts learning initially you see that\n",
        "96": "the network outputs a gray band like a\n",
        "99": "green uniform gray band throughout this\n",
        "101": "region and so the uniform gray fuzz\n",
        "103": "corresponds to the neural network having\n",
        "106": "been randomly initialized and initially\n",
        "108": "having no idea how to drive the car or\n",
        "110": "initially having no there's no idea what\n",
        "112": "direction to steer it and there's only\n",
        "115": "off there is learned for a while that it\n",
        "116": "will then start to output like a solid\n",
        "119": "went down in just a small part of the\n",
        "121": "corresponding to choosing a particular\n",
        "123": "steering direction and that corresponds\n",
        "125": "to when the neural network becomes more\n",
        "128": "confident in selecting you know it\n",
        "129": "wasn't abandoned want to the clear\n",
        "132": "location brought it in outputting a sort\n",
        "134": "of light gray fuzz but instead\n",
        "137": "outputting ale a white band that's more\n",
        "139": "confidently selecting one steering\n",
        "142": "Direction Alvin is a system of\n",
        "144": "artificial neural networks that learns\n",
        "147": "to steer by watching a person drive\n",
        "149": "Alvin is designed to control the navlab\n",
        "152": "to a modified Army Humvee equipped with\n",
        "156": "sensors computers and actuators for\n",
        "160": "autonomous navigation experiments the\n",
        "163": "initial step in configuring Alvin is\n",
        "166": "training a network just here during\n",
        "168": "training a person drives the vehicle\n",
        "175": " \n",
        "178": "once every two seconds Alban digitizes\n",
        "180": "the video invention of the road ahead\n",
        "183": "and records the person steering\n",
        "191": " \n",
        "193": "this training image is reduced in\n",
        "196": "resolution to 30 by 32 pixels and\n",
        "199": "provided as input to Alvin's three-layer\n",
        "202": "network using the back propagation\n",
        "204": "learning algorithm\n",
        "206": "Alvin is trained to output the same\n",
        "209": "steering direction as the human driver\n",
        "212": " \n",
        "215": "initially the network steering response\n",
        "223": " \n",
        "226": "after about two minutes of training the\n",
        "228": "network learns to accurately imitate the\n",
        "242": " \n",
        "245": "this same training procedure is repeated\n",
        "250": "for other road types after the networks\n",
        "252": "have been trained the operator pushes\n",
        "259": " \n",
        "263": "12 times per second Alvin digitizes an\n",
        "265": "image and feeds it to its neural\n",
        "272": " \n",
        "275": "each Network running in parallel\n",
        "278": "produces a steering direction and a\n",
        "279": "measure of its confidence in its\n",
        "288": " \n",
        "290": "the steering direction from the most\n",
        "293": "confident network in this case the\n",
        "294": "network training for the one lane road\n",
        "307": " \n",
        "310": "suddenly an intersection appears ahead\n",
        "322": " \n",
        "324": "as the vehicle approaches the\n",
        "326": "intersection the competence of the\n",
        "337": " \n",
        "340": "as it crosses the intersection and the\n",
        "342": "two-lane road ahead comes into view the\n",
        "350": " \n",
        "353": "when it's confidence Rises the to blame\n",
        "356": "Network is selected to steer safely\n",
        "358": "guiding the vehicle into its lane on the\n",
        "366": "two-lane road so that was autonomous\n",
        "367": "two-lane road so that was autonomous\n",
        "369": "driving using a neural network now of\n",
        "371": "course there are more recently more\n",
        "373": "modern attempts to do autonomous driving\n",
        "374": "there a few projects in the US and\n",
        "377": "Europe and so on they're giving more\n",
        "380": "robust driving patrollers than this but\n",
        "381": "I think is so pretty remarkable and\n",
        "383": "pretty amazing how a simple neural\n",
        "385": "network trained to a back propagation\n",
        "387": "can you know actually learn to drive a\n"
    },
    "qbvRdrd0yJ8": {
        "0": " \n",
        "2": "for linear regression we have previously\n",
        "4": "worked out two learning algorithms one\n",
        "6": "based on gradient descent and one based\n",
        "9": "on the normal equation in this video\n",
        "11": "we'll take those two algorithms and\n",
        "13": "generalize them to the case of\n",
        "17": "regularized linear regression here's the\n",
        "20": "optimization objective that we came up\n",
        "22": "with last time for regularized linear\n",
        "25": "regression this first part is our usual\n",
        "28": "objective for linear regression and we\n",
        "30": "now have this additional regularization\n",
        "33": "term where lambda is our regularization\n",
        "36": "parameter and we like to find parameters\n",
        "38": "theta that minimizes this cost function\n",
        "41": "this regular rise cost function J of\n",
        "43": "theta previously we were using gradient\n",
        "47": "descent for the original cost function\n",
        "49": "without the regularization term and we\n",
        "52": "had the following algorithm for regular\n",
        "54": "linear regression without regularization\n",
        "56": "we will repeatedly update the parameters\n",
        "60": "theta J as follows for J equals 0 1 2 up\n",
        "63": "through n let me take this and just\n",
        "67": "write the case for theta 0 separately so\n",
        "69": "you know I'm just going to write the\n",
        "72": "update for theta 0 separately then for\n",
        "74": "the updating for the parameters 1 2 3\n",
        "75": "the updating for the parameters 1 2 3\n",
        "78": "and so on up to n and this is so this is\n",
        "79": "I haven't changed anything yet right\n",
        "81": "this is just writing the update for\n",
        "83": "theta 0 separately from the updates for\n",
        "86": "theta 1 theta 2 theta 3 up to theta n\n",
        "89": "and the reason I want to do this is you\n",
        "90": "may remember that for our regularized\n",
        "93": "linear regression we penalize the\n",
        "95": "parameters theta 1 theta 2 and so on up\n",
        "98": "to theta n but we don't penalize theta 0\n",
        "101": "so when we modify this algorithm for\n",
        "103": "regularizes linear regression we're\n",
        "106": "going to end up treating theta 0\n",
        "110": "slightly differently concretely if we\n",
        "112": "want to take this algorithm and modify\n",
        "115": "it to use the regular rise objective all\n",
        "118": "we need to do is take this term at the\n",
        "120": "bottom and modify as follows we will\n",
        "124": "take this term and add minus lambda over\n",
        "129": "m times theta J and if you implement\n",
        "132": "this then you have gradient descent for\n",
        "133": "trying to\n",
        "136": "minimize the regular rise cost function\n",
        "139": "J of theta and concretely I'm not going\n",
        "142": "to do the calculus to prove it but\n",
        "145": "concretely if you look at this term the\n",
        "147": "term that written in square brackets if\n",
        "149": "you know calculus is possible to prove\n",
        "152": "that that term is the partial derivative\n",
        "155": "with respect to J of theta using the new\n",
        "158": "definition of J of theta with the\n",
        "162": "regularization term and similarly this\n",
        "166": "term up on top which I guess I'm drawing\n",
        "169": "the cyan box that's still the partial\n",
        "172": "derivative respect to theta 0 of J of\n",
        "175": "theta if you look at the update for\n",
        "177": "theta J is possible to show something\n",
        "178": "pretty interesting\n",
        "181": "concretely theta J gets updated as theta\n",
        "184": "J minus alpha times and then you have\n",
        "186": "this other term here that depends on\n",
        "189": "theta J so if you group all the terms\n",
        "191": "together at they depend on theta J you\n",
        "194": "can show that this update can be written\n",
        "196": "equivalently as follows and all I did\n",
        "199": "was at you know theta J here is of theta\n",
        "203": "J times 1 and this term is a lambda over\n",
        "205": "m there's also an alpha here so you end\n",
        "206": "m there's also an alpha here so you end\n",
        "208": "up with alpha lambda over m multiplied\n",
        "214": "into theta J and this term here 1 minus\n",
        "217": "alpha times lambda M is a pretty\n",
        "219": "interesting term it has a pretty\n",
        "221": " \n",
        "224": "concretely this term 1 minus alpha times\n",
        "227": "lambda over m is going to be a number\n",
        "229": "this usually is a number that's usually\n",
        "231": "like a lope and less than 1 right\n",
        "234": "because alpha times Londer over m is\n",
        "235": "going to be positive and usually you\n",
        "237": "know if your learning rate is small and\n",
        "238": "if M is large this is usually pretty\n",
        "240": "small so this term here it's going to be\n",
        "242": "a number it's usually a little bit less\n",
        "244": "than one so think of it as a number like\n",
        "248": "0.99 let's say and so the effect of our\n",
        "251": "update to theta J is we're going to say\n",
        "255": "that theta J gets replaced by theta J\n",
        "259": "times 0.99 right so theta J times 0.99\n",
        "262": "has the effect of shrinking theta J a\n",
        "265": "little bit to 1 0 so this makes theta J\n",
        "267": "bit smaller or more formally this makes\n",
        "269": "you know the squared norm of theta J a\n",
        "272": "little bit smaller and then after that\n",
        "275": "the second term here that's actually\n",
        "278": "exactly the same as the original\n",
        "280": "gradient descent update that we had\n",
        "282": "before we add in all this regularization\n",
        "287": "stuff so hopefully this gradient descent\n",
        "289": "hopefully this update makes sense\n",
        "291": "when we're using regular rise linear\n",
        "293": "regression what we're doing is on every\n",
        "295": "iteration we're multiplying theta J by a\n",
        "297": "number that's a little bit less than one\n",
        "299": "so we're shrinking the parameter a\n",
        "301": "little bit and then we're performing you\n",
        "304": "know similar update as before of course\n",
        "307": "that's just the intuition behind what\n",
        "308": "this particular update is doing\n",
        "311": "mathematically what is doing is exactly\n",
        "314": "gradient descent on the cost function J\n",
        "316": "of theta that we defined on the previous\n",
        "319": "slide that uses the regularization term\n",
        "322": "gradient descent was just one of our two\n",
        "325": "algorithms for fitting a linear\n",
        "327": "regression model the second algorithm\n",
        "330": "was the one based on the normal equation\n",
        "333": "where what we did was we created the\n",
        "335": "design matrix X where each row\n",
        "337": "corresponding to a separate training\n",
        "340": "example and we created a vector Y so\n",
        "343": "this is a vector that's in there's an M\n",
        "346": "dimensional vector and that contains the\n",
        "349": "labels from my training set so whereas X\n",
        "353": "is an M by n plus 1 dimensional matrix\n",
        "357": "Ziya m dimensional vector and in order\n",
        "361": "to minimize the cost function J we found\n",
        "366": "that one way to do so is to set theta to\n",
        "368": "be equal to this right we had X\n",
        "373": "transpose X inverse X transpose Y I'm\n",
        "374": "leaving room here to flow and stuff of\n",
        "378": "course and what this value for theta\n",
        "381": "does is this minimizes the cost function\n",
        "383": "J of theta where we were not using\n",
        "387": "regularization now that we are using the\n",
        "389": "regularization if you were to derive\n",
        "392": "what the minimum is and just to give you\n",
        "394": "a sense of how to derive the minimum the\n",
        "396": "way you derive it is you you know take\n",
        "398": "partial derivatives respect to each\n",
        "402": "parameter set this is 0 and then do a\n",
        "403": "bunch of math and you can then show that\n",
        "407": "it's a formula like this that minimizes\n",
        "412": "the cost function and concretely if you\n",
        "414": "are using regularization then this\n",
        "416": "formula changes as follows inside this\n",
        "418": "parenthesis you end up with a matrix\n",
        "423": "like this 0 1 1 1 and so on 1 until the\n",
        "425": "bottom so this thing over here is a\n",
        "428": "matrix whose upper leftmost entry is 0\n",
        "430": "the ones on the diagonals and then their\n",
        "432": "zeros everywhere else in this matrix\n",
        "434": "because I'm drawing this little bit\n",
        "438": "sloppy ly but as a concrete example if N\n",
        "442": "equals 2 then this matrix is going to be\n",
        "445": "a 3 by 3 matrix more generally this is\n",
        "449": "this matrix Isaiah n plus 1 by n plus 1\n",
        "453": "dimensional matrix so if N equals 2 then\n",
        "455": "that matrix becomes something that looks\n",
        "457": "like this would be 0 and then once on\n",
        "460": "the diagonals and then zeros on the rest\n",
        "462": "of the off diagonals and once again you\n",
        "463": "know I'm not going to show this\n",
        "465": "derivation which is frankly someone long\n",
        "466": "derivation which is frankly someone long\n",
        "467": "and involved but is possible to prove\n",
        "470": "that so if you are using the new\n",
        "472": "definition of J of theta with the\n",
        "475": "regularization objective then this new\n",
        "477": "formula for theta is the one that would\n",
        "480": "give you the global minimum of J of\n",
        "483": "theta so finally I want to just quickly\n",
        "486": "describe the issue of non inverter\n",
        "488": "this is relatively advanced materials so\n",
        "490": "you should consider this as optional and\n",
        "493": "feel free to skip it or if you listen to\n",
        "494": "it and you know possibly don't really\n",
        "495": "make sense don't worry about it either\n",
        "498": "but um earlier when I talked about the\n",
        "500": "normal equation method we also had an\n",
        "502": "optional video on the non-invertibility\n",
        "505": "issue so this is another optional part\n",
        "508": "that sort of an add-on to that earlier\n",
        "511": "optional video on non-invertibility now\n",
        "514": "consider setting where m the number of\n",
        "517": "examples is less than equal to n the\n",
        "519": "number of features if you have fewer\n",
        "522": "examples and features then this matrix x\n",
        "527": "transpose x will be non-invertible or\n",
        "530": "singular or the other term for this is\n",
        "533": "the matrix will be degenerate and if you\n",
        "534": "the matrix will be degenerate and if you\n",
        "536": "implement this in octave anyway and you\n",
        "538": "use the P in function to take the\n",
        "540": "it will kind of do the right thing but\n",
        "544": "it is not clear that it will give you a\n",
        "545": "very good hypothesis even though\n",
        "548": "numerically you know the octave P in\n",
        "550": "function will give you will give you a\n",
        "553": "result that kind of makes sense but if\n",
        "555": "you were doing this in a different\n",
        "557": "language and if you were or if you were\n",
        "560": "taking just the regular inverse which an\n",
        "562": "octave is denoted with the function inv\n",
        "564": "we're trying to take the regular inverse\n",
        "567": "of X transpose X then in this setting\n",
        "570": "you find that x transpose x is singular\n",
        "573": "is not invertible and if you're doing\n",
        "574": "this in a different programming language\n",
        "576": "and you know using some linear algebra\n",
        "578": "library to try to take the inverse of\n",
        "579": "library to try to take the inverse of\n",
        "580": "this matrix it just might not work\n",
        "583": "because that matrix is non-invertible or\n",
        "584": "singular\n",
        "587": "fortunately regularization also take\n",
        "590": "care of this for us and concretely so\n",
        "591": "long as the regularization parameter\n",
        "594": "lambda is strictly greater than 0 it is\n",
        "596": "actually possible to prove that this\n",
        "598": "matrix a transpose x plus lambda times\n",
        "601": "this you know funny matrix here is\n",
        "603": "possible to prove that this matrix will\n",
        "604": "not be singular and that this matrix\n",
        "608": "will be invertible so using\n",
        "610": "regularization also take cares of any\n",
        "613": "non-invertibility issues of the X\n",
        "615": "transpose X matrix as well so you now\n",
        "617": "know how to implement regularized linear\n",
        "618": "regression\n",
        "620": "using this you'll be able to avoid\n",
        "623": "overfitting even if you have lots of\n",
        "624": "features and a relatively small training\n",
        "627": "set and this should let you get linear\n",
        "628": "regression to work much better for many\n",
        "631": "problems in the next video we'll take\n",
        "633": "this regularization idea and apply it to\n",
        "635": "logistic regression so that you'll be\n",
        "637": "able to get logistic regression to avoid\n",
        "639": "overfitting and perform much better as\n"
    },
    "r5E2X1JdHAU": {
        "0": " \n",
        "2": "in this video and in the video after\n",
        "3": "this one I want to tell you about some\n",
        "5": "of the practical tricks for making\n",
        "8": "gradient descent work well in this video\n",
        "9": "I want to tell you about an idea called\n",
        "13": "feature scaling here's the idea if you\n",
        "14": "have a problem where you have multiple\n",
        "17": "features if you make sure that the\n",
        "19": "features are on a similar scale by which\n",
        "21": "I mean make sure that the different\n",
        "23": "features take on similar ranges of\n",
        "25": "values then gradient descent can\n",
        "28": "converge more quickly concretely let's\n",
        "30": "say you have a problem with two features\n",
        "32": "where x1 is the size of Houston takes on\n",
        "36": "values between say 0 to 2000 and x2 is\n",
        "37": "the number of bedrooms and maybe that\n",
        "40": "takes on values between 1 and 5 if you\n",
        "42": "plot the contours of the cost function J\n",
        "46": "of theta then the contours may look like\n",
        "50": "this where let's see J of theta is a\n",
        "52": "function of parameters theta 0 theta 1\n",
        "55": "and theta 2 I'm going to ignore theta 0\n",
        "57": "so let's forget about theta 0 and\n",
        "59": "pretend as a function of only theta 1\n",
        "62": "and theta 2 but if X 1 can take on a\n",
        "65": "much larger range of values in X 2 it\n",
        "67": "turns out that the contours of the cost\n",
        "70": "function J of theta can take on this\n",
        "73": "sort of a very very skewed elliptical\n",
        "76": "shape except that with a sort of mm to 5\n",
        "77": "shape except that with a sort of mm to 5\n",
        "79": "ratio it can be even more skewed so this\n",
        "82": "is very very tall and skinny ellipses or\n",
        "85": "these very tall skinny ovals can form\n",
        "87": "the contours of the cost function J of\n",
        "90": "theta and if you've run gradient descent\n",
        "94": "on this sort of cost function your\n",
        "98": "gradients may end up taking a long time\n",
        "100": "and can sort of oscillate back and forth\n",
        "103": "and they can take a long time before we\n",
        "106": "can finally find this way to the global\n",
        "108": "minimum in fact you'd imagine if these\n",
        "110": "contours are exaggerated even more able\n",
        "113": "to draw your incredibly skinny tall\n",
        "117": "skinny contours and it can be even more\n",
        "119": "extreme than that then you know gradient\n",
        "121": "descent can just well it turns out\n",
        "122": "gradient descent we just have a much\n",
        "125": "harder time taking us way meandering\n",
        "127": "around you can take a long time to find\n",
        "130": "this way to the global minimum\n",
        "135": "in these settings a useful thing to do\n",
        "138": "is to scale the features concretely if\n",
        "140": "you instead define the feature x1 to be\n",
        "141": "you instead define the feature x1 to be\n",
        "143": "the size of the house divided by 2,000\n",
        "146": "and define x2 to be maybe the number of\n",
        "149": "bedrooms divided by 5 then the contours\n",
        "153": "of the cost function J can become much\n",
        "156": "more much less skew so the contours may\n",
        "159": "look more like circles and if you run\n",
        "160": "gradient descent on the cost function\n",
        "164": "like this then gradient descent you can\n",
        "166": "show mathematically you can find a much\n",
        "168": "more direct path to the global minimum\n",
        "170": "rather than taking a much more\n",
        "172": "convoluted path we're sort of trying to\n",
        "173": "follow a much more complicated\n",
        "176": "trajectory to get to the global minimum\n",
        "179": "so by scaling the feature is so that\n",
        "181": "they're taken similar ranges of values\n",
        "183": "in this example we end up with both\n",
        "189": "features x1 and x2 between 0 &amp; 1 you can\n",
        "191": "wind up with an implementation of\n",
        "193": "gradient descent they can converge much\n",
        "199": "faster more generally when we're\n",
        "201": "performing feature scaling what we often\n",
        "203": "want to do is get every feature into a\n",
        "207": "proximately a minus 1 to +1 range and\n",
        "210": "concretely you know your feature x0\n",
        "211": "that's always equal to 1 so that's\n",
        "215": "already in that range but you may end up\n",
        "216": "dividing other features by different\n",
        "218": "numbers to get them into this range and\n",
        "221": "the numbers minus 1 and +1 on two\n",
        "224": "important so if you have a feature x1\n",
        "227": "that winds up being between 0 and 3 say\n",
        "228": "that's not a problem review end up\n",
        "230": "having a different feature that winds up\n",
        "231": "having a different feature that winds up\n",
        "233": "being between you know minus 2 and\n",
        "235": "positive 0.5 again this is close enough\n",
        "237": "to minus 1 and plus 1 that you know\n",
        "239": "that's fine that's fine\n",
        "241": "as only if you have a different feature\n",
        "246": "say x3 that is between say their ranges\n",
        "250": "from minus 100 to plus 100 then you know\n",
        "252": "this is a very different range of values\n",
        "254": "than minus 1 and plus 1 so so this might\n",
        "255": "be a less\n",
        "258": "scale feature and similarly if your\n",
        "260": "features take on a very very small range\n",
        "263": "of values so if x4 takes on values\n",
        "268": "between minus 0.0001 and positive 0.0001\n",
        "271": "then again this thing's on a much\n",
        "273": "smaller range of values than the minus 1\n",
        "274": "to +1 range and again I would consider\n",
        "278": "this feature wholly scaled so you want\n",
        "280": "the range of values you know it can be a\n",
        "282": "bit bigger than plus 1 is smaller than\n",
        "285": "plus 1 but just not much bigger like\n",
        "288": "plus 100 here and not too much smaller\n",
        "291": "like 0.001 over there different people\n",
        "293": "have different rules of thumb but the\n",
        "294": "one that I use is that if a feature\n",
        "295": "one that I use is that if a feature\n",
        "297": "takes on the range of values from say\n",
        "299": "minus 3 to plus 3 I usually think that\n",
        "300": "should be just fine\n",
        "302": "but maybe if it takes on much larger\n",
        "304": "values than plus 3 or minus VMs not to\n",
        "308": "worry and if it takes on values from say\n",
        "311": "minus one third to one third you know I\n",
        "313": "think that's fine too or zero to one\n",
        "315": "third of the minus one third to zero or\n",
        "316": "just that's that's typical range of\n",
        "319": "values cycle zero okay but if it takes\n",
        "321": "on a much tinier range of values like x4\n",
        "323": "here and then again on might start to\n",
        "326": "worry so the take-home message is don't\n",
        "328": "worry if your features are not exactly\n",
        "330": "on the same scale or exactly in the same\n",
        "331": "range of values but so long as they're\n",
        "334": "all close enough to this gradient\n",
        "336": "descent should work okay in addition to\n",
        "339": "dividing by so that the maximum value\n",
        "341": "when performing feature scaling\n",
        "344": "sometimes people will also do what's\n",
        "346": "called mean normalization and what I\n",
        "348": "mean by that is that you might take a\n",
        "351": "feature X I and replace it with X I\n",
        "354": "minus mu I to make your features have\n",
        "357": "approximately zero mean and obviously we\n",
        "359": "won't apply this to the feature x0\n",
        "362": "because the feature x0 is always equal\n",
        "364": "to one so it cannot have an average of\n",
        "367": "value of 0 but concretely for other\n",
        "370": "features if the range of sizes of a\n",
        "373": "house takes on values between zero to\n",
        "375": "two thousand and if you know the average\n",
        "378": "size of a house is equal to one thousand\n",
        "384": "then you might use this formula size set\n",
        "386": "the feature x1 to the size minus the\n",
        "387": "average value\n",
        "391": "/ mm and similarly if you know on\n",
        "395": "average if your houses have one to five\n",
        "399": "bedrooms and if number of and if on\n",
        "401": "average a house has say two bedrooms\n",
        "404": "then you might use this formula to mean\n",
        "409": "normalize the your second feature x2 in\n",
        "411": "both of these cases you therefore wind\n",
        "413": "up with features x1 and x2 they can take\n",
        "415": "on values roughly between minus point 5\n",
        "417": "and positive point five forms a key not\n",
        "419": "true x2 can actually be slightly larger\n",
        "420": "than point five but you know close\n",
        "423": "enough and the more general rule is that\n",
        "426": "you might take a feature x1 and replace\n",
        "433": "it with X 1 minus nu 1 over s 1 where to\n",
        "436": "define these terms nu 1 is the average\n",
        "443": "value of x1 in the training set and s1\n",
        "448": "is the range of values of that feature\n",
        "450": "and by range I mean let's say the\n",
        "452": "maximum value minus the minimum value or\n",
        "454": "for those of you that know what the\n",
        "456": "standard deviation of a variable is\n",
        "459": "setting s1 to be the standard deviation\n",
        "461": "of the variable would be fine too but\n",
        "463": "taking you of this max minus min would\n",
        "465": "be fine and similarly for the second\n",
        "469": "feature x2 you replace x2 with this are\n",
        "472": "sort of subtract the mean of the feature\n",
        "474": "and divided by the range of values\n",
        "477": "meaning the max minus min and this sort\n",
        "479": "of formula will get your features you\n",
        "481": "know maybe not exactly but maybe roughly\n",
        "484": "into these sorts of ranges and by the\n",
        "486": "way for those of you that are being\n",
        "487": "super careful technically if we're\n",
        "491": "taking the range as max minus min this 5\n",
        "492": "here will actually become a four so that\n",
        "495": "if max is five minutes one then a range\n",
        "497": "of bedroom values is actual equal to\n",
        "499": "four but all of these are approximate\n",
        "501": "and any value that gets the features\n",
        "503": "into anything close to these sorts of\n",
        "504": "into anything close to these sorts of\n",
        "507": "ranges will do fine and the the feature\n",
        "509": "scaling doesn't have to be too exact in\n",
        "511": "order to get gradient descent to run\n",
        "514": "quite a lot faster\n",
        "516": "so now you know about feature scaling\n",
        "519": "and if you apply the simple trick\n",
        "520": "it can make gradient descent run much\n",
        "523": "faster and converge in a lot fewer\n",
        "526": "iterations that was feature scaling in\n",
        "527": "the next video I'll tell you about\n",
        "530": "another trick to make gradient descent\n"
    },
    "rng04VJxUt4": {
        "0": " \n",
        "2": "in this video I'd like to tell you about\n",
        "4": "the principal components analysis\n",
        "7": "algorithm and by the end of this video\n",
        "9": "you know how to implement PCA for\n",
        "11": "yourself and use it to reduce the\n",
        "13": "dimension of your data but for applying\n",
        "16": "PCA there's a data pre-processing step\n",
        "18": "which you should always stay given a\n",
        "21": "training set of M unlabeled examples is\n",
        "23": "important to always perform mean\n",
        "26": "normalization and then depending on your\n",
        "28": "data may be perform feature scaling as\n",
        "31": "well so this is very similar to the mean\n",
        "33": "normalization and feature scaling\n",
        "36": "process that we had for a supervised\n",
        "37": "learning in fact it's actually exactly\n",
        "40": "the same procedure except that we're\n",
        "42": "doing it now to our unlabeled data x1\n",
        "45": "through XM so for me normalization we\n",
        "47": "first computes the mean of each feature\n",
        "51": "and then we replace each feature X with\n",
        "54": "X minus this mean and so this makes each\n",
        "56": "feature now have exactly zero mean and\n",
        "59": "then second if the different features\n",
        "62": "have very different scales so for\n",
        "64": "example if x1 is the size of a house and\n",
        "65": "x2 is number of bedrooms to use our\n",
        "68": "earlier example we then also scale each\n",
        "70": "feature to have a comparable range of\n",
        "72": "values and so similar to what we had\n",
        "75": "with supervised learning or tickets I\n",
        "79": "subscript J that's the J feature and so\n",
        "81": "we would subtract off the mean now\n",
        "83": "that's what we have on top and then\n",
        "86": "divide by SJ here SJ is some measure of\n",
        "88": "their range of values of feature J so it\n",
        "91": "could be the max - min value or more\n",
        "93": "commonly it is the standard deviation of\n",
        "94": "each and J\n",
        "96": "having done this sort of data\n",
        "98": "pre-processing here's what the PCA\n",
        "99": "pre-processing here's what the PCA\n",
        "101": "algorithm does we saw from the previous\n",
        "103": "video that what PCA does is it tries to\n",
        "105": "find a lower dimensional subspace onto\n",
        "107": "which to project the data so as to\n",
        "110": "minimize these squared projection errors\n",
        "112": "to some of the square projection errors\n",
        "115": "as the square of the lengths of\n",
        "117": "provide segments and so what we wanted\n",
        "122": "to do specifically is find a vector u1\n",
        "125": "which specifies that direction or in the\n",
        "128": "2d case we want to find two vectors u 1\n",
        "133": "and u 2 to define the surface onto which\n",
        "138": "to project the data so just as a quick\n",
        "140": "reminder of what reducing the dimension\n",
        "142": "of the data means for this example on\n",
        "144": "the Left we were given the examples X I\n",
        "149": "which are in r2 and what we'd like to do\n",
        "152": "is find a set that numbers Zi in our\n",
        "156": "with wish to represent our data right so\n",
        "157": "that's what a reduction from 2d to 1d\n",
        "161": "means and so specifically by projecting\n",
        "162": "means and so specifically by projecting\n",
        "164": "the data onto this red line there we\n",
        "167": "need only one number to specify the\n",
        "168": "position of the points on the line so\n",
        "171": "I'm going to call that number Z of Z 1 Z\n",
        "173": "here this is a real number so that's\n",
        "176": "like a 1 dimensional vector so z1 just\n",
        "178": "refers to the first component of this\n",
        "180": "you know 1 by 1 matrix where this 1\n",
        "183": "dimensional vector and so we need only\n",
        "185": "one number to specify the position of a\n",
        "189": "point so if this example here was my\n",
        "192": "example x1 then maybe that gets mapped\n",
        "195": "here and at this example was next to\n",
        "198": "maybe is on this map here and so this\n",
        "201": "point here would be z1 and this point\n",
        "204": "here will be Z 2 and similarly you know\n",
        "207": "we would have those other points for\n",
        "211": "these are maybe Z X 3 X 2 X X 3 X 4 X 5\n",
        "215": "get mapped to z1 z2 z3 so what PCA has\n",
        "218": "to do is we need to come over a way to\n",
        "220": "compute two things one is to compute\n",
        "224": "these vectors u1 and in this case u 1\n",
        "227": "and u 2 and the other is how do we\n",
        "229": "compute these numbers\n",
        "232": "so on the example on the left we're\n",
        "235": "reducing the data from 2d to 1d in the\n",
        "237": "example on the right we will be reducing\n",
        "240": "data from three dimensional so f size in\n",
        "245": "r3 to Zi which is now two dimensional so\n",
        "247": "these V Z vectors will now be\n",
        "251": "two-dimensional so in z1 z2 like so and\n",
        "253": "so we need to give a way to compute\n",
        "256": "these new representations that z1 and z2\n",
        "259": "of the data as well so how do we compute\n",
        "261": "all of these quantities it turns out\n",
        "263": "that a mathematical derivation also the\n",
        "266": "mathematical proof for what is the right\n",
        "269": "value for u 1 u 2 Z 1 Z 2 and so on that\n",
        "271": "mathematical proof is very complicated\n",
        "274": "and beyond the scope of the course but\n",
        "276": "but once you've done all that math\n",
        "278": "derivation it turns out that the\n",
        "281": "procedure to actually find the value of\n",
        "283": "u 1 that you want it's not that hard\n",
        "285": "even though so the mathematical proof\n",
        "287": "that this value is the correct value is\n",
        "289": "somewhat more involved in more than one\n",
        "292": "we get into but let me just describe the\n",
        "294": "specific procedure that you have to\n",
        "296": "implement in order to compute all of\n",
        "298": "these things the vectors u 1 u 2 and\n",
        "301": "these vectors Z here's the procedure\n",
        "303": "let's say we want to reduce the data\n",
        "306": "from n dimensional to K dimensional what\n",
        "308": "we're going to do is first compute\n",
        "310": "something called the covariance matrix\n",
        "312": "and the covariance matrix is commonly\n",
        "315": "denoted by this Greek alphabet which is\n",
        "318": "a capital Greek alphabet Sigma it's a\n",
        "319": "bit unfortunate that the Greek alphabet\n",
        "322": "Sigma looks exactly like the summation\n",
        "325": "symbol so this is the Greek alphabet\n",
        "327": "Sigma is used to denote a matrix and\n",
        "330": "this here is a summation symbol so\n",
        "333": "hopefully in these slides there won't be\n",
        "335": "ambiguity about which is Sigma or the\n",
        "337": "matrix the covariance matrix M which is\n",
        "338": "a summation symbol\n",
        "340": "and hopefully you'll be clear from\n",
        "342": "context you know when I'm using each one\n",
        "345": "how to compute this this matrix Sigma\n",
        "347": "and let's say we were to store it in an\n",
        "350": "octave variable called Y or f I GMA what\n",
        "353": "we need to do is then compute something\n",
        "355": "called the eigenvectors of a matrix of\n",
        "358": "the matrix Sigma and in octave the way\n",
        "360": "you do that is you use this command us V\n",
        "364": "equals ng of Sigma SVD by the way stands\n",
        "368": "for a singular value decomposition this\n",
        "372": "is a much more advanced singular value\n",
        "375": "decomposition this is much more advanced\n",
        "377": "linear algebra than you actually need to\n",
        "380": "know but it turns out that when Sigma is\n",
        "382": "a covariance matrix there are a few ways\n",
        "385": "to compute these eigenvectors and if\n",
        "387": "you're an expert in linear algebra and\n",
        "389": "if you've heard of eigenvectors before\n",
        "391": "you may know that there's another octave\n",
        "394": "function called F which can also be used\n",
        "396": "to compute the same thing and turns out\n",
        "399": "that the SVD function and the I function\n",
        "401": "will give you the same eigen vectors or\n",
        "404": "the SVD is a little bit more numerically\n",
        "406": "stable so I tend to use SVD although I\n",
        "407": "have a few friends that use the AI\n",
        "410": "function to do this as well but when you\n",
        "412": "apply this to a covariance matrix Sigma\n",
        "414": "they'll give you the same thing that's\n",
        "415": "because the covariance matrix always\n",
        "418": "satisfies a mathematical property called\n",
        "421": "a symmetric positive semi definite you\n",
        "422": "really don't need to know what that\n",
        "425": "means but the SVD and the I functions\n",
        "427": "are different functions but when they're\n",
        "430": "applied to a covariance matrix which can\n",
        "432": "be proved to always satisfy this sort of\n",
        "434": "a mathematical property they'll always\n",
        "437": "give you the same thing okay that was\n",
        "438": "probably much more linear algebra than\n",
        "441": "you needed to know in case none of that\n",
        "442": "made sense don't worry about it all you\n",
        "444": "need to know is that this is a command\n",
        "445": "you should\n",
        "447": "implement in octave and if you're\n",
        "448": "implement in octave and if you're\n",
        "449": "looking up if you're implementing this\n",
        "451": "in a different language than octave or\n",
        "454": "MATLAB what you should do is find it in\n",
        "456": "numerical linear algebra library that\n",
        "459": "can compute the s-video singular value\n",
        "461": "decomposition and there are many such\n",
        "463": "libraries you know for almost for\n",
        "465": "probably all the major burying languages\n",
        "467": "and she will use that to find routine to\n",
        "471": "compute the matrices you snv given of\n",
        "473": "the covariance matrix Sigma\n",
        "475": "so just to fill in some more details\n",
        "478": "this covariance matrix Sigma will be an\n",
        "483": "N by n matrix and one way to see that is\n",
        "485": "if you look at the definition this is an\n",
        "490": "N by 1 vector and this here X I\n",
        "493": "transpose is 1 by N and so the product\n",
        "495": "of these two things is going to be an N\n",
        "500": "by n matrix right since 1 by NX I\n",
        "502": "transpose 1 1 is 1 by n so that's a and\n",
        "504": "by n matrix and we add up a bunch of\n",
        "506": "these you still have an N by n matrix\n",
        "511": "and what the SVD outputs is 3 matrices u\n",
        "513": "s and V and the thing we really need out\n",
        "517": "of the SVD is the new matrix so the U\n",
        "521": "matrix will be also an N by n matrix and\n",
        "523": "if we look at the columns of the U\n",
        "527": "matrix it turns out that the columns of\n",
        "531": "the U matrix will be exactly those\n",
        "536": "vectors you want u 2 and so on that we\n",
        "539": "want so you will be an invite\n",
        "542": "matrix and if we want to reduce the data\n",
        "544": "from n dimensions down to K dimensions\n",
        "547": "then what we need to do is just take the\n",
        "551": "first K vectors and that will give us u1\n",
        "556": "up to u K which are the K directions\n",
        "558": "onto which we want to project the data\n",
        "560": "so to describe the rest of the procedure\n",
        "564": "from this SVD numerical linear algebra\n",
        "566": "routine we get this matrix U and I'm\n",
        "568": "going to call is columns you want\n",
        "571": "through u n so just to wrap up the\n",
        "574": "description of the rest of seizure from\n",
        "576": "the SVD numerical linear algebra routine\n",
        "580": "we get these matrices u s and D and so\n",
        "582": "here's the matrix U and we're going to\n",
        "585": "use the first K columns of this matrix\n",
        "589": "to get u 1 through u K now the other\n",
        "591": "thing we need to do is we need to come\n",
        "594": "up with a way to take my original data\n",
        "597": "set X which is an RN and find a lower\n",
        "599": "dimensional representation Z which is in\n",
        "603": "R K for this data so the way I'm going\n",
        "604": "to do that is I'm going to take the\n",
        "607": "first K columns of the U matrix simula\n",
        "611": "construct this matrix over them stack up\n",
        "617": "u 1 u 2 and so on up to u K in columns\n",
        "619": "it's really basically taking you know\n",
        "620": "this part of the matrix right the first\n",
        "625": "K columns of this matrix and so this is\n",
        "629": "going to be an N by K matrix I'm going\n",
        "631": "to give this matrix a name I'm going to\n",
        "636": "call this matrix u subscript reduce sort\n",
        "638": "of a reduced version of the u matrix\n",
        "640": "maybe until they use B user to reduce\n",
        "642": "the dimension of my data and the way I'm\n",
        "644": "going to compute Z is going to let Z be\n",
        "649": "equal to this u reduced matrix transpose\n",
        "652": "times X or alternatively you know to\n",
        "654": "write down what this transpose means\n",
        "656": "when I take the transpose of this new\n",
        "658": "reduce matrix well I'm going to end up\n",
        "661": "with is these vectors now in rows I have\n",
        "667": "u 1 transpose down to UK transpose and\n",
        "670": "think that times X and that's how I get\n",
        "673": "my vector Z just to make sure that these\n",
        "676": "dimensions make sense this matrix here\n",
        "679": "is going to be K by N and X here is\n",
        "682": "going to be n by 1 and so their product\n",
        "688": "here will be K by 1 and so Z is K\n",
        "690": "dimensional the K dimensional vector\n",
        "692": "which is exactly what we wanted and of\n",
        "694": "course these X's here right can be\n",
        "696": "examples in our training set can be\n",
        "697": "examples in our training set can be\n",
        "698": "examples my cross-validation set can be\n",
        "700": "examples on a test set and for example\n",
        "702": "if you know I want to take training\n",
        "707": "example I I can write this as X I X I\n",
        "710": "and that's what will give me Z I over\n",
        "713": "there so to summarize here's the PCA\n",
        "716": "algorithm on one slide after me\n",
        "718": "normalization says ensure every feature\n",
        "720": "a zero mean and optionally feature\n",
        "722": "scaling which you really should do\n",
        "724": "feature scaling of your features take on\n",
        "726": "very different ranges of values this\n",
        "729": "after this pre-processing we compute the\n",
        "734": "covariance matrix Sigma like so and by\n",
        "737": "the way if your data is given you know\n",
        "738": "as a matrix like this if you have your\n",
        "739": "as a matrix like this if you have your\n",
        "743": "data given in rows like this to give a\n",
        "745": "matrix capital X which is your entire\n",
        "749": "training set latenan rows X 1 transpose\n",
        "752": "down to exempt transpose this this\n",
        "755": "covariance matrix Sigma actually has a\n",
        "757": "nice vectorizing implementation so that\n",
        "759": "you can implement in octave you can\n",
        "763": "limit when Sigma equals 1 over m\n",
        "766": "times X which is this matrix up here\n",
        "772": "transpose times X and this simple\n",
        "773": "expression that's a vectorized\n",
        "775": "implementation for how to compute the\n",
        "779": "matrix Sigma I'm not going to prove that\n",
        "780": "this is a correct factorization but you\n",
        "782": "want you can either you know numerically\n",
        "784": "test this out yourself by trying out an\n",
        "786": "octave and making sure that both this\n",
        "788": "and this implementations give the same\n",
        "789": "answers or you can try to prove it\n",
        "791": "yourself mathematically either way but\n",
        "793": "this is a correct vectorized\n",
        "795": "implementation file to compute Sigma\n",
        "798": "next we can apply the SVD routine to get\n",
        "802": "us n V and then we grab the first K\n",
        "805": "columns of the U matrix to get u reduce\n",
        "809": "then finally this defines how we go from\n",
        "812": "a feature vector X to this reduced\n",
        "814": "dimension representation Z and similar\n",
        "818": "to came in severe applying PCA the way\n",
        "820": "you'd apply this is with vectors X in RN\n",
        "824": "right so this is not done with the X 0\n",
        "828": "equals 1 Convention so that was a PCA\n",
        "831": "algorithm one thing I didn't do was give\n",
        "833": "a mathematical proof that this procedure\n",
        "835": "actually gives the projection of the\n",
        "838": "data onto you know the K dimensional\n",
        "840": "subspace onto the K dimensional surface\n",
        "842": "that actually minimizes the square\n",
        "845": "projection error the mathematical proof\n",
        "846": "of that is beyond the scope of this\n",
        "849": "course but fortunately the PCA algorithm\n",
        "851": "can be implemented in you know not too\n",
        "853": "many lines of octave code and if you\n",
        "856": "implement it in octave or MATLAB you\n",
        "857": "actually get a very effective\n",
        "861": "dimensionality reduction algorithm\n",
        "865": "so that was the PCA algorithm one thing\n",
        "867": "I didn't do was give a mathematical\n",
        "870": "proof that you know the u1 u2 and so on\n",
        "872": "the disease and some one that you get\n",
        "874": "out of this procedure there's really the\n",
        "877": "choices that will minimize the squared\n",
        "879": "projection error right remember we said\n",
        "881": "what PCA tries to do is it tries to find\n",
        "882": "the surface or a line onto which to\n",
        "883": "the surface or a line onto which to\n",
        "885": "project the data so as to minimize that\n",
        "887": "squared projection error so I didn't\n",
        "889": "prove that this actually achieves that\n",
        "891": "and the mathematical proof of that is\n",
        "893": "beyond the scope of this course but so\n",
        "895": "fortunately the PCA algorithm can be\n",
        "897": "implemented in you know not too many\n",
        "900": "lines of octave code and if you just\n",
        "902": "implement this this is actually what\n",
        "904": "will work what this will work well and\n",
        "906": "if you implement this algorithm you get\n",
        "907": "a very effective dimensionality\n",
        "910": "reduction algorithm that does do the\n",
        "912": "right thing of minimizing this squared\n"
    },
    "sZSKGNbrwus": {
        "0": " \n",
        "1": "by now you've seen a lot of different\n",
        "4": "learning algorithms and if you've been\n",
        "5": "following along these videos you should\n",
        "7": "consider yourself an expert on many\n",
        "9": "state-of-the-art machine learning\n",
        "13": "techniques but even among people they\n",
        "14": "know a certain learning algorithm\n",
        "16": "there's often a huge difference between\n",
        "18": "someone that really knows how to\n",
        "20": "powerfully and effectively apply that\n",
        "23": "algorithm versus someone that's less\n",
        "25": "familiar with some of the material than\n",
        "27": "about to teach and who doesn't really\n",
        "28": "understand how to apply these algorithms\n",
        "30": "and can end up wasting a lot well at the\n",
        "32": "time trying things out that don't really\n",
        "33": "time trying things out that don't really\n",
        "35": "make sense what I'd like to do is make\n",
        "37": "sure that if you're developing a machine\n",
        "39": "learning system then you know how to\n",
        "41": "choose one of the most promising avenues\n",
        "44": "to spend your time pursuing and in this\n",
        "46": "and the next few videos I'm going to\n",
        "48": "give a number of practical suggestions\n",
        "52": "advice guidelines on how to do that and\n",
        "54": "concretely one way to focus on is the\n",
        "57": "problem of suppose you're developing a\n",
        "58": "machine learning system or trying to\n",
        "60": "improve the performance of a machine\n",
        "62": "learning system how do you go about\n",
        "64": "deciding what are the promising avenues\n",
        "70": "to trainings to explain this let's\n",
        "73": "continue using our example of learning\n",
        "75": "to predict housing prices and let's say\n",
        "77": "you've implemented regularized linear\n",
        "80": "regression thus minimizing that cost\n",
        "83": "function G but suppose that after you\n",
        "85": "take your learn parameters if you test\n",
        "87": "your hypothesis on the new set of houses\n",
        "89": "suppose you find that it's making huge\n",
        "91": "errors in this prediction of the housing\n",
        "94": "prices the question is what should you\n",
        "97": "then try next in order to improve the\n",
        "99": "learning algorithm there are many things\n",
        "101": "that one can think of that could improve\n",
        "103": "the performance of your learning\n",
        "106": "algorithm one thing they could try is to\n",
        "107": "get more training examples and\n",
        "109": "concretely you can imagine maybe you\n",
        "111": "know sending out phone surveys going on\n",
        "113": "door-to-door to try to get more data on\n",
        "117": "how much different houses so for and the\n",
        "119": "sad thing is I've seen a lot of people\n",
        "121": "spend a lot of time collecting more\n",
        "123": "training examples thinking oh if we have\n",
        "125": "twice as much or 10 times as much\n",
        "126": "training data\n",
        "128": "certainly gonna help right but sometimes\n",
        "130": "getting our training data doesn't\n",
        "132": "actually help and in the next few videos\n",
        "135": "we'll see why and we'll see how you can\n",
        "137": "avoid spending a lot of time collecting\n",
        "139": "more training data in settings where\n",
        "142": "it's just not gonna help other things\n",
        "145": "you might try our two well maybe try a\n",
        "147": "smaller set of features so if you have\n",
        "150": "some set of features X 1 X 2 X 3 and so\n",
        "152": "on maybe a large number of features\n",
        "154": "maybe you want to spend time carefully\n",
        "156": "selecting some small subset of them to\n",
        "159": "prevent overfitting or maybe you need to\n",
        "161": "get additional features maybe the\n",
        "162": "current set of features aren't\n",
        "165": "informative enough and we want to\n",
        "166": "collect more data in the sense of\n",
        "169": "getting more features and once again\n",
        "171": "this is a solid project that can scale\n",
        "172": "up to huge projects you can imagine\n",
        "173": "up to huge projects you can imagine\n",
        "175": "again phone surveys to find them on the\n",
        "178": "houses or extra land surveys to find out\n",
        "180": "more about the pieces of land and so on\n",
        "182": "so in a huge project and once again it\n",
        "184": "would be nice to know in advance if this\n",
        "186": "is going to help before we spend a lot\n",
        "189": "of time doing something like this we can\n",
        "191": "also try adding polynomial features\n",
        "193": "things like x1 squared x2 squared and\n",
        "195": "Prada features x1 x2 you can the\n",
        "196": "students spend quite a lot of time\n",
        "198": "thinking about that and we can also try\n",
        "200": "other things that decreasing lambda the\n",
        "202": "regularization parameter or increasing\n",
        "205": "long term given a menu of options like\n",
        "208": "these some of which can easily scale up\n",
        "209": "to it like maybe six months or longer\n",
        "212": "projects unfortunately the most common\n",
        "214": "method that people use to pick one of\n",
        "216": "these is to go by gut feeling in which\n",
        "219": "what many people will do is sort of\n",
        "221": "randomly pick one of these options like\n",
        "222": "maybe say oh let's go and get more\n",
        "224": "training data and easily spend six\n",
        "225": "training data and easily spend six\n",
        "227": "months collecting more training data or\n",
        "229": "maybe someone else or randomly say well\n",
        "231": "let's go collect a lot more features on\n",
        "233": "these houses in our data set and I have\n",
        "234": "a lot of times\n",
        "237": "sadly seen people's then literally six\n",
        "239": "months doing one of these avenues that\n",
        "240": "they had picked from sort of a random\n",
        "243": "only to discover six months later that\n",
        "245": "that really wasn't a promising Avenue to\n",
        "248": "pursue fortunately there's a pretty\n",
        "250": "simple technique that\n",
        "253": "can let you fairly quickly rule out half\n",
        "256": "of the things on this list as being\n",
        "258": "potentially promising things to pursue\n",
        "259": "there's a fairly simple technique that\n",
        "262": "if you run can easily rule out many of\n",
        "265": "these options and potentially save you a\n",
        "267": "lot of time pursuing something that's\n",
        "270": "just not gonna work in the next two\n",
        "272": "videos after this I'm going to first\n",
        "275": "talk about how to evaluate learning\n",
        "278": "algorithms and in the next few videos\n",
        "281": "after that I'm going to talk about these\n",
        "283": "techniques which are called the machine\n",
        "286": "learning Diagnostics and what a\n",
        "290": "diagnostic is is a test we can run to\n",
        "293": "get insight into what isn't what is that\n",
        "295": "what isn't working with an algorithm and\n",
        "297": "which will often give you insight as to\n",
        "299": "what are promising things to try to\n",
        "301": "improve in learning algorithms\n",
        "304": "performance we talked about specific\n",
        "307": "Diagnostics later in this video sequence\n",
        "309": "but I should mention advance that\n",
        "311": "Diagnostics can take time to implement\n",
        "314": "can sometimes you know take quite a lot\n",
        "316": "of time to implement and understand but\n",
        "318": "doing so can be a very good use of your\n",
        "320": "time when you're developing learning\n",
        "322": "algorithms because they can often save\n",
        "324": "you from spending many months pursuing\n",
        "327": "an avenue that you could have found out\n",
        "329": "much earlier I just was not going to be\n",
        "331": "fruitful\n",
        "334": "so in the next few videos I'm going to\n",
        "336": "first talk about how to evaluate your\n",
        "338": "learning algorithms and after that and\n",
        "340": "then talk about some of these\n",
        "342": "diagnostics which will hopefully let you\n",
        "344": "much more effectively select whether\n",
        "346": "useful things that trimix\n",
        "348": "in order if you go is to improve the\n",
        "350": "performance of your machine learning\n"
    },
    "t1IT5hZfS48": {
        "0": " \n",
        "1": "let's start talking about logistic\n",
        "4": "regression in this video I'd like to\n",
        "6": "show you the hypothesis representation\n",
        "8": "that is what is the function we're going\n",
        "9": "that is what is the function we're going\n",
        "11": "to use to represent our hypothesis where\n",
        "15": "we have a classification problem earlier\n",
        "17": "we said that we would like our\n",
        "21": "classifier to output values that are\n",
        "23": "between 0 &amp; 1 so we'd like to come up\n",
        "26": "with a hypothesis that satisfies this\n",
        "28": "property that these predictions are\n",
        "31": "maybe between 0 &amp; 1 when we were using\n",
        "33": "linear regression this was the form of a\n",
        "36": "hypothesis where H of X is Theta\n",
        "39": "transpose X for logistic regression I'm\n",
        "41": "going to modify this a little bit and\n",
        "44": "make the hypothesis G of theta transpose\n",
        "48": "X where I'm going to define the function\n",
        "50": "G as follows\n",
        "53": "G of Z of Z is a real number is equal to\n",
        "58": "1 over 1 plus e to the negative Z this\n",
        "62": "is called the sigmoid function or the\n",
        "66": "logistic function and the term logistic\n",
        "68": "function that's what gives rise to the\n",
        "71": "name logistic regression and by the way\n",
        "74": "the terms sigmoid function and logistic\n",
        "77": "function are basically synonyms and mean\n",
        "79": "the same thing so the two terms are\n",
        "81": "basically interchangeable and either a\n",
        "84": "term can be used to refer to this\n",
        "86": "function G and if we take these two\n",
        "89": "equations and put them together then\n",
        "91": "here's just an alternative way of\n",
        "94": "writing out the form of my hypothesis\n",
        "98": "I'm saying that H of X is 1 over 1 plus\n",
        "102": "e to the negative theta transpose X and\n",
        "106": "all done is I've taken this variable Z Z\n",
        "108": "here is a real number and plugged in\n",
        "111": "theta transpose X so I end up with you\n",
        "113": "know theta transpose X in place of Z\n",
        "116": "there lastly let me show you what the\n",
        "118": "sigmoid function looks like I'm going to\n",
        "120": "plot it on this figure here the sigmoid\n",
        "123": "function G of Z also called the logistic\n",
        "125": "function it looks like this is false off\n",
        "128": "me or zero and then it rises until I\n",
        "130": "process 0.5 and the\n",
        "132": "origin and then it flattens I'll be\n",
        "134": "gained like so so that's what the\n",
        "136": "sigmoid function looks like and you\n",
        "139": "notice that the sigmoid function well it\n",
        "143": "add some tones at 1 and asymptotes at 0\n",
        "146": "as Z I guess the horizontal axis is Z as\n",
        "147": "as Z I guess the horizontal axis is Z as\n",
        "149": "Z goes to minus infinity G of Z\n",
        "152": "approaches 0 and as G of Z approaches\n",
        "156": "infinity G of Z approaches 1 and so\n",
        "158": "because G of Z outputs values that are\n",
        "162": "you know between 0 &amp; 1 we also have that\n",
        "167": "H of X must be between 0 &amp; 1 finally\n",
        "170": "given this hypothesis representation\n",
        "174": "what we need to do as before is fit the\n",
        "179": "parameters theta to our data so given a\n",
        "181": "training set we need to pick a value for\n",
        "184": "the parameters theta and this hypothesis\n",
        "186": "will then they let us make predictions\n",
        "188": "we'll talk about a learning algorithm\n",
        "190": "later for fitting the parameters theta\n",
        "194": "but first let's talk a bit about the\n",
        "197": "interpretation of this model\n",
        "200": "here's how I'm going to interpret the\n",
        "205": "output of my hypothesis H of X when my\n",
        "208": "hypothesis output some number I am going\n",
        "210": "to treat that number as the estimated\n",
        "215": "probability then Y is equal to one on a\n",
        "218": "new input example X here's what I mean\n",
        "221": "here's an example let's say we're using\n",
        "224": "the tumor classification example so we\n",
        "226": "may have a feature vector X which is\n",
        "229": "this X 0 equals 1 as always and then our\n",
        "232": "one feature is the size of the tumor\n",
        "234": "suppose I have a patient come in and you\n",
        "237": "know they have some tumor size and I\n",
        "239": "feed their feature vector X into my\n",
        "241": "hypothesis and suppose my hypothesis\n",
        "244": "outputs the number zero point seven I'm\n",
        "246": "going to interpret my hypothesis as\n",
        "248": "follows I'm going to say that this\n",
        "251": "hypothesis is telling me that for a\n",
        "254": "patient with features X the probability\n",
        "257": "that y equals one is zero point seven in\n",
        "259": "other words I'm going to tell my patient\n",
        "263": "that that tumor Savi has a 70% chance or\n",
        "266": "a zero point seven chance of being\n",
        "269": "malignant to write this out slightly\n",
        "270": "more formally Oh to write this out in\n",
        "272": "math I'm going to interpret my\n",
        "277": "hypothesis output as p of y equals 1\n",
        "282": "given X parametrized by theta so for\n",
        "283": "those of you that are familiar with\n",
        "285": "probability this equation may make sense\n",
        "287": "if you're low less familiar probability\n",
        "290": "here's how I read this up read this\n",
        "292": "expression this is the probability that\n",
        "295": "Y is equal to 1 given X sort of given\n",
        "297": "that my patient has you know features X\n",
        "299": "so given my patient has a particular\n",
        "302": "tumor size represented by my features X\n",
        "305": "and this probability is parameterize by\n",
        "309": "theta so I'm basically going to count on\n",
        "311": "my hypothesis to give me estimates of\n",
        "314": "the probability that Y is equal to 1 now\n",
        "315": "the probability that Y is equal to 1 now\n",
        "318": "since this is a classification task we\n",
        "321": "know that y must be either 0 or 1 right\n",
        "323": "those are the only two values that Y\n",
        "325": "could possibly take on either in the\n",
        "327": "training set or for new patients that\n",
        "328": "may walk into my office\n",
        "330": "in order to the doctor's office in the\n",
        "334": "future so given H of X we can therefore\n",
        "336": "compute the probability that Y is equal\n",
        "338": "to zero as well\n",
        "342": "concretely because Y must be either 0 or\n",
        "345": "1 we know that the probability of y\n",
        "347": "equals 0 plus the probability of y\n",
        "350": "equals 1 must add up to 1 this first\n",
        "351": "equation looks a little bit more\n",
        "353": "complicated but it's basically saying\n",
        "355": "that probably if y equals 0 for a\n",
        "357": "particular patient with features X and\n",
        "360": "you know given our parameters theta plus\n",
        "362": "the probably of y equals 1 for that same\n",
        "364": "patient with features X and given fitted\n",
        "367": "parameters theta must add up to 1 if\n",
        "368": "this equation looks a little bit\n",
        "369": "complicated\n",
        "372": "feel free to mentally imagine it without\n",
        "375": "that X and theta and this is just saying\n",
        "376": "that the probability of y equals 0 plus\n",
        "378": "the probably y equals 1 must be equal to\n",
        "381": "1 and we know this to be true because Y\n",
        "383": "has to be either 0 or 1 and so the\n",
        "385": "chance of YB equals 0 plus the chance\n",
        "388": "that Y is 1 you know goes to must add up\n",
        "392": "to 1 and so if you just take this term\n",
        "395": "and move it to the right-hand side then\n",
        "397": "you end up with this equation this is\n",
        "400": "probably y equals 0 is 1 minus probably\n",
        "404": "y equals 1 and thus if our hypothesis if\n",
        "407": "H of X gives us that term you can\n",
        "409": "therefore quite simply compute the\n",
        "411": "probability or compute the estimated\n",
        "414": "probability that Y is equal to 0 as well\n",
        "417": "so you now know what the hypothesis\n",
        "419": "representation is for logistic\n",
        "421": "regression and we've seen what the\n",
        "424": "mathematical formula is defining the\n",
        "426": "hypothesis for the just regression in\n",
        "428": "the next video I'd like to try to give\n",
        "430": "you better intuition about what the\n",
        "433": "hypothesis function looks like and I\n",
        "434": "want to tell you about something called\n",
        "436": "the decision boundary and we'll look at\n",
        "439": "some visualizations together to try to\n",
        "440": "get a better sense of what this\n",
        "442": "hypothesis function of logistic\n"
    },
    "u73PU6Qwl1I": {
        "0": " \n",
        "2": "by now you've seen a couple different\n",
        "4": "learning algorithms linear regression\n",
        "7": "and logistic regression that work well\n",
        "9": "for many problems but when you apply\n",
        "10": "them to certain machine learning\n",
        "12": "applications they can run into a problem\n",
        "15": "called overfitting they can cause them\n",
        "18": "to perform very poorly what I'd like to\n",
        "21": "do in this video is explain to you what\n",
        "23": "is this overfitting problem and in the\n",
        "26": "next few videos after this we'll talk\n",
        "27": "about the technique called\n",
        "29": "regularization that will allow us to\n",
        "32": "ameliorate or to reduce this overfitting\n",
        "34": "problem and get these learning\n",
        "37": "algorithms to maybe work much better so\n",
        "38": "what is overfitting\n",
        "42": "let's keep using our running example of\n",
        "45": "predicting housing prices with linear\n",
        "47": "regression where we want to predict the\n",
        "49": "price as a function of the size of the\n",
        "52": "hulls one thing we could do is fit a\n",
        "54": "linear function to this data and if we\n",
        "56": "do that maybe we get that sort of\n",
        "59": "straight line fit to the data but this\n",
        "61": "isn't a very good model looking at the\n",
        "63": "data it seems pretty clear that as the\n",
        "66": "size of the holes increases the housing\n",
        "69": "prices plateau or kind of flattens out\n",
        "72": "as we move to the right and so this\n",
        "74": "algorithm doesn't fit the training set\n",
        "77": "very well and we call this problem under\n",
        "80": "fitting and another term for this is\n",
        "84": "that this algorithm has high bias um\n",
        "87": "both of these roughly mean that is just\n",
        "89": "not even fitting the training data very\n",
        "90": "well\n",
        "93": "the term bias is kind of a historical or\n",
        "96": "technical one but the idea is that if\n",
        "97": "we're fitting a straight line to the\n",
        "100": "data then as as if the algorithm has a\n",
        "102": "very strong preconception or a very\n",
        "105": "strong bias that housing prices are\n",
        "107": "going to vary linearly with their size\n",
        "109": "and despite the data to the contrary\n",
        "112": "despite the evidence to the contrary is\n",
        "115": "preconceptions slow or bias still causes\n",
        "118": "it to fit a straight line and this ends\n",
        "121": "up being a poor fit to the data now in\n",
        "122": "the middle we could fit a quadratic\n",
        "125": "function to the data and with this data\n",
        "127": "set we fit a quadratic function maybe we\n",
        "129": "get that kind of curve and that works\n",
        "132": "pretty well and\n",
        "134": "the other extreme would be if we were to\n",
        "137": "fit say a fourth order polynomial to the\n",
        "138": "data so here we have five parameters\n",
        "142": "theta 0 through theta 4 and with dads we\n",
        "144": "can actually fill the curve that passes\n",
        "145": "through all five of our training\n",
        "147": "examples we might get a curve that looks\n",
        "148": "examples we might get a curve that looks\n",
        "153": "like this that on the one hand seems to\n",
        "154": "do a very good job\n",
        "156": "fitting the training set then it passes\n",
        "158": "through all of my data at least but this\n",
        "160": "is still a very Wiggly curve right so\n",
        "161": "I'm going up and down all over the place\n",
        "164": "and we don't actually think that's such\n",
        "165": "a good model for predicting housing\n",
        "169": "prices so this problem we call\n",
        "172": "overfitting and another term for this is\n",
        "176": "that this algorithm has high variance\n",
        "177": "that this algorithm has high variance\n",
        "180": "the term high variance is another sort\n",
        "182": "of historical or technical one but the\n",
        "184": "intuition is that if we're fitting such\n",
        "186": "a high order polynomial then the\n",
        "187": "a high order polynomial then the\n",
        "189": "hypothesis can fit you know it's almost\n",
        "191": "as if it can fit almost any function and\n",
        "194": "the space of possible hypotheses is just\n",
        "196": "too large where it's too variable and we\n",
        "198": "don't have enough data to constrain it\n",
        "201": "to give us a good hypothesis so that's\n",
        "203": "called overfitting and in the middle\n",
        "205": "there isn't really a name but I'm just\n",
        "207": "going to write you're just right where a\n",
        "208": "second-degree polynomial quadratic\n",
        "210": "function seems to be just right for\n",
        "214": "fitting this data to recap a bit the\n",
        "218": "problem of overfitting comes when if we\n",
        "220": "have too many features then the learn\n",
        "222": "hypothesis may fit the training set very\n",
        "226": "well so your cost function may actually\n",
        "228": "be very close to zero or maybe even zero\n",
        "231": "exactly but you may then end up with a\n",
        "234": "curve like this that you know tries too\n",
        "237": "hard to fit the training set so that it\n",
        "239": "even fails to generalize to new examples\n",
        "242": "that it fails to predict prices on new\n",
        "244": "examples well and hear the term\n",
        "247": "generalized refers to how well a\n",
        "250": "hypothesis applies even to new examples\n",
        "253": "that is do data to houses that it hasn't\n",
        "256": "seen in the training set\n",
        "258": "on this slide we looked at overfitting\n",
        "260": "for the case of linear regression a\n",
        "263": "similar thing can apply to logistic\n",
        "265": "regression as well here's a logistic\n",
        "267": "regression example with two features x1\n",
        "270": "and x2 one thing we could do is fit\n",
        "273": "logistic regression with just a simple\n",
        "276": "hypothesis like this where as usual G is\n",
        "278": "my sigmoid function and if you do that\n",
        "281": "you end up with the hypothesis trying to\n",
        "283": "use maybe just a straight line to\n",
        "284": "separate the positive and the negative\n",
        "286": "examples and this doesn't look like a\n",
        "289": "very good fit to the hypothesis and so\n",
        "291": "once again this is an example of\n",
        "294": "underfitting or of a hypothesis having\n",
        "297": "high bias in contrast if you were to add\n",
        "300": "to your features these quadratic terms\n",
        "303": "then you could get a decision boundary\n",
        "305": "that might look more like this and you\n",
        "307": "know that's a pretty good fit to the\n",
        "310": "data probably what - probably about as\n",
        "312": "good as we could get on this training\n",
        "315": "set and finally at the other extreme if\n",
        "317": "you were to fit a very high order\n",
        "319": "polynomial if you were to generate lots\n",
        "321": "of high order polynomial terms as\n",
        "324": "features then logistic regression may\n",
        "327": "contort itself we try really hard to\n",
        "331": "find a decision boundary that um fits\n",
        "333": "your training data go to great lengths\n",
        "335": "to contort itself to fit every single\n",
        "338": "training example well and you know if\n",
        "340": "the features x1 and x2 offer predicting\n",
        "343": "maybe the cancer to the you know cancer\n",
        "346": "is a malignant benign breast tumors this\n",
        "348": "doesn't this really doesn't look like a\n",
        "350": "very good hypothesis for making\n",
        "353": "predictions and so once again this is an\n",
        "355": "instance of overfitting and of\n",
        "358": "hypothesis having high variance and not\n",
        "361": "really and being unlikely to generalize\n",
        "363": "well to new examples\n",
        "366": "later in this course when we talk about\n",
        "369": "debugging and diagnosing things they can\n",
        "371": "go wrong with learning algorithms will\n",
        "373": "give you specific tools to recognize\n",
        "375": "when overfitting and also when\n",
        "377": "underfitting may be occurring but for\n",
        "379": "now let's talk about the problem of if\n",
        "382": "we think overfitting is occurring what\n",
        "385": "can we do to address it in the previous\n",
        "387": "examples we had one or two dimensional\n",
        "389": "data so we could just plot the\n",
        "391": "hypothesis and see what was going on and\n",
        "394": "select the appropriate degree polynomial\n",
        "397": "so earlier for the housing prices\n",
        "398": "example we could just plot the\n",
        "401": "hypothesis and you know maybe see that\n",
        "403": "it was fitting the sort of very Wiggly\n",
        "404": "function that goes all over the place to\n",
        "406": "predict housing prices and we could then\n",
        "408": "use figures like these to select an\n",
        "411": "appropriate degree polynomial so\n",
        "414": "plotting the hypothesis could be one way\n",
        "416": "to try to decide what degree polynomial\n",
        "420": "to use but that doesn't always work and\n",
        "422": "in fact more often we may have learning\n",
        "424": "problems that where we just have a lot\n",
        "427": "of features and there is not just a\n",
        "429": "matter of selecting what degree\n",
        "432": "polynomial and in fact it when we have\n",
        "434": "so many features it also becomes much\n",
        "437": "harder to plot the data and becomes much\n",
        "439": "harder to visualize it to decide what\n",
        "443": "features to keep or not so concretely if\n",
        "444": "we're trying to predict housing prices\n",
        "447": "sometimes we can just have a lot of\n",
        "448": "different features and all of these\n",
        "450": "features seem you know maybe they seem\n",
        "452": "kind of useful but if we have a lot of\n",
        "454": "features and very little training data\n",
        "457": "then overfitting can become a problem in\n",
        "460": "order to address overfitting there are\n",
        "462": "two main options for things that we can\n",
        "466": "do the first option is to try to reduce\n",
        "467": "the number of features\n",
        "469": "concretely one thing we could do is\n",
        "471": "manually look through the list of\n",
        "474": "features and use that to try to decide\n",
        "475": "which are the more important features\n",
        "478": "and therefore which are the features we\n",
        "480": "should keep and which other features we\n",
        "482": "should throw out later in this class\n",
        "484": "we'll also talk about model selection\n",
        "487": "algorithms which are algorithms for\n",
        "489": "automatically deciding which features\n",
        "491": "the key and which features to throw out\n",
        "494": "this idea of reducing the number of\n",
        "496": "features can work well and can reduce\n",
        "497": "overfitting\n",
        "499": "and when we talk about model selection\n",
        "501": "we'll go into this in much greater depth\n",
        "504": "but the disadvantage is that by throwing\n",
        "506": "away some of the features is also\n",
        "508": "throwing away some of the information\n",
        "510": "you have about the problem for example\n",
        "513": "maybe all of those features are actually\n",
        "514": "useful for predicting the price of a\n",
        "516": "house so maybe we don't actually want to\n",
        "519": "throw some of our information or throw\n",
        "522": "some of our features away the second\n",
        "524": "option which we'll talk in which we'll\n",
        "525": "option which we'll talk in which we'll\n",
        "526": "talk about in the next few videos is\n",
        "530": "regularization here we're going to keep\n",
        "532": "all the features but we're going to\n",
        "535": "reduce the magnitude or the values of\n",
        "539": "the parameters theta J and this method\n",
        "541": "works well we'll see when we have a lot\n",
        "543": "of features each of which contributes a\n",
        "546": "little bit to predicting the value of y\n",
        "548": "like we had like we saw in the housing\n",
        "550": "price prediction example where we could\n",
        "552": "have a lot of features each of which are\n",
        "554": "you know somewhat useful so maybe we\n",
        "558": "don't want to throw them away so this\n",
        "561": "describes the idea of regularization at\n",
        "564": "a very high level and I realized that\n",
        "565": "all of these details probably don't make\n",
        "567": "sense to you yet but in the next video\n",
        "571": "we'll start to formulate exactly how to\n",
        "573": "apply regularization and exactly what\n",
        "576": "regularization means and then we'll\n",
        "578": "start to figure out how to use this to\n",
        "580": "make our learning algorithms work well\n"
    },
    "uoTBdCODGvk": {
        "0": " \n",
        "1": "suppose you like to decide what degree\n",
        "4": "of polynomial to fit to a data set some\n",
        "7": "of what features to include to gives you\n",
        "9": "a learning algorithm or suppose you'd\n",
        "10": "like to choose the regularization\n",
        "13": "parameter lambda for learning algorithm\n",
        "16": "how do you do that these are called\n",
        "18": "model selection problems and in our\n",
        "21": "discussion of how to do this we'll talk\n",
        "22": "about not just how to split your data\n",
        "25": "into a training test set but how to\n",
        "27": "switch a data into what we'll discover\n",
        "30": "is called the train validation and test\n",
        "32": "sets we'll see in this video just what\n",
        "34": "these things are and how to use them to\n",
        "37": "do model selection we've already seen a\n",
        "39": "lot of times the problem of overfitting\n",
        "41": "in which just because the learning\n",
        "44": "algorithm fits a training set well that\n",
        "47": "doesn't mean is a good hypothesis more\n",
        "49": "generally this is why the training set\n",
        "52": "error is not a good predictor for how\n",
        "54": "well the hypothesis will do on new\n",
        "58": "examples concretely if you fit some set\n",
        "60": "of parameters theta0 theta1 theta2 and\n",
        "63": "so on to your training set then the fact\n",
        "65": "that your hypothesis does well on the\n",
        "68": "training set well this doesn't mean much\n",
        "70": "in terms of predicting how well your\n",
        "72": "hypothesis will generalize to new\n",
        "75": "examples not seen in the training set\n",
        "77": "and the more general principle is that\n",
        "80": "once your parameters will fit to some\n",
        "81": "set of data\n",
        "82": "maybe the trainings that may be\n",
        "84": "something else then the error of your\n",
        "87": "hypothesis as measured on that same data\n",
        "89": "set such as the training error that's\n",
        "90": "set such as the training error that's\n",
        "93": "unlikely that's unlikely to be a good\n",
        "95": "estimate of your actual generalization\n",
        "97": "error that is of how well the hypothesis\n",
        "102": " \n",
        "105": "now let's consider the model selection\n",
        "106": "problem let's say you're trying to\n",
        "109": "choose what degree polynomial to fit to\n",
        "111": "data to choose a linear function a\n",
        "113": "quadratic function a cubic function all\n",
        "115": "the way up to a tenth order polynomial\n",
        "119": "so it's as if that's one extra parameter\n",
        "121": "in this algorithm which I'm going to\n",
        "123": "denote D which is what degree of\n",
        "124": "denote D which is what degree of\n",
        "127": "polynomial do you want to pick so as as\n",
        "132": "if does this a in addition to the theta\n",
        "134": "parameters is as if there's one more\n",
        "135": "parameter D that you're trying to\n",
        "138": "determine using your data set so the\n",
        "140": "first option is D equals one if you fit\n",
        "142": "a linear function we can choose T equals\n",
        "145": "two equals three all the way up to D\n",
        "147": "equals ten so we'd like to fit this\n",
        "150": "extra so the parameter at which I'm\n",
        "153": "denoting by D and concretely let's say\n",
        "155": "that you want to choose a model that is\n",
        "158": "choose a degree of polynomial choose one\n",
        "160": "of these ten models and fit that model\n",
        "164": "and also get some estimate of how well\n",
        "166": "your fitted hypothesis will generalize\n",
        "168": "to new examples\n",
        "171": "here's one thing you could do what you\n",
        "174": "could first take your first model and\n",
        "176": "minimize the training error and this\n",
        "178": "would give you some parameter vector\n",
        "181": "theta and you could then take your\n",
        "184": "second model the quadratic function and\n",
        "187": "fit that to your training set and this\n",
        "188": "will give you some other parameter\n",
        "190": "vector theta in order to distinguish\n",
        "192": "between these different parameter\n",
        "194": "vectors I'm going to use the superscript\n",
        "197": "1 superscript 2 there where theta a\n",
        "199": "superscript 1 just means the parameters\n",
        "201": "I get by fitting this model to my\n",
        "204": "training data and theta superscript 2\n",
        "206": "just means the parameters I get by\n",
        "207": "fitting this quadratic function to my\n",
        "208": "fitting this quadratic function to my\n",
        "211": "training data and so on then by fitting\n",
        "213": "a cubic model I get parameters theta 3\n",
        "216": "up to you know say theta 10\n",
        "220": "and one thing you could do is then take\n",
        "222": "these parameters and look at the test\n",
        "224": "set error so I can compute on my test\n",
        "231": "set J test with one J test of theta two\n",
        "233": "and so on\n",
        "238": "J test of theta three and so on\n",
        "241": "so I'm going to take each of my\n",
        "243": "hypotheses with the corresponding\n",
        "245": "parameters and just measure the\n",
        "249": "performance on the test set now one\n",
        "251": "thing I could do then is in order to\n",
        "254": "select one of these models I could then\n",
        "257": "see which model has the lowest test set\n",
        "258": "error and let's just say for this\n",
        "261": "example that I ended up choosing the\n",
        "263": "fifth order polynomial so this seems\n",
        "266": "reasonable so far but now let's say I\n",
        "269": "want to take my fit to hypothesis this\n",
        "272": "is a fifth order model and let's say I\n",
        "274": "want to ask how well does this model\n",
        "277": "generalize one thing I could do is look\n",
        "279": "at how well my fifth order polynomial\n",
        "283": "hypothesis had done on my test set but\n",
        "286": "the problem is this will not be a fair\n",
        "288": "estimate of how well my hypothesis\n",
        "293": "generalizes and the reason is what we've\n",
        "296": "done is we fit this extra parameter D\n",
        "299": "that is this degree of polynomial and\n",
        "302": "we'll fit that parameter D using the\n",
        "304": "test set namely we chose the value of D\n",
        "306": "they gave us the best possible\n",
        "310": "performance on the test set and so the\n",
        "312": "performance of my parameter vector theta\n",
        "315": "five on the test set that's likely to be\n",
        "318": "an overly optimistic estimate of\n",
        "320": "generalization error right so because I\n",
        "322": "have fit this parameter D to my test set\n",
        "326": "it is no longer fair to evaluate my\n",
        "328": "hypothesis on this test sentence because\n",
        "330": "I fit my parameters to the test set I've\n",
        "333": "chosen the degree D of polynomial using\n",
        "336": "the test set and so my hypothesis is\n",
        "338": "likely to do better on this test set\n",
        "340": "than it would on new age\n",
        "342": "apples that hasn't seen before and the\n",
        "343": "switches which is where I really care\n",
        "346": "about so just to reiterate on the\n",
        "349": "previous slide we saw that if we fit\n",
        "350": "some set of parameters you know say\n",
        "352": "theta 0 theta 1 and so on to some\n",
        "355": "training set then the performance of the\n",
        "357": "fitted model on the training set is not\n",
        "360": "predictive of how well the hypothesis\n",
        "362": "will generalize to new examples is\n",
        "364": "because these parameters will fit to the\n",
        "366": "training set so they're likely to do\n",
        "368": "well on the training set even if the\n",
        "369": "parameters don't do well on other\n",
        "373": "examples and in the procedure I just\n",
        "375": "described on the slide we just did the\n",
        "377": "same thing and specifically what we did\n",
        "380": "was we fit this parameter D to the test\n",
        "383": "set and by having fit the parameter the\n",
        "386": "test set this means that the performance\n",
        "388": "of the hypothesis on that test set may\n",
        "390": "not be a fair estimate of how well the\n",
        "393": "hypothesis is likely to do on examples\n",
        "396": "we haven't seen the form to address this\n",
        "399": "problem in a model selection setting if\n",
        "401": "we want to evaluate the hypothesis this\n",
        "404": "is what we usually do instead given the\n",
        "407": "data set instead of just putting it into\n",
        "409": "a training test set what we're going to\n",
        "411": "do is instead split it into three pieces\n",
        "415": "and the first piece is going to be\n",
        "419": "called the training set as usual so let\n",
        "421": "me call this first part the training set\n",
        "425": "and then we're going to toggle the\n",
        "428": "second piece of data which is called the\n",
        "433": "cross-validation set\n",
        "435": "and I'm going to abbreviate\n",
        "439": "cross-validation CV and the second piece\n",
        "442": "of this data I'm going to call the cross\n",
        "448": "validation set cross validation and I'm\n",
        "451": "going to abbreviate cross validation at\n",
        "453": "CVS sometimes it's also called the\n",
        "455": "validation sets instead of cross\n",
        "457": "validation set and then the last problem\n",
        "461": "I'm going to call my usual test set and\n",
        "463": "a pretty pretty typical ratio in which\n",
        "465": "to split these things would be to send\n",
        "467": "60% of your data to your training set\n",
        "470": "maybe 20% to your cross-validation set\n",
        "473": "and 20% to your test set these numbers\n",
        "474": "can vary a little bit but this sort of\n",
        "478": "Asian pretty typical and so our training\n",
        "481": "sets will now be only maybe 60% of the\n",
        "484": "data and our cross-validation set or a\n",
        "487": "validation set will have some number of\n",
        "489": "examples I'm going to denote that M\n",
        "492": "subscript CV so that's the number of\n",
        "495": "Trance validation examples and that's\n",
        "497": "following our earlier notation\n",
        "500": "convention I'm going to use X I CD comma\n",
        "505": "Y I CV following our earlier notation of\n",
        "508": "convention I'm going to use X I CD comma\n",
        "512": "Y I see V to denote the I cross\n",
        "516": "validation example and finally we also\n",
        "518": "have a test set over here with M\n",
        "521": "subscript s being the number of test\n",
        "524": "examples so now that we've defined the\n",
        "526": "training validation or cross validation\n",
        "529": "and test sets we can also define the\n",
        "531": "training error cross validation error\n",
        "534": "and test error so here's my training\n",
        "536": "error and I'm just writing this as J\n",
        "539": "subscript train of data this is pretty\n",
        "541": "much the same things usually the same\n",
        "543": "thing as the J of theta that you're\n",
        "545": "writing so far this is just a training\n",
        "547": "set error new as measured on your\n",
        "550": "training set and then J subscript CV is\n",
        "552": "my cross validation error it's pretty\n",
        "553": "much what you'd expect just like the\n",
        "555": "training error except measured on\n",
        "558": "validation data set and here's my test\n",
        "562": "set error same as before so when faced\n",
        "563": "with a multiple selection problem like\n",
        "566": "this what we're going to do is instead\n",
        "569": "of using the test set to select the\n",
        "571": "model where is there going to use the\n",
        "573": "validation set or the cross-validation\n",
        "577": "set to set the model concretely we're\n",
        "579": "going to first take our first hypothesis\n",
        "583": "take this first model and say minimize\n",
        "585": "the cost function and this will give me\n",
        "588": "some parameter vector theta for the\n",
        "590": "linear model and as before but for the\n",
        "592": "superscript one just to denote that this\n",
        "595": "is the parameter for the linear model we\n",
        "596": "do the same thing for the quadratic\n",
        "599": "model get some parameter vector theta to\n",
        "601": "get some print parameter vector theta\n",
        "605": "three and so on down to say the tenth\n",
        "607": "order polynomial and what I'm going to\n",
        "610": "do is instead of testing these\n",
        "612": "hypotheses on the test set I'm instead\n",
        "613": "going to test them on the\n",
        "616": "cross-validation set when I measure J\n",
        "619": "subscript CV to see how well each of\n",
        "622": "these hypotheses do on my cross\n",
        "626": " \n",
        "629": "and then I'm going to pick the\n",
        "631": "hypothesis with the lowest class\n",
        "633": "validation error so for this example\n",
        "635": "let's say for the sake of argument that\n",
        "639": "it was my fourth order polynomial that\n",
        "642": "had the lowest cross-validation error so\n",
        "643": "in that case I'm going to pick this\n",
        "646": "fourth order polynomial model and\n",
        "649": "finally what this means is that that\n",
        "652": "parameter D remember D was the degree of\n",
        "654": "polynomial right so V equals to D equals\n",
        "657": "3 up to D equals 10 what we've done is\n",
        "659": "we fit that parameter D with set D\n",
        "661": "equals 4 and we did so using the\n",
        "664": "cross-validation set and so this degree\n",
        "666": "of polynomial so the parameter is no\n",
        "669": "longer fit to the test set and so we've\n",
        "673": "now saved away the test set and we can\n",
        "676": "use the test set to measure or to\n",
        "678": "estimate the generalization error of the\n",
        "680": "model that was selected by this\n",
        "682": "algorithm\n",
        "685": "so that was model selection and how you\n",
        "687": "can take your data is split into a\n",
        "690": "training validation and test set and use\n",
        "692": "your cross-validation data to select a\n",
        "694": "model and evaluate it on the test set\n",
        "697": "one final note on I should say that in\n",
        "700": "the machine learning as in this practice\n",
        "703": "today there are many people that will do\n",
        "705": "earlier thing that I talked about and\n",
        "706": "said that you know isn't such a good\n",
        "709": "idea of selecting your model using the\n",
        "712": "test set and then using the same test\n",
        "714": "set to report the error as though\n",
        "717": "selecting your degree of polynomial on\n",
        "719": "the test set and then reporting the\n",
        "720": "error on the test set as though that\n",
        "722": "were a good estimate of generalization\n",
        "724": "error that's how the practice is\n",
        "726": "unfortunately maybe many people do do it\n",
        "728": "if you have a massive massive test set\n",
        "730": "this may be not a terrible thing to do\n",
        "733": "but many practitioners most\n",
        "736": "practitioners of machine learning tend\n",
        "738": "to advise against that and this because\n",
        "740": "the better practice they have separate\n",
        "742": "train validation and test sets will just\n",
        "744": "warn you that sometimes people do you\n",
        "746": "know use the same data for the purpose\n",
        "748": "of the validation set and for the\n",
        "750": "purpose of test sets you only have a\n",
        "752": "training set and the test set that's\n",
        "753": "because there's good practice but you\n",
        "756": "will see if some people do it but if\n",
        "757": "possible I would recommend against doing\n"
    },
    "wGw6R8AbcuI": {
        "0": " \n",
        "2": "in the previous video I talked about\n",
        "5": "error analysis and the importance of\n",
        "8": "having error metrics that is of having a\n",
        "11": "single real number evaluation metric for\n",
        "12": "your learning algorithm to tell how well\n",
        "16": "is doing in the context of evaluation\n",
        "18": "and of error metrics there's one\n",
        "20": "important case where is particularly\n",
        "23": "tricky to come up with any appropriate\n",
        "26": "error metric or evaluation metric for\n",
        "29": "your learning algorithm that case is the\n",
        "32": "case of what's called skewed classes let\n",
        "36": "me tell you what that means considering\n",
        "38": "the problem of cancer classification\n",
        "41": "where we have features of medical\n",
        "43": "patients and we want to decide whether\n",
        "45": "or not they have cancer so this is like\n",
        "48": "the malignant versus benign tumor\n",
        "50": "classification example that we had\n",
        "52": "earlier so let's say y equals 1 of the\n",
        "54": "patient has cancer and y equals 0 if\n",
        "57": "they do not we might train a logistic\n",
        "60": "regression classifier and let's say we\n",
        "62": "test our classifier on a test set and\n",
        "64": "find that we get one percent error so\n",
        "68": "we're making 99% correct diagnosis seems\n",
        "69": "like a really impressive as our rate\n",
        "72": "would correct 99% of the time but now\n",
        "76": "let's say we find out that only 0.5% of\n",
        "79": "patients in our training and test sets\n",
        "80": "actually have cancer\n",
        "82": "so only half a percentage of the\n",
        "84": "patients that come through our screening\n",
        "88": "process have cancer in this case the 1\n",
        "89": "percent error no longer not so\n",
        "92": "impressive and the particular here's a\n",
        "93": "piece of code she is actually a piece of\n",
        "96": "non learning code that takes us in for\n",
        "98": "the features X and then ignores it it\n",
        "101": "just says y equals 0 and always predicts\n",
        "104": "the Oh nobody has cancer and this\n",
        "107": "algorithm would actually get 0.5 percent\n",
        "110": "error so this is even better than the 1\n",
        "112": "percent error that we're getting just\n",
        "113": "now and this is a non learning algorithm\n",
        "114": "now and this is a non learning algorithm\n",
        "115": "as you know it's just predicting y\n",
        "119": "equals 0 all the time so the setting of\n",
        "122": "when the ratio of positive to negative\n",
        "125": "examples is very close to one of the two\n",
        "127": "extremes where in\n",
        "130": "case the number of positive examples is\n",
        "131": "much much smaller than the number of\n",
        "134": "negative examples because y equals 1 so\n",
        "137": "rarely this is what we call the case of\n",
        "141": "skewed classes we just have a lot more\n",
        "144": "of examples from one class than from the\n",
        "146": "other class and by just predicting y\n",
        "148": "equals 0 all the time or maybe right\n",
        "150": "predicting y equals 1 all the time and\n",
        "153": "however we can do pretty well so the\n",
        "155": "problem with using classification error\n",
        "157": "or classification accuracy as our\n",
        "160": "evaluation metric is the following let's\n",
        "162": "say you have one learning algorithm\n",
        "166": "that's getting 99.2% accuracy so that's\n",
        "171": "a 1.8 percent error let's say you make a\n",
        "173": "change to your algorithm and you now are\n",
        "180": "getting 99.5% accuracy that is 0.5\n",
        "183": "percent error\n",
        "186": "so is this an improvement to the\n",
        "188": "algorithm or not one of the nice things\n",
        "190": "about having a single real number\n",
        "192": "evaluation metric because this helps us\n",
        "194": "to quickly decide if we just made a good\n",
        "196": "change the algorithm or not by going\n",
        "200": "from 99.2% accuracy to 99.5% accuracy\n",
        "202": "you know that we just do something\n",
        "205": "useful or do we just replace our codes\n",
        "207": "or something that just predicts y equals\n",
        "210": "0 more often so if you have very skewed\n",
        "213": "classes it becomes much harder to use\n",
        "216": "just classification accuracy because you\n",
        "217": "can get very high classification\n",
        "221": "accuracies or very low errors and it's\n",
        "224": "not always clear if doing so as really\n",
        "226": "improving the quality of your classifier\n",
        "228": "because predicting y equals 0 all the\n",
        "231": "time is doesn't seem that could better\n",
        "234": "like a particularly good classifier but\n",
        "236": "just predicting y equals 0 more often\n",
        "239": "can bring your error down to you know\n",
        "242": "as low as 0.5% what would faiths are\n",
        "245": "such a few classes therefore we would\n",
        "247": "want to come up with a different error\n",
        "249": "measuring or a different evaluation\n",
        "252": "metric one such evaluation metric are\n",
        "255": "what's called precision we call let me\n",
        "258": "explain what that is let's say we're\n",
        "260": "evaluating a classifier on the test set\n",
        "263": "for the examples in the test set the\n",
        "267": "actual class of that example in the test\n",
        "270": "set is going to be here 1 or 0 right if\n",
        "271": "set is going to be here 1 or 0 right if\n",
        "272": "there's a binary classification problem\n",
        "275": "and what our learning algorithm will do\n",
        "279": "is it will you know predict some value\n",
        "282": "for the class and our learning algorithm\n",
        "284": "will predict the value for each example\n",
        "287": "my test set and the predicted value will\n",
        "291": "also be either 1 or 0 so let me draw a\n",
        "294": "two-by-two table as follows depending on\n",
        "296": "and I'll fill in these entries depending\n",
        "298": "on what was the actual class and was the\n",
        "301": "predicted class if we have an example\n",
        "303": "where the actual class is 1 and the\n",
        "306": "predicted class is 1 then that's called\n",
        "309": "a an example that's a true positive\n",
        "311": "meaning our hovervan predicted that is\n",
        "313": "positive and in reality the example is\n",
        "316": "positive if our learning algorithm\n",
        "318": "predicted that something is negative\n",
        "320": "called 0 and the actual class is also\n",
        "323": "called 0 then that's what's called a\n",
        "324": "true negative we predict\n",
        "328": "zero and it actually is zero to learn\n",
        "330": "the other two boxes if our learning\n",
        "332": "algorithm predicts that the cost is 1\n",
        "336": "but the actual class is zero then that's\n",
        "339": "called a false positive so that means\n",
        "341": "our algorithm thought the patient has\n",
        "343": "cancelled up in the reality the patient\n",
        "347": "not and finally the last box is a zero\n",
        "350": "one that's called a false negative\n",
        "354": "because our algorithm predicted zero but\n",
        "359": "the actual class was one and so we have\n",
        "363": "this law to the 2x2 table based on what\n",
        "364": "was the actual class and what was the\n",
        "368": "predicted class so here's a different\n",
        "370": "way of evaluating the performance of our\n",
        "372": "algorithm we're going to compute two\n",
        "374": "numbers the first is called precision\n",
        "377": "and what that says is of all the\n",
        "379": "patients where we predicted that they\n",
        "381": "have cancer what fraction of them\n",
        "385": "actually have cancer so let me write\n",
        "387": "this down the precision of a classifier\n",
        "391": "is the number of true positives divided\n",
        "396": "by the number that we predicted as\n",
        "400": "positive right so if all the patients\n",
        "401": "that you know we went to those patients\n",
        "403": "that we told them we think you have\n",
        "405": "cancer of all those patients what\n",
        "407": "fraction of them actually have cancer so\n",
        "410": "that's called precision and another way\n",
        "413": "to write this would be true positives\n",
        "416": "and then in the denominator is the\n",
        "419": "number of predicted positives and so\n",
        "421": "that will be the sum of you know the\n",
        "424": "entries in this first row of the table\n",
        "426": "so being true positives divided by\n",
        "427": "so being true positives divided by\n",
        "429": "through positives I'm going to\n",
        "431": "abbreviate abbreviate positive as POS\n",
        "435": "and then plus false positives again\n",
        "440": "abbreviating positive using POS so\n",
        "442": "that's called precision and Intel high\n",
        "444": "precision would be good that means\n",
        "445": "they've got all the patients that we\n",
        "446": "went to we said you know we're very\n",
        "448": "sorry we think your cancer higher\n",
        "450": "precision means that the that tallying\n",
        "453": "of that group of patients most of them\n",
        "454": "we had actually made accurate\n",
        "456": "predictions on them\n",
        "459": "they do have cancer the second number\n",
        "461": "we're gonna compute to this count recall\n",
        "464": "and what we call says is if all the\n",
        "466": "patients in it's the same the test set\n",
        "468": "or in the cross-validation set but of\n",
        "470": "all the patients in the dataset that\n",
        "473": "actually have cancer what fraction of\n",
        "476": "them that we correctly detect as having\n",
        "477": "cancer so if all the patients have\n",
        "479": "cancer how many of them did we actually\n",
        "481": "go to them and you know correctly tell\n",
        "485": "them that we think they need treatment\n",
        "488": "so writing this down recall is defined\n",
        "491": "as the number of positives that excuse\n",
        "493": "me the number of true positives meaning\n",
        "496": "right true positive meaning the number\n",
        "498": "of people they have cancer and that we\n",
        "500": "correctly predicted half cancer and we\n",
        "503": "take that and divide that by divide that\n",
        "510": "by the number of actual positives so\n",
        "513": "this is the right number of actual\n",
        "515": "positives of all the people to do have\n",
        "517": "cancer what fraction do we correctly\n",
        "521": "flag and you know center treatment so to\n",
        "522": "rewrite this in a different form the\n",
        "525": "denominator would be number of actual\n",
        "527": "positives as you know the sum of the\n",
        "529": "entries in this first column over here\n",
        "532": "and sell writing this out differently\n",
        "533": "this is therefore the number of true\n",
        "541": "positives divided by the number of true\n",
        "546": "positives plus the number\n",
        "550": "false negatives and so once again having\n",
        "553": "a high recalls would be a good thing\n",
        "556": "so by computing precision and recall\n",
        "558": "this will usually give us a better sense\n",
        "561": "of how well our classifier is doing and\n",
        "563": "in particular if we have a learning\n",
        "566": "algorithm that predicts y equals zero\n",
        "568": "all the time and it predicts no one has\n",
        "569": "cancer\n",
        "572": "then this classifier will have a recall\n",
        "575": "equal to zero because there won't be any\n",
        "577": "true positives and so that's a quick way\n",
        "579": "to for us to recognize that you know a\n",
        "581": "classifier that predicts y equals zero\n",
        "583": "all the time just isn't a very good\n",
        "585": "classifier\n",
        "588": "and more generally even for settings\n",
        "592": "where we have very skewed classes is not\n",
        "594": "possible for an algorithm to sort of\n",
        "596": "quote cheat and somehow get a very high\n",
        "599": "position and a very high recall by doing\n",
        "601": "some simple thing like predicting y\n",
        "603": "equals 0 all the time or predicting y\n",
        "604": "equals 1 all the time\n",
        "607": "and so we're much more sure that a\n",
        "609": "classifier with high precision high\n",
        "612": "recall actually is a good classifier and\n",
        "614": "discus gives us a more useful evaluation\n",
        "616": "metric there's more direct way to\n",
        "619": "actually understand whether you know our\n",
        "622": "algorithm may be doing well so one final\n",
        "624": "note in the definition of precision and\n",
        "627": "recall that we would define precision\n",
        "629": "and recall usually we use the convention\n",
        "632": "that Y is equal to 1 in the presence of\n",
        "635": "the more rare class so if we're trying\n",
        "636": "to detect some rare conditions such as\n",
        "638": "cancer hopefully that's a rare condition\n",
        "641": "for just precision and recall are\n",
        "644": "defined setting y equals 1 rather than y\n",
        "646": "equals 0 to be so the presence of that\n",
        "648": "rare class that we're trying to detect\n",
        "653": "and by using precision and recall we\n",
        "655": "found what happens is that even if we\n",
        "658": "have very few classes it's not possible\n",
        "660": "for an algorithm to you know quote\n",
        "662": "cheats and putting white because 1 all\n",
        "664": "the time will predict y equals 0 all the\n",
        "666": "time and get high precision a recall and\n",
        "668": "in particular if a classifier is getting\n",
        "671": "high position and high recall then we're\n",
        "673": "actually confident that the algorithm\n",
        "675": "has to be doing well even if we have\n",
        "679": "very few constants so for the promised\n",
        "681": "few classes precision precision and\n",
        "684": "recall gives us more direct insight into\n",
        "686": "how the learning algorithm is doing and\n",
        "688": "there's often a much better way to\n",
        "690": "evaluate our learning algorithms then\n",
        "692": "looking at classification error\n",
        "694": "mission accuracy when the classes are\n"
    },
    "xI9-I-gcwaw": {
        "0": " \n",
        "2": "in an earlier video I had said that PCA\n",
        "4": "can be sometimes used to speed up the\n",
        "6": "running time of a learning algorithm in\n",
        "9": "this video I'd like to explain how to\n",
        "12": "actually do that and also say some also\n",
        "14": "just try to get some advice about how to\n",
        "18": "apply PCA here's how you can use PCA to\n",
        "20": "speed up a learning algorithm and this\n",
        "22": "is a supervised learning algorithm speed\n",
        "25": "up is actually the most common user that\n",
        "28": "you know I personally make of PCA let's\n",
        "30": "say you have a supervised learning\n",
        "31": "problem notice a supervised learning\n",
        "32": "problem notice a supervised learning\n",
        "35": "problem with infos X and labels Y and\n",
        "38": "let's say that your example is X I I'm\n",
        "40": "very high dimensional so let's say X I\n",
        "44": "are you know 10,000 dimensional feature\n",
        "46": "vectors one example of that would be if\n",
        "48": "you were doing some computer vision\n",
        "51": "problem where you have 100 100 images\n",
        "55": "and so if you have 100 100 that's 10,000\n",
        "58": "pixels and so if X I are you know\n",
        "61": "feature vectors that contain your 10,000\n",
        "64": "pixel intensity values then you have\n",
        "67": "10,000 dimensional feature vectors so\n",
        "69": "with very high dimensional feature\n",
        "71": "vectors like this a running learning a\n",
        "73": "learning algorithm can be slow right\n",
        "74": "just if you fee you know 10,000\n",
        "76": "dimensional feature vectors into\n",
        "78": "logistic regression or a neural network\n",
        "79": "or support vector machine or what have\n",
        "82": "just because that's a lot of data that's\n",
        "84": "10,000 numbers um it can make your\n",
        "86": "learning algorithm run more slowly\n",
        "88": "fortunately with PCA we'll be able to\n",
        "90": "reduce the dimension of this data and so\n",
        "93": "make our algorithms run more efficiently\n",
        "96": "here's how you do that we're going to\n",
        "98": "first take our labeled training set and\n",
        "100": "extract just the input so I'm just going\n",
        "103": "to extract the X's and you know\n",
        "106": "temporarily put aside the Y's so this\n",
        "108": "will now give us an unlabeled training\n",
        "111": "set this x1 through XM which are maybe\n",
        "114": "this of 10,000 dimensional data 10,000\n",
        "115": "dimensional example\n",
        "118": "so just extract the input vectors x1\n",
        "122": "through XM then we're going to apply PCA\n",
        "124": "and this will give me a reduced\n",
        "126": "dimension representation of data so\n",
        "128": "instead of 10,000 dimensional feature\n",
        "129": "vectors\n",
        "131": "I now have maybe one thousand\n",
        "132": "dimensional feature vectors so let's\n",
        "136": "look at 10x savings so this gives me if\n",
        "138": "you will a new training set so whereas\n",
        "140": "previously I might have had an example\n",
        "143": "x1 y1 my first training input is now\n",
        "145": "represented by z1 instead we'll have a\n",
        "148": "new sort of training example which is z1\n",
        "151": "pairs of y1 and similarly zi2 y2 and so\n",
        "154": "on up to z my m because my training\n",
        "156": "examples are now represented with this\n",
        "159": "much lower dimensional representation z1\n",
        "162": "z2 up to ZM finally I can take this sum\n",
        "165": "reduce dimension training set and feed\n",
        "167": "it to a learning algorithm maybe our new\n",
        "169": "network may be logistic regression and I\n",
        "172": "can learn the hypothesis H that takes us\n",
        "174": "input these low dimensional\n",
        "176": "representations Z and tries to make\n",
        "179": "predictions and so if I were using\n",
        "181": "logistic regression for example I would\n",
        "183": "train a hypothesis that outputs you know\n",
        "185": "1 over 1 plus e to the negative theta\n",
        "190": "transpose Z or theta transpose it that\n",
        "192": "takes this input to one of these Z\n",
        "193": "vectors and tries to make a prediction\n",
        "197": "and finally if you have a new example\n",
        "200": "maybe a new test example X what you do\n",
        "203": "is you would take your new test example\n",
        "207": "X map it through the same mapping that\n",
        "208": "was found by PCA to get you your\n",
        "212": "corresponding Z and that Z then get fed\n",
        "214": "to this hypothesis and this hypothesis\n",
        "217": "then makes a prediction on your in place\n",
        "220": "one final note what PCA does is it\n",
        "226": "defines a mapping from X to Z and this\n",
        "228": "mapping from X to Z should be defined by\n",
        "231": "running PCA only on the training set and\n",
        "234": "in particular this mapping that PCA is\n",
        "236": "learning right this mapping what that\n",
        "237": "does is it computes the scylla\n",
        "239": "parameters right that's the feature\n",
        "241": "scaling and mean normalization and\n",
        "243": "there's also computing this matrix you\n",
        "245": "reduce but all of these things that you\n",
        "248": "views that's like a parameter that is\n",
        "250": "learned by PCA and we should be fitting\n",
        "253": "our parameters only to our training set\n",
        "254": "and not to our cross-validation\n",
        "257": "intestines and so these things that you\n",
        "259": "reduced and so on that should be\n",
        "262": "obtained by running PCA only on your\n",
        "264": "training set and then having found new\n",
        "266": "reduce or having found you know the\n",
        "268": "parameters for feature scaling or the\n",
        "272": "mean normalization and scaling the scale\n",
        "273": "that you divide the features by to get\n",
        "275": "them onto couple scales but having found\n",
        "277": "all those parameters on the training set\n",
        "280": "you can then apply the same mapping to\n",
        "282": "other examples that may be in your\n",
        "285": "cross-validation set or in your or in\n",
        "287": "your test set episode just to summarize\n",
        "290": "when you're running PCA run your PCA\n",
        "292": "only on the training set portion of your\n",
        "294": "data not the cross-validation set or the\n",
        "296": "tested portion in your data and that\n",
        "298": "defines the mapping from X to Z you can\n",
        "300": "then apply that mapping to your\n",
        "302": "cross-validation set and your test set\n",
        "304": "and by the way in this example I talked\n",
        "307": "about reducing the data from 10,000\n",
        "310": "dimensional to 1,000 mention all this is\n",
        "312": "actually not that unrealistic for many\n",
        "314": "problems we can actually reduce the\n",
        "316": "dimensional data you know by 5 X maybe\n",
        "319": "by 10 X and still retain most of the\n",
        "320": "variance then we can do this without\n",
        "323": "barely affecting the performance in\n",
        "325": "terms of classification accuracy let's\n",
        "328": "say barely affecting the classification\n",
        "330": "accuracy of the learning algorithm and\n",
        "333": "by working with low dimensional data a\n",
        "334": "learning algorithm can often run much\n",
        "337": "much faster to summarize we've so far\n",
        "340": "talked about the following applications\n",
        "342": "of PCA first is the compression\n",
        "344": "application where we might do so to\n",
        "346": "reduce the memory or the disk space\n",
        "348": "needed to slow data and we just talked\n",
        "350": "about how to use this to speed up a\n",
        "351": "learning hour\n",
        "354": "in these applications in order to choose\n",
        "359": "K often will do so according to figuring\n",
        "361": "out what is the percentage of variance\n",
        "365": "retained and so it's but this is a\n",
        "367": "learning out from speed-up application\n",
        "370": "you know often will maintain 99% of the\n",
        "371": "variance that would be a very typical\n",
        "374": "choice for how to choose okay so that's\n",
        "376": "how you choose K for these compression\n",
        "379": "applications whereas for visualization\n",
        "382": "applications well usually we know how to\n",
        "385": "plot only two dimensional data or three\n",
        "387": "dimensional data and so for\n",
        "388": "visualization applications we would\n",
        "391": "usually choose K equals 2 or K equals 3\n",
        "392": "because you know you can plot them be 2d\n",
        "395": "and 3d data sets so that summarizes the\n",
        "398": "main applications of PCA as well as how\n",
        "401": "to choose the value of K for these\n",
        "403": "different applications I should mention\n",
        "404": "that there's often that there's one\n",
        "408": "frequent width use of PCA and you\n",
        "411": "sometimes hear about others doing this\n",
        "412": "hopefully not too often but as I\n",
        "414": "mentioned this so that you know not to\n",
        "416": "do it and there's one bad use of PCA\n",
        "419": "which is to try to use it to prevent\n",
        "421": "overfitting here's the reasoning this is\n",
        "423": "not dissing race it's not a great way to\n",
        "425": "use PCA but here's the reasoning behind\n",
        "428": "this method which is you know if we have\n",
        "430": "X I then maybe we'll have n features but\n",
        "433": "if we compress the data and use zi0\n",
        "435": "instead then that reduces the number of\n",
        "438": "features to K which could be much lower\n",
        "441": "dimensional and so if we have a much\n",
        "443": "smaller number of features of K as you\n",
        "447": "know mm and n is 10,000 then well if we\n",
        "448": "have only a thousand dimensional data\n",
        "449": "have only a thousand dimensional data\n",
        "451": "maybe we're less likely to overfit and\n",
        "453": "if we were using 10,000 dimensional data\n",
        "455": "with like a thousand features you know\n",
        "458": "and so some people think of PCA as a way\n",
        "461": "to prevent overfitting but just\n",
        "463": "emphasize this is a bad application of\n",
        "466": "PCA and I do not recommend doing this\n",
        "468": "and it's not that this method works\n",
        "470": "badly you know if you were to use this\n",
        "472": "method to reduce the dimensional data\n",
        "474": "to try to prevent over-fitting it might\n",
        "476": "actually work okay but this just is not\n",
        "479": "a good way to address overfitting and\n",
        "480": "instead if you're worried about\n",
        "483": "overfitting is a much better way to\n",
        "484": "address it to use the regularization\n",
        "487": "instead of using PCA to reduce the\n",
        "490": "dimension of the data and the reason is\n",
        "493": "if you think about how PCA works it does\n",
        "495": "not use the labels why right now you're\n",
        "497": "just looking at your inputs X I and\n",
        "499": "you're using that to find a lower\n",
        "501": "dimensional approximation to your data\n",
        "504": "so what PCA does is it throws away some\n",
        "507": "of some information it throws away or\n",
        "509": "reduces the dimension of your data\n",
        "511": "without knowing what the values of Y is\n",
        "515": "so this is probably okay using PCA this\n",
        "519": "way is probably okay if say 99% of the\n",
        "520": "variance obtained if you're keeping most\n",
        "523": "of the variance but it might also throw\n",
        "525": "away some valuable information and it\n",
        "526": "turns out that you know if you're\n",
        "529": "retaining 99% of the variance or 95% of\n",
        "531": "the Bairns or whatever it turns out that\n",
        "534": "just using regularization will often\n",
        "537": "give you at least as good a method for\n",
        "538": "preventing overfitting and\n",
        "540": "regularization will often work just work\n",
        "542": "better because when you're applying\n",
        "544": "linear regression or which is regression\n",
        "546": "or some other method with regularization\n",
        "548": "well this minimization problem actually\n",
        "551": "knows what the values of Y are and so is\n",
        "553": "less likely to throw away some valuable\n",
        "555": "information whereas PCA doesn't make use\n",
        "557": "of the labels and it's more likely to\n",
        "560": "throw away valuable information so just\n",
        "562": "summarize it is a good use of pca if\n",
        "564": "your main motivation is to speed up your\n",
        "567": "learning algorithm but using PCA to\n",
        "569": "prevent overfitting that's definitely\n",
        "571": "good use of PCA and using regularization\n",
        "575": "instead this is really what what most\n",
        "577": "people what many people will recommend\n",
        "582": "doing this fit finally one loss list use\n",
        "585": "of PCA so I should say PCA is a very\n",
        "587": "useful algorithm I often use it you know\n",
        "589": "for the compression of the visualization\n",
        "592": "purposes but what I sometimes see is\n",
        "594": "also people sometimes use PCA where it\n",
        "595": "shouldn't be\n",
        "598": "so here's a pretty common thing that I\n",
        "599": "see right which is the someone's\n",
        "600": "designing machine learning system you\n",
        "602": "know they may break down the plan like\n",
        "604": "this you know let's design a learning\n",
        "606": "system get a training set and then you\n",
        "608": "know what I'm going to do is run PCA\n",
        "610": "then train which is Russian in the test\n",
        "613": "online test data so often at the very\n",
        "614": "start of a project someone will just\n",
        "617": "write out a project plan that says let's\n",
        "619": "do these small steps with pca inside\n",
        "622": "before writing on a project plan that\n",
        "625": "incorporates pca like this one very good\n",
        "627": "question to ask is well what if we were\n",
        "630": "to just do the whole thing without using\n",
        "634": "PCA and often people do not consider\n",
        "636": "this step before like coming up with a\n",
        "637": "complicated project planning and\n",
        "641": "implementing PCA and so on and some time\n",
        "643": "and so specifically what I often often\n",
        "644": "and so specifically what I often often\n",
        "646": "advise people is before you implement\n",
        "649": "PCA I would first suggest that you know\n",
        "651": "do whatever it is take whatever is you\n",
        "653": "want to do and first consider doing it\n",
        "656": "with your original raw data X I and only\n",
        "658": "if that doesn't do what you want dint\n",
        "660": "influent PC and consider using some\n",
        "662": "other so before using PCA you know\n",
        "664": "instead of reducing the dimension of the\n",
        "667": "data I would consider well let's ditch\n",
        "670": "this PCA step and I would consider let's\n",
        "672": "just train my learning algorithm on my\n",
        "675": "original data let's just use my original\n",
        "678": "row a plus X I and I would recommend\n",
        "680": "instead of putting PCA into the\n",
        "682": "algorithm just try doing whatever it is\n",
        "684": "you're doing with the X I first and only\n",
        "686": "if you have a reason to believe that\n",
        "687": "doesn't work so if only if your learning\n",
        "690": "algorithm ends up running too slowly or\n",
        "692": "only if the memory requirement or the\n",
        "694": "disk space requirement is too large so\n",
        "696": "you want to compress your representation\n",
        "699": "that only if using the X I doesn't work\n",
        "700": "only if you have evidence or a strong\n",
        "702": "reason to believe that using the X I\n",
        "705": "won't work then implement PCA and\n",
        "706": "consider using the compress\n",
        "708": "representation because what I do see is\n",
        "710": "sometimes people start off with a\n",
        "713": "the in Corpus PC inside you know\n",
        "716": "sometimes they whatever they're doing\n",
        "718": "with it work just fine even without\n",
        "721": "using PC instead so just consider that\n",
        "723": "as an alternative as well before you go\n",
        "725": "and spend a long time to influence you\n",
        "727": "and figure out what K is and so so\n",
        "730": "that's if a PCA um just like these last\n",
        "732": "sets of comments PCA is an incredibly\n",
        "734": "useful algorithm when you use it for the\n",
        "736": "appropriate applications and I should\n",
        "740": "use PCA pretty often and for me I use it\n",
        "741": "mostly to speed up the running time of\n",
        "743": "my learning algorithms but I think\n",
        "745": "almost as common an application of PCA\n",
        "749": "is to use it to compress data to reduce\n",
        "751": "the memory of disk space requirements or\n",
        "755": "to use it to visualize data and PC is\n",
        "757": "one of the most commonly used among the\n",
        "758": "most powerful unsupervised learning\n",
        "761": "algorithms and with what you've learned\n",
        "763": "in these videos I think hopefully you'll\n",
        "766": "be able to implement PCA and use them to\n"
    },
    "x_Eamf8MHwU": {
        "0": " \n",
        "1": "in the previous video we talked about a\n",
        "3": "cost function for the neural network in\n",
        "6": "this video let's start to talk about an\n",
        "8": "algorithm for trying to minimize the\n",
        "10": "cost function in particular we'll talk\n",
        "13": "about the back propagation algorithm\n",
        "15": "here's the cost function that we wrote\n",
        "18": "down in the previous video what we'd\n",
        "20": "like to do is try to find parameters\n",
        "23": "theta to try to minimize J of theta in\n",
        "25": "order to use either gradient descent or\n",
        "27": "one of the advanced optimization\n",
        "29": "algorithms what we need to do therefore\n",
        "32": "is to my code that takes us input the\n",
        "34": "parameters theta and computes J of theta\n",
        "36": "and these partial derivative terms\n",
        "38": "remember that the parameters of the\n",
        "40": "neural network of these things theta\n",
        "43": "superscript L subscript IJ that that's a\n",
        "46": "real number and so these are the partial\n",
        "48": "derivative terms we need to compute in\n",
        "50": "order to compute the cost function J of\n",
        "53": "theta we just use this formula up here\n",
        "56": "and so what I want to do for most of\n",
        "58": "this video is focused on talking about\n",
        "59": "how we can compute these partial\n",
        "62": "derivative terms let's start by talking\n",
        "65": "about the case of when we have only one\n",
        "67": "training example so imagine if you will\n",
        "70": "that our entire training set comprises\n",
        "72": "only one training example which is a\n",
        "75": "pair XY not going to write x1 y1 just\n",
        "77": "writers right at one training example s\n",
        "80": "XY and let's step through the sequence\n",
        "82": "of calculations we would do with this\n",
        "84": "one training example\n",
        "87": "the first thing we do is we apply\n",
        "90": "forward propagation in order to compute\n",
        "92": "what our hypothesis actually outputs\n",
        "96": "given this input X concretely remember\n",
        "98": "recall that a1 is the activation values\n",
        "101": "of this first layer that is the input\n",
        "102": "layer so I'm going to set that to X and\n",
        "106": "then we're going to compute Z 2 equals\n",
        "109": "theta 1 a 1 and a 2 equals G the sigmoid\n",
        "110": "theta 1 a 1 and a 2 equals G the sigmoid\n",
        "112": "activation function applied to Z 2 and\n",
        "114": "this will give us our activations for\n",
        "116": "the first hidden layer that is for layer\n",
        "119": "2 of the network and we also add those\n",
        "122": "bias terms next we apply two more steps\n",
        "125": "of this forward propagation to compute a\n",
        "131": "3 and a 4 which is also the output of a\n",
        "135": "hypothesis it connects so this is our\n",
        "138": "vectorized implementation of forward\n",
        "140": "propagation and allows us to compute the\n",
        "143": "activation values for all of the neurons\n",
        "149": "in our neural network next in order to\n",
        "150": "compute the derivatives we're going to\n",
        "154": "use an algorithm called back propagation\n",
        "156": "the intuition of the back propagation\n",
        "159": "algorithm is that for each node we're\n",
        "161": "going to compute the term Delta\n",
        "164": "superscript L subscript J that's going\n",
        "167": "to somehow represent the error of node J\n",
        "169": "in layer L so recall that any\n",
        "172": "superscript L subscript J that that's\n",
        "175": "the activation of the JV unit in layer L\n",
        "178": "and so this Delta term is in some sense\n",
        "181": "going to capture our error in the\n",
        "184": "activation of that node or so how we\n",
        "186": "might wish the activation of that node\n",
        "188": "were slightly different concretely\n",
        "191": "taking the example neural network that\n",
        "192": "we have on the right which has four\n",
        "195": "layers and so capital L is equal to four\n",
        "197": "for each output unit we're going to\n",
        "200": "compute this Delta term so Delta for the\n",
        "202": "JV unit in the fourth layer is equal to\n",
        "205": "just the activation of that unit\n",
        "207": "- what was the actual value observed in\n",
        "211": "our training example so this term here\n",
        "216": "is can also be written H of X subscript\n",
        "219": "J right so this Delta term is just the\n",
        "221": "difference between what our hypothesis\n",
        "225": "outputs and what was the value of y in\n",
        "227": "our training set where this Y subscript\n",
        "230": "J is the jave element of the vector\n",
        "235": "value Y in our labeled training set and\n",
        "240": "by the way if you think of Delta a and Y\n",
        "243": "as vectors then you can also take this\n",
        "244": "and come up with a vectorized\n",
        "248": "implementation of it which is just Delta\n",
        "254": "for get set as a for - why we're here\n",
        "257": "each of these Delta for a for and why\n",
        "260": "each of these is a vector whose\n",
        "262": "dimension is equal to the number of\n",
        "264": "output units in your network\n",
        "268": "so we've now computed the error terms\n",
        "271": "Delta for for our network what we do\n",
        "274": "next is compute the Delta terms for the\n",
        "277": "earlier layers in our network here's the\n",
        "279": "formula for computing Delta 3 is Delta 3\n",
        "281": "is equal to theta 3 transpose times\n",
        "284": "Delta 4 and this dot times this is the\n",
        "287": "element wise multiplication operation\n",
        "289": "that we know from MATLAB so Delta 3\n",
        "293": "transpose Delta fold as a vector Z prime\n",
        "297": "Z 3 that's also a vector and so that x\n",
        "298": "is an element wise multiplication\n",
        "300": "between these two vectors\n",
        "301": "between these two vectors\n",
        "304": "this term G prime of Z 3 that formally\n",
        "306": "is actually the derivative of the\n",
        "308": "activation function G evaluated at the\n",
        "312": "input values given by Z 3 if you know\n",
        "313": "calculus you can try to work it out\n",
        "315": "yourself and see if you can simplify it\n",
        "316": "in the same answer that I get\n",
        "319": "but so I'll just tell you pragmatically\n",
        "321": "what that means what you do to compute\n",
        "323": "this G prime these derivative terms is\n",
        "328": "just a 3 dot times 1 minus 8 3 where a 3\n",
        "331": "is the vector of activations 1 is a\n",
        "334": "vector once and a 3 is again\n",
        "337": "deactivation the vector of activation\n",
        "339": "values for that layer next you apply a\n",
        "343": "similar formula to compute Delta 2 where\n",
        "346": "again that can be computed using a\n",
        "349": "similar formula or you know this a 2\n",
        "354": "like so and I didn't prove it here but\n",
        "357": "you can actually possible to prove you\n",
        "359": "know calculus that this expression is\n",
        "361": "equal to mathematically the derivative\n",
        "363": "of the G function of the activation\n",
        "366": "function which I'm denoting by G Prime\n",
        "371": "and finally that's it and there is no\n",
        "374": "delta 1 term because the first layer\n",
        "376": "corresponds to the input layer and\n",
        "378": "that's just the features we observed in\n",
        "379": "our training set so that doesn't have\n",
        "381": "any error associated with it it's not\n",
        "383": "like we don't really want to try to\n",
        "385": "change those values and so we have Delta\n",
        "388": "terms only for layers 2 3 &amp; 4 in this\n",
        "389": "example\n",
        "392": "the name backpropagation comes from the\n",
        "394": "fact that we start with computing the\n",
        "396": "Delta term for the output layer and then\n",
        "398": "we go back a layer and compute the Delta\n",
        "400": "terms for the third hidden layer and\n",
        "402": "then we go back another step to compute\n",
        "404": "Delta 2 and so we're sort of back\n",
        "406": "propagating the errors from the output\n",
        "409": "layer to layer 3 to layer 2 hence the\n",
        "412": "name back propagation finally the\n",
        "414": "derivation is um surprisingly\n",
        "416": "complicated and surprisingly involved\n",
        "418": "but if you just do these few steps of\n",
        "421": "computation is possible to prove very\n",
        "423": "frankly somewhat complicated\n",
        "425": "mathematical proof it's possible to\n",
        "428": "prove that if you ignore regularization\n",
        "431": "then the partial derivative terms you\n",
        "434": "want are exactly given by the\n",
        "437": "activations and these Delta terms this\n",
        "442": "is ignoring lambda or alternatively if\n",
        "445": "the regularization term lambda will\n",
        "447": "equal to 0 will fix this detail later\n",
        "449": "about the regularization term but so by\n",
        "451": "performing back propagation and\n",
        "453": "computing these Delta terms you can you\n",
        "455": "know pretty quickly compute these\n",
        "457": "partial derivative terms for all of your\n",
        "460": "parameters so this is a lot of detail\n",
        "462": "let's take everything and put it all\n",
        "465": "together to talk about how to implement\n",
        "467": "back propagation to compute derivatives\n",
        "470": "with respect to your parameters and for\n",
        "472": "the case of when we have a large\n",
        "474": "training set not just a training set of\n",
        "477": "one example here's what we do suppose we\n",
        "479": "have a training set of M examples like\n",
        "481": "that shown here the first thing we're\n",
        "484": "going to do is going to set these Delta\n",
        "487": "L subscript IJ so this triangle symbol\n",
        "490": "that's actually the capital of Greek\n",
        "492": "alphabet Delta the symbol we had on the\n",
        "494": "previous line was the lower case Delta\n",
        "497": "so the triangle is capital Delta when I\n",
        "499": "set this equal to 0 for all values of L\n",
        "504": "IJ eventually this capital Delta L IJ\n",
        "508": "will be used to compute\n",
        "511": "the partial derivative term partial\n",
        "515": "derivative respect to theta L IJ of J of\n",
        "520": "theta so as we'll see in a second these\n",
        "522": "deltas are going to be used as\n",
        "525": "accumulators that will slowly add things\n",
        "526": "to in order to compute these partial\n",
        "530": "derivatives next we're going to loop\n",
        "532": "through our training set so we'll say\n",
        "535": "for I equals 1 through m and so for the\n",
        "538": "iFit eration we're going to be working\n",
        "542": "with the training example X I comma 1 I\n",
        "544": "so the first thing I'm going to do is\n",
        "547": "set a 1 which is the activations of the\n",
        "549": "input layer set that to equal to X I\n",
        "552": "that is the inputs for our training\n",
        "555": "example and then we're going to perform\n",
        "558": "forward propagation to compute the\n",
        "561": "activations for layer 2 layer 3 and so\n",
        "564": "on up to the final layer layer capital L\n",
        "567": "next we're going to use the output label\n",
        "570": "Y I from the specific example that we're\n",
        "572": "looking at to compute the error term\n",
        "575": "Delta L for the output layer so Delta L\n",
        "578": "is what our hypotheses output minus what\n",
        "582": "the target label was and then we're\n",
        "583": "going to use the back propagation\n",
        "586": "algorithm to compute Delta L minus 1\n",
        "588": "Delta L minus 2 and so on down to Delta\n",
        "591": "2 and once again there is no Delta 1\n",
        "593": "because we don't associate an error term\n",
        "597": "with the input layer and finally we're\n",
        "600": "going to use these capital Delta terms\n",
        "603": "to accumulate these partial derivative\n",
        "605": "terms that we wrote down on the previous\n",
        "608": "line and by the way if you look at this\n",
        "610": "expression this possible to vectorize\n",
        "613": "this 2 concretely if you think of Delta\n",
        "618": "IJ as a matrix indexed by subscript IJ\n",
        "621": "then if Delta L is a matrix so you can\n",
        "624": "rewrite this as Delta L gets updated as\n",
        "625": "rewrite this as Delta L gets updated as\n",
        "629": "Delta L plus lower case Delta L plus 1\n",
        "634": "times a L transpose so that's a\n",
        "636": "vectorized implementation of this that\n",
        "638": "automatically does this update for all\n",
        "641": "values of I and J finally after\n",
        "644": "executing the body of the for loop we\n",
        "646": "did go outside the for loop and we\n",
        "648": "compute the following we'll compute\n",
        "651": "capital D as follows and we have two\n",
        "654": "separate cases for J equals 0 and J\n",
        "657": "naught equals 0 the case of J equals 0\n",
        "660": "corresponds to the bias term so when J\n",
        "661": "equals 0 that's why we're missing this\n",
        "666": "extra regularization term finally while\n",
        "668": "the formal proof is pretty complicated\n",
        "670": "what you can show is that once you've\n",
        "674": "computed these D terms that is exactly\n",
        "677": "the partial derivative of the cost\n",
        "679": "function with respect to each of your\n",
        "681": "parameters and so you can use those in\n",
        "684": "either gradient descent or in one of the\n",
        "687": "advanced optimization algorithms\n",
        "690": "so that's the backpropagation algorithm\n",
        "692": "and how you compute derivatives of\n",
        "694": "movements of your cost function for a\n",
        "696": "neural network I know this looks like it\n",
        "698": "was a lot of details and this is a lot\n",
        "700": "of steps strung together but both in the\n",
        "702": "programming assignments right out and\n",
        "704": "later in this video will give you a\n",
        "706": "summary of this so you can have all the\n",
        "708": "pieces in the algorithm together so that\n",
        "710": "you know exactly what you want need to\n",
        "712": "implement if you want to implement back\n",
        "714": "propagation to compute the derivatives\n",
        "717": "of your neural networks cost function\n"
    },
    "yR2ipCoFvNo": {
        "0": " \n",
        "1": "in the previous video we gave a\n",
        "3": "mathematical definition in the cost\n",
        "5": "function in this video let's look at\n",
        "7": "some examples to get better intuition\n",
        "9": "about what the cost function is doing\n",
        "15": "and why we want to use it to recap here\n",
        "17": "some we had last time we want to fit a\n",
        "19": "straight line to our data so we had this\n",
        "21": "form of the hypothesis with these\n",
        "24": "parameters theta 0 and theta 1 and with\n",
        "26": "different choices of the parameters we\n",
        "28": "end up with different straight line fits\n",
        "29": "to the data\n",
        "32": "each event like so and there's a cost\n",
        "34": "function and that was our optimization\n",
        "38": "objective for this video in order to\n",
        "40": "better visualize the cost function J I'm\n",
        "42": "going to work with a simplified\n",
        "44": "hypothesis function like that shown on\n",
        "45": "the right so I'm going to use my\n",
        "48": "simplified hypothesis which is just\n",
        "51": "theta 1 times X you can if you want to\n",
        "53": "think of this as setting the parameter\n",
        "56": "theta 0 equal to 0 so I have only one\n",
        "59": "parameter theta 1 and my cost function\n",
        "61": "is similar to before except that now H\n",
        "64": "of X that is now equal to just ADA 1\n",
        "67": "times X and I have only one parameter\n",
        "69": "theta one and so my optimization\n",
        "72": "objective is the minimize J of theta 1\n",
        "76": "in pictures what this means is that if\n",
        "79": "theta 0 equals 0 that corresponds to\n",
        "82": "choosing only hypothesis functions that\n",
        "84": "pass through the origin that pass\n",
        "88": "through the point 0 0 using the\n",
        "90": "simplified definition of hypothesis and\n",
        "92": "cost function let's try to understand\n",
        "96": "the cost function concept better it\n",
        "98": "turns out the two key functions we want\n",
        "100": "to understand the first is the\n",
        "102": "hypothesis function the second is the\n",
        "105": "cost function so notice that the\n",
        "109": "hypothesis right H of X for a fixed\n",
        "112": "value of theta 1 this is a function of X\n",
        "115": "so the hypothesis is a function of what\n",
        "118": "is the size of the house X and contrast\n",
        "122": "the cost function J that's a function of\n",
        "125": "the parameter theta 1 which controls the\n",
        "129": "slope of the straight line let's plot\n",
        "131": "these functions and try to understand\n",
        "132": "them both better\n",
        "135": "let's start with the hypothesis on the\n",
        "137": "left let's say here's my training set\n",
        "140": "with three points at 1 1 2 2 &amp; 3 3 let's\n",
        "142": "pick a value of theta one so one set of\n",
        "144": "theta one equals one and if that's my\n",
        "147": "choice for theta one then my hypothesis\n",
        "149": "is going to look like this straight line\n",
        "152": "over here okay and I want point out when\n",
        "154": "I'm plotting my hypothesis function my\n",
        "156": "x-axis my horizontal axis is labeled\n",
        "158": "axes label you know size of the holes\n",
        "163": "over here now I've temporarily set theta\n",
        "166": "1 equals 1 what I want to do is figure\n",
        "170": "out you know what is J of theta one when\n",
        "172": "theta 1 equals 1 so let's go ahead and\n",
        "174": "compute what the cost function is for\n",
        "178": "the value 1 well as usual my cost\n",
        "182": "function is defined as follows write\n",
        "185": "some from some of my training set of\n",
        "189": "this usual squared error term and this\n",
        "196": "is therefore equal to this of theta 1 X\n",
        "201": "I - why I and if you simplify this turns\n",
        "207": "out to be that 0 squared plus 0 squared\n",
        "209": "is 0 squared which is of course just\n",
        "212": "equal to 0 now inside the cost function\n",
        "214": "it turns out each of these terms here is\n",
        "217": "equal to 0 because for the specific\n",
        "219": "training set I have for my Damini\n",
        "222": "training examples are 1 1 2 2 3 3 if\n",
        "225": "theta 1 is equal to 1 then you know H of\n",
        "233": "X where H of X I is equal to Y I exactly\n",
        "239": "right is better right and so H of X\n",
        "242": "minus y each of these terms is equal to\n",
        "246": "0 which is why I find that J of 1 is\n",
        "254": "equal to 0 so we now know that J of 1 is\n",
        "258": "equal to 0 let's plot that what I'm\n",
        "260": "going to do on the right is plot my cost\n",
        "263": "function J and notice because my cost\n",
        "264": "function\n",
        "267": "function of my parameter theta one when\n",
        "269": "I plot my cost function the horizontal\n",
        "272": "axis is now labeled with theta one so I\n",
        "275": "have J of one is equals zero so let's go\n",
        "277": "ahead and plot that you can end up with\n",
        "282": "an x over there now let's look at some\n",
        "285": "other examples theta one can take on a\n",
        "287": "range of different values right so theta\n",
        "289": "one can take on your negative values\n",
        "292": "zero positive values so what if theta\n",
        "296": "one is equal to zero point five what\n",
        "298": "happens then let's go ahead and plot\n",
        "300": "that I'm now going to set theta one\n",
        "302": "equals zero point five and in that case\n",
        "305": "my hypothesis now looks like this as a\n",
        "309": "line with slope equals to 0.5 and let's\n",
        "314": "compute J of 0.5 so that is going to be\n",
        "318": "1 over 2m of my usual cost function it\n",
        "320": "turns out that the cost function is\n",
        "323": "going to be the sum of squared values of\n",
        "328": "the height of this line plus the summer\n",
        "330": "squared of the height of that line plus\n",
        "332": "the summer square of the height of that\n",
        "333": "line right because this is this vertical\n",
        "335": "distance that's the difference between\n",
        "342": "you know why I and the predicted value H\n",
        "346": "of X I right so the first example is\n",
        "350": "going to open 5 minus 1 squared because\n",
        "353": "my hypothesis predicted 0.5 whereas the\n",
        "357": "actual value was 1 so my second example\n",
        "361": "I get 1 minus 2 squared because my\n",
        "363": "hypothesis predicted 1 but the actual\n",
        "366": "housing price was 2 and then finally\n",
        "372": "plus 1.5 minus 3 squared and so that's\n",
        "373": "plus 1.5 minus 3 squared and so that's\n",
        "377": "equal to 1 over 2 times 3 because my\n",
        "380": "training set size I have three training\n",
        "383": "examples and then that's times\n",
        "385": "simplifying within the parentheses is\n",
        "391": "3.5 so that's 3.5 over 6 which is about\n",
        "392": "0.6\n",
        "396": "to eight so now we know that J of 0.5 is\n",
        "401": "about 0.68 let's go and plot that I\n",
        "403": "always used to be mapper does actually\n",
        "406": "open five eight so I'm going to plot\n",
        "410": "that which is maybe about over there\n",
        "415": "okay now let's do one more\n",
        "419": "how about if theta one is equal to zero\n",
        "425": "what is J of zero equal to it turns out\n",
        "429": "that if theta one is equal to zero then\n",
        "433": "H of X is just equal to you know this\n",
        "435": "flat line right there just goes\n",
        "438": "horizontally like this and so measuring\n",
        "445": "the errors we have that J of zero is\n",
        "447": "equal to one over 2m times 1 squared\n",
        "451": "plus 2 squared plus three squared which\n",
        "457": "is 1/6 times 14 which is about two point\n",
        "460": "three so let's go ahead and plot that as\n",
        "464": "well you end up with a value around 2.3\n",
        "467": "and of course we can keep on doing this\n",
        "469": "for other values of theta one it turns\n",
        "471": "out that so you can have your negative\n",
        "474": "values of theta one as well so if theta\n",
        "478": "one is negative then H of X would be\n",
        "482": "equal to say minus 0.5 times X right if\n",
        "484": "theta one is minus 0.5 and so that\n",
        "487": "corresponds to a hypothesis you know\n",
        "490": "with a slope of negative 0.5 and you can\n",
        "492": "actually keep on computing these errors\n",
        "495": "this turns out to be for 0.5 it turns\n",
        "498": "out to really high ever it works out to\n",
        "500": "be something like five point two five\n",
        "503": "and so on and the different values of\n",
        "507": "theta one you can compute these things\n",
        "511": "right turns out the computer range of\n",
        "514": "values you get something like that\n",
        "517": "and by computing the range of values you\n",
        "522": " \n",
        "525": "what this function J of theta looks like\n",
        "530": "and that's what J of theta is to recap\n",
        "533": "for each value of theta one right each\n",
        "537": "value of theta one corresponds to a\n",
        "539": "different hypothesis or to a different\n",
        "543": "straight line fit on the left and for\n",
        "545": "each value of theta one we could then\n",
        "548": "derive a different value of J of theta\n",
        "554": "one and for example you know theta one\n",
        "557": "equals one corresponding to this\n",
        "559": "straight line fit through the data\n",
        "565": "whereas theta one equals 0.5 this point\n",
        "568": "shown in magenta responded to maybe that\n",
        "574": "line and theta one equals zero which\n",
        "576": "will show in blue that corresponds to\n",
        "580": "this horizontal line right so if we each\n",
        "582": "value of theta one we wound up with a\n",
        "585": "different value of J of theta one and we\n",
        "587": "could then use this to trace out the\n",
        "591": "spot on the right now you remember the\n",
        "594": "optimization objective for our learning\n",
        "596": "algorithm is we want to choose the value\n",
        "601": "of theta one that minimizes J of theta\n",
        "604": "one right this was our objective\n",
        "607": "function for linear regression well\n",
        "610": "looking at this curve the value that\n",
        "613": "minimizes J of theta one is you know\n",
        "616": "theta one equals to one and lo and\n",
        "619": "behold that is indeed the best possible\n",
        "621": "straight line fit through our data by\n",
        "623": "setting theta one equals one and just\n",
        "625": "for this particular training set we\n",
        "627": "actually end up fitting it perfectly and\n",
        "630": "that's why minimizing J of theta one\n",
        "633": "corresponds to finding a straight line\n",
        "638": "that fits the data well so to wrap up in\n",
        "641": "this video we looked at some plots to\n",
        "643": "understand the cost function to do so we\n",
        "645": "simplify the algorithm so that it had\n",
        "648": "only one parameter theta one and we set\n",
        "651": "the parameters theta zero to zero in the\n",
        "653": "next video we'll go back to the original\n",
        "654": "problem for me\n",
        "656": "and look at some visualizations\n",
        "659": "involving both theta 0 and theta 1 that\n",
        "661": "is without setting theta 0 0 and\n",
        "663": "hopefully that would give you an even\n",
        "665": "better sense of what the cost function J\n",
        "667": "is doing in the original linear\n"
    },
    "yoYA1MFpYRg": {
        "0": " \n",
        "1": "we've talked about how to evaluate\n",
        "3": "learning algorithms thoughtful normal\n",
        "5": "selection tougher more about buyers in\n",
        "7": "theory ins so how does this help us\n",
        "10": "figure out what a potentially fruitful\n",
        "11": "potentially not through full things to\n",
        "13": "try to do to improve the performance of\n",
        "16": "a learning algorithm let's go back to\n",
        "18": "our original motivating example and go\n",
        "22": "for the result so here is our earlier\n",
        "24": "example of maybe having fits regularized\n",
        "26": "linear regression and finding that it\n",
        "28": "doesn't work as well as we're hoping we\n",
        "31": "said that we had this menu of options so\n",
        "33": "is there some way to figure out which of\n",
        "35": "these might be fruitful options the\n",
        "37": "first thing long list was getting more\n",
        "40": "training examples and what this is good\n",
        "44": "for this this helps to fix high variance\n",
        "47": "and concretely if you instead have a\n",
        "49": "high bias problem and don't have any\n",
        "51": "variance problem then we saw in the\n",
        "53": "previous video that getting more\n",
        "55": "training examples well maybe just isn't\n",
        "57": "going to help much at all so the first\n",
        "60": "option is useful only if you say plot\n",
        "61": "the learning curves and figure out that\n",
        "63": "you have at least a bit of a variance\n",
        "65": "meaning that if the cross-validation\n",
        "67": "error is you know quite a bit bigger\n",
        "69": "than your training center how about\n",
        "71": "trying a smaller set of features well\n",
        "73": "trying to smaller set of features that's\n",
        "76": "again something that fixes high variance\n",
        "78": "and in other words if you figure out by\n",
        "80": "looking learning curves or something\n",
        "81": "else that in you instead have a high\n",
        "83": "bias problem then for goodness sakes\n",
        "85": "don't waste your time trying to\n",
        "88": "carefully select out a smaller set of\n",
        "89": "features to use because if there's a\n",
        "92": "high bias problem using future fewer\n",
        "94": "features it's not going to help whereas\n",
        "96": "in contrast if you look at the learning\n",
        "97": "curves or something else and figure out\n",
        "99": "that you have a high variance problem\n",
        "102": "then indeed try to select on the\n",
        "103": "smallest of the features that might\n",
        "105": "indeed be a very good user your time how\n",
        "108": "about trying to get additional features\n",
        "110": "so adding features usually not always\n",
        "112": "but usually we think of this as a\n",
        "117": "solution for fixing high bias problems\n",
        "119": "so that if you're adding extra features\n",
        "121": "is usually because\n",
        "123": "your current hypothesis is too simple\n",
        "125": "and so we want to try to get additional\n",
        "128": "features to make our hypothesis better\n",
        "130": "able to fit the training set and\n",
        "133": "similarly adding polynomial features\n",
        "135": "this is another way of adding features\n",
        "138": "and so that's another way to try to fix\n",
        "142": "a high bias problem and if concretely if\n",
        "144": "your learning curves show you that\n",
        "146": "instead of a high variance problem then\n",
        "148": "you know doing this is maybe a less good\n",
        "150": "use of your time\n",
        "152": "and finally decreasing and increasing\n",
        "154": "longer these are quick and easy to try\n",
        "156": "because these are less likely to be a\n",
        "158": "ways of you know many months of your\n",
        "161": "life but decreasing lambda you already\n",
        "166": "know fixes high bias in case this isn't\n",
        "167": "clear to you you have do encourage you\n",
        "169": "to pause the video and think through\n",
        "171": "this that convince yourself that\n",
        "174": "decreasing lambda helps fix high bias\n",
        "177": "whereas increasing around there fixes\n",
        "181": "high variance and if you aren't sure why\n",
        "184": "this is a case do post a video and make\n",
        "185": "sure you can convince yourself and this\n",
        "187": "is the case well take a look at the\n",
        "189": "curves that we were plotting at the end\n",
        "191": "of the previous video and try to make\n",
        "193": "sure you understand why these is not the\n",
        "196": "case finally let's take everything we've\n",
        "198": "learned and related back to neural\n",
        "201": "networks and here just give some\n",
        "202": "practical advice for how I usually\n",
        "205": "choose the architecture or the\n",
        "207": "connectivity pattern of the neural\n",
        "208": "connectivity pattern of the neural\n",
        "211": "networks I use so if you fit in your\n",
        "214": "network one option would be to fit say a\n",
        "216": "pretty small neural network with you\n",
        "218": "know relatively few hidden units maybe\n",
        "221": "just one hidden you if you're fitting on\n",
        "223": "your network one option would be to fit\n",
        "226": "a relatively small neural network with\n",
        "230": "say relatively few maybe only one hidden\n",
        "233": "layer and maybe only a relatively few\n",
        "235": "number of hidden units so a network like\n",
        "237": "this might have relatively few\n",
        "239": "parameters and be more prone to\n",
        "241": "underfitting the main advantage of these\n",
        "243": "small neural networks\n",
        "245": "that the computationally cheaper an\n",
        "247": "alternative would be to fit a maybe\n",
        "249": "relatively large neural network with\n",
        "252": "either a more hidden unit so if that's a\n",
        "254": "lot of hidden units in one layer or with\n",
        "257": "more hidden layers and so these neural\n",
        "258": "networks tend to have more parameters\n",
        "260": "and therefore be more prone to\n",
        "261": "overfitting\n",
        "264": "what does advantage often on a major one\n",
        "266": "but something to think about is that if\n",
        "269": "you have a large number of neurons in\n",
        "270": "your network then they can be more\n",
        "273": "computationally expensive although\n",
        "275": "within reason this is often hopefully\n",
        "277": "not a huge problem the main potential\n",
        "279": "problem of these much larger neural\n",
        "280": "networks is that they could be more\n",
        "283": "prone to overfitting and it turns out\n",
        "286": "that if you apply in neural network very\n",
        "288": "often using a larger neural network\n",
        "290": "often is actually the larger the better\n",
        "292": "but the result of fitting you can then\n",
        "294": "use regularization to address\n",
        "297": "overfitting and usually using a larger\n",
        "299": "neural network but using regularization\n",
        "302": "to address overfitting that's often more\n",
        "304": "effective than using a smaller neural\n",
        "306": "network and the main possible\n",
        "307": "disadvantage is that it can be more\n",
        "310": "computationally expensive and finally\n",
        "312": "one of the other decisions is say the\n",
        "314": "number of hidden you the number of\n",
        "316": "hidden layers you want to have right so\n",
        "318": "do you want one hidden layer or do you\n",
        "320": "want three hidden layers as we have\n",
        "321": "shown here or do you want two hidden\n",
        "325": "layers and usually I think I said in the\n",
        "327": "previous video using a single hidden\n",
        "330": "layer as a reasonable default but if you\n",
        "331": "want to choose the number of hidden\n",
        "333": "layers one other thing you can try is\n",
        "335": "find yourself a training\n",
        "338": "cross-validation and test set split and\n",
        "340": "try training neural networks with one\n",
        "341": "hidden layer or two hidden layers with\n",
        "343": "three hidden layers and see which of\n",
        "346": "those neural networks performs best on\n",
        "348": "the cross-validation set so you take you\n",
        "350": "know three neural networks with one two\n",
        "352": "and three hidden layers and compute the\n",
        "355": "cross-validation error and jzd on all of\n",
        "358": "them and use that to select which of\n",
        "360": "these is you think the best neural\n",
        "361": "network\n",
        "365": "so that's it for bias-variance and waste\n",
        "367": "like learning Carosa try to diagnose\n",
        "369": "these problems that's all that's what\n",
        "371": "these things imply for what might be\n",
        "373": "fruitful or not for full things to try\n",
        "375": "to improve the performance of a learning\n",
        "378": "algorithm if you understood the content\n",
        "381": "of the last few videos and if you apply\n",
        "383": "them you actually be much more effective\n",
        "385": "already and getting learning algorithms\n",
        "387": "to work on problems than even a large\n",
        "389": "fraction maybe the majority of\n",
        "391": "practitioners and machine learning here\n",
        "393": "at Silicon Valley today doing these\n",
        "396": "things as their full-time jobs so I hope\n",
        "399": "that this he said pieces of advice on\n",
        "401": "bias-variance learning curves and\n",
        "404": "Diagnostics will help you to much more\n",
        "406": "effectively and powerfully apply\n",
        "408": "learning algorithms and get them to work\n"
    },
    "yuH4iRcggMw": {
        "0": " \n",
        "1": "in this video we'll define something\n",
        "4": "called the cost function this will let\n",
        "5": "us figure out how to fit the best\n",
        "10": "possible straight line to our data in\n",
        "12": "linear regression we have a training set\n",
        "13": "like that shown here remember our\n",
        "16": "notation M was the number of training\n",
        "20": "so maybe M equals 47 and the form of our\n",
        "22": "hypothesis which we use to make\n",
        "26": "predictions is this linear function to\n",
        "28": "introduce a little bit more terminology\n",
        "33": "these theta 0 and theta 1 write these\n",
        "34": "theta eyes are what I call the\n",
        "39": "parameters of the model and what we're\n",
        "41": "going to do in this video is talk about\n",
        "44": "how to go about choosing these two\n",
        "47": "parameter values theta 0 and theta 1\n",
        "49": "with different choices of the parameters\n",
        "51": "theta 0 and theta 1 we get different\n",
        "53": "hypotheses different hypothesis\n",
        "56": "functions I know some of you will\n",
        "58": "probably be already familiar with what\n",
        "59": "I'm going to do on the slide but just a\n",
        "62": "review here are a few examples if theta\n",
        "66": "0 is 1 point 5 and theta 1 is 0 then the\n",
        "68": "hypothesis function will look like this\n",
        "71": "and because your hypothesis function\n",
        "76": "will be H of x equals 1 point 5 plus 0\n",
        "79": "times X which is this constant value\n",
        "83": "function there's just fat at 1.5 if\n",
        "86": "theta 0 equals 0 theta 1 equals 0.5 then\n",
        "89": "the hypotheses will look like this and\n",
        "92": "it should pass through this point to one\n",
        "95": "since you now have H of X really H sub\n",
        "97": "substrate theta of X but sometimes out\n",
        "100": "well I'll just omit theta for gravity so\n",
        "103": "H of X would be equal to just 0.5 times\n",
        "104": "X which looks like that\n",
        "107": "and finally if theta is equals 1 and\n",
        "110": "theta 1 equals 0.5 then we end up with a\n",
        "114": "hypothesis that looks like this let's\n",
        "118": "see I should pass through the 2 to point\n",
        "121": "like so and this is my\n",
        "124": "you H of X on my new subscript theta of\n",
        "126": "X all right where you remember I said\n",
        "128": "that this is H subscript theta of X but\n",
        "130": "as a shorthand sometimes I'll just write\n",
        "135": "this as H of X in linear regression we\n",
        "137": "have a training set like maybe the one I\n",
        "140": "plotted here what we want to do is come\n",
        "141": "plotted here what we want to do is come\n",
        "144": "up with values for the parameters theta\n",
        "146": "zero and theta one so then the straight\n",
        "149": "line we get all of this corresponds to a\n",
        "151": "straight line that somehow fits the data\n",
        "155": "well maybe that line over there so how\n",
        "157": "do we come up with you know values theta\n",
        "160": "0 theta 1 that corresponds to a good fit\n",
        "163": "to the data the idea is we're going to\n",
        "166": "choose our parameters theta 0 theta 1 so\n",
        "169": "that H of X meaning the value we predict\n",
        "172": "on implies X that this is at least close\n",
        "177": "to the values Y for the examples in our\n",
        "179": "training set for our training examples\n",
        "181": "so in our training set we're given a\n",
        "183": "number of examples where we know X\n",
        "185": "decides the house and we know the actual\n",
        "187": "price that was so for so let's try to\n",
        "191": "choose values to the parameters so that\n",
        "193": "at least in the training set given the\n",
        "194": "XS in the training set we make\n",
        "197": "reasonably accurate predictions for the\n",
        "201": "Y values let's formalize this sin linear\n",
        "203": "regression what we're going to do is I'm\n",
        "206": "going to want to solve a minimization\n",
        "208": "problem so the write minimize over\n",
        "218": " \n",
        "220": "to be small right I want the difference\n",
        "223": "between H of x and y to be small and one\n",
        "226": "thing I might do is try to minimize the\n",
        "228": "squared difference between the output of\n",
        "230": "my hypothesis and the actual price of a\n",
        "233": "house okay so let's throw in some\n",
        "235": "details you remember that I was using\n",
        "239": "the notation x I comma Y I to represent\n",
        "243": "the I've training example so what I want\n",
        "247": "really is to sum over my training set\n",
        "251": "sum from I equals 1 to M of the squared\n",
        "254": "difference between this is the\n",
        "258": "prediction of my hypothesis when it is\n",
        "262": "input the size of house number I right -\n",
        "265": "the actual price that house number I was\n",
        "268": "so full and I want to minimize the sum\n",
        "270": "over my training set sum from I equals 1\n",
        "273": "through m of the difference of the\n",
        "275": "squared error squared difference between\n",
        "277": "the predicted price of the house and the\n",
        "279": "price that it was actually so for and\n",
        "283": "just remind you of your notation M here\n",
        "287": "was the size of my training set right so\n",
        "289": "little m there is the my number of\n",
        "293": "training examples that hash sign is the\n",
        "294": "abbreviation for number of training\n",
        "299": "examples okay and to make some of our\n",
        "301": "later map a little bit easier I'm going\n",
        "304": "to actually look at you know 1 over m\n",
        "307": "times AB so we'll try to minimize my\n",
        "309": "average ever visually minimize 1 over 2m\n",
        "312": "the putting the to the constant you know\n",
        "314": "1/2 in front it just makes some of the\n",
        "317": "math mo bit easier so minimizing 1/2 of\n",
        "319": "something right should give you the same\n",
        "321": "values for the parameters theta 0 and\n",
        "324": "theta 1 as minimizing that function and\n",
        "326": "just make sure this this this equation\n",
        "330": "is clear write this expression in here\n",
        "334": "take subscript theta of X this is my\n",
        "338": "this is our usual write that is equal to\n",
        "339": "this\n",
        "344": "plus beta 1 X I and this notation\n",
        "347": "minimize over theta0 and theta1 this\n",
        "349": "means you'll find me the values of theta\n",
        "352": "0 and theta 1 that causes this\n",
        "354": "expression to be minimized and this\n",
        "355": "expression depends on theta 0 and theta\n",
        "359": "1 okay so just to recap we're posing\n",
        "361": "this problem as find me the values of\n",
        "364": "theta 0 and theta one so that the\n",
        "368": "average already 1 over 2m times the sum\n",
        "370": "of squared errors between my predictions\n",
        "371": "on the training set minus the actual\n",
        "374": "values the houses on the training set is\n",
        "377": "minimized so this is going to be my\n",
        "380": "overall objective function for linear\n",
        "384": "regression and just to rewrite this out\n",
        "385": "a little bit more cleanly\n",
        "388": "what I'm going to do is by convention we\n",
        "391": "usually define a cost function which is\n",
        "394": "going to be exactly this that formula\n",
        "398": "that I have up here and what I want to\n",
        "405": "do is minimize over theta0 and theta1 my\n",
        "410": "function J of theta 0 comma theta 1\n",
        "416": "where just read this out this is my cost\n",
        "418": " \n",
        "423": "so this cost function is also called the\n",
        "427": "squared error function sometimes called\n",
        "431": "the squared error cost function and it\n",
        "434": "turns out that why do we take the\n",
        "436": "squares of the errors it turns out that\n",
        "438": "the squared error cost function is a\n",
        "440": "reasonable choice and will work well for\n",
        "441": "most problems for most regression\n",
        "443": "problems there are other cost functions\n",
        "445": "that will work pretty well but the\n",
        "447": "squared error cost function is probably\n",
        "449": "the most commonly used one for\n",
        "452": "regression problems layton's is also\n",
        "453": "talked about alternative cost functions\n",
        "456": "as well but this this choice that we\n",
        "458": "just had should be a pretty reasonable\n",
        "460": "thing to try for most linear regression\n",
        "463": "problems okay so that's the cost\n",
        "466": "function so far we've just seen a\n",
        "469": "mathematical definition of you know this\n",
        "471": "cost function and in case this this\n",
        "474": "function j of theta0 theta1 in case this\n",
        "476": "function seems a little bit abstract and\n",
        "478": "you still don't have a good sense of\n",
        "480": "what is doing in the next video in the\n",
        "483": "next couple of videos much going to go a\n",
        "485": "little bit deeper into what the cost\n",
        "488": "function J is doing and trying to give\n",
        "489": "you better intuition about what is\n"
    }
}
